[
  {
    "title": "Learn More | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nLearn More\n1: Philosophy\n2: Architecture\n3: Components\n4: Control Plane\n5: Image Factory\n6: Controllers and Resources\n7: Networking Resources\n8: Network Connectivity\n9: KubeSpan\n10: Process Capabilities\n11: talosctl\n12: FAQs\n13: Knowledge Base\n1 - Philosophy\nLearn about the philosophy behind the need for Talos Linux.\nDistributed\n\nTalos is intended to be operated in a distributed manner: it is built for a high-availability dataplane first. Its etcd cluster is built in an ad-hoc manner, with each appointed node joining on its own directive (with proper security validations enforced, of course). Like Kubernetes, workloads are intended to be distributed across any number of compute nodes.\n\nThere should be no single points of failure, and the level of required coordination is as low as each platform allows.\n\nImmutable\n\nTalos takes immutability very seriously. Talos itself, even when installed on a disk, always runs from a SquashFS image, meaning that even if a directory is mounted to be writable, the image itself is never modified. All images are signed and delivered as single, versioned files. We can always run integrity checks on our image to verify that it has not been modified.\n\nWhile Talos does allow a few, highly-controlled write points to the filesystem, we strive to make them as non-unique and non-critical as possible. We call the writable partition the “ephemeral” partition precisely because we want to make sure none of us ever uses it for unique, non-replicated, non-recreatable data. Thus, if all else fails, we can always wipe the disk and get back up and running.\n\nMinimal\n\nWe are always trying to reduce Talos’ footprint. Because nearly the entire OS is built from scratch in Go, we are in a good position. We have no shell. We have no SSH. We have none of the GNU utilities, not even a rollup tool such as busybox. Everything in Talos is there because it is necessary, and nothing is included which isn’t.\n\nAs a result, the OS right now produces a SquashFS image size of less than 80 MB.\n\nEphemeral\n\nEverything Talos writes to its disk is either replicated or reconstructable. Since the controlplane is highly available, the loss of any node will cause neither service disruption nor loss of data. No writes are even allowed to the vast majority of the filesystem. We even call the writable partition “ephemeral” to keep this idea always in focus.\n\nSecure\n\nTalos has always been designed with security in mind. With its immutability, its minimalism, its signing, and its componenture, we are able to simply bypass huge classes of vulnerabilities. Moreover, because of the way we have designed Talos, we are able to take advantage of a number of additional settings, such as the recommendations of the Kernel Self Protection Project (kspp) and completely disabling dynamic modules.\n\nThere are no passwords in Talos. All networked communication is encrypted and key-authenticated. The Talos certificates are short-lived and automatically-rotating. Kubernetes is always constructed with its own separate PKI structure which is enforced.\n\nDeclarative\n\nEverything which can be configured in Talos is done through a single YAML manifest. There is no scripting and no procedural steps. Everything is defined by the one declarative YAML file. This configuration includes that of both Talos itself and the Kubernetes which it forms.\n\nThis is achievable because Talos is tightly focused to do one thing: run Kubernetes, in the easiest, most secure, most reliable way it can.\n\nNot based on X distro\n\nTalos Linux isn’t based on any other distribution. We think of ourselves as being the second-generation of container-optimised operating systems, where things like CoreOS, Flatcar, and Rancher represent the first generation (but the technology is not derived from any of those.)\n\nTalos Linux is actually a ground-up rewrite of the userspace, from PID 1. We run the Linux kernel, but everything downstream of that is our own custom code, written in Go, rigorously-tested, and published as an immutable, integrated image. The Linux kernel launches what we call machined, for instance, not systemd. There is no systemd on our system. There are no GNU utilities, no shell, no SSH, no packages, nothing you could associate with any other distribution.\n\nAn Operating System designed for Kubernetes\n\nTechnically, Talos Linux installs to a computer like any other operating system. Unlike other operating systems, Talos is not meant to run alone, on a single machine. A design goal of Talos Linux is eliminating the management of individual nodes as much as possible. In order to do that, Talos Linux operates as a cluster of machines, with lots of checking and coordination between them, at all levels.\n\nThere is only a cluster. Talos is meant to do one thing: maintain a Kubernetes cluster, and it does this very, very well.\n\nThe entirety of the configuration of any machine is specified by a single configuration file, which can often be the same configuration file used across many machines. Much like a biological system, if some component misbehaves, just cut it out and let a replacement grow. Rebuilds of Talos are remarkably fast, whether they be new machines, upgrades, or reinstalls. Never get hung up on an individual machine.\n\n2 - Architecture\nLearn the system architecture of Talos Linux itself.\n\nTalos is designed to be atomic in deployment and modular in composition.\n\nIt is atomic in that the entirety of Talos is distributed as a single, self-contained image, which is versioned, signed, and immutable.\n\nIt is modular in that it is composed of many separate components which have clearly defined gRPC interfaces which facilitate internal flexibility and external operational guarantees.\n\nAll of the main Talos components communicate with each other by gRPC, through a socket on the local machine. This imposes a clear separation of concerns and ensures that changes over time which affect the interoperation of components are a part of the public git record. The benefit is that each component may be iterated and changed as its needs dictate, so long as the external API is controlled. This is a key component in reducing coupling and maintaining modularity.\n\nFile system partitions\n\nTalos uses these partitions with the following labels:\n\nEFI - stores EFI boot data.\nBIOS - used for GRUB’s second stage boot.\nBOOT - used for the boot loader, stores initramfs and kernel data.\nMETA - stores metadata about the talos node, such as node id’s.\nSTATE - stores machine configuration, node identity data for cluster discovery and KubeSpan info\nEPHEMERAL - stores ephemeral state information, mounted at /var\nThe File System\n\nOne of the unique design decisions in Talos is the layout of the root file system. There are three “layers” to the Talos root file system. At its core the rootfs is a read-only squashfs. The squashfs is then mounted as a loop device into memory. This provides Talos with an immutable base.\n\nThe next layer is a set of tmpfs file systems for runtime specific needs. Aside from the standard pseudo file systems such as /dev, /proc, /run, /sys and /tmp, a special /system is created for internal needs. One reason for this is that we need special files such as /etc/hosts, and /etc/resolv.conf to be writable (remember that the rootfs is read-only). For example, at boot Talos will write /system/etc/hosts and then bind mount it over /etc/hosts. This means that instead of making all of /etc writable, Talos only makes very specific files writable under /etc.\n\nAll files under /system are completely recreated on each boot. For files and directories that need to persist across boots, Talos creates overlayfs file systems. The /etc/kubernetes is a good example of this. Directories like this are overlayfs backed by an XFS file system mounted at /var.\n\nThe /var directory is owned by Kubernetes with the exception of the above overlayfs file systems. This directory is writable and used by etcd (in the case of control plane nodes), the kubelet, and the CRI (containerd). Its content survives machine reboots, but it is wiped and lost on machine upgrades and resets, unless the --preserve option of talosctl upgrade or the --system-labels-to-wipe option of talosctl reset is used.\n\n3 - Components\nUnderstand the system components that make up Talos Linux.\n\nIn this section, we discuss the various components that underpin Talos.\n\nComponents\n\nTalos Linux and Kubernetes are tightly integrated.\n\nIn the following, the focus is on the Talos Linux specific components.\n\nComponent\tDescription\napid\tWhen interacting with Talos, the gRPC API endpoint you interact with directly is provided by apid. apid acts as the gateway for all component interactions and forwards the requests to machined.\ncontainerd\tAn industry-standard container runtime with an emphasis on simplicity, robustness, and portability. To learn more, see the containerd website.\nmachined\tTalos replacement for the traditional Linux init-process. Specially designed to run Kubernetes and does not allow starting arbitrary user services.\nkernel\tThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project.\ntrustd\tTo run and operate a Kubernetes cluster, a certain level of trust is required. Based on the concept of a ‘Root of Trust’, trustd is a simple daemon responsible for establishing trust within the system.\nudevd\tImplementation of eudev into machined. eudev is Gentoo’s fork of udev, systemd’s device file manager for the Linux kernel. It manages device nodes in /dev and handles all user space actions when adding or removing devices. To learn more, see the Gentoo Wiki.\napid\n\nWhen interacting with Talos, the gRPC api endpoint you will interact with directly is apid. Apid acts as the gateway for all component interactions. Apid provides a mechanism to route requests to the appropriate destination when running on a control plane node.\n\nWe’ll use some examples below to illustrate what apid is doing.\n\nWhen a user wants to interact with a Talos component via talosctl, there are two flags that control the interaction with apid. The -e | --endpoints flag specifies which Talos node ( via apid ) should handle the connection. Typically this is a public-facing server. The -n | --nodes flag specifies which Talos node(s) should respond to the request. If --nodes is omitted, the first endpoint will be used.\n\nNote: Typically, there will be an endpoint already defined in the Talos config file. Optionally, nodes can be included here as well.\n\nFor example, if a user wants to interact with machined, a command like talosctl -e cluster.talos.dev memory may be used.\n\n$ talosctl -e cluster.talos.dev memory\n\nNODE                TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\ncluster.talos.dev   7938    1768   2390   145      53        3724    6571\n\n\nIn this case, talosctl is interacting with apid running on cluster.talos.dev and forwarding the request to the machined api.\n\nIf we wanted to extend our example to retrieve memory from another node in our cluster, we could use the command talosctl -e cluster.talos.dev -n node02 memory.\n\n$ talosctl -e cluster.talos.dev -n node02 memory\n\nNODE    TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode02  7938    1768   2390   145      53        3724    6571\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to apid running on node02, which forwards the request to the machined api.\n\nWe can further extend our example to retrieve memory for all nodes in our cluster by appending additional -n node flags or using a comma separated list of nodes ( -n node01,node02,node03 ):\n\n$ talosctl -e cluster.talos.dev -n node01 -n node02 -n node03 memory\n\nNODE     TOTAL    USED    FREE     SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode01   7938     871     4071     137      49        2945    7042\n\nnode02   257844   14408   190796   18138    49        52589   227492\n\nnode03   257844   1830    255186   125      49        777     254556\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to node01, node02, and node03, which then forwards the request to their local machined api.\n\ncontainerd\n\nContainerd provides the container runtime to launch workloads on Talos and Kubernetes.\n\nTalos services are namespaced under the system namespace in containerd, whereas the Kubernetes services are namespaced under the k8s.io namespace.\n\nmachined\n\nA common theme throughout the design of Talos is minimalism. We believe strongly in the UNIX philosophy that each program should do one job well. The init included in Talos is one example of this, and we are calling it “machined”.\n\nWe wanted to create a focused init that had one job - run Kubernetes. To that extent, machined is relatively static in that it does not allow for arbitrary user-defined services. Only the services necessary to run Kubernetes and manage the node are available. This includes:\n\ncontainerd\netcd\nkubelet\nnetworkd\ntrustd\nudevd\n\nThe machined process handles all machine configuration, API handling, resource and controller management.\n\nkernel\n\nThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project (KSSP).\n\ntrustd\n\nSecurity is one of the highest priorities within Talos. To run a Kubernetes cluster, a certain level of trust is required to operate a cluster. For example, orchestrating the bootstrap of a highly available control plane requires sensitive PKI data distribution.\n\nTo that end, we created trustd. Based on a Root of Trust concept, trustd is a simple daemon responsible for establishing trust within the system. Once trust is established, various methods become available to the trustee. For example, it can accept a write request from another node to place a file on disk.\n\nAdditional methods and capabilities will be added to the trustd component to support new functionality in the rest of the Talos environment.\n\nudevd\n\nUdevd handles the kernel device notifications and sets up the necessary links in /dev.\n\n4 - Control Plane\nUnderstand the Kubernetes Control Plane.\n\nThis guide provides information about the Kubernetes control plane, and details on how Talos runs and bootstraps the Kubernetes control plane.\n\nWhat is a control plane node?\n\nA control plane node is a node which:\n\nruns etcd, the Kubernetes database\nruns the Kubernetes control plane\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nserves as an administrative proxy to the worker nodes\n\nThese nodes are critical to the operation of your cluster. Without control plane nodes, Kubernetes will not respond to changes in the system, and certain central services may not be available.\n\nTalos nodes which have .machine.type of controlplane are control plane nodes. (check via talosctl get member)\n\nControl plane nodes are tainted by default to prevent workloads from being scheduled onto them. This is both to protect the control plane from workloads consuming resources and starving the control plane processes, and also to reduce the risk of a vulnerability exposes the control plane’s credentials to a workload.\n\nThe Control Plane and Etcd\n\nA critical design concept of Kubernetes (and Talos) is the etcd database.\n\nProperly managed (which Talos Linux does), etcd should never have split brain or noticeable down time. In order to do this, etcd maintains the concept of “membership” and of “quorum”. To perform any operation, read or write, the database requires quorum. That is, a majority of members must agree on the current leader, and absenteeism (members that are down, or not reachable) counts as a negative. For example, if there are three members, at least two out of the three must agree on the current leader. If two disagree or fail to answer, the etcd database will lock itself until quorum is achieved in order to protect the integrity of the data.\n\nThis design means that having two controlplane nodes is worse than having only one, because if either goes down, your database will lock (and the chance of one of two nodes going down is greater than the chance of just a single node going down). Similarly, a 4 node etcd cluster is worse than a 3 node etcd cluster - a 4 node cluster requires 3 nodes to be up to achieve quorum (in order to have a majority), while the 3 node cluster requires 2 nodes: i.e. both can support a single node failure and keep running - but the chance of a node failing in a 4 node cluster is higher than that in a 3 node cluster.\n\nAnother note about etcd: due to the need to replicate data amongst members, performance of etcd decreases as the cluster scales. A 5 node cluster can commit about 5% less writes per second than a 3 node cluster running on the same hardware.\n\nRecommendations for your control plane\nRun your clusters with three or five control plane nodes. Three is enough for most use cases. Five will give you better availability (in that it can tolerate two node failures simultaneously), but cost you more both in the number of nodes required, and also as each node may require more hardware resources to offset the performance degradation seen in larger clusters.\nImplement good monitoring and put processes in place to deal with a failed node in a timely manner (and test them!)\nEven with robust monitoring and procedures for replacing failed nodes in place, backup etcd and your control plane node configuration to guard against unforeseen disasters.\nMonitor the performance of your etcd clusters. If etcd performance is slow, vertically scale the nodes, not the number of nodes.\nIf a control plane node fails, remove it first, then add the replacement node. (This ensures that the failed node does not “vote” when adding in the new node, minimizing the chances of a quorum violation.)\nIf replacing a node that has not failed, add the new one, then remove the old.\nBootstrapping the Control Plane\n\nEvery new cluster must be bootstrapped only once, which is achieved by telling a single control plane node to initiate the bootstrap.\n\nBootstrapping itself does not do anything with Kubernetes. Bootstrapping only tells etcd to form a cluster, so don’t judge the success of a bootstrap by the failure of Kubernetes to start. Kubernetes relies on etcd, so bootstrapping is required, but it is not sufficient for Kubernetes to start. If your Kubernetes cluster fails to form for other reasons (say, a bad configuration option or unavailable container repository), if the bootstrap API call returns successfully, you do NOT need to bootstrap again: just fix the config or let Kubernetes retry.\n\nHigh-level Overview\n\nTalos cluster bootstrap flow:\n\nThe etcd service is started on control plane nodes. Instances of etcd on control plane nodes build the etcd cluster.\nThe kubelet service is started.\nControl plane components are started as static pods via the kubelet, and the kube-apiserver component connects to the local (running on the same node) etcd instance.\nThe kubelet issues client certificate using the bootstrap token using the control plane endpoint (via kube-apiserver and kube-controller-manager).\nThe kubelet registers the node in the API server.\nKubernetes control plane schedules pods on the nodes.\nCluster Bootstrapping\n\nAll nodes start the kubelet service. The kubelet tries to contact the control plane endpoint, but as it is not up yet, it keeps retrying.\n\nOne of the control plane nodes is chosen as the bootstrap node, and promoted using the bootstrap API (talosctl bootstrap). The bootstrap node initiates the etcd bootstrap process by initializing etcd as the first member of the cluster.\n\nOnce etcd is bootstrapped, the bootstrap node has no special role and acts the same way as other control plane nodes.\n\nServices etcd on non-bootstrap nodes try to get Endpoints resource via control plane endpoint, but that request fails as control plane endpoint is not up yet.\n\nAs soon as etcd is up on the bootstrap node, static pod definitions for the Kubernetes control plane components (kube-apiserver, kube-controller-manager, kube-scheduler) are rendered to disk. The kubelet service on the bootstrap node picks up the static pod definitions and starts the Kubernetes control plane components. As soon as kube-apiserver is launched, the control plane endpoint comes up.\n\nThe bootstrap node acquires an etcd mutex and injects the bootstrap manifests into the API server. The set of the bootstrap manifests specify the Kubernetes join token and kubelet CSR auto-approval. The kubelet service on all the nodes is now able to issue client certificates for themselves and register nodes in the API server.\n\nOther bootstrap manifests specify additional resources critical for Kubernetes operations (i.e. CNI, PSP, etc.)\n\nThe etcd service on non-bootstrap nodes is now able to discover other members of the etcd cluster via the Kubernetes Endpoints resource. The etcd cluster is now formed and consists of all control plane nodes.\n\nAll control plane nodes render static pod manifests for the control plane components. Each node now runs a full set of components to make the control plane HA.\n\nThe kubelet service on worker nodes is now able to issue the client certificate and register itself with the API server.\n\nScaling Up the Control Plane\n\nWhen new nodes are added to the control plane, the process is the same as the bootstrap process above: the etcd service discovers existing members of the control plane via the control plane endpoint, joins the etcd cluster, and the control plane components are scheduled on the node.\n\nScaling Down the Control Plane\n\nScaling down the control plane involves removing a node from the cluster. The most critical part is making sure that the node which is being removed leaves the etcd cluster. The recommended way to do this is to use:\n\ntalosctl -n IP.of.node.to.remove reset\nkubectl delete node\n\nWhen using talosctl reset command, the targeted control plane node leaves the etcd cluster as part of the reset sequence, and its disks are erased.\n\nUpgrading Talos on Control Plane Nodes\n\nWhen a control plane node is upgraded, Talos leaves etcd, wipes the system disk, installs a new version of itself, and reboots. The upgraded node then joins the etcd cluster on reboot. So upgrading a control plane node is equivalent to scaling down the control plane node followed by scaling up with a new version of Talos.\n\n5 - Image Factory\nImage Factory generates customized Talos Linux images based on configured schematics.\n\nThe Image Factory provides a way to download Talos Linux artifacts. Artifacts can be generated with customizations defined by a “schematic”. A schematic can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”.\n\nThe following assets are provided:\n\nISO\nkernel, initramfs, and kernel command line\nUKI\ndisk images in various formats (e.g. AWS, GCP, VMware, etc.)\ninstaller container images\n\nThe supported frontends are:\n\nHTTP\nPXE\nContainer Registry\n\nThe official instance of Image Factory is available at https://factory.talos.dev.\n\nSee Boot Assets for an example of how to use the Image Factory to boot and upgrade Talos on different platforms. Full API documentation for the Image Factory is available at GitHub.\n\nSchematics\n\nSchematics are YAML files that define customizations to be applied to a Talos Linux image. Schematics can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”, which is a Talos Linux image with the customizations applied.\n\nSchematics are content-addressable, that is, the content of the schematic is used to generate a unique ID. The schematic should be uploaded to the Image Factory first, and then the ID can be used to reference the schematic in a model.\n\nSchematics can be generated using the Image Factory UI, or using the Image Factory API:\n\ncustomization:\n\n  extraKernelArgs: # optional\n\n    - vga=791\n\n  meta: # optional, allows to set initial Talos META\n\n    - key: 0xa\n\n      value: \"{}\"\n\n  systemExtensions: # optional\n\n    officialExtensions: # optional\n\n      - siderolabs/gvisor\n\n      - siderolabs/amd-ucode\n\n\nThe “vanilla” schematic is:\n\ncustomization:\n\n\nand has an ID of 376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba.\n\nThe schematic can be applied by uploading it to the Image Factory:\n\ncurl -X POST --data-binary @schematic.yaml https://factory.talos.dev/schematics\n\n\nAs the schematic is content-addressable, the same schematic can be uploaded multiple times, and the Image Factory will return the same ID.\n\nModels\n\nModels are Talos Linux images with customizations applied. The inputs to generate a model are:\n\nschematic ID\nTalos Linux version\nmodel type (e.g. ISO, UKI, etc.)\narchitecture (e.g. amd64, arm64)\nvarious model type specific options (e.g. disk image format, disk image size, etc.)\nFrontends\n\nImage Factory provides several frontends to retrieve models:\n\nHTTP frontend to download models (e.g. download an ISO or a disk image)\nPXE frontend to boot bare-metal machines (PXE script references kernel/initramfs from HTTP frontend)\nRegistry frontend to fetch customized installer images (for initial Talos Linux installation and upgrades)\n\nThe links to different models are available in the Image Factory UI, and a full list of possible models is documented at GitHub.\n\nIn this guide we will provide a list of examples:\n\namd64 ISO (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64.iso\narm64 AWS image (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/aws-arm64.raw.xz\namd64 PXE boot script (for Talos v1.6.2, “vanilla” schematic) https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64\nTalos installer image (for Talos v1.6.2, “vanilla” schematic, architecture is detected automatically): factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2\n\nThe installer image can be used to install Talos Linux on a bare-metal machine, or to upgrade an existing Talos Linux installation. As the Talos version and schematic ID can be changed, via an upgrade process, the installer image can be used to upgrade to any version of Talos Linux, or replace a set of installed system extensions.\n\nUI\n\nThe Image Factory UI is available at https://factory.talos.dev. The UI provides a way to list supported Talos Linux versions, list of system extensions available for each release, and a way to generate schematic based on the selected system extensions.\n\nThe UI operations are equivalent to API operations.\n\nFind Schematic ID from Talos Installation\n\nImage Factory always appends “virtual” system extension with the version matching schematic ID used to generate the model. So, for any running Talos Linux instance the schematic ID can be found by looking at the list of system extensions:\n\n$ talosctl get extensions\n\nNAMESPACE   TYPE              ID   VERSION   NAME       VERSION\n\nruntime     ExtensionStatus   0    1         schematic  376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba\n\nRestrictions\n\nSome models don’t include every customization of the schematic:\n\ninstaller and initramfs images only support system extensions (kernel args and META are ignored)\nkernel assets don’t depend on the schematic\n\nOther models have full support for all customizations:\n\nany disk image format\nISO, PXE boot script\n\nWhen installing Talos Linux using ISO/PXE boot, Talos will be installed on the disk using the installer image, so the installer image in the machine configuration should be using the same schematic as the ISO/PXE boot image.\n\nSome system extensions are not available for all Talos Linux versions, so an attempt to generate a model with an unsupported system extension will fail. List of supported Talos versions and supported system extensions for each version is available in the Image Factory UI and API.\n\nUnder the Hood\n\nImage Factory is based on the Talos imager container which provides both the Talos base boot assets, and the ability to generate custom assets based on a configuration. Image Factory manages a set of imager container images to acquire base Talos Linux boot assets (kernel, initramfs), a set of Talos Linux system extension images, and a set of schematics. When a model is requested, Image Factory uses the imager container to generate the requested assets based on the schematic and the Talos Linux version.\n\nSecurity\n\nImage Factory verifies signatures of all source container images fetched:\n\nimager container images (base boot assets)\nextensions system extensions catalogs\ninstaller contianer images (base installer layer)\nTalos Linux system extension images\n\nInternally, Image Factory caches generated boot assets and signs all cached images using a private key. Image Factory verifies the signature of the cached images before serving them to clients.\n\nImage Factory signs generated installer images, and verifies the signature of the installer images before serving them to clients.\n\nImage Factory does not provide a way to list all schematics, as schematics may contain sensitive information (e.g. private kernel boot arguments). As the schematic ID is content-addressable, it is not possible to guess the ID of a schematic without knowing the content of the schematic.\n\nRunning your own Image Factory\n\nImage Factory can be deployed on-premises to provide in-house asset generation.\n\nImage Factory requires following components:\n\nan OCI registry to store schematics (private)\nan OCI registry to store cached assets (private)\nan OCI registry to store installer images (should allow public read-only access)\na container image signing key: ECDSA P-256 private key in PEM format\n\nImage Factory is configured using command line flags, use --help to see a list of available flags. Image Factory should be configured to use proper authentication to push to the OCI registries:\n\nby mounting proper credentials via ~/.docker/config.json\nby supplying GITHUB_TOKEN (for ghcr.io)\n\nImage Factory performs HTTP redirects to the public registry endpoint for installer images, so the public endpoint should be available to Talos Linux machines to pull the installer images.\n\n6 - Controllers and Resources\nDiscover how Talos Linux uses the concepts on Controllers and Resources.\n\nTalos implements concepts of resources and controllers to facilitate internal operations of the operating system. Talos resources and controllers are very similar to Kubernetes resources and controllers, but there are some differences. The content of this document is not required to operate Talos, but it is useful for troubleshooting.\n\nStarting with Talos 0.9, most of the Kubernetes control plane bootstrapping and operations is implemented via controllers and resources which allows Talos to be reactive to configuration changes, environment changes (e.g. time sync).\n\nResources\n\nA resource captures a piece of system state. Each resource belongs to a “Type” which defines resource contents. Resource state can be split in two parts:\n\nmetadata: fixed set of fields describing resource - namespace, type, ID, etc.\nspec: contents of the resource (depends on resource type).\n\nResource is uniquely identified by (namespace, type, id). Namespaces provide a way to avoid conflicts on duplicate resource IDs.\n\nAt the moment of this writing, all resources are local to the node and stored in memory. So on every reboot resource state is rebuilt from scratch (the only exception is MachineConfig resource which reflects current machine config).\n\nControllers\n\nControllers run as independent lightweight threads in Talos. The goal of the controller is to reconcile the state based on inputs and eventually update outputs.\n\nA controller can have any number of resource types (and namespaces) as inputs. In other words, it watches specified resources for changes and reconciles when these changes occur. A controller might also have additional inputs: running reconcile on schedule, watching etcd keys, etc.\n\nA controller has a single output: a set of resources of fixed type in a fixed namespace. Only one controller can manage resource type in the namespace, so conflicts are avoided.\n\nQuerying Resources\n\nTalos CLI tool talosctl provides read-only access to the resource API which includes getting specific resource, listing resources and watching for changes.\n\nTalos stores resources describing resource types and namespaces in meta namespace:\n\n$ talosctl get resourcedefinitions\n\nNODE         NAMESPACE   TYPE                 ID                                               VERSION\n\n172.20.0.2   meta        ResourceDefinition   bootstrapstatuses.v1alpha1.talos.dev             1\n\n172.20.0.2   meta        ResourceDefinition   etcdsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   kubernetescontrolplaneconfigs.config.talos.dev   1\n\n172.20.0.2   meta        ResourceDefinition   kubernetessecrets.secrets.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   machineconfigs.config.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   machinetypes.config.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   manifests.kubernetes.talos.dev                   1\n\n172.20.0.2   meta        ResourceDefinition   manifeststatuses.kubernetes.talos.dev            1\n\n172.20.0.2   meta        ResourceDefinition   namespaces.meta.cosi.dev                         1\n\n172.20.0.2   meta        ResourceDefinition   resourcedefinitions.meta.cosi.dev                1\n\n172.20.0.2   meta        ResourceDefinition   rootsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   secretstatuses.kubernetes.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   services.v1alpha1.talos.dev                      1\n\n172.20.0.2   meta        ResourceDefinition   staticpods.kubernetes.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   staticpodstatuses.kubernetes.talos.dev           1\n\n172.20.0.2   meta        ResourceDefinition   timestatuses.v1alpha1.talos.dev                  1\n\n$ talosctl get namespaces\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\n172.20.0.2   meta        Namespace   controlplane   1\n\n172.20.0.2   meta        Namespace   meta           1\n\n172.20.0.2   meta        Namespace   runtime        1\n\n172.20.0.2   meta        Namespace   secrets        1\n\n\nMost of the time namespace flag (--namespace) can be omitted, as ResourceDefinition contains default namespace which is used if no namespace is given:\n\n$ talosctl get resourcedefinitions resourcedefinitions.meta.cosi.dev -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: meta\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    id: resourcedefinitions.meta.cosi.dev\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    displayType: ResourceDefinition\n\n    aliases:\n\n        - resourcedefinitions\n\n        - resourcedefinition\n\n        - resourcedefinitions.meta\n\n        - resourcedefinitions.meta.cosi\n\n        - rd\n\n        - rds\n\n    printColumns: []\n\n    defaultNamespace: meta\n\n\nResource definition also contains type aliases which can be used interchangeably with canonical resource name:\n\n$ talosctl get ns config\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\nOutput\n\nCommand talosctl get supports following output modes:\n\ntable (default) prints resource list as a table\nyaml prints pretty formatted resources with details, including full metadata spec. This format carries most details from the backend resource (e.g. comments in MachineConfig resource)\njson prints same information as yaml, some additional details (e.g. comments) might be lost. This format is useful for automated processing with tools like jq.\nWatching Changes\n\nIf flag --watch is appended to the talosctl get command, the command switches to watch mode. If list of resources was requested, talosctl prints initial contents of the list and then appends resource information for every change:\n\n$ talosctl get svc -w\n\nNODE         *   NAMESPACE   TYPE      ID     VERSION   RUNNING   HEALTHY\n\n172.20.0.2   +   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   trustd   2   true   true\n\n172.20.0.2   +   runtime   Service   udevd   2   true   true\n\n172.20.0.2   -   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   timed   1   true   false\n\n172.20.0.2       runtime   Service   timed   2   true   true\n\n\nColumn * specifies event type:\n\n+ is created\n- is deleted\nis updated\n\nIn YAML/JSON output, field event is added to the resource representation to describe the event type.\n\nExamples\n\nGetting machine config:\n\n$ talosctl get machineconfig -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: MachineConfigs.config.talos.dev\n\n    id: v1alpha1\n\n    version: 2\n\n    phase: running\n\nspec:\n\n    version: v1alpha1 # Indicates the schema used to decode the contents.\n\n    debug: false # Enable verbose logging to the console.\n\n    persist: true # Indicates whether to pull the machine config upon every boot.\n\n    # Provides machine specific configuration options.\n\n...\n\n\nGetting control plane static pod statuses:\n\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE      TYPE              ID                                                           VERSION   READY\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-1            3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-1   3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-1            4         True\n\n\nGetting static pod definition for kube-apiserver:\n\n$ talosctl get sp kube-apiserver -n 172.20.0.2 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: StaticPods.kubernetes.talos.dev\n\n    id: kube-apiserver\n\n    version: 3\n\n    phase: running\n\n    finalizers:\n\n        - k8s.StaticPodStatus(\"kube-apiserver\")\n\nspec:\n\n    apiVersion: v1\n\n    kind: Pod\n\n    metadata:\n\n        annotations:\n\n            talos.dev/config-version: \"1\"\n\n            talos.dev/secrets-version: \"2\"\n\n...\n\nInspecting Controller Dependencies\n\nTalos can report current dependencies between controllers and resources for debugging purposes:\n\n$ talosctl inspect dependencies\n\ndigraph  {\n\n\n\n  n1[label=\"config.K8sControlPlaneController\",shape=\"box\"];\n\n  n3[label=\"config.MachineTypeController\",shape=\"box\"];\n\n  n2[fillcolor=\"azure2\",label=\"config:KubernetesControlPlaneConfigs.config.talos.dev\",shape=\"note\",style=\"filled\"];\n\n...\n\n\nThis outputs graph in graphviz format which can be rendered to PNG with command:\n\ntalosctl inspect dependencies | dot -T png > deps.png\n\n\nGraph can be enhanced by replacing resource types with actual resource instances:\n\ntalosctl inspect dependencies --with-resources | dot -T png > deps.png\n\n\n7 - Networking Resources\nDelve deeper into networking of Talos Linux.\n\nTalos network configuration subsystem is powered by COSI. Talos translates network configuration from multiple sources: machine configuration, cloud metadata, network automatic configuration (e.g. DHCP) into COSI resources.\n\nNetwork configuration and network state can be inspected using talosctl get command.\n\nNetwork machine configuration can be modified using talosctl edit mc command (also variants talosctl patch mc, talosctl apply-config) without a reboot. As API access requires network connection, --mode=try can be used to test the configuration with automatic rollback to avoid losing network access to the node.\n\nResources\n\nThere are six basic network configuration items in Talos:\n\nAddress (IP address assigned to the interface/link);\nRoute (route to a destination);\nLink (network interface/link configuration);\nResolver (list of DNS servers);\nHostname (node hostname and domainname);\nTimeServer (list of NTP servers).\n\nEach network configuration item has two counterparts:\n\n*Status (e.g. LinkStatus) describes the current state of the system (Linux kernel state);\n*Spec (e.g. LinkSpec) defines the desired configuration.\nResource\tStatus\tSpec\nAddress\tAddressStatus\tAddressSpec\nRoute\tRouteStatus\tRouteSpec\nLink\tLinkStatus\tLinkSpec\nResolver\tResolverStatus\tResolverSpec\nHostname\tHostnameStatus\tHostnameSpec\nTimeServer\tTimeServerStatus\tTimeServerSpec\n\nStatus resources have aliases with the Status suffix removed, so for example AddressStatus is also available as Address.\n\nTalos networking controllers reconcile the state so that *Status equals the desired *Spec.\n\nObserving State\n\nThe current network configuration state can be observed by querying *Status resources via talosctl:\n\n$ talosctl get addresses\n\nNODE         NAMESPACE   TYPE            ID                                       VERSION   ADDRESS                        LINK\n\n172.20.0.2   network     AddressStatus   eth0/172.20.0.2/24                       1         172.20.0.2/24                  eth0\n\n172.20.0.2   network     AddressStatus   eth0/fe80::9804:17ff:fe9d:3058/64        2         fe80::9804:17ff:fe9d:3058/64   eth0\n\n172.20.0.2   network     AddressStatus   flannel.1/10.244.4.0/32                  1         10.244.4.0/32                  flannel.1\n\n172.20.0.2   network     AddressStatus   flannel.1/fe80::10b5:44ff:fe62:6fb8/64   2         fe80::10b5:44ff:fe62:6fb8/64   flannel.1\n\n172.20.0.2   network     AddressStatus   lo/127.0.0.1/8                           1         127.0.0.1/8                    lo\n\n172.20.0.2   network     AddressStatus   lo/::1/128                               1         ::1/128                        lo\n\n\nIn the output there are addresses set up by Talos (e.g. eth0/172.20.0.2/24) and addresses set up by other facilities (e.g. flannel.1/10.244.4.0/32 set up by CNI).\n\nTalos networking controllers watch the kernel state and update resources accordingly.\n\nAdditional details about the address can be accessed via the YAML output:\n\n# talosctl get address eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressStatuses.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 1\n\n    owner: network.AddressStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    address: 172.20.0.2/24\n\n    local: 172.20.0.2\n\n    broadcast: 172.20.0.255\n\n    linkIndex: 4\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n\nResources can be watched for changes with the --watch flag to see how configuration changes over time.\n\nOther networking status resources can be inspected with talosctl get routes, talosctl get links, etc. For example:\n\n$ talosctl get resolvers\n\nNODE         NAMESPACE   TYPE             ID          VERSION   RESOLVERS\n\n172.20.0.2   network     ResolverStatus   resolvers   2         [\"8.8.8.8\",\"1.1.1.1\"]\n\n# talosctl get links -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: LinkStatuses.net.talos.dev\n\n    id: eth0\n\n    version: 2\n\n    owner: network.LinkStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    index: 4\n\n    type: ether\n\n    linkIndex: 0\n\n    flags: UP,BROADCAST,RUNNING,MULTICAST,LOWER_UP\n\n    hardwareAddr: 4e:95:8e:8f:e4:47\n\n    broadcastAddr: ff:ff:ff:ff:ff:ff\n\n    mtu: 1500\n\n    queueDisc: pfifo_fast\n\n    operationalState: up\n\n    kind: \"\"\n\n    slaveKind: \"\"\n\n    driver: virtio_net\n\n    linkState: true\n\n    speedMbit: 4294967295\n\n    port: Other\n\n    duplex: Unknown\n\nInspecting Configuration\n\nThe desired networking configuration is combined from multiple sources and presented as *Spec resources:\n\n$ talosctl get addressspecs\n\nNODE         NAMESPACE   TYPE          ID                   VERSION\n\n172.20.0.2   network     AddressSpec   eth0/172.20.0.2/24   2\n\n172.20.0.2   network     AddressSpec   lo/127.0.0.1/8       2\n\n172.20.0.2   network     AddressSpec   lo/::1/128           2\n\n\nThese AddressSpecs are applied to the Linux kernel to reach the desired state. If, for example, an AddressSpec is removed, the address is removed from the Linux network interface as well.\n\n*Spec resources can’t be manipulated directly, they are generated automatically by Talos from multiple configuration sources (see a section below for details).\n\nIf a *Spec resource is queried in YAML format, some additional information is available:\n\n# talosctl get addressspecs eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressSpecs.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 2\n\n    owner: network.AddressMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.AddressSpecController\n\nspec:\n\n    address: 172.20.0.2/24\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n    layer: operator\n\n\nAn important field is the layer field, which describes a configuration layer this spec is coming from: in this case, it’s generated by a network operator (see below) and is set by the DHCPv4 operator.\n\nConfiguration Merging\n\nSpec resources described in the previous section show the final merged configuration state, while initial specs are put to a different unmerged namespace network-config. Spec resources in the network-config namespace are merged with conflict resolution to produce the final merged representation in the network namespace.\n\nLet’s take HostnameSpec as an example. The final merged representation is:\n\n# talosctl get hostnamespec -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: hostname\n\n    version: 2\n\n    owner: network.HostnameMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.HostnameSpecController\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that the final configuration for the hostname is talos-default-controlplane-1. And this is the hostname that was actually applied. This can be verified by querying a HostnameStatus resource:\n\n$ talosctl get hostnamestatus\n\nNODE         NAMESPACE   TYPE             ID         VERSION   HOSTNAME                 DOMAINNAME\n\n172.20.0.2   network     HostnameStatus   hostname   1         talos-default-controlplane-1\n\n\nInitial configuration for the hostname in the network-config namespace is:\n\n# talosctl get hostnamespec -o yaml --namespace network-config\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: default/hostname\n\n    version: 2\n\n    owner: network.HostnameConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-172-20-0-2\n\n    domainname: \"\"\n\n    layer: default\n\n---\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: dhcp4/eth0/hostname\n\n    version: 1\n\n    owner: network.OperatorSpecController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that there are two specs for the hostname:\n\none from the default configuration layer which defines the hostname as talos-172-20-0-2 (default driven by the default node address);\nanother one from the layer operator that defines the hostname as talos-default-controlplane-1 (DHCP).\n\nTalos merges these two specs into a final HostnameSpec based on the configuration layer and merge rules. Here is the order of precedence from low to high:\n\ndefault (defaults provided by Talos);\ncmdline (from the kernel command line);\nplatform (driven by the cloud provider);\noperator (various dynamic configuration options: DHCP, Virtual IP, etc);\nconfiguration (derived from the machine configuration).\n\nSo in our example the operator layer HostnameSpec overrides the default layer producing the final hostname talos-default-controlplane-1.\n\nThe merge process applies to all six core networking specs. For each spec, the layer controls the merge behavior If multiple configuration specs appear at the same layer, they can be merged together if possible, otherwise merge result is stable but not defined (e.g. if DHCP on multiple interfaces provides two different hostnames for the node).\n\nLinkSpecs are merged across layers, so for example, machine configuration for the interface MTU overrides an MTU set by the DHCP server.\n\nNetwork Operators\n\nNetwork operators provide dynamic network configuration which can change over time as the node is running:\n\nDHCPv4\nDHCPv6\nVirtual IP\n\nNetwork operators produce specs for addresses, routes, links, etc., which are then merged and applied according to the rules described above.\n\nOperators are configured with OperatorSpec resources which describe when operators should run and additional configuration for the operator:\n\n# talosctl get operatorspecs -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: OperatorSpecs.net.talos.dev\n\n    id: dhcp4/eth0\n\n    version: 1\n\n    owner: network.OperatorConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    operator: dhcp4\n\n    linkName: eth0\n\n    requireUp: true\n\n    dhcp4:\n\n        routeMetric: 1024\n\n\nOperatorSpec resources are generated by Talos based on machine configuration mostly. DHCP4 operator is created automatically for all physical network links which are not configured explicitly via the kernel command line or the machine configuration. This also means that on the first boot, without a machine configuration, a DHCP request is made on all physical network interfaces by default.\n\nSpecs generated by operators are prefixed with the operator ID (dhcp4/eth0 in the example above) in the unmerged network-config namespace:\n\n$ talosctl -n 172.20.0.2 get addressspecs --namespace network-config\n\nNODE         NAMESPACE        TYPE          ID                              VERSION\n\n172.20.0.2   network-config   AddressSpec   dhcp4/eth0/eth0/172.20.0.2/24   1\n\nOther Network Resources\n\nThere are some additional resources describing the network subsystem state.\n\nThe NodeAddress resource presents node addresses excluding link-local and loopback addresses:\n\n$ talosctl get nodeaddresses\n\nNODE          NAMESPACE   TYPE          ID             VERSION   ADDRESSES\n\n10.100.2.23   network     NodeAddress   accumulative   6         [\"10.100.2.23\",\"147.75.98.173\",\"147.75.195.143\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   current        5         [\"10.100.2.23\",\"147.75.98.173\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   default        1         [\"10.100.2.23\"]\n\ndefault is the node default address;\ncurrent is the set of addresses a node currently has;\naccumulative is the set of addresses a node had over time (it might include virtual IPs which are not owned by the node at the moment).\n\nNodeAddress resources are used to pick up the default address for etcd peer URL, to populate SANs field in the generated certificates, etc.\n\nAnother important resource is Nodename which provides Node name in Kubernetes:\n\n$ talosctl get nodename\n\nNODE          NAMESPACE      TYPE       ID         VERSION   NODENAME\n\n10.100.2.23   controlplane   Nodename   nodename   1         infra-green-cp-mmf7v\n\n\nDepending on the machine configuration nodename might be just a hostname or the FQDN of the node.\n\nNetworkStatus aggregates the current state of the network configuration:\n\n# talosctl get networkstatus -o yaml\n\nnode: 10.100.2.23\n\nmetadata:\n\n    namespace: network\n\n    type: NetworkStatuses.net.talos.dev\n\n    id: status\n\n    version: 5\n\n    owner: network.StatusController\n\n    phase: running\n\n    created: 2021-06-24T18:56:00Z\n\n    updated: 2021-06-24T18:56:02Z\n\nspec:\n\n    addressReady: true\n\n    connectivityReady: true\n\n    hostnameReady: true\n\n    etcFilesReady: true\n\nNetwork Controllers\n\nFor each of the six basic resource types, there are several controllers:\n\n*StatusController populates *Status resources observing the Linux kernel state.\n*ConfigController produces the initial unmerged *Spec resources in the network-config namespace based on defaults, kernel command line, and machine configuration.\n*MergeController merges *Spec resources into the final representation in the network namespace.\n*SpecController applies merged *Spec resources to the kernel state.\n\nFor the network operators:\n\nOperatorConfigController produces OperatorSpec resources based on machine configuration and deafauls.\nOperatorSpecController runs network operators watching OperatorSpec resources and producing various *Spec resources in the network-config namespace.\nConfiguration Sources\n\nThere are several configuration sources for the network configuration, which are described in this section.\n\nDefaults\nlo interface is assigned addresses 127.0.0.1/8 and ::1/128;\nhostname is set to the talos-<IP> where IP is the default node address;\nresolvers are set to 8.8.8.8, 1.1.1.1;\ntime servers are set to pool.ntp.org;\nDHCP4 operator is run on any physical interface which is not configured explicitly.\nCmdline\n\nThe kernel command line is parsed for the following options:\n\nip= option is parsed for node IP, default gateway, hostname, DNS servers, NTP servers;\nbond= option is parsed for bonding interfaces and their options;\ntalos.hostname= option is used to set node hostname;\ntalos.network.interface.ignore= can be used to make Talos skip network interface configuration completely.\nPlatform\n\nPlatform configuration delivers cloud environment-specific options (e.g. the hostname).\n\nPlatform configuration is specific to the environment metadata: for example, on Equinix Metal, Talos automatically configures public and private IPs, routing, link bonding, hostname.\n\nPlatform configuration is cached across reboots in /system/state/platform-network.yaml.\n\nOperator\n\nNetwork operators provide configuration for all basic resource types.\n\nMachine Configuration\n\nThe machine configuration is parsed for link configuration, addresses, routes, hostname, resolvers and time servers. Any changes to .machine.network configuration can be applied in immediate mode.\n\nNetwork Configuration Debugging\n\nMost of the network controller operations and failures are logged to the kernel console, additional logs with debug level are available with talosctl logs controller-runtime command. If the network configuration can’t be established and the API is not available, debug level logs can be sent to the console with debug: true option in the machine configuration.\n\n8 - Network Connectivity\nDescription of the Networking Connectivity needed by Talos Linux\nConfiguring Network Connectivity\n\nThe simplest way to deploy Talos is by ensuring that all the remote components of the system (talosctl, the control plane nodes, and worker nodes) all have layer 2 connectivity. This is not always possible, however, so this page lays out the minimal network access that is required to configure and operate a talos cluster.\n\nNote: These are the ports required for Talos specifically, and should be configured in addition to the ports required by kuberenetes. See the kubernetes docs for information on the ports used by kubernetes itself.\n\nControl plane node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\ttalosctl, control plane nodes\nTCP\tInbound\t50001*\ttrustd\tWorker nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\nWorker node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\tControl plane nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\n9 - KubeSpan\nUnderstand more about KubeSpan for Talos Linux.\nWireGuard Peer Discovery\n\nThe key pieces of information needed for WireGuard generally are:\n\nthe public key of the host you wish to connect to\nan IP address and port of the host you wish to connect to\n\nThe latter is really only required of one side of the pair. Once traffic is received, that information is learned and updated by WireGuard automatically.\n\nKubernetes, though, also needs to know which traffic goes to which WireGuard peer. Because this information may be dynamic, we need a way to keep this information up to date.\n\nIf we already have a connection to Kubernetes, it’s fairly easy: we can just keep that information in Kubernetes. Otherwise, we have to have some way to discover it.\n\nTalos Linux implements a multi-tiered approach to gathering this information. Each tier can operate independently, but the amalgamation of the mechanisms produces a more robust set of connection criteria.\n\nThese mechanisms are:\n\nan external service\na Kubernetes-based system\n\nSee discovery service to learn more about the external service.\n\nThe Kubernetes-based system utilizes annotations on Kubernetes Nodes which describe each node’s public key and local addresses.\n\nOn top of this, KubeSpan can optionally route Pod subnets. This is usually taken care of by the CNI, but there are many situations where the CNI fails to be able to do this itself, across networks.\n\nNAT, Multiple Routes, Multiple IPs\n\nOne of the difficulties in communicating across networks is that there is often not a single address and port which can identify a connection for each node on the system. For instance, a node sitting on the same network might see its peer as 192.168.2.10, but a node across the internet may see it as 2001:db8:1ef1::10.\n\nWe need to be able to handle any number of addresses and ports, and we also need to have a mechanism to try them. WireGuard only allows us to select one at a time.\n\nKubeSpan implements a controller which continuously discovers and rotates these IP:port pairs until a connection is established. It then starts trying again if that connection ever fails.\n\nPacket Routing\n\nAfter we have established a WireGuard connection, we have to make sure that the right packets get sent to the WireGuard interface.\n\nWireGuard supplies a convenient facility for tagging packets which come from it, which is great. But in our case, we need to be able to allow traffic which both does not come from WireGuard and also is not destined for another Kubernetes node to flow through the normal mechanisms.\n\nUnlike many corporate or privacy-oriented VPNs, we need to allow general internet traffic to flow normally.\n\nAlso, as our cluster grows, this set of IP addresses can become quite large and quite dynamic. This would be very cumbersome and slow in iptables. Luckily, the kernel supplies a convenient mechanism by which to define this arbitrarily large set of IP addresses: IP sets.\n\nTalos collects all of the IPs and subnets which are considered “in-cluster” and maintains these in the kernel as an IP set.\n\nNow that we have the IP set defined, we need to tell the kernel how to use it.\n\nThe traditional way of doing this would be to use iptables. However, there is a big problem with IPTables. It is a common namespace in which any number of other pieces of software may dump things. We have no surety that what we add will not be wiped out by something else (from Kubernetes itself, to the CNI, to some workload application), be rendered unusable by higher-priority rules, or just generally cause trouble and conflicts.\n\nInstead, we use a three-pronged system which is both more foundational and less centralised.\n\nNFTables offers a separately namespaced, decentralised way of marking packets for later processing based on IP sets. Instead of a common set of well-known tables, NFTables uses hooks into the kernel’s netfilter system, which are less vulnerable to being usurped, bypassed, or a source of interference than IPTables, but which are rendered down by the kernel to the same underlying XTables system.\n\nOur NFTables system is where we store the IP sets. Any packet which enters the system, either by forward from inside Kubernetes or by generation from the host itself, is compared against a hash table of this IP set. If it is matched, it is marked for later processing by our next stage. This is a high-performance system which exists fully in the kernel and which ultimately becomes an eBPF program, so it scales well to hundreds of nodes.\n\nThe next stage is the kernel router’s route rules. These are defined as a common ordered list of operations for the whole operating system, but they are intended to be tightly constrained and are rarely used by applications in any case. The rules we add are very simple: if a packet is marked by our NFTables system, send it to an alternate routing table.\n\nThis leads us to our third and final stage of packet routing. We have a custom routing table with two rules:\n\nsend all IPv4 traffic to the WireGuard interface\nsend all IPv6 traffic to the WireGuard interface\n\nSo in summary, we:\n\nmark packets destined for Kubernetes applications or Kubernetes nodes\nsend marked packets to a special routing table\nsend anything which is sent to that routing table through the WireGuard interface\n\nThis gives us an isolated, resilient, tolerant, and non-invasive way to route Kubernetes traffic safely, automatically, and transparently through WireGuard across almost any set of network topologies.\n\nDesign Decisions\nRouting\n\nRouting for Wireguard is a touch complicated when the set of possible peer endpoints includes at least one member of the set of destinations. That is, packets from Wireguard to a peer endpoint should not be sent to Wireguard, lest a loop be created.\n\nIn order to handle this situation, Wireguard provides the ability to mark packets which it generates, so their routing can be handled separately.\n\nIn our case, though, we actually want the inverse of this: we want to route Wireguard packets however the normal networking routes and rules say they should be routed, while packets destined for the other side of Wireguard Peers should be forced into Wireguard interfaces.\n\nWhile IP Rules allow you to invert matches, they do not support matching based on IP sets. That means, to use simple rules, we would have to add a rule for each destination, which could reach into hundreds or thousands of rules to manage. This is not really much of a performance issue, but it is a management issue, since it is expected that we would not be the only manager of rules in the system, and rules offer no facility to tag for ownership.\n\nIP Sets are supported by IPTables, and we could integrate there. However, IPTables exists in a global namespace, which makes it fragile having multiple parties manipulating it. The newer NFTables replacement for IPTables, though, allows users to independently hook into various points of XTables, keeping all such rules and sets independent. This means that regardless of what CNIs or other user-side routing rules may do, our KubeSpan setup will not be messed up.\n\nTherefore, we utilise NFTables (which natively supports IP sets and owner grouping) instead, to mark matching traffic which should be sent to the Wireguard interface. This way, we can keep all our KubeSpan set logic in one place, allowing us to simply use a single ip rule match: for our fwmark, and sending those matched packets to a separate routing table with one rule: default to the wireguard interface.\n\nSo we have three components:\n\nA routing table for Wireguard-destined packets\nAn NFTables table which defines the set of destinations packets to which will be marked with our firewall mark.\nHook into PreRouting (type Filter)\nHook into Outgoing (type Route)\nOne IP Rule which sends packets marked with our firewall mark to our Wireguard routing table.\nRouting Table\n\nThe routing table (number 180 by default) is simple, containing a single route for each family: send everything through the Wireguard interface.\n\nNFTables\n\nThe logic inside NFTables is fairly simple. First, everything is compiled into a single table: talos_kubespan.\n\nNext, two chains are set up: one for the prerouting hook (kubespan_prerouting) and the other for the outgoing hook (kubespan_outgoing).\n\nWe define two sets of target IP prefixes: one for IPv6 (kubespan_targets_ipv6) and the other for IPv4 (kubespan_targets_ipv4).\n\nLast, we add rules to each chain which basically specify:\n\nIf the packet is marked as from Wireguard, just accept it and terminate the chain.\nIf the packet matches an IP in either of the target IP sets, mark that packet with the to Wireguard mark.\nRules\n\nThere are two route rules defined: one to match IPv6 packets and the other to match IPv4 packets.\n\nThese rules say the same thing for each: if the packet is marked that it should go to Wireguard, send it to the Wireguard routing table.\n\nFirewall Mark\n\nKubeSpan is using only two bits of the firewall mark with the mask 0x00000060.\n\nNote: if other software on the node is using the bits 0x60 of the firewall mark, this might cause conflicts and break KubeSpan.\n\nAt the moment of the writing, it was confirmed that Calico CNI is using bits 0xffff0000 and Cilium CNI is using bits 0xf00, so KubeSpan is compatible with both. Flannel CNI uses 0x4000 mask, so it is also compatible.\n\nIn the routing rules table, we match on the mark 0x40 with the mask 0x60:\n\n32500: from all fwmark 0x40/0x60 lookup 180\n\n\nIn the NFTables table, we match with the same mask 0x60 and we set the mask by only modifying bits from the 0x60 mask:\n\nmeta mark & 0x00000060 == 0x00000020 accept\n\nip daddr @kubespan_targets_ipv4 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\nip6 daddr @kubespan_targets_ipv6 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\n10 - Process Capabilities\nUnderstand the Linux process capabilities restrictions with Talos Linux.\n\nLinux defines a set of process capabilities that can be used to fine-tune the process permissions.\n\nTalos Linux for security reasons restricts any process from gaining the following capabilities:\n\nCAP_SYS_MODULE (loading kernel modules)\nCAP_SYS_BOOT (rebooting the system)\n\nThis means that any process including privileged Kubernetes pods will not be able to get these capabilities.\n\nIf you see the following error on starting a pod, make sure it doesn’t have any of the capabilities listed above in the spec:\n\nError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply caps: operation not permitted: unknown\n\n\nNote: even with CAP_SYS_MODULE capability, Linux kernel module loading is restricted by requiring a valid signature. Talos Linux creates a throw away signing key during kernel build, so it’s not possible to build/sign a kernel module for Talos Linux outside of the build process.\n\n11 - talosctl\nThe design and use of the Talos Linux control application.\n\nThe talosctl tool acts as a reference implementation for the Talos API, but it also handles a lot of conveniences for the use of Talos and its clusters.\n\nVideo Walkthrough\n\nTo see some live examples of talosctl usage, view the following video:\n\nClient Configuration\n\nTalosctl configuration is located in $XDG_CONFIG_HOME/talos/config.yaml if $XDG_CONFIG_HOME is defined. Otherwise it is in $HOME/.talos/config. The location can always be overridden by the TALOSCONFIG environment variable or the --talosconfig parameter.\n\nLike kubectl, talosctl uses the concept of configuration contexts, so any number of Talos clusters can be managed with a single configuration file. It also comes with some intelligent tooling to manage the merging of new contexts into the config. The default operation is a non-destructive merge, where if a context of the same name already exists in the file, the context to be added is renamed by appending an index number. You can easily overwrite instead, as well. See the talosctl config help for more information.\n\nEndpoints and Nodes\n\nendpoints are the communication endpoints to which the client directly talks. These can be load balancers, DNS hostnames, a list of IPs, etc. If multiple endpoints are specified, the client will automatically load balance and fail over between them. It is recommended that these point to the set of control plane nodes, either directly or through a load balancer.\n\nEach endpoint will automatically proxy requests destined to another node through it, so it is not necessary to change the endpoint configuration just because you wish to talk to a different node within the cluster.\n\nEndpoints do, however, need to be members of the same Talos cluster as the target node, because these proxied connections reply on certificate-based authentication.\n\nThe node is the target node on which you wish to perform the API call. While you can configure the target node (or even set of target nodes) inside the ’talosctl’ configuration file, it is recommended not to do so, but to explicitly declare the target node(s) using the -n or --nodes command-line parameter.\n\nWhen specifying nodes, their IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied first through the endpoints.\n\nKubeconfig\n\nThe configuration for accessing a Talos Kubernetes cluster is obtained with talosctl. By default, talosctl will safely merge the cluster into the default kubeconfig. Like talosctl itself, in the event of a naming conflict, the new context name will be index-appended before insertion. The --force option can be used to overwrite instead.\n\nYou can also specify an alternate path by supplying it as a positional parameter.\n\nThus, like Talos clusters themselves, talosctl makes it easy to manage any number of kubernetes clusters from the same workstation.\n\nCommands\n\nPlease see the CLI reference for the entire list of commands which are available from talosctl.\n\n12 - FAQs\nFrequently Asked Questions about Talos Linux.\nHow is Talos different from other container optimized Linux distros?\n\nTalos integrates tightly with Kubernetes, and is not meant to be a general-purpose operating system. The most important difference is that Talos is fully controlled by an API via a gRPC interface, instead of an ordinary shell. We don’t ship SSH, and there is no console access. Removing components such as these has allowed us to dramatically reduce the footprint of Talos, and in turn, improve a number of other areas like security, predictability, reliability, and consistency across platforms. It’s a big change from how operating systems have been managed in the past, but we believe that API-driven OSes are the future.\n\nWhy no shell or SSH?\n\nSince Talos is fully API-driven, all maintenance and debugging operations are possible via the OS API. We would like for Talos users to start thinking about what a “machine” is in the context of a Kubernetes cluster. That is, that a Kubernetes cluster can be thought of as one massive machine, and the nodes are merely additional, undifferentiated resources. We don’t want humans to focus on the nodes, but rather on the machine that is the Kubernetes cluster. Should an issue arise at the node level, talosctl should provide the necessary tooling to assist in the identification, debugging, and remediation of the issue. However, the API is based on the Principle of Least Privilege, and exposes only a limited set of methods. We envision Talos being a great place for the application of control theory in order to provide a self-healing platform.\n\nWhy the name “Talos”?\n\nTalos was an automaton created by the Greek God of the forge to protect the island of Crete. He would patrol the coast and enforce laws throughout the land. We felt it was a fitting name for a security focused operating system designed to run Kubernetes.\n\nWhy does Talos rely on a separate configuration from Kubernetes?\n\nThe talosconfig file contains client credentials to access the Talos Linux API. Sometimes Kubernetes might be down for a number of reasons (etcd issues, misconfiguration, etc.), while Talos API access will always be available. The Talos API is a way to access the operating system and fix issues, e.g. fixing access to Kubernetes. When Talos Linux is running fine, using the Kubernetes APIs (via kubeconfig) is all you should need to deploy and manage Kubernetes workloads.\n\nHow does Talos handle certificates?\n\nDuring the machine config generation process, Talos generates a set of certificate authorities (CAs) that remains valid for 10 years. Talos is responsible for managing certificates for etcd, Talos API (apid), node certificates (kubelet), and other components. It also handles the automatic rotation of server-side certificates.\n\nHowever, client certificates such as talosconfig and kubeconfig are the user’s responsibility, and by default, they have a validity period of 1 year.\n\nTo renew the talosconfig certificate, the follow this process. To renew kubeconfig, use talosctl kubeconfig command, and the time-to-live (TTL) is defined in the configuration.\n\nHow can I set the timezone of my Talos Linux clusters?\n\nTalos doesn’t support timezones, and will always run in UTC. This ensures consistency of log timestamps for all Talos Linux clusters, simplifying debugging. Your containers can run with any timezone configuration you desire, but the timezone of Talos Linux is not configurable.\n\nHow do I see Talos kernel configuration?\nUsing Talos API\n\nCurrent kernel config can be read with talosctl -n <NODE> read /proc/config.gz.\n\nFor example:\n\ntalosctl -n NODE read /proc/config.gz | zgrep E1000\n\nUsing GitHub\n\nFor amd64, see https://github.com/siderolabs/pkgs/blob/main/kernel/build/config-amd64. Use appropriate branch to see the kernel config matching your Talos release.\n\n13 - Knowledge Base\nRecipes for common configuration tasks with Talos Linux.\nDisabling GracefulNodeShutdown on a node\n\nTalos Linux enables Graceful Node Shutdown Kubernetes feature by default.\n\nIf this feature should be disabled, modify the kubelet part of the machine configuration with:\n\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      feature-gates: GracefulNodeShutdown=false\n\n    extraConfig:\n\n      shutdownGracePeriod: 0s\n\n      shutdownGracePeriodCriticalPods: 0s\n\nGenerating Talos Linux ISO image with custom kernel arguments\n\nPass additional kernel arguments using --extra-kernel-arg flag:\n\n$ docker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --extra-kernel-arg console=ttyS1 --extra-kernel-arg console=tty0 | tar xz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/vmlinuz to /mnt/boot/vmlinuz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/initramfs.xz to /mnt/boot/initramfs.xz\n\n2022/05/25 13:18:47 creating grub.cfg\n\n2022/05/25 13:18:47 creating ISO\n\n\nISO will be output to the file talos-<arch>.iso in the current directory.\n\nLogging Kubernetes audit logs with loki\n\nIf using loki-stack helm chart to gather logs from the Kubernetes cluster, you can use the helm values to configure loki-stack to log Kubernetes API server audit logs:\n\npromtail:\n\n  extraArgs:\n\n    - -config.expand-env\n\n  # this is required so that the promtail process can read the kube-apiserver audit logs written as `nobody` user\n\n  containerSecurityContext:\n\n    capabilities:\n\n      add:\n\n        - DAC_READ_SEARCH\n\n  extraVolumes:\n\n    - name: audit-logs\n\n      hostPath:\n\n        path: /var/log/audit/kube\n\n  extraVolumeMounts:\n\n    - name: audit-logs\n\n      mountPath: /var/log/audit/kube\n\n      readOnly: true\n\n  config:\n\n    snippets:\n\n      extraScrapeConfigs: |\n\n        - job_name: auditlogs\n\n          static_configs:\n\n            - targets:\n\n                - localhost\n\n              labels:\n\n                job: auditlogs\n\n                host: ${HOSTNAME}\n\n                __path__: /var/log/audit/kube/*.log        \n\nSetting CPU scaling governer\n\nWhile its possible to set CPU scaling governer via .machine.sysfs it’s sometimes cumbersome to set it for all CPU’s individually. A more elegant approach would be set it via a kernel commandline parameter. This also means that the options are applied way early in the boot process.\n\nThis can be set in the machineconfig via the snippet below:\n\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - cpufreq.default_governor=performance\n\n\nNote: Talos needs to be upgraded for the extraKernelArgs to take effect.\n\nDisable admissionControl on control plane nodes\n\nTalos Linux enables admission control in the API Server by default.\n\nAlthough it is not recommended from a security point of view, admission control can be removed by patching your control plane machine configuration:\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch-control-plane '[{\"op\": \"remove\", \"path\": \"/cluster/apiServer/admissionControl\"}]'\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "v1alpha1 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/v1alpha1/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nv1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\n1: Config\n1 - Config\nConfig defines the v1alpha1.Config Talos machine configuration document.\nversion: v1alpha1\n\nmachine: # ...\n\ncluster: # ...\nField\tType\tDescription\tValue(s)\nversion\tstring\tIndicates the schema used to decode the contents.\tv1alpha1\n\ndebug\tbool\t\nEnable verbose logging to the console.\n\ttrue\nyes\nfalse\nno\n\nmachine\tMachineConfig\tProvides machine specific configuration options.\t\ncluster\tClusterConfig\tProvides cluster specific configuration options.\t\nmachine\n\nMachineConfig represents the machine-specific config values.\n\nmachine:\n\n    type: controlplane\n\n    # InstallConfig represents the installation options for preparing a node.\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ntype\tstring\t\nDefines the role of the machine within the cluster.\n\tcontrolplane\nworker\n\ntoken\tstring\t\nThe token is used by a machine to join the PKI of the cluster.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe root certificate authority of the PKI.\nShow example(s)\n\t\ncertSANs\t[]string\t\nExtra certificate subject alternative names for the machine’s certificate.\nShow example(s)\n\t\ncontrolPlane\tMachineControlPlaneConfig\tProvides machine specific control plane configuration options.\nShow example(s)\n\t\nkubelet\tKubeletConfig\tUsed to provide additional options to the kubelet.\nShow example(s)\n\t\npods\t[]Unstructured\t\nUsed to provide static pod definitions to be run by the kubelet directly bypassing the kube-apiserver.\nShow example(s)\n\t\nnetwork\tNetworkConfig\tProvides machine specific network configuration options.\nShow example(s)\n\t\ndisks\t[]MachineDisk\t\nUsed to partition, format and mount additional disks.\nShow example(s)\n\t\ninstall\tInstallConfig\t\nUsed to provide instructions for installations.\nShow example(s)\n\t\nfiles\t[]MachineFile\t\nAllows the addition of user specified files.\nShow example(s)\n\t\nenv\tEnv\t\nThe env field allows for the addition of environment variables.\nShow example(s)\n\tGRPC_GO_LOG_VERBOSITY_LEVEL\nGRPC_GO_LOG_SEVERITY_LEVEL\nhttp_proxy\nhttps_proxy\nno_proxy\n\ntime\tTimeConfig\tUsed to configure the machine’s time settings.\nShow example(s)\n\t\nsysctls\tmap[string]string\tUsed to configure the machine’s sysctls.\nShow example(s)\n\t\nsysfs\tmap[string]string\tUsed to configure the machine’s sysfs.\nShow example(s)\n\t\nregistries\tRegistriesConfig\t\nUsed to configure the machine’s container image registry mirrors.\nShow example(s)\n\t\nsystemDiskEncryption\tSystemDiskEncryptionConfig\t\nMachine system disk encryption configuration.\nShow example(s)\n\t\nfeatures\tFeaturesConfig\tFeatures describe individual Talos features that can be switched on or off.\nShow example(s)\n\t\nudev\tUdevConfig\tConfigures the udev system.\nShow example(s)\n\t\nlogging\tLoggingConfig\tConfigures the logging system.\nShow example(s)\n\t\nkernel\tKernelConfig\tConfigures the kernel.\nShow example(s)\n\t\nseccompProfiles\t[]MachineSeccompProfile\tConfigures the seccomp profiles for the machine.\nShow example(s)\n\t\nnodeLabels\tmap[string]string\tConfigures the node labels for the machine.\nShow example(s)\n\t\nnodeTaints\tmap[string]string\tConfigures the node taints for the machine. Effect is optional.\nShow example(s)\n\t\ncontrolPlane\n\nMachineControlPlaneConfig machine specific configuration options.\n\nmachine:\n\n    controlPlane:\n\n        # Controller manager machine specific configuration options.\n\n        controllerManager:\n\n            disabled: false # Disable kube-controller-manager on the node.\n\n        # Scheduler machine specific configuration options.\n\n        scheduler:\n\n            disabled: true # Disable kube-scheduler on the node.\nField\tType\tDescription\tValue(s)\ncontrollerManager\tMachineControllerManagerConfig\tController manager machine specific configuration options.\t\nscheduler\tMachineSchedulerConfig\tScheduler machine specific configuration options.\t\ncontrollerManager\n\nMachineControllerManagerConfig represents the machine specific ControllerManager config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-controller-manager on the node.\t\nscheduler\n\nMachineSchedulerConfig represents the machine specific Scheduler config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-scheduler on the node.\t\nkubelet\n\nKubeletConfig represents the kubelet config values.\n\nmachine:\n\n    kubelet:\n\n        image: ghcr.io/siderolabs/kubelet:v1.29.0 # The `image` field is an optional reference to an alternative kubelet image.\n\n        # The `extraArgs` field is used to provide additional flags to the kubelet.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n\n\n        # # The `ClusterDNS` field is an optional reference to an alternative kubelet clusterDNS ip list.\n\n        # clusterDNS:\n\n        #     - 10.96.0.10\n\n        #     - 169.254.2.53\n\n\n\n        # # The `extraMounts` field is used to add additional mounts to the kubelet container.\n\n        # extraMounts:\n\n        #     - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n        #       type: bind # Type specifies the mount kind.\n\n        #       source: /var/lib/example # Source specifies the source path of the mount.\n\n        #       # Options are fstab style mount options.\n\n        #       options:\n\n        #         - bind\n\n        #         - rshared\n\n        #         - rw\n\n\n\n        # # The `extraConfig` field is used to provide kubelet configuration overrides.\n\n        # extraConfig:\n\n        #     serverTLSBootstrap: true\n\n\n\n        # # The `KubeletCredentialProviderConfig` field is used to provide kubelet credential configuration.\n\n        # credentialProviderConfig:\n\n        #     apiVersion: kubelet.config.k8s.io/v1\n\n        #     kind: CredentialProviderConfig\n\n        #     providers:\n\n        #         - apiVersion: credentialprovider.kubelet.k8s.io/v1\n\n        #           defaultCacheDuration: 12h\n\n        #           matchImages:\n\n        #             - '*.dkr.ecr.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.*.amazonaws.com.cn'\n\n        #             - '*.dkr.ecr-fips.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.us-iso-east-1.c2s.ic.gov'\n\n        #             - '*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov'\n\n        #           name: ecr-credential-provider\n\n\n\n        # # The `nodeIP` field is used to configure `--node-ip` flag for the kubelet.\n\n        # nodeIP:\n\n        #     # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n        #     validSubnets:\n\n        #         - 10.0.0.0/8\n\n        #         - '!10.0.0.3/32'\n\n        #         - fdc7::/16\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe image field is an optional reference to an alternative kubelet image.\nShow example(s)\n\t\nclusterDNS\t[]string\tThe ClusterDNS field is an optional reference to an alternative kubelet clusterDNS ip list.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tThe extraArgs field is used to provide additional flags to the kubelet.\nShow example(s)\n\t\nextraMounts\t[]ExtraMount\t\nThe extraMounts field is used to add additional mounts to the kubelet container.\nShow example(s)\n\t\nextraConfig\tUnstructured\t\nThe extraConfig field is used to provide kubelet configuration overrides.\nShow example(s)\n\t\ncredentialProviderConfig\tUnstructured\tThe KubeletCredentialProviderConfig field is used to provide kubelet credential configuration.\nShow example(s)\n\t\ndefaultRuntimeSeccompProfileEnabled\tbool\tEnable container runtime default Seccomp profile.\ttrue\nyes\nfalse\nno\n\nregisterWithFQDN\tbool\t\nThe registerWithFQDN field is used to force kubelet to use the node FQDN for registration.\n\ttrue\nyes\nfalse\nno\n\nnodeIP\tKubeletNodeIPConfig\t\nThe nodeIP field is used to configure --node-ip flag for the kubelet.\nShow example(s)\n\t\nskipNodeRegistration\tbool\t\nThe skipNodeRegistration is used to run the kubelet without registering with the apiserver.\n\ttrue\nyes\nfalse\nno\n\ndisableManifestsDirectory\tbool\t\nThe disableManifestsDirectory field configures the kubelet to get static pod manifests from the /etc/kubernetes/manifests directory.\n\ttrue\nyes\nfalse\nno\n\nextraMounts[]\n\nExtraMount wraps OCI Mount specification.\n\nmachine:\n\n    kubelet:\n\n        extraMounts:\n\n            - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n              type: bind # Type specifies the mount kind.\n\n              source: /var/lib/example # Source specifies the source path of the mount.\n\n              # Options are fstab style mount options.\n\n              options:\n\n                - bind\n\n                - rshared\n\n                - rw\nField\tType\tDescription\tValue(s)\ndestination\tstring\tDestination is the absolute path where the mount will be placed in the container.\t\ntype\tstring\tType specifies the mount kind.\t\nsource\tstring\tSource specifies the source path of the mount.\t\noptions\t[]string\tOptions are fstab style mount options.\t\nuidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\ngidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\nuidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\ngidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\nnodeIP\n\nKubeletNodeIPConfig represents the kubelet node IP configuration.\n\nmachine:\n\n    kubelet:\n\n        nodeIP:\n\n            # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n            validSubnets:\n\n                - 10.0.0.0/8\n\n                - '!10.0.0.3/32'\n\n                - fdc7::/16\nField\tType\tDescription\tValue(s)\nvalidSubnets\t[]string\t\nThe validSubnets field configures the networks to pick kubelet node IP from.\n\t\nnetwork\n\nNetworkConfig represents the machine’s networking config values.\n\nmachine:\n\n    network:\n\n        hostname: worker-1 # Used to statically set the hostname for the machine.\n\n        # `interfaces` is used to define the network interface configuration.\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\n\n        # Used to statically set the nameservers for the machine.\n\n        nameservers:\n\n            - 9.8.7.6\n\n            - 8.7.6.5\n\n\n\n        # # Allows for extra entries to be added to the `/etc/hosts` file\n\n        # extraHostEntries:\n\n        #     - ip: 192.168.1.100 # The IP of the host.\n\n        #       # The host alias.\n\n        #       aliases:\n\n        #         - example\n\n        #         - example.domain.tld\n\n\n\n        # # Configures KubeSpan feature.\n\n        # kubespan:\n\n        #     enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nhostname\tstring\tUsed to statically set the hostname for the machine.\t\ninterfaces\t[]Device\t\ninterfaces is used to define the network interface configuration.\nShow example(s)\n\t\nnameservers\t[]string\t\nUsed to statically set the nameservers for the machine.\nShow example(s)\n\t\nextraHostEntries\t[]ExtraHost\tAllows for extra entries to be added to the /etc/hosts file\nShow example(s)\n\t\nkubespan\tNetworkKubeSpan\tConfigures KubeSpan feature.\nShow example(s)\n\t\ndisableSearchDomain\tbool\t\nDisable generating a default search domain in /etc/resolv.conf\n\ttrue\nyes\nfalse\nno\n\ninterfaces[]\n\nDevice represents a network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\ninterface\tstring\t\nThe interface name.\nShow example(s)\n\t\ndeviceSelector\tNetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\naddresses\t[]string\t\nAssigns static IP addresses to the interface.\nShow example(s)\n\t\nroutes\t[]Route\t\nA list of routes associated with the interface.\nShow example(s)\n\t\nbond\tBond\tBond specific options.\nShow example(s)\n\t\nbridge\tBridge\tBridge specific options.\nShow example(s)\n\t\nvlans\t[]Vlan\tVLAN specific options.\t\nmtu\tint\t\nThe interface’s MTU.\n\t\ndhcp\tbool\t\nIndicates if DHCP should be used to configure the interface.\nShow example(s)\n\t\nignore\tbool\tIndicates if the interface should be ignored (skips configuration).\t\ndummy\tbool\t\nIndicates if the interface is a dummy interface.\n\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\nShow example(s)\n\t\nwireguard\tDeviceWireguardConfig\t\nWireguard specific configuration.\nShow example(s)\n\t\nvip\tDeviceVIPConfig\tVirtual (shared) IP address configuration.\nShow example(s)\n\t\ndeviceSelector\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                  driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                - network: 10.2.0.0/16 # The route's network (destination).\n\n                  gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nbond\n\nBond contains the various options for configuring a bonded interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                # The interfaces that make up the bond.\n\n                interfaces:\n\n                    - enp2s0\n\n                    - enp2s1\n\n                mode: 802.3ad # A bond option.\n\n                lacpRate: fast # A bond option.\n\n\n\n                # # Picks a network device using the selector.\n\n\n\n                # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n                # deviceSelectors:\n\n                #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                #       driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bond.\t\ndeviceSelectors\t[]NetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\narpIPTarget\t[]string\t\nA bond option.\n\t\nmode\tstring\t\nA bond option.\n\t\nxmitHashPolicy\tstring\t\nA bond option.\n\t\nlacpRate\tstring\t\nA bond option.\n\t\nadActorSystem\tstring\t\nA bond option.\n\t\narpValidate\tstring\t\nA bond option.\n\t\narpAllTargets\tstring\t\nA bond option.\n\t\nprimary\tstring\t\nA bond option.\n\t\nprimaryReselect\tstring\t\nA bond option.\n\t\nfailOverMac\tstring\t\nA bond option.\n\t\nadSelect\tstring\t\nA bond option.\n\t\nmiimon\tuint32\t\nA bond option.\n\t\nupdelay\tuint32\t\nA bond option.\n\t\ndowndelay\tuint32\t\nA bond option.\n\t\narpInterval\tuint32\t\nA bond option.\n\t\nresendIgmp\tuint32\t\nA bond option.\n\t\nminLinks\tuint32\t\nA bond option.\n\t\nlpInterval\tuint32\t\nA bond option.\n\t\npacketsPerSlave\tuint32\t\nA bond option.\n\t\nnumPeerNotif\tuint8\t\nA bond option.\n\t\ntlbDynamicLb\tuint8\t\nA bond option.\n\t\nallSlavesActive\tuint8\t\nA bond option.\n\t\nuseCarrier\tbool\t\nA bond option.\n\t\nadActorSysPrio\tuint16\t\nA bond option.\n\t\nadUserPortKey\tuint16\t\nA bond option.\n\t\npeerNotifyDelay\tuint32\t\nA bond option.\n\t\ndeviceSelectors[]\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                    driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                    - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                      driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nbridge\n\nBridge contains the various options for configuring a bridge interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bridge:\n\n                # The interfaces that make up the bridge.\n\n                interfaces:\n\n                    - enxda4042ca9a51\n\n                    - enxae2a6774c259\n\n                # A bridge option.\n\n                stp:\n\n                    enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bridge.\t\nstp\tSTP\t\nA bridge option.\n\t\nstp\n\nSTP contains the various options for configuring the STP properties of a bridge interface.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tWhether Spanning Tree Protocol (STP) is enabled.\t\nvlans[]\n\nVlan represents vlan settings for a device.\n\nField\tType\tDescription\tValue(s)\naddresses\t[]string\tThe addresses in CIDR notation or as plain IPs to use.\t\nroutes\t[]Route\tA list of routes associated with the VLAN.\t\ndhcp\tbool\tIndicates if DHCP should be used.\t\nvlanId\tuint16\tThe VLAN’s ID.\t\nmtu\tuint32\tThe VLAN’s MTU.\t\nvip\tDeviceVIPConfig\tThe VLAN’s virtual IP address configuration.\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\n\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - routes:\n\n                    - network: 0.0.0.0/0 # The route's network (destination).\n\n                      gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                    - network: 10.2.0.0/16 # The route's network (destination).\n\n                      gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - vip:\n\n                    ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - dhcpOptions:\n\n                    routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - dhcpOptions:\n\n                routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\nwireguard\n\nDeviceWireguardConfig contains settings for configuring Wireguard network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                listenPort: 51111 # Specifies a device's listening port.\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n                      persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nField\tType\tDescription\tValue(s)\nprivateKey\tstring\t\nSpecifies a private key configuration (base64 encoded).\n\t\nlistenPort\tint\tSpecifies a device’s listening port.\t\nfirewallMark\tint\tSpecifies a device’s firewall mark.\t\npeers\t[]DeviceWireguardPeer\tSpecifies a list of peer configurations to apply to a device.\t\npeers[]\n\nDeviceWireguardPeer a WireGuard device peer configuration.\n\nField\tType\tDescription\tValue(s)\npublicKey\tstring\t\nSpecifies the public key of this peer.\n\t\nendpoint\tstring\tSpecifies the endpoint of this peer entry.\t\npersistentKeepaliveInterval\tDuration\t\nSpecifies the persistent keepalive interval for this peer.\n\t\nallowedIPs\t[]string\tAllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vip:\n\n                ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\nextraHostEntries[]\n\nExtraHost represents a host entry in /etc/hosts.\n\nmachine:\n\n    network:\n\n        extraHostEntries:\n\n            - ip: 192.168.1.100 # The IP of the host.\n\n              # The host alias.\n\n              aliases:\n\n                - example\n\n                - example.domain.tld\nField\tType\tDescription\tValue(s)\nip\tstring\tThe IP of the host.\t\naliases\t[]string\tThe host alias.\t\nkubespan\n\nNetworkKubeSpan struct describes KubeSpan configuration.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the KubeSpan feature.\n\t\nadvertiseKubernetesNetworks\tbool\t\nControl whether Kubernetes pod CIDRs are announced over KubeSpan from the node.\n\t\nallowDownPeerBypass\tbool\t\nSkip sending traffic via KubeSpan if the peer connection state is not up.\n\t\nharvestExtraEndpoints\tbool\t\nKubeSpan can collect and publish extra endpoints for each member of the cluster\n\t\nmtu\tuint32\t\nKubeSpan link MTU size.\n\t\nfilters\tKubeSpanFilters\t\nKubeSpan advanced filtering of network addresses .\n\t\nfilters\n\nKubeSpanFilters struct describes KubeSpan advanced network addresses filtering.\n\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nFilter node addresses which will be advertised as KubeSpan endpoints for peer-to-peer Wireguard connections.\nShow example(s)\n\t\ndisks[]\n\nMachineDisk represents the options available for partitioning, formatting, and mounting extra disks.\n\nmachine:\n\n    disks:\n\n        - device: /dev/sdb # The name of the disk to use.\n\n          # A list of partitions to create on the disk.\n\n          partitions:\n\n            - mountpoint: /var/mnt/extra # Where to mount the partition.\n\n\n\n              # # The size of partition: either bytes or human readable representation. If `size:` is omitted, the partition is sized to occupy the full disk.\n\n\n\n              # # Human readable representation.\n\n              # size: 100 MB\n\n              # # Precise value in bytes.\n\n              # size: 1073741824\nField\tType\tDescription\tValue(s)\ndevice\tstring\tThe name of the disk to use.\t\npartitions\t[]DiskPartition\tA list of partitions to create on the disk.\t\npartitions[]\n\nDiskPartition represents the options for a disk partition.\n\nField\tType\tDescription\tValue(s)\nsize\tDiskSize\tThe size of partition: either bytes or human readable representation. If size: is omitted, the partition is sized to occupy the full disk.\nShow example(s)\n\t\nmountpoint\tstring\tWhere to mount the partition.\t\ninstall\n\nInstallConfig represents the installation options for preparing a node.\n\nmachine:\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ndisk\tstring\tThe disk used for installations.\nShow example(s)\n\t\ndiskSelector\tInstallDiskSelector\t\nLook up disk using disk attributes like model, size, serial and others.\nShow example(s)\n\t\nextraKernelArgs\t[]string\t\nAllows for supplying extra kernel args via the bootloader.\nShow example(s)\n\t\nimage\tstring\t\nAllows for supplying the image used to perform the installation.\nShow example(s)\n\t\nextensions\t[]InstallExtensionConfig\tAllows for supplying additional system extension images to install on top of base Talos image.\nShow example(s)\n\t\nwipe\tbool\t\nIndicates if the installation disk should be wiped at installation time.\n\ttrue\nyes\nfalse\nno\n\nlegacyBIOSSupport\tbool\t\nIndicates if MBR partition should be marked as bootable (active).\n\t\ndiskSelector\n\nInstallDiskSelector represents a disk query parameters for the install disk lookup.\n\nmachine:\n\n    install:\n\n        diskSelector:\n\n            size: '>= 1TB' # Disk size.\n\n            model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n\n\n            # # Disk bus path.\n\n            # busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0\n\n            # busPath: /pci0000:00/*\nField\tType\tDescription\tValue(s)\nsize\tInstallDiskSizeMatcher\tDisk size.\nShow example(s)\n\t\nname\tstring\tDisk name /sys/block/<dev>/device/name.\t\nmodel\tstring\tDisk model /sys/block/<dev>/device/model.\t\nserial\tstring\tDisk serial number /sys/block/<dev>/serial.\t\nmodalias\tstring\tDisk modalias /sys/block/<dev>/device/modalias.\t\nuuid\tstring\tDisk UUID /sys/block/<dev>/uuid.\t\nwwid\tstring\tDisk WWID /sys/block/<dev>/wwid.\t\ntype\tInstallDiskType\tDisk Type.\tssd\nhdd\nnvme\nsd\n\nbusPath\tstring\tDisk bus path.\nShow example(s)\n\t\nextensions[]\n\nInstallExtensionConfig represents a configuration for a system extension.\n\nmachine:\n\n    install:\n\n        extensions:\n\n            - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\nimage\tstring\tSystem extension image.\t\nfiles[]\n\nMachineFile represents a file to write to disk.\n\nmachine:\n\n    files:\n\n        - content: '...' # The contents of the file.\n\n          permissions: 0o666 # The file's permissions in octal.\n\n          path: /tmp/file.txt # The path of the file.\n\n          op: append # The operation to use\nField\tType\tDescription\tValue(s)\ncontent\tstring\tThe contents of the file.\t\npermissions\tFileMode\tThe file’s permissions in octal.\t\npath\tstring\tThe path of the file.\t\nop\tstring\tThe operation to use\tcreate\nappend\noverwrite\n\ntime\n\nTimeConfig represents the options for configuring time on a machine.\n\nmachine:\n\n    time:\n\n        disabled: false # Indicates if the time service is disabled for the machine.\n\n        # Specifies time (NTP) servers to use for setting the system time.\n\n        servers:\n\n            - time.cloudflare.com\n\n        bootTimeout: 2m0s # Specifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\t\nIndicates if the time service is disabled for the machine.\n\t\nservers\t[]string\t\nSpecifies time (NTP) servers to use for setting the system time.\n\t\nbootTimeout\tDuration\t\nSpecifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\n\t\nregistries\n\nRegistriesConfig represents the image pull options.\n\nmachine:\n\n    registries:\n\n        # Specifies mirror configuration for each registry host namespace.\n\n        mirrors:\n\n            docker.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.local\n\n        # Specifies TLS & auth configuration for HTTPS image registries.\n\n        config:\n\n            registry.local:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n                # The auth configuration for this registry.\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nmirrors\tmap[string]RegistryMirrorConfig\t\nSpecifies mirror configuration for each registry host namespace.\nShow example(s)\n\t\nconfig\tmap[string]RegistryConfig\t\nSpecifies TLS & auth configuration for HTTPS image registries.\nShow example(s)\n\t\nmirrors.*\n\nRegistryMirrorConfig represents mirror configuration for a registry.\n\nmachine:\n\n    registries:\n\n        mirrors:\n\n            ghcr.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.insecure\n\n                    - https://ghcr.io/v2/\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nList of endpoints (URLs) for registry mirrors to use.\n\t\noverridePath\tbool\t\nUse the exact path specified for the endpoint (don’t append /v2/).\n\t\nconfig.*\n\nRegistryConfig specifies auth & TLS config per registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            registry.insecure:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n\n\n                # # The auth configuration for this registry.\n\n                # auth:\n\n                #     username: username # Optional registry authentication.\n\n                #     password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\ntls\tRegistryTLSConfig\tThe TLS configuration for the registry.\nShow example(s)\n\t\nauth\tRegistryAuthConfig\t\nThe auth configuration for this registry.\nShow example(s)\n\t\ntls\n\nRegistryTLSConfig specifies TLS config for HTTPS registries.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nField\tType\tDescription\tValue(s)\nclientIdentity\tPEMEncodedCertificateAndKey\t\nEnable mutual TLS authentication with the registry.\nShow example(s)\n\t\nca\tBase64Bytes\t\nCA registry certificate to add the list of trusted certificates.\n\t\ninsecureSkipVerify\tbool\tSkip TLS server certificate verification (not recommended).\t\nauth\n\nRegistryAuthConfig specifies authentication configuration for a registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nusername\tstring\t\nOptional registry authentication.\n\t\npassword\tstring\t\nOptional registry authentication.\n\t\nauth\tstring\t\nOptional registry authentication.\n\t\nidentityToken\tstring\t\nOptional registry authentication.\n\t\nsystemDiskEncryption\n\nSystemDiskEncryptionConfig specifies system disk partitions encryption settings.\n\nmachine:\n\n    systemDiskEncryption:\n\n        # Ephemeral partition encryption.\n\n        ephemeral:\n\n            provider: luks2 # Encryption provider to use for the encryption.\n\n            # Defines the encryption keys generation and storage method.\n\n            keys:\n\n                - # Deterministically generated key from the node UUID and PartitionLabel.\n\n                  nodeID: {}\n\n                  slot: 0 # Key slot number for LUKS2 encryption.\n\n\n\n                  # # KMS managed encryption key.\n\n                  # kms:\n\n                  #     endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\n\n\n\n            # # Cipher kind to use for the encryption. Depends on the encryption provider.\n\n            # cipher: aes-xts-plain64\n\n\n\n            # # Defines the encryption sector size.\n\n            # blockSize: 4096\n\n\n\n            # # Additional --perf parameters for the LUKS2 encryption.\n\n            # options:\n\n            #     - no_read_workqueue\n\n            #     - no_write_workqueue\nField\tType\tDescription\tValue(s)\nstate\tEncryptionConfig\tState partition encryption.\t\nephemeral\tEncryptionConfig\tEphemeral partition encryption.\t\nstate\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        state:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nephemeral\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        ephemeral:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nfeatures\n\nFeaturesConfig describes individual Talos features that can be switched on or off.\n\nmachine:\n\n    features:\n\n        rbac: true # Enable role-based access control (RBAC).\n\n\n\n        # # Configure Talos API access from Kubernetes pods.\n\n        # kubernetesTalosAPIAccess:\n\n        #     enabled: true # Enable Talos API access from Kubernetes pods.\n\n        #     # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n        #     allowedRoles:\n\n        #         - os:reader\n\n        #     # The list of Kubernetes namespaces Talos API access is available from.\n\n        #     allowedKubernetesNamespaces:\n\n        #         - kube-system\nField\tType\tDescription\tValue(s)\nrbac\tbool\tEnable role-based access control (RBAC).\t\nstableHostname\tbool\tEnable stable default hostname.\t\nkubernetesTalosAPIAccess\tKubernetesTalosAPIAccessConfig\t\nConfigure Talos API access from Kubernetes pods.\nShow example(s)\n\t\napidCheckExtKeyUsage\tbool\tEnable checks for extended key usage of client certificates in apid.\t\ndiskQuotaSupport\tbool\t\nEnable XFS project quota support for EPHEMERAL partition and user disks.\n\t\nkubePrism\tKubePrism\t\nKubePrism - local proxy/load balancer on defined port that will distribute\n\t\nkubernetesTalosAPIAccess\n\nKubernetesTalosAPIAccessConfig describes the configuration for the Talos API access from Kubernetes pods.\n\nmachine:\n\n    features:\n\n        kubernetesTalosAPIAccess:\n\n            enabled: true # Enable Talos API access from Kubernetes pods.\n\n            # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n            allowedRoles:\n\n                - os:reader\n\n            # The list of Kubernetes namespaces Talos API access is available from.\n\n            allowedKubernetesNamespaces:\n\n                - kube-system\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable Talos API access from Kubernetes pods.\t\nallowedRoles\t[]string\t\nThe list of Talos API roles which can be granted for access from Kubernetes pods.\n\t\nallowedKubernetesNamespaces\t[]string\tThe list of Kubernetes namespaces Talos API access is available from.\t\nkubePrism\n\nKubePrism describes the configuration for the KubePrism load balancer.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable KubePrism support - will start local load balacing proxy.\t\nport\tint\tKubePrism port.\t\nudev\n\nUdevConfig describes how the udev system should be configured.\n\nmachine:\n\n    udev:\n\n        # List of udev rules to apply to the udev system\n\n        rules:\n\n            - SUBSYSTEM==\"drm\", KERNEL==\"renderD*\", GROUP=\"44\", MODE=\"0660\"\nField\tType\tDescription\tValue(s)\nrules\t[]string\tList of udev rules to apply to the udev system\t\nlogging\n\nLoggingConfig struct configures Talos logging.\n\nmachine:\n\n    logging:\n\n        # Logging destination.\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345 # Where to send logs. Supported protocols are \"tcp\" and \"udp\".\n\n              format: json_lines # Logs format.\nField\tType\tDescription\tValue(s)\ndestinations\t[]LoggingDestination\tLogging destination.\t\ndestinations[]\n\nLoggingDestination struct configures Talos logging destination.\n\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\tWhere to send logs. Supported protocols are “tcp” and “udp”.\nShow example(s)\n\t\nformat\tstring\tLogs format.\tjson_lines\n\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://1.2.3.4:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://cluster1.internal:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: udp://127.0.0.1:12345\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nkernel\n\nKernelConfig struct configures Talos Linux kernel.\n\nmachine:\n\n    kernel:\n\n        # Kernel modules to load.\n\n        modules:\n\n            - name: brtfs # Module name.\nField\tType\tDescription\tValue(s)\nmodules\t[]KernelModuleConfig\tKernel modules to load.\t\nmodules[]\n\nKernelModuleConfig struct configures Linux kernel modules to load.\n\nField\tType\tDescription\tValue(s)\nname\tstring\tModule name.\t\nparameters\t[]string\tModule parameters, changes applied after reboot.\t\nseccompProfiles[]\n\nMachineSeccompProfile defines seccomp profiles for the machine.\n\nmachine:\n\n    seccompProfiles:\n\n        - name: audit.json # The `name` field is used to provide the file name of the seccomp profile.\n\n          # The `value` field is used to provide the seccomp profile.\n\n          value:\n\n            defaultAction: SCMP_ACT_LOG\nField\tType\tDescription\tValue(s)\nname\tstring\tThe name field is used to provide the file name of the seccomp profile.\t\nvalue\tUnstructured\tThe value field is used to provide the seccomp profile.\t\ncluster\n\nClusterConfig represents the cluster-wide config values.\n\ncluster:\n\n    # ControlPlaneConfig represents the control plane configuration options.\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\n\n    clusterName: talos.local\n\n    # ClusterNetworkConfig represents kube networking configuration options.\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\nid\tstring\tGlobally unique identifier for this cluster (base64 encoded random 32 bytes).\t\nsecret\tstring\t\nShared secret of cluster (base64 encoded random 32 bytes).\n\t\ncontrolPlane\tControlPlaneConfig\tProvides control plane specific configuration options.\nShow example(s)\n\t\nclusterName\tstring\tConfigures the cluster’s name.\t\nnetwork\tClusterNetworkConfig\tProvides cluster specific network configuration options.\nShow example(s)\n\t\ntoken\tstring\tThe bootstrap token used to join the cluster.\nShow example(s)\n\t\naescbcEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nsecretboxEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\tThe base64 encoded root certificate authority used by Kubernetes.\nShow example(s)\n\t\naggregatorCA\tPEMEncodedCertificateAndKey\t\nThe base64 encoded aggregator certificate authority used by Kubernetes for front-proxy certificate generation.\nShow example(s)\n\t\nserviceAccount\tPEMEncodedKey\tThe base64 encoded private key for service account token generation.\nShow example(s)\n\t\napiServer\tAPIServerConfig\tAPI server specific configuration options.\nShow example(s)\n\t\ncontrollerManager\tControllerManagerConfig\tController manager server specific configuration options.\nShow example(s)\n\t\nproxy\tProxyConfig\tKube-proxy server-specific configuration options\nShow example(s)\n\t\nscheduler\tSchedulerConfig\tScheduler server specific configuration options.\nShow example(s)\n\t\ndiscovery\tClusterDiscoveryConfig\tConfigures cluster member discovery.\nShow example(s)\n\t\netcd\tEtcdConfig\tEtcd specific configuration options.\nShow example(s)\n\t\ncoreDNS\tCoreDNS\tCore DNS specific configuration options.\nShow example(s)\n\t\nexternalCloudProvider\tExternalCloudProviderConfig\tExternal cloud provider configuration.\nShow example(s)\n\t\nextraManifests\t[]string\t\nA list of urls that point to additional manifests.\nShow example(s)\n\t\nextraManifestHeaders\tmap[string]string\tA map of key value pairs that will be added while fetching the extraManifests.\nShow example(s)\n\t\ninlineManifests\t[]ClusterInlineManifest\t\nA list of inline Kubernetes manifests.\nShow example(s)\n\t\nadminKubeconfig\tAdminKubeconfigConfig\t\nSettings for admin kubeconfig generation.\nShow example(s)\n\t\nallowSchedulingOnControlPlanes\tbool\tAllows running workload on control-plane nodes.\nShow example(s)\n\ttrue\nyes\nfalse\nno\n\ncontrolPlane\n\nControlPlaneConfig represents the control plane configuration options.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\t\nEndpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\nShow example(s)\n\t\nlocalAPIServerPort\tint\t\nThe port that the API server listens on internally.\n\t\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: https://cluster1.internal:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: udp://127.0.0.1:12345\ncluster:\n\n    controlPlane:\n\n        endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nnetwork\n\nClusterNetworkConfig represents kube networking configuration options.\n\ncluster:\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\ncni\tCNIConfig\t\nThe CNI used.\nShow example(s)\n\t\ndnsDomain\tstring\t\nThe domain used by Kubernetes DNS.\nShow example(s)\n\t\npodSubnets\t[]string\tThe pod subnet CIDR.\nShow example(s)\n\t\nserviceSubnets\t[]string\tThe service subnet CIDR.\nShow example(s)\n\t\ncni\n\nCNIConfig represents the CNI configuration options.\n\ncluster:\n\n    network:\n\n        cni:\n\n            name: custom # Name of CNI to use.\n\n            # URLs containing manifests to apply for the CNI.\n\n            urls:\n\n                - https://docs.projectcalico.org/archive/v3.20/manifests/canal.yaml\nField\tType\tDescription\tValue(s)\nname\tstring\tName of CNI to use.\tflannel\ncustom\nnone\n\nurls\t[]string\t\nURLs containing manifests to apply for the CNI.\n\t\nflannel\tFlannelCNIConfig\t\ndescription:\n\tFlannel configuration options.\n\nflannel\n\nFlannelCNIConfig represents the Flannel CNI configuration options.\n\nField\tType\tDescription\tValue(s)\nextraArgs\t[]string\tExtra arguments for ‘flanneld’.\nShow example(s)\n\t\napiServer\n\nAPIServerConfig represents the kube apiserver configuration options.\n\ncluster:\n\n    apiServer:\n\n        image: registry.k8s.io/kube-apiserver:v1.29.0 # The container image used in the API server manifest.\n\n        # Extra arguments to supply to the API server.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n            http2-max-streams-per-connection: \"32\"\n\n        # Extra certificate subject alternative names for the API server's certificate.\n\n        certSANs:\n\n            - 1.2.3.4\n\n            - 4.5.6.7\n\n\n\n        # # Configure the API server admission plugins.\n\n        # admissionControl:\n\n        #     - name: PodSecurity # Name is the name of the admission controller.\n\n        #       # Configuration is an embedded configuration object to be used as the plugin's\n\n        #       configuration:\n\n        #         apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n        #         defaults:\n\n        #             audit: restricted\n\n        #             audit-version: latest\n\n        #             enforce: baseline\n\n        #             enforce-version: latest\n\n        #             warn: restricted\n\n        #             warn-version: latest\n\n        #         exemptions:\n\n        #             namespaces:\n\n        #                 - kube-system\n\n        #             runtimeClasses: []\n\n        #             usernames: []\n\n        #         kind: PodSecurityConfiguration\n\n\n\n        # # Configure the API server audit policy.\n\n        # auditPolicy:\n\n        #     apiVersion: audit.k8s.io/v1\n\n        #     kind: Policy\n\n        #     rules:\n\n        #         - level: Metadata\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the API server manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the API server.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the API server static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\ncertSANs\t[]string\tExtra certificate subject alternative names for the API server’s certificate.\t\ndisablePodSecurityPolicy\tbool\tDisable PodSecurityPolicy in the API server and default manifests.\t\nadmissionControl\t[]AdmissionPluginConfig\tConfigure the API server admission plugins.\nShow example(s)\n\t\nauditPolicy\tUnstructured\tConfigure the API server audit policy.\nShow example(s)\n\t\nresources\tResourcesConfig\tConfigure the API server resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nadmissionControl[]\n\nAdmissionPluginConfig represents the API server admission plugin configuration.\n\ncluster:\n\n    apiServer:\n\n        admissionControl:\n\n            - name: PodSecurity # Name is the name of the admission controller.\n\n              # Configuration is an embedded configuration object to be used as the plugin's\n\n              configuration:\n\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n                defaults:\n\n                    audit: restricted\n\n                    audit-version: latest\n\n                    enforce: baseline\n\n                    enforce-version: latest\n\n                    warn: restricted\n\n                    warn-version: latest\n\n                exemptions:\n\n                    namespaces:\n\n                        - kube-system\n\n                    runtimeClasses: []\n\n                    usernames: []\n\n                kind: PodSecurityConfiguration\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName is the name of the admission controller.\n\t\nconfiguration\tUnstructured\t\nConfiguration is an embedded configuration object to be used as the plugin’s\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ncontrollerManager\n\nControllerManagerConfig represents the kube controller manager configuration options.\n\ncluster:\n\n    controllerManager:\n\n        image: registry.k8s.io/kube-controller-manager:v1.29.0 # The container image used in the controller manager manifest.\n\n        # Extra arguments to supply to the controller manager.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the controller manager manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the controller manager.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the controller manager static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the controller manager resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\nproxy\n\nProxyConfig represents the kube proxy configuration options.\n\ncluster:\n\n    proxy:\n\n        image: registry.k8s.io/kube-proxy:v1.29.0 # The container image used in the kube-proxy manifest.\n\n        mode: ipvs # proxy mode of kube-proxy.\n\n        # Extra arguments to supply to kube-proxy.\n\n        extraArgs:\n\n            proxy-mode: iptables\n\n\n\n        # # Disable kube-proxy deployment on cluster bootstrap.\n\n        # disabled: false\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-proxy deployment on cluster bootstrap.\nShow example(s)\n\t\nimage\tstring\tThe container image used in the kube-proxy manifest.\nShow example(s)\n\t\nmode\tstring\t\nproxy mode of kube-proxy.\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to kube-proxy.\t\nscheduler\n\nSchedulerConfig represents the kube scheduler configuration options.\n\ncluster:\n\n    scheduler:\n\n        image: registry.k8s.io/kube-scheduler:v1.29.0 # The container image used in the scheduler manifest.\n\n        # Extra arguments to supply to the scheduler.\n\n        extraArgs:\n\n            feature-gates: AllBeta=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the scheduler manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the scheduler.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the scheduler static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the scheduler resources.\t\nconfig\tUnstructured\tSpecify custom kube-scheduler configuration.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ndiscovery\n\nClusterDiscoveryConfig struct configures cluster membership discovery.\n\ncluster:\n\n    discovery:\n\n        enabled: true # Enable the cluster membership discovery feature.\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            # Kubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\n            kubernetes: {}\n\n            # Service registry is using an external service to push and pull information about cluster members.\n\n            service:\n\n                endpoint: https://discovery.talos.dev/ # External service endpoint.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the cluster membership discovery feature.\n\t\nregistries\tDiscoveryRegistriesConfig\tConfigure registries used for cluster member discovery.\t\nregistries\n\nDiscoveryRegistriesConfig struct configures cluster membership discovery.\n\nField\tType\tDescription\tValue(s)\nkubernetes\tRegistryKubernetesConfig\t\nKubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\t\nservice\tRegistryServiceConfig\tService registry is using an external service to push and pull information about cluster members.\t\nkubernetes\n\nRegistryKubernetesConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable Kubernetes discovery registry.\t\nservice\n\nRegistryServiceConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable external service discovery registry.\t\nendpoint\tstring\tExternal service endpoint.\nShow example(s)\n\t\netcd\n\nEtcdConfig represents the etcd configuration options.\n\ncluster:\n\n    etcd:\n\n        image: gcr.io/etcd-development/etcd:v3.5.11 # The container image used to create the etcd service.\n\n        # The `ca` is the root certificate authority of the PKI.\n\n        ca:\n\n            crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n            key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n        # Extra arguments to supply to etcd.\n\n        extraArgs:\n\n            election-timeout: \"5000\"\n\n\n\n        # # The `advertisedSubnets` field configures the networks to pick etcd advertised IP from.\n\n        # advertisedSubnets:\n\n        #     - 10.0.0.0/8\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used to create the etcd service.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe ca is the root certificate authority of the PKI.\nShow example(s)\n\t\nextraArgs\tmap[string]string\t\nExtra arguments to supply to etcd.\n\t\nadvertisedSubnets\t[]string\t\nThe advertisedSubnets field configures the networks to pick etcd advertised IP from.\nShow example(s)\n\t\nlistenSubnets\t[]string\t\nThe listenSubnets field configures the networks for the etcd to listen for peer and client connections.\n\t\ncoreDNS\n\nCoreDNS represents the CoreDNS config values.\n\ncluster:\n\n    coreDNS:\n\n        image: registry.k8s.io/coredns/coredns:v1.11.1 # The `image` field is an override to the default coredns image.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable coredns deployment on cluster bootstrap.\t\nimage\tstring\tThe image field is an override to the default coredns image.\t\nexternalCloudProvider\n\nExternalCloudProviderConfig contains external cloud provider configuration.\n\ncluster:\n\n    externalCloudProvider:\n\n        enabled: true # Enable external cloud provider.\n\n        # A list of urls that point to additional manifests for an external cloud provider.\n\n        manifests:\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/rbac.yaml\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/aws-cloud-controller-manager-daemonset.yaml\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable external cloud provider.\ttrue\nyes\nfalse\nno\n\nmanifests\t[]string\t\nA list of urls that point to additional manifests for an external cloud provider.\nShow example(s)\n\t\ninlineManifests[]\n\nClusterInlineManifest struct describes inline bootstrap manifests for the user.\n\ncluster:\n\n    inlineManifests:\n\n        - name: namespace-ci # Name of the manifest.\n\n          contents: |- # Manifest contents as a string.\n\n            apiVersion: v1\n\n            kind: Namespace\n\n            metadata:\n\n            \tname: ci\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName of the manifest.\nShow example(s)\n\t\ncontents\tstring\tManifest contents as a string.\nShow example(s)\n\t\nadminKubeconfig\n\nAdminKubeconfigConfig contains admin kubeconfig settings.\n\ncluster:\n\n    adminKubeconfig:\n\n        certLifetime: 1h0m0s # Admin kubeconfig certificate lifetime (default is 1 year).\nField\tType\tDescription\tValue(s)\ncertLifetime\tDuration\t\nAdmin kubeconfig certificate lifetime (default is 1 year).\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Reference | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nReference\n1: API\n2: CLI\n3: Configuration\n3.1: network\n3.1.1: NetworkDefaultActionConfig\n3.1.2: NetworkRuleConfig\n3.2: runtime\n3.2.1: EventSinkConfig\n3.2.2: KmsgLogConfig\n3.3: siderolink\n3.3.1: SideroLinkConfig\n3.4: v1alpha1\n3.4.1: Config\n4: Kernel\n1 - API\nTalos gRPC API reference.\nTable of Contents\n\ncommon/common.proto\n\nData\n\nDataResponse\n\nEmpty\n\nEmptyResponse\n\nError\n\nMetadata\n\nNetIP\n\nNetIPPort\n\nNetIPPrefix\n\nPEMEncodedCertificateAndKey\n\nPEMEncodedKey\n\nURL\n\nCode\n\nContainerDriver\n\nContainerdNamespace\n\nFile-level Extensions\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\nConfigSpec\nControlPlane\nIdentitySpec\nInfoSpec\nKubeSpanAffiliateSpec\nMemberSpec\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\nMachineType\nNethelpersADSelect\nNethelpersARPAllTargets\nNethelpersARPValidate\nNethelpersAddressFlag\nNethelpersBondMode\nNethelpersBondXmitHashPolicy\nNethelpersConntrackState\nNethelpersDuplex\nNethelpersFailOverMAC\nNethelpersFamily\nNethelpersLACPRate\nNethelpersLinkType\nNethelpersMatchOperator\nNethelpersNfTablesChainHook\nNethelpersNfTablesChainPriority\nNethelpersNfTablesVerdict\nNethelpersOperationalState\nNethelpersPort\nNethelpersPrimaryReselect\nNethelpersProtocol\nNethelpersRouteFlag\nNethelpersRouteProtocol\nNethelpersRouteType\nNethelpersRoutingTable\nNethelpersScope\nNethelpersVLANProtocol\nNetworkConfigLayer\nNetworkOperator\nRuntimeMachineStage\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\nConfigSpec.ExtraArgsEntry\nMemberSpec\nPKIStatusSpec\nSpecSpec\nSpecSpec.ExtraArgsEntry\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\nConstraint\nLayer\nMetadata\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\nEtcFileStatusSpec\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\nProcessorSpec\nSystemInformationSpec\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\nAPIServerConfigSpec.EnvironmentVariablesEntry\nAPIServerConfigSpec.ExtraArgsEntry\nAdmissionControlConfigSpec\nAdmissionPluginSpec\nAuditPolicyConfigSpec\nBootstrapManifestsConfigSpec\nConfigStatusSpec\nControllerManagerConfigSpec\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nControllerManagerConfigSpec.ExtraArgsEntry\nEndpointSpec\nExtraManifest\nExtraManifest.ExtraHeadersEntry\nExtraManifestsConfigSpec\nExtraVolume\nKubePrismConfigSpec\nKubePrismEndpoint\nKubePrismEndpointsSpec\nKubePrismStatusesSpec\nKubeletConfigSpec\nKubeletConfigSpec.ExtraArgsEntry\nKubeletSpecSpec\nManifestSpec\nManifestStatusSpec\nNodeIPConfigSpec\nNodeIPSpec\nNodeLabelSpecSpec\nNodeStatusSpec\nNodeStatusSpec.AnnotationsEntry\nNodeStatusSpec.LabelsEntry\nNodeTaintSpecSpec\nNodenameSpec\nResources\nResources.LimitsEntry\nResources.RequestsEntry\nSchedulerConfigSpec\nSchedulerConfigSpec.EnvironmentVariablesEntry\nSchedulerConfigSpec.ExtraArgsEntry\nSecretsStatusSpec\nSingleManifest\nStaticPodServerStatusSpec\nStaticPodSpec\nStaticPodStatusSpec\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\nEndpointSpec\nIdentitySpec\nPeerSpecSpec\nPeerStatusSpec\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\nAddressStatusSpec\nBondMasterSpec\nBondSlave\nBridgeMasterSpec\nBridgeSlave\nDHCP4OperatorSpec\nDHCP6OperatorSpec\nHardwareAddrSpec\nHostnameSpecSpec\nHostnameStatusSpec\nLinkRefreshSpec\nLinkSpecSpec\nLinkStatusSpec\nNfTablesAddressMatch\nNfTablesChainSpec\nNfTablesClampMSS\nNfTablesConntrackStateMatch\nNfTablesIfNameMatch\nNfTablesLayer4Match\nNfTablesLimitMatch\nNfTablesMark\nNfTablesPortMatch\nNfTablesRule\nNodeAddressFilterSpec\nNodeAddressSpec\nOperatorSpecSpec\nPortRange\nProbeSpecSpec\nProbeStatusSpec\nResolverSpecSpec\nResolverStatusSpec\nRouteSpecSpec\nRouteStatusSpec\nSTPSpec\nStatusSpec\nTCPProbeSpec\nTimeServerSpecSpec\nTimeServerStatusSpec\nVIPEquinixMetalSpec\nVIPHCloudSpec\nVIPOperatorSpec\nVLANSpec\nWireguardPeer\nWireguardSpec\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\nCPUStat\nMemorySpec\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\nMount\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\nEventSinkConfigSpec\nKernelModuleSpecSpec\nKernelParamSpecSpec\nKernelParamStatusSpec\nKmsgLogConfigSpec\nMachineStatusSpec\nMachineStatusStatus\nMaintenanceServiceConfigSpec\nMetaKeySpec\nMetaLoadedSpec\nMountStatusSpec\nPlatformMetadataSpec\nSecurityStateSpec\nUniqueMachineTokenSpec\nUnmetCondition\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\nCertSANSpec\nEtcdCertsSpec\nEtcdRootSpec\nKubeletSpec\nKubernetesCertsSpec\nKubernetesDynamicCertsSpec\nKubernetesRootSpec\nMaintenanceRootSpec\nMaintenanceServiceCertsSpec\nOSRootSpec\nTrustdCertsSpec\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\nStatusSpec\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\ninspect/inspect.proto\n\nControllerDependencyEdge\n\nControllerRuntimeDependenciesResponse\n\nControllerRuntimeDependency\n\nDependencyEdgeType\n\nInspectService\n\nmachine/machine.proto\n\nAddressEvent\n\nApplyConfiguration\n\nApplyConfigurationRequest\n\nApplyConfigurationResponse\n\nBPFInstruction\n\nBootstrap\n\nBootstrapRequest\n\nBootstrapResponse\n\nCNIConfig\n\nCPUInfo\n\nCPUInfoResponse\n\nCPUStat\n\nCPUsInfo\n\nClusterConfig\n\nClusterNetworkConfig\n\nConfigLoadErrorEvent\n\nConfigValidationErrorEvent\n\nConnectRecord\n\nConnectRecord.Process\n\nContainer\n\nContainerInfo\n\nContainersRequest\n\nContainersResponse\n\nControlPlaneConfig\n\nCopyRequest\n\nDHCPOptionsConfig\n\nDiskStat\n\nDiskStats\n\nDiskStatsResponse\n\nDiskUsageInfo\n\nDiskUsageRequest\n\nDmesgRequest\n\nEtcdAlarm\n\nEtcdAlarmDisarm\n\nEtcdAlarmDisarmResponse\n\nEtcdAlarmListResponse\n\nEtcdDefragment\n\nEtcdDefragmentResponse\n\nEtcdForfeitLeadership\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\n\nEtcdLeaveCluster\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\n\nEtcdMember\n\nEtcdMemberAlarm\n\nEtcdMemberListRequest\n\nEtcdMemberListResponse\n\nEtcdMemberStatus\n\nEtcdMembers\n\nEtcdRecover\n\nEtcdRecoverResponse\n\nEtcdRemoveMember\n\nEtcdRemoveMemberByID\n\nEtcdRemoveMemberByIDRequest\n\nEtcdRemoveMemberByIDResponse\n\nEtcdRemoveMemberRequest\n\nEtcdRemoveMemberResponse\n\nEtcdSnapshotRequest\n\nEtcdStatus\n\nEtcdStatusResponse\n\nEvent\n\nEventsRequest\n\nFeaturesInfo\n\nFileInfo\n\nGenerateClientConfiguration\n\nGenerateClientConfigurationRequest\n\nGenerateClientConfigurationResponse\n\nGenerateConfiguration\n\nGenerateConfigurationRequest\n\nGenerateConfigurationResponse\n\nHostname\n\nHostnameResponse\n\nImageListRequest\n\nImageListResponse\n\nImagePull\n\nImagePullRequest\n\nImagePullResponse\n\nInstallConfig\n\nListRequest\n\nLoadAvg\n\nLoadAvgResponse\n\nLogsRequest\n\nMachineConfig\n\nMachineStatusEvent\n\nMachineStatusEvent.MachineStatus\n\nMachineStatusEvent.MachineStatus.UnmetCondition\n\nMemInfo\n\nMemory\n\nMemoryResponse\n\nMetaDelete\n\nMetaDeleteRequest\n\nMetaDeleteResponse\n\nMetaWrite\n\nMetaWriteRequest\n\nMetaWriteResponse\n\nMountStat\n\nMounts\n\nMountsResponse\n\nNetDev\n\nNetstat\n\nNetstatRequest\n\nNetstatRequest.Feature\n\nNetstatRequest.L4proto\n\nNetstatRequest.NetNS\n\nNetstatResponse\n\nNetworkConfig\n\nNetworkDeviceConfig\n\nNetworkDeviceStats\n\nNetworkDeviceStatsResponse\n\nPacketCaptureRequest\n\nPhaseEvent\n\nPlatformInfo\n\nProcess\n\nProcessInfo\n\nProcessesResponse\n\nReadRequest\n\nReboot\n\nRebootRequest\n\nRebootResponse\n\nReset\n\nResetPartitionSpec\n\nResetRequest\n\nResetResponse\n\nRestart\n\nRestartEvent\n\nRestartRequest\n\nRestartResponse\n\nRollback\n\nRollbackRequest\n\nRollbackResponse\n\nRouteConfig\n\nSequenceEvent\n\nServiceEvent\n\nServiceEvents\n\nServiceHealth\n\nServiceInfo\n\nServiceList\n\nServiceListResponse\n\nServiceRestart\n\nServiceRestartRequest\n\nServiceRestartResponse\n\nServiceStart\n\nServiceStartRequest\n\nServiceStartResponse\n\nServiceStateEvent\n\nServiceStop\n\nServiceStopRequest\n\nServiceStopResponse\n\nShutdown\n\nShutdownRequest\n\nShutdownResponse\n\nSoftIRQStat\n\nStat\n\nStats\n\nStatsRequest\n\nStatsResponse\n\nSystemStat\n\nSystemStatResponse\n\nTaskEvent\n\nUpgrade\n\nUpgradeRequest\n\nUpgradeResponse\n\nVersion\n\nVersionInfo\n\nVersionResponse\n\nApplyConfigurationRequest.Mode\n\nConnectRecord.State\n\nConnectRecord.TimerActive\n\nEtcdMemberAlarm.AlarmType\n\nListRequest.Type\n\nMachineConfig.MachineType\n\nMachineStatusEvent.MachineStage\n\nNetstatRequest.Filter\n\nPhaseEvent.Action\n\nRebootRequest.Mode\n\nResetRequest.WipeMode\n\nSequenceEvent.Action\n\nServiceStateEvent.Action\n\nTaskEvent.Action\n\nUpgradeRequest.RebootMode\n\nMachineService\n\nsecurity/security.proto\n\nCertificateRequest\n\nCertificateResponse\n\nSecurityService\n\nstorage/storage.proto\n\nDisk\n\nDisks\n\nDisksResponse\n\nDisk.DiskType\n\nStorageService\n\ntime/time.proto\n\nTime\n\nTimeRequest\n\nTimeResponse\n\nTimeService\n\nScalar Value Types\n\nTop\n\ncommon/common.proto\n\nData\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\nbytes\tbytes\t\t\n\nDataResponse\nField\tType\tLabel\tDescription\nmessages\tData\trepeated\t\n\nEmpty\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\n\nEmptyResponse\nField\tType\tLabel\tDescription\nmessages\tEmpty\trepeated\t\n\nError\nField\tType\tLabel\tDescription\ncode\tCode\t\t\nmessage\tstring\t\t\ndetails\tgoogle.protobuf.Any\trepeated\t\n\nMetadata\n\nCommon metadata message nested in all reply message types\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\thostname of the server response comes from (injected by proxy)\nerror\tstring\t\terror is set if request failed to the upstream (rest of response is undefined)\nstatus\tgoogle.rpc.Status\t\terror as gRPC Status\n\nNetIP\nField\tType\tLabel\tDescription\nip\tbytes\t\t\n\nNetIPPort\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nport\tint32\t\t\n\nNetIPPrefix\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nprefix_length\tint32\t\t\n\nPEMEncodedCertificateAndKey\nField\tType\tLabel\tDescription\ncrt\tbytes\t\t\nkey\tbytes\t\t\n\nPEMEncodedKey\nField\tType\tLabel\tDescription\nkey\tbytes\t\t\n\nURL\nField\tType\tLabel\tDescription\nfull_path\tstring\t\t\n\nCode\nName\tNumber\tDescription\nFATAL\t0\t\nLOCKED\t1\t\nCANCELED\t2\t\n\nContainerDriver\nName\tNumber\tDescription\nCONTAINERD\t0\t\nCRI\t1\t\n\nContainerdNamespace\nName\tNumber\tDescription\nNS_UNKNOWN\t0\t\nNS_SYSTEM\t1\t\nNS_CRI\t2\t\n\nFile-level Extensions\nExtension\tType\tBase\tNumber\tDescription\nremove_deprecated_enum\tstring\t.google.protobuf.EnumOptions\t93117\tIndicates the Talos version when this deprecated enum will be removed from API.\nremove_deprecated_enum_value\tstring\t.google.protobuf.EnumValueOptions\t93117\tIndicates the Talos version when this deprecated enum value will be removed from API.\nremove_deprecated_field\tstring\t.google.protobuf.FieldOptions\t93117\tIndicates the Talos version when this deprecated filed will be removed from API.\nremove_deprecated_message\tstring\t.google.protobuf.MessageOptions\t93117\tIndicates the Talos version when this deprecated message will be removed from API.\nremove_deprecated_method\tstring\t.google.protobuf.MethodOptions\t93117\tIndicates the Talos version when this deprecated method will be removed from API.\nremove_deprecated_service\tstring\t.google.protobuf.ServiceOptions\t93117\tIndicates the Talos version when this deprecated service will be removed from API.\n\nTop\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\n\nAffiliateSpec describes Affiliate state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nnodename\tstring\t\t\noperating_system\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\nkube_span\tKubeSpanAffiliateSpec\t\t\ncontrol_plane\tControlPlane\t\t\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration.\n\nField\tType\tLabel\tDescription\ndiscovery_enabled\tbool\t\t\nregistry_kubernetes_enabled\tbool\t\t\nregistry_service_enabled\tbool\t\t\nservice_endpoint\tstring\t\t\nservice_endpoint_insecure\tbool\t\t\nservice_encryption_key\tbytes\t\t\nservice_cluster_id\tstring\t\t\n\nControlPlane\n\nControlPlane describes ControlPlane data if any.\n\nField\tType\tLabel\tDescription\napi_server_port\tint64\t\t\n\nIdentitySpec\n\nIdentitySpec describes status of rendered secrets.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\n\nInfoSpec\n\nInfoSpec describes cluster information.\n\nField\tType\tLabel\tDescription\ncluster_id\tstring\t\t\ncluster_name\tstring\t\t\n\nKubeSpanAffiliateSpec\n\nKubeSpanAffiliateSpec describes additional information specific for the KubeSpan.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\naddress\tcommon.NetIP\t\t\nadditional_addresses\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\n\nMemberSpec\n\nMemberSpec describes Member state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\noperating_system\tstring\t\t\ncontrol_plane\tControlPlane\t\t\n\nTop\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nSeccompProfileSpec represents the SeccompProfile.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nvalue\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\n\nKubespanPeerState is KubeSpan peer current state.\n\nName\tNumber\tDescription\nPEER_STATE_UNKNOWN\t0\t\nPEER_STATE_UP\t1\t\nPEER_STATE_DOWN\t2\t\n\nMachineType\n\nMachineType represents a machine type.\n\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\tTypeUnknown represents undefined node type, when there is no machine configuration yet.\nTYPE_INIT\t1\tTypeInit type designates the first control plane node to come up. You can think of it like a bootstrap node. This node will perform the initial steps to bootstrap the cluster – generation of TLS assets, starting of the control plane, etc.\nTYPE_CONTROL_PLANE\t2\tTypeControlPlane designates the node as a control plane member. This means it will host etcd along with the Kubernetes controlplane components such as API Server, Controller Manager, Scheduler.\nTYPE_WORKER\t3\tTypeWorker designates the node as a worker node. This means it will be an available compute node for scheduling workloads.\n\nNethelpersADSelect\n\nNethelpersADSelect is ADSelect.\n\nName\tNumber\tDescription\nAD_SELECT_STABLE\t0\t\nAD_SELECT_BANDWIDTH\t1\t\nAD_SELECT_COUNT\t2\t\n\nNethelpersARPAllTargets\n\nNethelpersARPAllTargets is an ARP targets mode.\n\nName\tNumber\tDescription\nARP_ALL_TARGETS_ANY\t0\t\nARP_ALL_TARGETS_ALL\t1\t\n\nNethelpersARPValidate\n\nNethelpersARPValidate is an ARP Validation mode.\n\nName\tNumber\tDescription\nARP_VALIDATE_NONE\t0\t\nARP_VALIDATE_ACTIVE\t1\t\nARP_VALIDATE_BACKUP\t2\t\nARP_VALIDATE_ALL\t3\t\n\nNethelpersAddressFlag\n\nNethelpersAddressFlag wraps IFF_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ADDRESSFLAG_UNSPECIFIED\t0\t\nADDRESS_TEMPORARY\t1\t\nADDRESS_NO_DAD\t2\t\nADDRESS_OPTIMISTIC\t4\t\nADDRESS_DAD_FAILED\t8\t\nADDRESS_HOME\t16\t\nADDRESS_DEPRECATED\t32\t\nADDRESS_TENTATIVE\t64\t\nADDRESS_PERMANENT\t128\t\nADDRESS_MANAGEMENT_TEMP\t256\t\nADDRESS_NO_PREFIX_ROUTE\t512\t\nADDRESS_MC_AUTO_JOIN\t1024\t\nADDRESS_STABLE_PRIVACY\t2048\t\n\nNethelpersBondMode\n\nNethelpersBondMode is a bond mode.\n\nName\tNumber\tDescription\nBOND_MODE_ROUNDROBIN\t0\t\nBOND_MODE_ACTIVE_BACKUP\t1\t\nBOND_MODE_XOR\t2\t\nBOND_MODE_BROADCAST\t3\t\nBOND_MODE8023_AD\t4\t\nBOND_MODE_TLB\t5\t\nBOND_MODE_ALB\t6\t\n\nNethelpersBondXmitHashPolicy\n\nNethelpersBondXmitHashPolicy is a bond hash policy.\n\nName\tNumber\tDescription\nBOND_XMIT_POLICY_LAYER2\t0\t\nBOND_XMIT_POLICY_LAYER34\t1\t\nBOND_XMIT_POLICY_LAYER23\t2\t\nBOND_XMIT_POLICY_ENCAP23\t3\t\nBOND_XMIT_POLICY_ENCAP34\t4\t\n\nNethelpersConntrackState\n\nNethelpersConntrackState is a conntrack state.\n\nName\tNumber\tDescription\nNETHELPERS_CONNTRACKSTATE_UNSPECIFIED\t0\t\nCONNTRACK_STATE_NEW\t8\t\nCONNTRACK_STATE_RELATED\t4\t\nCONNTRACK_STATE_ESTABLISHED\t2\t\nCONNTRACK_STATE_INVALID\t1\t\n\nNethelpersDuplex\n\nNethelpersDuplex wraps ethtool.Duplex for YAML marshaling.\n\nName\tNumber\tDescription\nHALF\t0\t\nFULL\t1\t\nUNKNOWN\t255\t\n\nNethelpersFailOverMAC\n\nNethelpersFailOverMAC is a MAC failover mode.\n\nName\tNumber\tDescription\nFAIL_OVER_MAC_NONE\t0\t\nFAIL_OVER_MAC_ACTIVE\t1\t\nFAIL_OVER_MAC_FOLLOW\t2\t\n\nNethelpersFamily\n\nNethelpersFamily is a network family.\n\nName\tNumber\tDescription\nNETHELPERS_FAMILY_UNSPECIFIED\t0\t\nFAMILY_INET4\t2\t\nFAMILY_INET6\t10\t\n\nNethelpersLACPRate\n\nNethelpersLACPRate is a LACP rate.\n\nName\tNumber\tDescription\nLACP_RATE_SLOW\t0\t\nLACP_RATE_FAST\t1\t\n\nNethelpersLinkType\n\nNethelpersLinkType is a link type.\n\nName\tNumber\tDescription\nLINK_NETROM\t0\t\nLINK_ETHER\t1\t\nLINK_EETHER\t2\t\nLINK_AX25\t3\t\nLINK_PRONET\t4\t\nLINK_CHAOS\t5\t\nLINK_IEE802\t6\t\nLINK_ARCNET\t7\t\nLINK_ATALK\t8\t\nLINK_DLCI\t15\t\nLINK_ATM\t19\t\nLINK_METRICOM\t23\t\nLINK_IEEE1394\t24\t\nLINK_EUI64\t27\t\nLINK_INFINIBAND\t32\t\nLINK_SLIP\t256\t\nLINK_CSLIP\t257\t\nLINK_SLIP6\t258\t\nLINK_CSLIP6\t259\t\nLINK_RSRVD\t260\t\nLINK_ADAPT\t264\t\nLINK_ROSE\t270\t\nLINK_X25\t271\t\nLINK_HWX25\t272\t\nLINK_CAN\t280\t\nLINK_PPP\t512\t\nLINK_CISCO\t513\t\nLINK_HDLC\t513\t\nLINK_LAPB\t516\t\nLINK_DDCMP\t517\t\nLINK_RAWHDLC\t518\t\nLINK_TUNNEL\t768\t\nLINK_TUNNEL6\t769\t\nLINK_FRAD\t770\t\nLINK_SKIP\t771\t\nLINK_LOOPBCK\t772\t\nLINK_LOCALTLK\t773\t\nLINK_FDDI\t774\t\nLINK_BIF\t775\t\nLINK_SIT\t776\t\nLINK_IPDDP\t777\t\nLINK_IPGRE\t778\t\nLINK_PIMREG\t779\t\nLINK_HIPPI\t780\t\nLINK_ASH\t781\t\nLINK_ECONET\t782\t\nLINK_IRDA\t783\t\nLINK_FCPP\t784\t\nLINK_FCAL\t785\t\nLINK_FCPL\t786\t\nLINK_FCFABRIC\t787\t\nLINK_FCFABRIC1\t788\t\nLINK_FCFABRIC2\t789\t\nLINK_FCFABRIC3\t790\t\nLINK_FCFABRIC4\t791\t\nLINK_FCFABRIC5\t792\t\nLINK_FCFABRIC6\t793\t\nLINK_FCFABRIC7\t794\t\nLINK_FCFABRIC8\t795\t\nLINK_FCFABRIC9\t796\t\nLINK_FCFABRIC10\t797\t\nLINK_FCFABRIC11\t798\t\nLINK_FCFABRIC12\t799\t\nLINK_IEE802TR\t800\t\nLINK_IEE80211\t801\t\nLINK_IEE80211PRISM\t802\t\nLINK_IEE80211_RADIOTAP\t803\t\nLINK_IEE8021154\t804\t\nLINK_IEE8021154MONITOR\t805\t\nLINK_PHONET\t820\t\nLINK_PHONETPIPE\t821\t\nLINK_CAIF\t822\t\nLINK_IP6GRE\t823\t\nLINK_NETLINK\t824\t\nLINK6_LOWPAN\t825\t\nLINK_VOID\t65535\t\nLINK_NONE\t65534\t\n\nNethelpersMatchOperator\n\nNethelpersMatchOperator is a netfilter match operator.\n\nName\tNumber\tDescription\nOPERATOR_EQUAL\t0\t\nOPERATOR_NOT_EQUAL\t1\t\n\nNethelpersNfTablesChainHook\n\nNethelpersNfTablesChainHook wraps nftables.ChainHook for YAML marshaling.\n\nName\tNumber\tDescription\nCHAIN_HOOK_PREROUTING\t0\t\nCHAIN_HOOK_INPUT\t1\t\nCHAIN_HOOK_FORWARD\t2\t\nCHAIN_HOOK_OUTPUT\t3\t\nCHAIN_HOOK_POSTROUTING\t4\t\n\nNethelpersNfTablesChainPriority\n\nNethelpersNfTablesChainPriority wraps nftables.ChainPriority for YAML marshaling.\n\nName\tNumber\tDescription\nNETHELPERS_NFTABLESCHAINPRIORITY_UNSPECIFIED\t0\t\nCHAIN_PRIORITY_FIRST\t-2147483648\t\nCHAIN_PRIORITY_CONNTRACK_DEFRAG\t-400\t\nCHAIN_PRIORITY_RAW\t-300\t\nCHAIN_PRIORITY_SE_LINUX_FIRST\t-225\t\nCHAIN_PRIORITY_CONNTRACK\t-200\t\nCHAIN_PRIORITY_MANGLE\t-150\t\nCHAIN_PRIORITY_NAT_DEST\t-100\t\nCHAIN_PRIORITY_FILTER\t0\t\nCHAIN_PRIORITY_SECURITY\t50\t\nCHAIN_PRIORITY_NAT_SOURCE\t100\t\nCHAIN_PRIORITY_SE_LINUX_LAST\t225\t\nCHAIN_PRIORITY_CONNTRACK_HELPER\t300\t\nCHAIN_PRIORITY_LAST\t2147483647\t\n\nNethelpersNfTablesVerdict\n\nNethelpersNfTablesVerdict wraps nftables.Verdict for YAML marshaling.\n\nName\tNumber\tDescription\nVERDICT_DROP\t0\t\nVERDICT_ACCEPT\t1\t\n\nNethelpersOperationalState\n\nNethelpersOperationalState wraps rtnetlink.OperationalState for YAML marshaling.\n\nName\tNumber\tDescription\nOPER_STATE_UNKNOWN\t0\t\nOPER_STATE_NOT_PRESENT\t1\t\nOPER_STATE_DOWN\t2\t\nOPER_STATE_LOWER_LAYER_DOWN\t3\t\nOPER_STATE_TESTING\t4\t\nOPER_STATE_DORMANT\t5\t\nOPER_STATE_UP\t6\t\n\nNethelpersPort\n\nNethelpersPort wraps ethtool.Port for YAML marshaling.\n\nName\tNumber\tDescription\nTWISTED_PAIR\t0\t\nAUI\t1\t\nMII\t2\t\nFIBRE\t3\t\nBNC\t4\t\nDIRECT_ATTACH\t5\t\nNONE\t239\t\nOTHER\t255\t\n\nNethelpersPrimaryReselect\n\nNethelpersPrimaryReselect is an ARP targets mode.\n\nName\tNumber\tDescription\nPRIMARY_RESELECT_ALWAYS\t0\t\nPRIMARY_RESELECT_BETTER\t1\t\nPRIMARY_RESELECT_FAILURE\t2\t\n\nNethelpersProtocol\n\nNethelpersProtocol is a inet protocol.\n\nName\tNumber\tDescription\nNETHELPERS_PROTOCOL_UNSPECIFIED\t0\t\nPROTOCOL_ICMP\t1\t\nPROTOCOL_TCP\t6\t\nPROTOCOL_UDP\t17\t\nPROTOCOL_ICM_PV6\t58\t\n\nNethelpersRouteFlag\n\nNethelpersRouteFlag wraps RTM_F_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ROUTEFLAG_UNSPECIFIED\t0\t\nROUTE_NOTIFY\t256\t\nROUTE_CLONED\t512\t\nROUTE_EQUALIZE\t1024\t\nROUTE_PREFIX\t2048\t\nROUTE_LOOKUP_TABLE\t4096\t\nROUTE_FIB_MATCH\t8192\t\nROUTE_OFFLOAD\t16384\t\nROUTE_TRAP\t32768\t\n\nNethelpersRouteProtocol\n\nNethelpersRouteProtocol is a routing protocol.\n\nName\tNumber\tDescription\nPROTOCOL_UNSPEC\t0\t\nPROTOCOL_REDIRECT\t1\t\nPROTOCOL_KERNEL\t2\t\nPROTOCOL_BOOT\t3\t\nPROTOCOL_STATIC\t4\t\nPROTOCOL_RA\t9\t\nPROTOCOL_MRT\t10\t\nPROTOCOL_ZEBRA\t11\t\nPROTOCOL_BIRD\t12\t\nPROTOCOL_DNROUTED\t13\t\nPROTOCOL_XORP\t14\t\nPROTOCOL_NTK\t15\t\nPROTOCOL_DHCP\t16\t\nPROTOCOL_MRTD\t17\t\nPROTOCOL_KEEPALIVED\t18\t\nPROTOCOL_BABEL\t42\t\nPROTOCOL_OPENR\t99\t\nPROTOCOL_BGP\t186\t\nPROTOCOL_ISIS\t187\t\nPROTOCOL_OSPF\t188\t\nPROTOCOL_RIP\t189\t\nPROTOCOL_EIGRP\t192\t\n\nNethelpersRouteType\n\nNethelpersRouteType is a route type.\n\nName\tNumber\tDescription\nTYPE_UNSPEC\t0\t\nTYPE_UNICAST\t1\t\nTYPE_LOCAL\t2\t\nTYPE_BROADCAST\t3\t\nTYPE_ANYCAST\t4\t\nTYPE_MULTICAST\t5\t\nTYPE_BLACKHOLE\t6\t\nTYPE_UNREACHABLE\t7\t\nTYPE_PROHIBIT\t8\t\nTYPE_THROW\t9\t\nTYPE_NAT\t10\t\nTYPE_X_RESOLVE\t11\t\n\nNethelpersRoutingTable\n\nNethelpersRoutingTable is a routing table ID.\n\nName\tNumber\tDescription\nTABLE_UNSPEC\t0\t\nTABLE_DEFAULT\t253\t\nTABLE_MAIN\t254\t\nTABLE_LOCAL\t255\t\n\nNethelpersScope\n\nNethelpersScope is an address scope.\n\nName\tNumber\tDescription\nSCOPE_GLOBAL\t0\t\nSCOPE_SITE\t200\t\nSCOPE_LINK\t253\t\nSCOPE_HOST\t254\t\nSCOPE_NOWHERE\t255\t\n\nNethelpersVLANProtocol\n\nNethelpersVLANProtocol is a VLAN protocol.\n\nName\tNumber\tDescription\nNETHELPERS_VLANPROTOCOL_UNSPECIFIED\t0\t\nVLAN_PROTOCOL8021_Q\t33024\t\nVLAN_PROTOCOL8021_AD\t34984\t\n\nNetworkConfigLayer\n\nNetworkConfigLayer describes network configuration layers, with lowest priority first.\n\nName\tNumber\tDescription\nCONFIG_DEFAULT\t0\t\nCONFIG_CMDLINE\t1\t\nCONFIG_PLATFORM\t2\t\nCONFIG_OPERATOR\t3\t\nCONFIG_MACHINE_CONFIGURATION\t4\t\n\nNetworkOperator\n\nNetworkOperator enumerates Talos network operators.\n\nName\tNumber\tDescription\nOPERATOR_DHCP4\t0\t\nOPERATOR_DHCP6\t1\t\nOPERATOR_VIP\t2\t\n\nRuntimeMachineStage\n\nRuntimeMachineStage describes the stage of the machine boot/run process.\n\nName\tNumber\tDescription\nMACHINE_STAGE_UNKNOWN\t0\t\nMACHINE_STAGE_BOOTING\t1\t\nMACHINE_STAGE_INSTALLING\t2\t\nMACHINE_STAGE_MAINTENANCE\t3\t\nMACHINE_STAGE_RUNNING\t4\t\nMACHINE_STAGE_REBOOTING\t5\t\nMACHINE_STAGE_SHUTTING_DOWN\t6\t\nMACHINE_STAGE_RESETTING\t7\t\nMACHINE_STAGE_UPGRADING\t8\t\n\nTop\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\n\nConfigSpec describes (some) configuration settings of etcd.\n\nField\tType\tLabel\tDescription\nadvertise_valid_subnets\tstring\trepeated\t\nadvertise_exclude_subnets\tstring\trepeated\t\nimage\tstring\t\t\nextra_args\tConfigSpec.ExtraArgsEntry\trepeated\t\nlisten_valid_subnets\tstring\trepeated\t\nlisten_exclude_subnets\tstring\trepeated\t\n\nConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nMemberSpec\n\nMemberSpec holds information about an etcd member.\n\nField\tType\tLabel\tDescription\nmember_id\tstring\t\t\n\nPKIStatusSpec\n\nPKIStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSpecSpec\n\nSpecSpec describes (some) Specuration settings of etcd.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nadvertised_addresses\tcommon.NetIP\trepeated\t\nimage\tstring\t\t\nextra_args\tSpecSpec.ExtraArgsEntry\trepeated\t\nlisten_peer_addresses\tcommon.NetIP\trepeated\t\nlisten_client_addresses\tcommon.NetIP\trepeated\t\n\nSpecSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nTop\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\n\nCompatibility describes extension compatibility.\n\nField\tType\tLabel\tDescription\ntalos\tConstraint\t\t\n\nConstraint\n\nConstraint describes compatibility constraint.\n\nField\tType\tLabel\tDescription\nversion\tstring\t\t\n\nLayer\n\nLayer defines overlay mount layer.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nmetadata\tMetadata\t\t\n\nMetadata\n\nMetadata describes base extension metadata.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nversion\tstring\t\t\nauthor\tstring\t\t\ndescription\tstring\t\t\ncompatibility\tCompatibility\t\t\n\nTop\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\n\nEtcFileSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ncontents\tbytes\t\t\nmode\tuint32\t\t\n\nEtcFileStatusSpec\n\nEtcFileStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nspec_version\tstring\t\t\n\nTop\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\n\nMemoryModuleSpec represents a single Memory.\n\nField\tType\tLabel\tDescription\nsize\tuint32\t\t\ndevice_locator\tstring\t\t\nbank_locator\tstring\t\t\nspeed\tuint32\t\t\nmanufacturer\tstring\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\nproduct_name\tstring\t\t\n\nProcessorSpec\n\nProcessorSpec represents a single processor.\n\nField\tType\tLabel\tDescription\nsocket\tstring\t\t\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nmax_speed\tuint32\t\t\nboot_speed\tuint32\t\t\nstatus\tuint32\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\npart_number\tstring\t\t\ncore_count\tuint32\t\t\ncore_enabled\tuint32\t\t\nthread_count\tuint32\t\t\n\nSystemInformationSpec\n\nSystemInformationSpec represents the system information obtained from smbios.\n\nField\tType\tLabel\tDescription\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nversion\tstring\t\t\nserial_number\tstring\t\t\nuuid\tstring\t\t\nwake_up_type\tstring\t\t\nsku_number\tstring\t\t\n\nTop\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\n\nAPIServerConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncloud_provider\tstring\t\t\ncontrol_plane_endpoint\tstring\t\t\netcd_servers\tstring\trepeated\t\nlocal_port\tint64\t\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tAPIServerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tAPIServerConfigSpec.EnvironmentVariablesEntry\trepeated\t\npod_security_policy_enabled\tbool\t\t\nadvertised_address\tstring\t\t\nresources\tResources\t\t\n\nAPIServerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAPIServerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAdmissionControlConfigSpec\n\nAdmissionControlConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tAdmissionPluginSpec\trepeated\t\n\nAdmissionPluginSpec\n\nAdmissionPluginSpec is a single admission plugin configuration Admission Control plugins.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nconfiguration\tgoogle.protobuf.Struct\t\t\n\nAuditPolicyConfigSpec\n\nAuditPolicyConfigSpec is audit policy configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tgoogle.protobuf.Struct\t\t\n\nBootstrapManifestsConfigSpec\n\nBootstrapManifestsConfigSpec is configuration for bootstrap manifests.\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\ncluster_domain\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nproxy_enabled\tbool\t\t\nproxy_image\tstring\t\t\nproxy_args\tstring\trepeated\t\ncore_dns_enabled\tbool\t\t\ncore_dns_image\tstring\t\t\ndns_service_ip\tstring\t\t\ndns_service_i_pv6\tstring\t\t\nflannel_enabled\tbool\t\t\nflannel_image\tstring\t\t\nflannel_cni_image\tstring\t\t\npod_security_policy_enabled\tbool\t\t\ntalos_api_service_enabled\tbool\t\t\nflannel_extra_args\tstring\trepeated\t\n\nConfigStatusSpec\n\nConfigStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nControllerManagerConfigSpec\n\nControllerManagerConfigSpec is configuration for kube-controller-manager.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\ncloud_provider\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tControllerManagerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tControllerManagerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\n\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nControllerManagerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nEndpointSpec\n\nEndpointSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nExtraManifest\n\nExtraManifest defines a single extra manifest to download.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurl\tstring\t\t\npriority\tstring\t\t\nextra_headers\tExtraManifest.ExtraHeadersEntry\trepeated\t\ninline_manifest\tstring\t\t\n\nExtraManifest.ExtraHeadersEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nExtraManifestsConfigSpec\n\nExtraManifestsConfigSpec is configuration for extra bootstrap manifests.\n\nField\tType\tLabel\tDescription\nextra_manifests\tExtraManifest\trepeated\t\n\nExtraVolume\n\nExtraVolume is a configuration of extra volume.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhost_path\tstring\t\t\nmount_path\tstring\t\t\nread_only\tbool\t\t\n\nKubePrismConfigSpec\n\nKubePrismConfigSpec describes KubePrismConfig data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tint64\t\t\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismEndpoint\n\nKubePrismEndpoint holds data for control plane endpoint.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tuint32\t\t\n\nKubePrismEndpointsSpec\n\nKubePrismEndpointsSpec describes KubePrismEndpoints configuration.\n\nField\tType\tLabel\tDescription\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismStatusesSpec\n\nKubePrismStatusesSpec describes KubePrismStatuses data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nhealthy\tbool\t\t\n\nKubeletConfigSpec\n\nKubeletConfigSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncluster_dns\tstring\trepeated\t\ncluster_domain\tstring\t\t\nextra_args\tKubeletConfigSpec.ExtraArgsEntry\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nextra_config\tgoogle.protobuf.Struct\t\t\ncloud_provider_external\tbool\t\t\ndefault_runtime_seccomp_enabled\tbool\t\t\nskip_node_registration\tbool\t\t\nstatic_pod_list_url\tstring\t\t\ndisable_manifests_directory\tbool\t\t\nenable_fs_quota_monitoring\tbool\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nKubeletConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nKubeletSpecSpec\n\nKubeletSpecSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nargs\tstring\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nexpected_nodename\tstring\t\t\nconfig\tgoogle.protobuf.Struct\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nManifestSpec\n\nManifestSpec holds the Kubernetes resources spec.\n\nField\tType\tLabel\tDescription\nitems\tSingleManifest\trepeated\t\n\nManifestStatusSpec\n\nManifestStatusSpec describes manifest application status.\n\nField\tType\tLabel\tDescription\nmanifests_applied\tstring\trepeated\t\n\nNodeIPConfigSpec\n\nNodeIPConfigSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\nvalid_subnets\tstring\trepeated\t\nexclude_subnets\tstring\trepeated\t\n\nNodeIPSpec\n\nNodeIPSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nNodeLabelSpecSpec\n\nNodeLabelSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec\n\nNodeStatusSpec describes Kubernetes NodeStatus.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nnode_ready\tbool\t\t\nunschedulable\tbool\t\t\nlabels\tNodeStatusSpec.LabelsEntry\trepeated\t\nannotations\tNodeStatusSpec.AnnotationsEntry\trepeated\t\n\nNodeStatusSpec.AnnotationsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec.LabelsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeTaintSpecSpec\n\nNodeTaintSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\neffect\tstring\t\t\nvalue\tstring\t\t\n\nNodenameSpec\n\nNodenameSpec describes Kubernetes nodename.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nhostname_version\tstring\t\t\nskip_node_registration\tbool\t\t\n\nResources\n\nResources is a configuration of cpu and memory resources.\n\nField\tType\tLabel\tDescription\nrequests\tResources.RequestsEntry\trepeated\t\nlimits\tResources.LimitsEntry\trepeated\t\n\nResources.LimitsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nResources.RequestsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec\n\nSchedulerConfigSpec is configuration for kube-scheduler.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\nextra_args\tSchedulerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tSchedulerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\nconfig\tgoogle.protobuf.Struct\t\t\n\nSchedulerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSecretsStatusSpec\n\nSecretsStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSingleManifest\n\nSingleManifest is a single manifest.\n\nField\tType\tLabel\tDescription\nobject\tgoogle.protobuf.Struct\t\t\n\nStaticPodServerStatusSpec\n\nStaticPodServerStatusSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\nurl\tstring\t\t\n\nStaticPodSpec\n\nStaticPodSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\npod\tgoogle.protobuf.Struct\t\t\n\nStaticPodStatusSpec\n\nStaticPodStatusSpec describes kubelet static pod status.\n\nField\tType\tLabel\tDescription\npod_status\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nallowed_api_roles\tstring\trepeated\t\nallowed_kubernetes_namespaces\tstring\trepeated\t\n\nTop\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\ncluster_id\tstring\t\t\nshared_secret\tstring\t\t\nforce_routing\tbool\t\t\nadvertise_kubernetes_networks\tbool\t\t\nmtu\tuint32\t\t\nendpoint_filters\tstring\trepeated\t\nharvest_extra_endpoints\tbool\t\t\n\nEndpointSpec\n\nEndpointSpec describes Endpoint state.\n\nField\tType\tLabel\tDescription\naffiliate_id\tstring\t\t\nendpoint\tcommon.NetIPPort\t\t\n\nIdentitySpec\n\nIdentitySpec describes KubeSpan keys and address.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nsubnet\tcommon.NetIPPrefix\t\t\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\n\nPeerSpecSpec\n\nPeerSpecSpec describes PeerSpec state.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIP\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\nlabel\tstring\t\t\n\nPeerStatusSpec\n\nPeerStatusSpec describes PeerStatus state.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.NetIPPort\t\t\nlabel\tstring\t\t\nstate\ttalos.resource.definitions.enums.KubespanPeerState\t\t\nreceive_bytes\tint64\t\t\ntransmit_bytes\tint64\t\t\nlast_handshake_time\tgoogle.protobuf.Timestamp\t\t\nlast_used_endpoint\tcommon.NetIPPort\t\t\nlast_endpoint_change\tgoogle.protobuf.Timestamp\t\t\n\nTop\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\n\nAddressSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\nannounce_with_arp\tbool\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nAddressStatusSpec\n\nAddressStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlocal\tcommon.NetIP\t\t\nbroadcast\tcommon.NetIP\t\t\nanycast\tcommon.NetIP\t\t\nmulticast\tcommon.NetIP\t\t\nlink_index\tuint32\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\n\nBondMasterSpec\n\nBondMasterSpec describes bond settings if Kind == “bond”.\n\nField\tType\tLabel\tDescription\nmode\ttalos.resource.definitions.enums.NethelpersBondMode\t\t\nhash_policy\ttalos.resource.definitions.enums.NethelpersBondXmitHashPolicy\t\t\nlacp_rate\ttalos.resource.definitions.enums.NethelpersLACPRate\t\t\narp_validate\ttalos.resource.definitions.enums.NethelpersARPValidate\t\t\narp_all_targets\ttalos.resource.definitions.enums.NethelpersARPAllTargets\t\t\nprimary_index\tuint32\t\t\nprimary_reselect\ttalos.resource.definitions.enums.NethelpersPrimaryReselect\t\t\nfail_over_mac\ttalos.resource.definitions.enums.NethelpersFailOverMAC\t\t\nad_select\ttalos.resource.definitions.enums.NethelpersADSelect\t\t\nmii_mon\tuint32\t\t\nup_delay\tuint32\t\t\ndown_delay\tuint32\t\t\narp_interval\tuint32\t\t\nresend_igmp\tuint32\t\t\nmin_links\tuint32\t\t\nlp_interval\tuint32\t\t\npackets_per_slave\tuint32\t\t\nnum_peer_notif\tfixed32\t\t\ntlb_dynamic_lb\tfixed32\t\t\nall_slaves_active\tfixed32\t\t\nuse_carrier\tbool\t\t\nad_actor_sys_prio\tfixed32\t\t\nad_user_port_key\tfixed32\t\t\npeer_notify_delay\tuint32\t\t\n\nBondSlave\n\nBondSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\nslave_index\tint64\t\t\n\nBridgeMasterSpec\n\nBridgeMasterSpec describes bridge settings if Kind == “bridge”.\n\nField\tType\tLabel\tDescription\nstp\tSTPSpec\t\t\n\nBridgeSlave\n\nBridgeSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\n\nDHCP4OperatorSpec\n\nDHCP4OperatorSpec describes DHCP4 operator options.\n\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nDHCP6OperatorSpec\n\nDHCP6OperatorSpec describes DHCP6 operator options.\n\nField\tType\tLabel\tDescription\nduid\tstring\t\t\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nHardwareAddrSpec\n\nHardwareAddrSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhardware_addr\tbytes\t\t\n\nHostnameSpecSpec\n\nHostnameSpecSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nHostnameStatusSpec\n\nHostnameStatusSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\n\nLinkRefreshSpec\n\nLinkRefreshSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ngeneration\tint64\t\t\n\nLinkSpecSpec\n\nLinkSpecSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nlogical\tbool\t\t\nup\tbool\t\t\nmtu\tuint32\t\t\nkind\tstring\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nparent_name\tstring\t\t\nbond_slave\tBondSlave\t\t\nbridge_slave\tBridgeSlave\t\t\nvlan\tVLANSpec\t\t\nbond_master\tBondMasterSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nLinkStatusSpec\n\nLinkStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nindex\tuint32\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nlink_index\tuint32\t\t\nflags\tuint32\t\t\nhardware_addr\tbytes\t\t\nbroadcast_addr\tbytes\t\t\nmtu\tuint32\t\t\nqueue_disc\tstring\t\t\nmaster_index\tuint32\t\t\noperational_state\ttalos.resource.definitions.enums.NethelpersOperationalState\t\t\nkind\tstring\t\t\nslave_kind\tstring\t\t\nbus_path\tstring\t\t\npciid\tstring\t\t\ndriver\tstring\t\t\ndriver_version\tstring\t\t\nfirmware_version\tstring\t\t\nproduct_id\tstring\t\t\nvendor_id\tstring\t\t\nproduct\tstring\t\t\nvendor\tstring\t\t\nlink_state\tbool\t\t\nspeed_megabits\tint64\t\t\nport\ttalos.resource.definitions.enums.NethelpersPort\t\t\nduplex\ttalos.resource.definitions.enums.NethelpersDuplex\t\t\nvlan\tVLANSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nbond_master\tBondMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\npermanent_addr\tbytes\t\t\n\nNfTablesAddressMatch\n\nNfTablesAddressMatch describes the match on the IP address.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\ninvert\tbool\t\t\n\nNfTablesChainSpec\n\nNfTablesChainSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ntype\tstring\t\t\nhook\ttalos.resource.definitions.enums.NethelpersNfTablesChainHook\t\t\npriority\ttalos.resource.definitions.enums.NethelpersNfTablesChainPriority\t\t\nrules\tNfTablesRule\trepeated\t\npolicy\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\n\nNfTablesClampMSS\n\nNfTablesClampMSS describes the TCP MSS clamping operation.\n\nMSS is limited by the MaxMTU so that:\n\nIPv4: MSS = MaxMTU - 40\nIPv6: MSS = MaxMTU - 60.\nField\tType\tLabel\tDescription\nmtu\tfixed32\t\t\n\nNfTablesConntrackStateMatch\n\nNfTablesConntrackStateMatch describes the match on the connection tracking state.\n\nField\tType\tLabel\tDescription\nstates\ttalos.resource.definitions.enums.NethelpersConntrackState\trepeated\t\n\nNfTablesIfNameMatch\n\nNfTablesIfNameMatch describes the match on the interface name.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NethelpersMatchOperator\t\t\ninterface_names\tstring\trepeated\t\n\nNfTablesLayer4Match\n\nNfTablesLayer4Match describes the match on the transport layer protocol.\n\nField\tType\tLabel\tDescription\nprotocol\ttalos.resource.definitions.enums.NethelpersProtocol\t\t\nmatch_source_port\tNfTablesPortMatch\t\t\nmatch_destination_port\tNfTablesPortMatch\t\t\n\nNfTablesLimitMatch\n\nNfTablesLimitMatch describes the match on the packet rate.\n\nField\tType\tLabel\tDescription\npacket_rate_per_second\tuint64\t\t\n\nNfTablesMark\n\nNfTablesMark encodes packet mark match/update operation.\n\nWhen used as a match computes the following condition: (mark & mask) ^ xor == value\n\nWhen used as an update computes the following operation: mark = (mark & mask) ^ xor.\n\nField\tType\tLabel\tDescription\nmask\tuint32\t\t\nxor\tuint32\t\t\nvalue\tuint32\t\t\n\nNfTablesPortMatch\n\nNfTablesPortMatch describes the match on the transport layer port.\n\nField\tType\tLabel\tDescription\nranges\tPortRange\trepeated\t\n\nNfTablesRule\n\nNfTablesRule describes a single rule in the nftables chain.\n\nField\tType\tLabel\tDescription\nmatch_o_if_name\tNfTablesIfNameMatch\t\t\nverdict\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\nmatch_mark\tNfTablesMark\t\t\nset_mark\tNfTablesMark\t\t\nmatch_source_address\tNfTablesAddressMatch\t\t\nmatch_destination_address\tNfTablesAddressMatch\t\t\nmatch_layer4\tNfTablesLayer4Match\t\t\nmatch_i_if_name\tNfTablesIfNameMatch\t\t\nclamp_mss\tNfTablesClampMSS\t\t\nmatch_limit\tNfTablesLimitMatch\t\t\nmatch_conntrack_state\tNfTablesConntrackStateMatch\t\t\nanon_counter\tbool\t\t\n\nNodeAddressFilterSpec\n\nNodeAddressFilterSpec describes a filter for NodeAddresses.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\n\nNodeAddressSpec\n\nNodeAddressSpec describes a set of node addresses.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIPPrefix\trepeated\t\n\nOperatorSpecSpec\n\nOperatorSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NetworkOperator\t\t\nlink_name\tstring\t\t\nrequire_up\tbool\t\t\ndhcp4\tDHCP4OperatorSpec\t\t\ndhcp6\tDHCP6OperatorSpec\t\t\nvip\tVIPOperatorSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nPortRange\n\nPortRange describes a range of ports.\n\nRange is [lo, hi].\n\nField\tType\tLabel\tDescription\nlo\tfixed32\t\t\nhi\tfixed32\t\t\n\nProbeSpecSpec\n\nProbeSpecSpec describes the Probe.\n\nField\tType\tLabel\tDescription\ninterval\tgoogle.protobuf.Duration\t\t\nfailure_threshold\tint64\t\t\ntcp\tTCPProbeSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nProbeStatusSpec\n\nProbeStatusSpec describes the Probe.\n\nField\tType\tLabel\tDescription\nsuccess\tbool\t\t\nlast_error\tstring\t\t\n\nResolverSpecSpec\n\nResolverSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nResolverStatusSpec\n\nResolverStatusSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\n\nRouteSpecSpec\n\nRouteSpecSpec describes the route.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\nmtu\tuint32\t\t\n\nRouteStatusSpec\n\nRouteStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_index\tuint32\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nmtu\tuint32\t\t\n\nSTPSpec\n\nSTPSpec describes Spanning Tree Protocol (STP) settings of a bridge.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\n\nStatusSpec\n\nStatusSpec describes network state.\n\nField\tType\tLabel\tDescription\naddress_ready\tbool\t\t\nconnectivity_ready\tbool\t\t\nhostname_ready\tbool\t\t\netc_files_ready\tbool\t\t\n\nTCPProbeSpec\n\nTCPProbeSpec describes the TCP Probe.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\ntimeout\tgoogle.protobuf.Duration\t\t\n\nTimeServerSpecSpec\n\nTimeServerSpecSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nTimeServerStatusSpec\n\nTimeServerStatusSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\n\nVIPEquinixMetalSpec\n\nVIPEquinixMetalSpec describes virtual (elastic) IP settings for Equinix Metal.\n\nField\tType\tLabel\tDescription\nproject_id\tstring\t\t\ndevice_id\tstring\t\t\napi_token\tstring\t\t\n\nVIPHCloudSpec\n\nVIPHCloudSpec describes virtual (elastic) IP settings for Hetzner Cloud.\n\nField\tType\tLabel\tDescription\ndevice_id\tint64\t\t\nnetwork_id\tint64\t\t\napi_token\tstring\t\t\n\nVIPOperatorSpec\n\nVIPOperatorSpec describes virtual IP operator options.\n\nField\tType\tLabel\tDescription\nip\tcommon.NetIP\t\t\ngratuitous_arp\tbool\t\t\nequinix_metal\tVIPEquinixMetalSpec\t\t\nh_cloud\tVIPHCloudSpec\t\t\n\nVLANSpec\n\nVLANSpec describes VLAN settings if Kind == “vlan”.\n\nField\tType\tLabel\tDescription\nvid\tfixed32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersVLANProtocol\t\t\n\nWireguardPeer\n\nWireguardPeer describes a single peer.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\npreshared_key\tstring\t\t\nendpoint\tstring\t\t\npersistent_keepalive_interval\tgoogle.protobuf.Duration\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\n\nWireguardSpec\n\nWireguardSpec describes Wireguard settings if Kind == “wireguard”.\n\nField\tType\tLabel\tDescription\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\nlisten_port\tint64\t\t\nfirewall_mark\tint64\t\t\npeers\tWireguardPeer\trepeated\t\n\nTop\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\n\nCPUSpec represents the last CPU stats snapshot.\n\nField\tType\tLabel\tDescription\ncpu\tCPUStat\trepeated\t\ncpu_total\tCPUStat\t\t\nirq_total\tuint64\t\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\n\nCPUStat\n\nCPUStat represents a single cpu stat.\n\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nMemorySpec\n\nMemorySpec represents the last Memory stats snapshot.\n\nField\tType\tLabel\tDescription\nmem_total\tuint64\t\t\nmem_used\tuint64\t\t\nmem_available\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswap_cached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactive_anon\tuint64\t\t\ninactive_anon\tuint64\t\t\nactive_file\tuint64\t\t\ninactive_file\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswap_total\tuint64\t\t\nswap_free\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanon_pages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\ns_reclaimable\tuint64\t\t\ns_unreclaim\tuint64\t\t\nkernel_stack\tuint64\t\t\npage_tables\tuint64\t\t\nnf_sunstable\tuint64\t\t\nbounce\tuint64\t\t\nwriteback_tmp\tuint64\t\t\ncommit_limit\tuint64\t\t\ncommitted_as\tuint64\t\t\nvmalloc_total\tuint64\t\t\nvmalloc_used\tuint64\t\t\nvmalloc_chunk\tuint64\t\t\nhardware_corrupted\tuint64\t\t\nanon_huge_pages\tuint64\t\t\nshmem_huge_pages\tuint64\t\t\nshmem_pmd_mapped\tuint64\t\t\ncma_total\tuint64\t\t\ncma_free\tuint64\t\t\nhuge_pages_total\tuint64\t\t\nhuge_pages_free\tuint64\t\t\nhuge_pages_rsvd\tuint64\t\t\nhuge_pages_surp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirect_map4k\tuint64\t\t\ndirect_map2m\tuint64\t\t\ndirect_map1g\tuint64\t\t\n\nTop\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\n\nLinuxIDMapping specifies UID/GID mappings.\n\nField\tType\tLabel\tDescription\ncontainer_id\tuint32\t\t\nhost_id\tuint32\t\t\nsize\tuint32\t\t\n\nMount\n\nMount specifies a mount for a container.\n\nField\tType\tLabel\tDescription\ndestination\tstring\t\t\ntype\tstring\t\t\nsource\tstring\t\t\noptions\tstring\trepeated\t\nuid_mappings\tLinuxIDMapping\trepeated\t\ngid_mappings\tLinuxIDMapping\trepeated\t\n\nTop\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\n\nDevicesStatusSpec is the spec for devices status.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\n\nEventSinkConfigSpec\n\nEventSinkConfigSpec describes configuration of Talos event log streaming.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nKernelModuleSpecSpec\n\nKernelModuleSpecSpec describes Linux kernel module to load.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nparameters\tstring\trepeated\t\n\nKernelParamSpecSpec\n\nKernelParamSpecSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\nignore_errors\tbool\t\t\n\nKernelParamStatusSpec\n\nKernelParamStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\ncurrent\tstring\t\t\ndefault\tstring\t\t\nunsupported\tbool\t\t\n\nKmsgLogConfigSpec\n\nKmsgLogConfigSpec describes configuration for kmsg log streaming.\n\nField\tType\tLabel\tDescription\ndestinations\tcommon.URL\trepeated\t\n\nMachineStatusSpec\n\nMachineStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nstage\ttalos.resource.definitions.enums.RuntimeMachineStage\t\t\nstatus\tMachineStatusStatus\t\t\n\nMachineStatusStatus\n\nMachineStatusStatus describes machine current status at the stage.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tUnmetCondition\trepeated\t\n\nMaintenanceServiceConfigSpec\n\nMaintenanceServiceConfigSpec describes configuration for maintenance service API.\n\nField\tType\tLabel\tDescription\nlisten_address\tstring\t\t\nreachable_addresses\tcommon.NetIP\trepeated\t\n\nMetaKeySpec\n\nMetaKeySpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\n\nMetaLoadedSpec\n\nMetaLoadedSpec is the spec for meta loaded. The Done field is always true when resource exists.\n\nField\tType\tLabel\tDescription\ndone\tbool\t\t\n\nMountStatusSpec\n\nMountStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nsource\tstring\t\t\ntarget\tstring\t\t\nfilesystem_type\tstring\t\t\noptions\tstring\trepeated\t\nencrypted\tbool\t\t\nencryption_providers\tstring\trepeated\t\n\nPlatformMetadataSpec\n\nPlatformMetadataSpec describes platform metadata properties.\n\nField\tType\tLabel\tDescription\nplatform\tstring\t\t\nhostname\tstring\t\t\nregion\tstring\t\t\nzone\tstring\t\t\ninstance_type\tstring\t\t\ninstance_id\tstring\t\t\nprovider_id\tstring\t\t\nspot\tbool\t\t\n\nSecurityStateSpec\n\nSecurityStateSpec describes the security state resource properties.\n\nField\tType\tLabel\tDescription\nsecure_boot\tbool\t\t\nuki_signing_key_fingerprint\tstring\t\t\npcr_signing_key_fingerprint\tstring\t\t\n\nUniqueMachineTokenSpec\n\nUniqueMachineTokenSpec is the spec for the machine unique token. Token can be empty if machine wasn’t assigned any.\n\nField\tType\tLabel\tDescription\ntoken\tstring\t\t\n\nUnmetCondition\n\nUnmetCondition is a failure which prevents machine from being ready at the stage.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nTop\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\n\nAPICertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nclient\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nCertSANSpec\n\nCertSANSpec describes fields of the cert SANs.\n\nField\tType\tLabel\tDescription\ni_ps\tcommon.NetIP\trepeated\t\ndns_names\tstring\trepeated\t\nfqdn\tstring\t\t\n\nEtcdCertsSpec\n\nEtcdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\netcd\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_peer\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_admin\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_api_server\tcommon.PEMEncodedCertificateAndKey\t\t\n\nEtcdRootSpec\n\nEtcdRootSpec describes etcd CA secrets.\n\nField\tType\tLabel\tDescription\netcd_ca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubeletSpec\n\nKubeletSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.URL\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\n\nKubernetesCertsSpec\n\nKubernetesCertsSpec describes generated Kubernetes certificates.\n\nField\tType\tLabel\tDescription\nscheduler_kubeconfig\tstring\t\t\ncontroller_manager_kubeconfig\tstring\t\t\nlocalhost_admin_kubeconfig\tstring\t\t\nadmin_kubeconfig\tstring\t\t\n\nKubernetesDynamicCertsSpec\n\nKubernetesDynamicCertsSpec describes generated KubernetesCerts certificates.\n\nField\tType\tLabel\tDescription\napi_server\tcommon.PEMEncodedCertificateAndKey\t\t\napi_server_kubelet_client\tcommon.PEMEncodedCertificateAndKey\t\t\nfront_proxy\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubernetesRootSpec\n\nKubernetesRootSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nendpoint\tcommon.URL\t\t\nlocal_endpoint\tcommon.URL\t\t\ncert_sa_ns\tstring\trepeated\t\ndns_domain\tstring\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nservice_account\tcommon.PEMEncodedKey\t\t\naggregator_ca\tcommon.PEMEncodedCertificateAndKey\t\t\naescbc_encryption_secret\tstring\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\nsecretbox_encryption_secret\tstring\t\t\napi_server_ips\tcommon.NetIP\trepeated\t\n\nMaintenanceRootSpec\n\nMaintenanceRootSpec describes maintenance service CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nMaintenanceServiceCertsSpec\n\nMaintenanceServiceCertsSpec describes maintenance service certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nOSRootSpec\n\nOSRootSpec describes operating system CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\ncert_sani_ps\tcommon.NetIP\trepeated\t\ncert_sandns_names\tstring\trepeated\t\ntoken\tstring\t\t\n\nTrustdCertsSpec\n\nTrustdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nTop\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\napi_endpoint\tstring\t\t\n\nTop\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\n\nAdjtimeStatusSpec describes Linux internal adjtime state.\n\nField\tType\tLabel\tDescription\noffset\tgoogle.protobuf.Duration\t\t\nfrequency_adjustment_ratio\tdouble\t\t\nmax_error\tgoogle.protobuf.Duration\t\t\nest_error\tgoogle.protobuf.Duration\t\t\nstatus\tstring\t\t\nconstant\tint64\t\t\nsync_status\tbool\t\t\nstate\tstring\t\t\n\nStatusSpec\n\nStatusSpec describes time sync state.\n\nField\tType\tLabel\tDescription\nsynced\tbool\t\t\nepoch\tint64\t\t\nsync_disabled\tbool\t\t\n\nTop\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\nServiceSpec describe service state.\n\nField\tType\tLabel\tDescription\nrunning\tbool\t\t\nhealthy\tbool\t\t\nunknown\tbool\t\t\n\nTop\n\ninspect/inspect.proto\n\nControllerDependencyEdge\nField\tType\tLabel\tDescription\ncontroller_name\tstring\t\t\nedge_type\tDependencyEdgeType\t\t\nresource_namespace\tstring\t\t\nresource_type\tstring\t\t\nresource_id\tstring\t\t\n\nControllerRuntimeDependenciesResponse\nField\tType\tLabel\tDescription\nmessages\tControllerRuntimeDependency\trepeated\t\n\nControllerRuntimeDependency\n\nThe ControllerRuntimeDependency message contains the graph of controller-resource dependencies.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nedges\tControllerDependencyEdge\trepeated\t\n\nDependencyEdgeType\nName\tNumber\tDescription\nOUTPUT_EXCLUSIVE\t0\t\nOUTPUT_SHARED\t3\t\nINPUT_STRONG\t1\t\nINPUT_WEAK\t2\t\nINPUT_DESTROY_READY\t4\t\n\nInspectService\n\nThe inspect service definition.\n\nInspectService provides auxiliary API to inspect OS internals.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nControllerRuntimeDependencies\t.google.protobuf.Empty\tControllerRuntimeDependenciesResponse\t\n\nTop\n\nmachine/machine.proto\n\nAddressEvent\n\nAddressEvent reports node endpoints aggregated from k8s.Endpoints and network.Hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\naddresses\tstring\trepeated\t\n\nApplyConfiguration\n\nApplyConfigurationResponse describes the response to a configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nwarnings\tstring\trepeated\tConfiguration validation warnings.\nmode\tApplyConfigurationRequest.Mode\t\tStates which mode was actually chosen.\nmode_details\tstring\t\tHuman-readable message explaining the result of the apply configuration call.\n\nApplyConfigurationRequest\n\nrpc applyConfiguration ApplyConfiguration describes a request to assert a new configuration upon a node.\n\nField\tType\tLabel\tDescription\ndata\tbytes\t\t\nmode\tApplyConfigurationRequest.Mode\t\t\ndry_run\tbool\t\t\ntry_mode_timeout\tgoogle.protobuf.Duration\t\t\n\nApplyConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tApplyConfiguration\trepeated\t\n\nBPFInstruction\nField\tType\tLabel\tDescription\nop\tuint32\t\t\njt\tuint32\t\t\njf\tuint32\t\t\nk\tuint32\t\t\n\nBootstrap\n\nThe bootstrap message containing the bootstrap status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nBootstrapRequest\n\nrpc Bootstrap\n\nField\tType\tLabel\tDescription\nrecover_etcd\tbool\t\tEnable etcd recovery from the snapshot. Snapshot should be uploaded before this call via EtcdRecover RPC.\nrecover_skip_hash_check\tbool\t\tSkip hash check on the snapshot (etcd). Enable this when recovering from data directory copy to skip integrity check.\n\nBootstrapResponse\nField\tType\tLabel\tDescription\nmessages\tBootstrap\trepeated\t\n\nCNIConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurls\tstring\trepeated\t\n\nCPUInfo\nField\tType\tLabel\tDescription\nprocessor\tuint32\t\t\nvendor_id\tstring\t\t\ncpu_family\tstring\t\t\nmodel\tstring\t\t\nmodel_name\tstring\t\t\nstepping\tstring\t\t\nmicrocode\tstring\t\t\ncpu_mhz\tdouble\t\t\ncache_size\tstring\t\t\nphysical_id\tstring\t\t\nsiblings\tuint32\t\t\ncore_id\tstring\t\t\ncpu_cores\tuint32\t\t\napic_id\tstring\t\t\ninitial_apic_id\tstring\t\t\nfpu\tstring\t\t\nfpu_exception\tstring\t\t\ncpu_id_level\tuint32\t\t\nwp\tstring\t\t\nflags\tstring\trepeated\t\nbugs\tstring\trepeated\t\nbogo_mips\tdouble\t\t\ncl_flush_size\tuint32\t\t\ncache_alignment\tuint32\t\t\naddress_sizes\tstring\t\t\npower_management\tstring\t\t\n\nCPUInfoResponse\nField\tType\tLabel\tDescription\nmessages\tCPUsInfo\trepeated\t\n\nCPUStat\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nCPUsInfo\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncpu_info\tCPUInfo\trepeated\t\n\nClusterConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\ncontrol_plane\tControlPlaneConfig\t\t\ncluster_network\tClusterNetworkConfig\t\t\nallow_scheduling_on_control_planes\tbool\t\t\n\nClusterNetworkConfig\nField\tType\tLabel\tDescription\ndns_domain\tstring\t\t\ncni_config\tCNIConfig\t\t\n\nConfigLoadErrorEvent\n\nConfigLoadErrorEvent is reported when the config loading has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConfigValidationErrorEvent\n\nConfigValidationErrorEvent is reported when config validation has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConnectRecord\nField\tType\tLabel\tDescription\nl4proto\tstring\t\t\nlocalip\tstring\t\t\nlocalport\tuint32\t\t\nremoteip\tstring\t\t\nremoteport\tuint32\t\t\nstate\tConnectRecord.State\t\t\ntxqueue\tuint64\t\t\nrxqueue\tuint64\t\t\ntr\tConnectRecord.TimerActive\t\t\ntimerwhen\tuint64\t\t\nretrnsmt\tuint64\t\t\nuid\tuint32\t\t\ntimeout\tuint64\t\t\ninode\tuint64\t\t\nref\tuint64\t\t\npointer\tuint64\t\t\nprocess\tConnectRecord.Process\t\t\nnetns\tstring\t\t\n\nConnectRecord.Process\nField\tType\tLabel\tDescription\npid\tuint32\t\t\nname\tstring\t\t\n\nContainer\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncontainers\tContainerInfo\trepeated\t\n\nContainerInfo\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nimage\tstring\t\t\npid\tuint32\t\t\nstatus\tstring\t\t\npod_id\tstring\t\t\nname\tstring\t\t\nnetwork_namespace\tstring\t\t\n\nContainersRequest\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nContainersResponse\nField\tType\tLabel\tDescription\nmessages\tContainer\trepeated\t\n\nControlPlaneConfig\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nCopyRequest\n\nCopyRequest describes a request to copy data out of Talos node\n\nCopy produces .tar.gz archive which is streamed back to the caller\n\nField\tType\tLabel\tDescription\nroot_path\tstring\t\tRoot path to start copying data out, it might be either a file or directory\n\nDHCPOptionsConfig\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\n\nDiskStat\nField\tType\tLabel\tDescription\nname\tstring\t\t\nread_completed\tuint64\t\t\nread_merged\tuint64\t\t\nread_sectors\tuint64\t\t\nread_time_ms\tuint64\t\t\nwrite_completed\tuint64\t\t\nwrite_merged\tuint64\t\t\nwrite_sectors\tuint64\t\t\nwrite_time_ms\tuint64\t\t\nio_in_progress\tuint64\t\t\nio_time_ms\tuint64\t\t\nio_time_weighted_ms\tuint64\t\t\ndiscard_completed\tuint64\t\t\ndiscard_merged\tuint64\t\t\ndiscard_sectors\tuint64\t\t\ndiscard_time_ms\tuint64\t\t\n\nDiskStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tDiskStat\t\t\ndevices\tDiskStat\trepeated\t\n\nDiskStatsResponse\nField\tType\tLabel\tDescription\nmessages\tDiskStats\trepeated\t\n\nDiskUsageInfo\n\nDiskUsageInfo describes a file or directory’s information for du command\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\n\nDiskUsageRequest\n\nDiskUsageRequest describes a request to list disk usage of directories and regular files\n\nField\tType\tLabel\tDescription\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\nall\tbool\t\tAll write sizes for all files, not just directories.\nthreshold\tint64\t\tThreshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative.\npaths\tstring\trepeated\tDiskUsagePaths is the list of directories to calculate disk usage for.\n\nDmesgRequest\n\ndmesg\n\nField\tType\tLabel\tDescription\nfollow\tbool\t\t\ntail\tbool\t\t\n\nEtcdAlarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarmResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarmDisarm\trepeated\t\n\nEtcdAlarmListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarm\trepeated\t\n\nEtcdDefragment\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdDefragmentResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdDefragment\trepeated\t\n\nEtcdForfeitLeadership\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember\tstring\t\t\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdForfeitLeadership\trepeated\t\n\nEtcdLeaveCluster\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdLeaveCluster\trepeated\t\n\nEtcdMember\n\nEtcdMember describes a single etcd member.\n\nField\tType\tLabel\tDescription\nid\tuint64\t\tmember ID.\nhostname\tstring\t\thuman-readable name of the member.\npeer_urls\tstring\trepeated\tthe list of URLs the member exposes to clients for communication.\nclient_urls\tstring\trepeated\tthe list of URLs the member exposes to the cluster for communication.\nis_learner\tbool\t\tlearner flag\n\nEtcdMemberAlarm\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nalarm\tEtcdMemberAlarm.AlarmType\t\t\n\nEtcdMemberListRequest\nField\tType\tLabel\tDescription\nquery_local\tbool\t\t\n\nEtcdMemberListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdMembers\trepeated\t\n\nEtcdMemberStatus\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nprotocol_version\tstring\t\t\ndb_size\tint64\t\t\ndb_size_in_use\tint64\t\t\nleader\tuint64\t\t\nraft_index\tuint64\t\t\nraft_term\tuint64\t\t\nraft_applied_index\tuint64\t\t\nerrors\tstring\trepeated\t\nis_learner\tbool\t\t\n\nEtcdMembers\n\nEtcdMembers contains the list of members registered on the host.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nlegacy_members\tstring\trepeated\tlist of member hostnames.\nmembers\tEtcdMember\trepeated\tthe list of etcd members registered on the node.\n\nEtcdRecover\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRecoverResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRecover\trepeated\t\n\nEtcdRemoveMember\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByID\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByIDRequest\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\n\nEtcdRemoveMemberByIDResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMemberByID\trepeated\t\n\nEtcdRemoveMemberRequest\nField\tType\tLabel\tDescription\nmember\tstring\t\t\n\nEtcdRemoveMemberResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMember\trepeated\t\n\nEtcdSnapshotRequest\n\nEtcdStatus\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_status\tEtcdMemberStatus\t\t\n\nEtcdStatusResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdStatus\trepeated\t\n\nEvent\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tgoogle.protobuf.Any\t\t\nid\tstring\t\t\nactor_id\tstring\t\t\n\nEventsRequest\nField\tType\tLabel\tDescription\ntail_events\tint32\t\t\ntail_id\tstring\t\t\ntail_seconds\tint32\t\t\nwith_actor_id\tstring\t\t\n\nFeaturesInfo\n\nFeaturesInfo describes individual Talos features that can be switched on or off.\n\nField\tType\tLabel\tDescription\nrbac\tbool\t\tRBAC is true if role-based access control is enabled.\n\nFileInfo\n\nFileInfo describes a file or directory’s information\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nmode\tuint32\t\tMode is the bitmap of UNIX mode/permission flags of the file\nmodified\tint64\t\tModified indicates the UNIX timestamp at which the file was last modified\nis_dir\tbool\t\tIsDir indicates that the file is a directory\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nlink\tstring\t\tLink is filled with symlink target\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\nuid\tuint32\t\tOwner uid\ngid\tuint32\t\tOwner gid\n\nGenerateClientConfiguration\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nca\tbytes\t\tPEM-encoded CA certificate.\ncrt\tbytes\t\tPEM-encoded generated client certificate.\nkey\tbytes\t\tPEM-encoded generated client key.\ntalosconfig\tbytes\t\tClient configuration (talosconfig) file content.\n\nGenerateClientConfigurationRequest\nField\tType\tLabel\tDescription\nroles\tstring\trepeated\tRoles in the generated client certificate.\ncrt_ttl\tgoogle.protobuf.Duration\t\tClient certificate TTL.\n\nGenerateClientConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateClientConfiguration\trepeated\t\n\nGenerateConfiguration\n\nGenerateConfiguration describes the response to a generate configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tbytes\trepeated\t\ntalosconfig\tbytes\t\t\n\nGenerateConfigurationRequest\n\nGenerateConfigurationRequest describes a request to generate a new configuration on a node.\n\nField\tType\tLabel\tDescription\nconfig_version\tstring\t\t\ncluster_config\tClusterConfig\t\t\nmachine_config\tMachineConfig\t\t\noverride_time\tgoogle.protobuf.Timestamp\t\t\n\nGenerateConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateConfiguration\trepeated\t\n\nHostname\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nhostname\tstring\t\t\n\nHostnameResponse\nField\tType\tLabel\tDescription\nmessages\tHostname\trepeated\t\n\nImageListRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\n\nImageListResponse\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\t\ndigest\tstring\t\t\nsize\tint64\t\t\ncreated_at\tgoogle.protobuf.Timestamp\t\t\n\nImagePull\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nImagePullRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\nreference\tstring\t\tImage reference to pull.\n\nImagePullResponse\nField\tType\tLabel\tDescription\nmessages\tImagePull\trepeated\t\n\nInstallConfig\nField\tType\tLabel\tDescription\ninstall_disk\tstring\t\t\ninstall_image\tstring\t\t\n\nListRequest\n\nListRequest describes a request to list the contents of a directory.\n\nField\tType\tLabel\tDescription\nroot\tstring\t\tRoot indicates the root directory for the list. If not indicated, ‘/’ is presumed.\nrecurse\tbool\t\tRecurse indicates that subdirectories should be recursed.\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\ntypes\tListRequest.Type\trepeated\tTypes indicates what file type should be returned. If not indicated, all files will be returned.\n\nLoadAvg\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nload1\tdouble\t\t\nload5\tdouble\t\t\nload15\tdouble\t\t\n\nLoadAvgResponse\nField\tType\tLabel\tDescription\nmessages\tLoadAvg\trepeated\t\n\nLogsRequest\n\nrpc logs The request message containing the process name.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\nfollow\tbool\t\t\ntail_lines\tint32\t\t\n\nMachineConfig\nField\tType\tLabel\tDescription\ntype\tMachineConfig.MachineType\t\t\ninstall_config\tInstallConfig\t\t\nnetwork_config\tNetworkConfig\t\t\nkubernetes_version\tstring\t\t\n\nMachineStatusEvent\n\nMachineStatusEvent reports changes to the MachineStatus resource.\n\nField\tType\tLabel\tDescription\nstage\tMachineStatusEvent.MachineStage\t\t\nstatus\tMachineStatusEvent.MachineStatus\t\t\n\nMachineStatusEvent.MachineStatus\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tMachineStatusEvent.MachineStatus.UnmetCondition\trepeated\t\n\nMachineStatusEvent.MachineStatus.UnmetCondition\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nMemInfo\nField\tType\tLabel\tDescription\nmemtotal\tuint64\t\t\nmemfree\tuint64\t\t\nmemavailable\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswapcached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactiveanon\tuint64\t\t\ninactiveanon\tuint64\t\t\nactivefile\tuint64\t\t\ninactivefile\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswaptotal\tuint64\t\t\nswapfree\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanonpages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\nsreclaimable\tuint64\t\t\nsunreclaim\tuint64\t\t\nkernelstack\tuint64\t\t\npagetables\tuint64\t\t\nnfsunstable\tuint64\t\t\nbounce\tuint64\t\t\nwritebacktmp\tuint64\t\t\ncommitlimit\tuint64\t\t\ncommittedas\tuint64\t\t\nvmalloctotal\tuint64\t\t\nvmallocused\tuint64\t\t\nvmallocchunk\tuint64\t\t\nhardwarecorrupted\tuint64\t\t\nanonhugepages\tuint64\t\t\nshmemhugepages\tuint64\t\t\nshmempmdmapped\tuint64\t\t\ncmatotal\tuint64\t\t\ncmafree\tuint64\t\t\nhugepagestotal\tuint64\t\t\nhugepagesfree\tuint64\t\t\nhugepagesrsvd\tuint64\t\t\nhugepagessurp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirectmap4k\tuint64\t\t\ndirectmap2m\tuint64\t\t\ndirectmap1g\tuint64\t\t\n\nMemory\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmeminfo\tMemInfo\t\t\n\nMemoryResponse\nField\tType\tLabel\tDescription\nmessages\tMemory\trepeated\t\n\nMetaDelete\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaDeleteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\n\nMetaDeleteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaDelete\trepeated\t\n\nMetaWrite\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaWriteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\nvalue\tbytes\t\t\n\nMetaWriteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaWrite\trepeated\t\n\nMountStat\n\nThe messages message containing the requested processes.\n\nField\tType\tLabel\tDescription\nfilesystem\tstring\t\t\nsize\tuint64\t\t\navailable\tuint64\t\t\nmounted_on\tstring\t\t\n\nMounts\n\nThe messages message containing the requested df stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tMountStat\trepeated\t\n\nMountsResponse\nField\tType\tLabel\tDescription\nmessages\tMounts\trepeated\t\n\nNetDev\nField\tType\tLabel\tDescription\nname\tstring\t\t\nrx_bytes\tuint64\t\t\nrx_packets\tuint64\t\t\nrx_errors\tuint64\t\t\nrx_dropped\tuint64\t\t\nrx_fifo\tuint64\t\t\nrx_frame\tuint64\t\t\nrx_compressed\tuint64\t\t\nrx_multicast\tuint64\t\t\ntx_bytes\tuint64\t\t\ntx_packets\tuint64\t\t\ntx_errors\tuint64\t\t\ntx_dropped\tuint64\t\t\ntx_fifo\tuint64\t\t\ntx_collisions\tuint64\t\t\ntx_carrier\tuint64\t\t\ntx_compressed\tuint64\t\t\n\nNetstat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nconnectrecord\tConnectRecord\trepeated\t\n\nNetstatRequest\nField\tType\tLabel\tDescription\nfilter\tNetstatRequest.Filter\t\t\nfeature\tNetstatRequest.Feature\t\t\nl4proto\tNetstatRequest.L4proto\t\t\nnetns\tNetstatRequest.NetNS\t\t\n\nNetstatRequest.Feature\nField\tType\tLabel\tDescription\npid\tbool\t\t\n\nNetstatRequest.L4proto\nField\tType\tLabel\tDescription\ntcp\tbool\t\t\ntcp6\tbool\t\t\nudp\tbool\t\t\nudp6\tbool\t\t\nudplite\tbool\t\t\nudplite6\tbool\t\t\nraw\tbool\t\t\nraw6\tbool\t\t\n\nNetstatRequest.NetNS\nField\tType\tLabel\tDescription\nhostnetwork\tbool\t\t\nnetns\tstring\trepeated\t\nallnetns\tbool\t\t\n\nNetstatResponse\nField\tType\tLabel\tDescription\nmessages\tNetstat\trepeated\t\n\nNetworkConfig\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ninterfaces\tNetworkDeviceConfig\trepeated\t\n\nNetworkDeviceConfig\nField\tType\tLabel\tDescription\ninterface\tstring\t\t\ncidr\tstring\t\t\nmtu\tint32\t\t\ndhcp\tbool\t\t\nignore\tbool\t\t\ndhcp_options\tDHCPOptionsConfig\t\t\nroutes\tRouteConfig\trepeated\t\n\nNetworkDeviceStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tNetDev\t\t\ndevices\tNetDev\trepeated\t\n\nNetworkDeviceStatsResponse\nField\tType\tLabel\tDescription\nmessages\tNetworkDeviceStats\trepeated\t\n\nPacketCaptureRequest\nField\tType\tLabel\tDescription\ninterface\tstring\t\tInterface name to perform packet capture on.\npromiscuous\tbool\t\tEnable promiscuous mode.\nsnap_len\tuint32\t\tSnap length in bytes.\nbpf_filter\tBPFInstruction\trepeated\tBPF filter.\n\nPhaseEvent\nField\tType\tLabel\tDescription\nphase\tstring\t\t\naction\tPhaseEvent.Action\t\t\n\nPlatformInfo\nField\tType\tLabel\tDescription\nname\tstring\t\t\nmode\tstring\t\t\n\nProcess\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nprocesses\tProcessInfo\trepeated\t\n\nProcessInfo\nField\tType\tLabel\tDescription\npid\tint32\t\t\nppid\tint32\t\t\nstate\tstring\t\t\nthreads\tint32\t\t\ncpu_time\tdouble\t\t\nvirtual_memory\tuint64\t\t\nresident_memory\tuint64\t\t\ncommand\tstring\t\t\nexecutable\tstring\t\t\nargs\tstring\t\t\n\nProcessesResponse\n\nrpc processes\n\nField\tType\tLabel\tDescription\nmessages\tProcess\trepeated\t\n\nReadRequest\nField\tType\tLabel\tDescription\npath\tstring\t\t\n\nReboot\n\nThe reboot message containing the reboot status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nRebootRequest\n\nrpc reboot\n\nField\tType\tLabel\tDescription\nmode\tRebootRequest.Mode\t\t\n\nRebootResponse\nField\tType\tLabel\tDescription\nmessages\tReboot\trepeated\t\n\nReset\n\nThe reset message containing the restart status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nResetPartitionSpec\n\nrpc reset\n\nField\tType\tLabel\tDescription\nlabel\tstring\t\t\nwipe\tbool\t\t\n\nResetRequest\nField\tType\tLabel\tDescription\ngraceful\tbool\t\tGraceful indicates whether node should leave etcd before the upgrade, it also enforces etcd checks before leaving.\nreboot\tbool\t\tReboot indicates whether node should reboot or halt after resetting.\nsystem_partitions_to_wipe\tResetPartitionSpec\trepeated\tSystem_partitions_to_wipe lists specific system disk partitions to be reset (wiped). If system_partitions_to_wipe is empty, all the partitions are erased.\nuser_disks_to_wipe\tstring\trepeated\tUserDisksToWipe lists specific connected block devices to be reset (wiped).\nmode\tResetRequest.WipeMode\t\tWipeMode defines which devices should be wiped.\n\nResetResponse\nField\tType\tLabel\tDescription\nmessages\tReset\trepeated\t\n\nRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRestartEvent\nField\tType\tLabel\tDescription\ncmd\tint64\t\t\n\nRestartRequest\n\nrpc restart The request message containing the process to restart.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nRestartResponse\n\nThe messages message containing the restart status.\n\nField\tType\tLabel\tDescription\nmessages\tRestart\trepeated\t\n\nRollback\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRollbackRequest\n\nrpc rollback\n\nRollbackResponse\nField\tType\tLabel\tDescription\nmessages\tRollback\trepeated\t\n\nRouteConfig\nField\tType\tLabel\tDescription\nnetwork\tstring\t\t\ngateway\tstring\t\t\nmetric\tuint32\t\t\n\nSequenceEvent\n\nrpc events\n\nField\tType\tLabel\tDescription\nsequence\tstring\t\t\naction\tSequenceEvent.Action\t\t\nerror\tcommon.Error\t\t\n\nServiceEvent\nField\tType\tLabel\tDescription\nmsg\tstring\t\t\nstate\tstring\t\t\nts\tgoogle.protobuf.Timestamp\t\t\n\nServiceEvents\nField\tType\tLabel\tDescription\nevents\tServiceEvent\trepeated\t\n\nServiceHealth\nField\tType\tLabel\tDescription\nunknown\tbool\t\t\nhealthy\tbool\t\t\nlast_message\tstring\t\t\nlast_change\tgoogle.protobuf.Timestamp\t\t\n\nServiceInfo\nField\tType\tLabel\tDescription\nid\tstring\t\t\nstate\tstring\t\t\nevents\tServiceEvents\t\t\nhealth\tServiceHealth\t\t\n\nServiceList\n\nrpc servicelist\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nservices\tServiceInfo\trepeated\t\n\nServiceListResponse\nField\tType\tLabel\tDescription\nmessages\tServiceList\trepeated\t\n\nServiceRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceRestartRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceRestartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceRestart\trepeated\t\n\nServiceStart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStartRequest\n\nrpc servicestart\n\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStart\trepeated\t\n\nServiceStateEvent\nField\tType\tLabel\tDescription\nservice\tstring\t\t\naction\tServiceStateEvent.Action\t\t\nmessage\tstring\t\t\nhealth\tServiceHealth\t\t\n\nServiceStop\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStopRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStopResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStop\trepeated\t\n\nShutdown\n\nrpc shutdown The messages message containing the shutdown status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nShutdownRequest\nField\tType\tLabel\tDescription\nforce\tbool\t\tForce indicates whether node should shutdown without first cordening and draining\n\nShutdownResponse\nField\tType\tLabel\tDescription\nmessages\tShutdown\trepeated\t\n\nSoftIRQStat\nField\tType\tLabel\tDescription\nhi\tuint64\t\t\ntimer\tuint64\t\t\nnet_tx\tuint64\t\t\nnet_rx\tuint64\t\t\nblock\tuint64\t\t\nblock_io_poll\tuint64\t\t\ntasklet\tuint64\t\t\nsched\tuint64\t\t\nhrtimer\tuint64\t\t\nrcu\tuint64\t\t\n\nStat\n\nThe messages message containing the requested stat.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nmemory_usage\tuint64\t\t\ncpu_usage\tuint64\t\t\npod_id\tstring\t\t\nname\tstring\t\t\n\nStats\n\nThe messages message containing the requested stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tStat\trepeated\t\n\nStatsRequest\n\nThe request message containing the containerd namespace.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nStatsResponse\nField\tType\tLabel\tDescription\nmessages\tStats\trepeated\t\n\nSystemStat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nboot_time\tuint64\t\t\ncpu_total\tCPUStat\t\t\ncpu\tCPUStat\trepeated\t\nirq_total\tuint64\t\t\nirq\tuint64\trepeated\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\nsoft_irq\tSoftIRQStat\t\t\n\nSystemStatResponse\nField\tType\tLabel\tDescription\nmessages\tSystemStat\trepeated\t\n\nTaskEvent\nField\tType\tLabel\tDescription\ntask\tstring\t\t\naction\tTaskEvent.Action\t\t\n\nUpgrade\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nack\tstring\t\t\nactor_id\tstring\t\t\n\nUpgradeRequest\n\nrpc upgrade\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\npreserve\tbool\t\t\nstage\tbool\t\t\nforce\tbool\t\t\nreboot_mode\tUpgradeRequest.RebootMode\t\t\n\nUpgradeResponse\nField\tType\tLabel\tDescription\nmessages\tUpgrade\trepeated\t\n\nVersion\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nversion\tVersionInfo\t\t\nplatform\tPlatformInfo\t\t\nfeatures\tFeaturesInfo\t\tFeatures describe individual Talos features that can be switched on or off.\n\nVersionInfo\nField\tType\tLabel\tDescription\ntag\tstring\t\t\nsha\tstring\t\t\nbuilt\tstring\t\t\ngo_version\tstring\t\t\nos\tstring\t\t\narch\tstring\t\t\n\nVersionResponse\nField\tType\tLabel\tDescription\nmessages\tVersion\trepeated\t\n\nApplyConfigurationRequest.Mode\nName\tNumber\tDescription\nREBOOT\t0\t\nAUTO\t1\t\nNO_REBOOT\t2\t\nSTAGED\t3\t\nTRY\t4\t\n\nConnectRecord.State\nName\tNumber\tDescription\nRESERVED\t0\t\nESTABLISHED\t1\t\nSYN_SENT\t2\t\nSYN_RECV\t3\t\nFIN_WAIT1\t4\t\nFIN_WAIT2\t5\t\nTIME_WAIT\t6\t\nCLOSE\t7\t\nCLOSEWAIT\t8\t\nLASTACK\t9\t\nLISTEN\t10\t\nCLOSING\t11\t\n\nConnectRecord.TimerActive\nName\tNumber\tDescription\nOFF\t0\t\nON\t1\t\nKEEPALIVE\t2\t\nTIMEWAIT\t3\t\nPROBE\t4\t\n\nEtcdMemberAlarm.AlarmType\nName\tNumber\tDescription\nNONE\t0\t\nNOSPACE\t1\t\nCORRUPT\t2\t\n\nListRequest.Type\n\nFile type.\n\nName\tNumber\tDescription\nREGULAR\t0\tRegular file (not directory, symlink, etc).\nDIRECTORY\t1\tDirectory.\nSYMLINK\t2\tSymbolic link.\n\nMachineConfig.MachineType\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\t\nTYPE_INIT\t1\t\nTYPE_CONTROL_PLANE\t2\t\nTYPE_WORKER\t3\t\n\nMachineStatusEvent.MachineStage\nName\tNumber\tDescription\nUNKNOWN\t0\t\nBOOTING\t1\t\nINSTALLING\t2\t\nMAINTENANCE\t3\t\nRUNNING\t4\t\nREBOOTING\t5\t\nSHUTTING_DOWN\t6\t\nRESETTING\t7\t\nUPGRADING\t8\t\n\nNetstatRequest.Filter\nName\tNumber\tDescription\nALL\t0\t\nCONNECTED\t1\t\nLISTENING\t2\t\n\nPhaseEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nRebootRequest.Mode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nResetRequest.WipeMode\nName\tNumber\tDescription\nALL\t0\t\nSYSTEM_DISK\t1\t\nUSER_DISKS\t2\t\n\nSequenceEvent.Action\nName\tNumber\tDescription\nNOOP\t0\t\nSTART\t1\t\nSTOP\t2\t\n\nServiceStateEvent.Action\nName\tNumber\tDescription\nINITIALIZED\t0\t\nPREPARING\t1\t\nWAITING\t2\t\nRUNNING\t3\t\nSTOPPING\t4\t\nFINISHED\t5\t\nFAILED\t6\t\nSKIPPED\t7\t\n\nTaskEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nUpgradeRequest.RebootMode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nMachineService\n\nThe machine service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nApplyConfiguration\tApplyConfigurationRequest\tApplyConfigurationResponse\t\nBootstrap\tBootstrapRequest\tBootstrapResponse\tBootstrap method makes control plane node enter etcd bootstrap mode. Node aborts etcd join sequence and creates single-node etcd cluster. If recover_etcd argument is specified, etcd is recovered from a snapshot uploaded with EtcdRecover.\nContainers\tContainersRequest\tContainersResponse\t\nCopy\tCopyRequest\t.common.Data stream\t\nCPUInfo\t.google.protobuf.Empty\tCPUInfoResponse\t\nDiskStats\t.google.protobuf.Empty\tDiskStatsResponse\t\nDmesg\tDmesgRequest\t.common.Data stream\t\nEvents\tEventsRequest\tEvent stream\t\nEtcdMemberList\tEtcdMemberListRequest\tEtcdMemberListResponse\t\nEtcdRemoveMember\tEtcdRemoveMemberRequest\tEtcdRemoveMemberResponse\tEtcdRemoveMember removes a member from the etcd cluster by hostname. Please use EtcdRemoveMemberByID instead.\nEtcdRemoveMemberByID\tEtcdRemoveMemberByIDRequest\tEtcdRemoveMemberByIDResponse\tEtcdRemoveMemberByID removes a member from the etcd cluster identified by member ID. This API should be used to remove members which don’t have an associated Talos node anymore. To remove a member with a running Talos node, use EtcdLeaveCluster API on the node to be removed.\nEtcdLeaveCluster\tEtcdLeaveClusterRequest\tEtcdLeaveClusterResponse\t\nEtcdForfeitLeadership\tEtcdForfeitLeadershipRequest\tEtcdForfeitLeadershipResponse\t\nEtcdRecover\t.common.Data stream\tEtcdRecoverResponse\tEtcdRecover method uploads etcd data snapshot created with EtcdSnapshot to the node. Snapshot can be later used to recover the cluster via Bootstrap method.\nEtcdSnapshot\tEtcdSnapshotRequest\t.common.Data stream\tEtcdSnapshot method creates etcd data snapshot (backup) from the local etcd instance and streams it back to the client. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmList\t.google.protobuf.Empty\tEtcdAlarmListResponse\tEtcdAlarmList lists etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmDisarm\t.google.protobuf.Empty\tEtcdAlarmDisarmResponse\tEtcdAlarmDisarm disarms etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdDefragment\t.google.protobuf.Empty\tEtcdDefragmentResponse\tEtcdDefragment defragments etcd data directory for the current node. Defragmentation is a resource-heavy operation, so it should only run on a specific node. This method is available only on control plane nodes (which run etcd).\nEtcdStatus\t.google.protobuf.Empty\tEtcdStatusResponse\tEtcdStatus returns etcd status for the current member. This method is available only on control plane nodes (which run etcd).\nGenerateConfiguration\tGenerateConfigurationRequest\tGenerateConfigurationResponse\t\nHostname\t.google.protobuf.Empty\tHostnameResponse\t\nKubeconfig\t.google.protobuf.Empty\t.common.Data stream\t\nList\tListRequest\tFileInfo stream\t\nDiskUsage\tDiskUsageRequest\tDiskUsageInfo stream\t\nLoadAvg\t.google.protobuf.Empty\tLoadAvgResponse\t\nLogs\tLogsRequest\t.common.Data stream\t\nMemory\t.google.protobuf.Empty\tMemoryResponse\t\nMounts\t.google.protobuf.Empty\tMountsResponse\t\nNetworkDeviceStats\t.google.protobuf.Empty\tNetworkDeviceStatsResponse\t\nProcesses\t.google.protobuf.Empty\tProcessesResponse\t\nRead\tReadRequest\t.common.Data stream\t\nReboot\tRebootRequest\tRebootResponse\t\nRestart\tRestartRequest\tRestartResponse\t\nRollback\tRollbackRequest\tRollbackResponse\t\nReset\tResetRequest\tResetResponse\t\nServiceList\t.google.protobuf.Empty\tServiceListResponse\t\nServiceRestart\tServiceRestartRequest\tServiceRestartResponse\t\nServiceStart\tServiceStartRequest\tServiceStartResponse\t\nServiceStop\tServiceStopRequest\tServiceStopResponse\t\nShutdown\tShutdownRequest\tShutdownResponse\t\nStats\tStatsRequest\tStatsResponse\t\nSystemStat\t.google.protobuf.Empty\tSystemStatResponse\t\nUpgrade\tUpgradeRequest\tUpgradeResponse\t\nVersion\t.google.protobuf.Empty\tVersionResponse\t\nGenerateClientConfiguration\tGenerateClientConfigurationRequest\tGenerateClientConfigurationResponse\tGenerateClientConfiguration generates talosctl client configuration (talosconfig).\nPacketCapture\tPacketCaptureRequest\t.common.Data stream\tPacketCapture performs packet capture and streams back pcap file.\nNetstat\tNetstatRequest\tNetstatResponse\tNetstat provides information about network connections.\nMetaWrite\tMetaWriteRequest\tMetaWriteResponse\tMetaWrite writes a META key-value pair.\nMetaDelete\tMetaDeleteRequest\tMetaDeleteResponse\tMetaDelete deletes a META key.\nImageList\tImageListRequest\tImageListResponse stream\tImageList lists images in the CRI.\nImagePull\tImagePullRequest\tImagePullResponse\tImagePull pulls an image into the CRI.\n\nTop\n\nsecurity/security.proto\n\nCertificateRequest\n\nThe request message containing the certificate signing request.\n\nField\tType\tLabel\tDescription\ncsr\tbytes\t\tCertificate Signing Request in PEM format.\n\nCertificateResponse\n\nThe response message containing signed certificate.\n\nField\tType\tLabel\tDescription\nca\tbytes\t\tCertificate of the CA that signed the requested certificate in PEM format.\ncrt\tbytes\t\tSigned X.509 requested certificate in PEM format.\n\nSecurityService\n\nThe security service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nCertificate\tCertificateRequest\tCertificateResponse\t\n\nTop\n\nstorage/storage.proto\n\nDisk\n\nDisk represents a disk.\n\nField\tType\tLabel\tDescription\nsize\tuint64\t\tSize indicates the disk size in bytes.\nmodel\tstring\t\tModel idicates the disk model.\ndevice_name\tstring\t\tDeviceName indicates the disk name (e.g. sda).\nname\tstring\t\tName as in /sys/block/<dev>/device/name.\nserial\tstring\t\tSerial as in /sys/block/<dev>/device/serial.\nmodalias\tstring\t\tModalias as in /sys/block/<dev>/device/modalias.\nuuid\tstring\t\tUuid as in /sys/block/<dev>/device/uuid.\nwwid\tstring\t\tWwid as in /sys/block/<dev>/device/wwid.\ntype\tDisk.DiskType\t\tType is a type of the disk: nvme, ssd, hdd, sd card.\nbus_path\tstring\t\tBusPath is the bus path of the disk.\nsystem_disk\tbool\t\tSystemDisk indicates that the disk is used as Talos system disk.\nsubsystem\tstring\t\tSubsystem is the symlink path in the /sys/block/<dev>/subsystem.\nreadonly\tbool\t\tReadonly specifies if the disk is read only.\n\nDisks\n\nDisksResponse represents the response of the Disks RPC.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndisks\tDisk\trepeated\t\n\nDisksResponse\nField\tType\tLabel\tDescription\nmessages\tDisks\trepeated\t\n\nDisk.DiskType\nName\tNumber\tDescription\nUNKNOWN\t0\t\nSSD\t1\t\nHDD\t2\t\nNVME\t3\t\nSD\t4\t\n\nStorageService\n\nStorageService represents the storage service.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nDisks\t.google.protobuf.Empty\tDisksResponse\t\n\nTop\n\ntime/time.proto\n\nTime\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nserver\tstring\t\t\nlocaltime\tgoogle.protobuf.Timestamp\t\t\nremotetime\tgoogle.protobuf.Timestamp\t\t\n\nTimeRequest\n\nThe response message containing the ntp server\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\n\nTimeResponse\n\nThe response message containing the ntp server, time, and offset\n\nField\tType\tLabel\tDescription\nmessages\tTime\trepeated\t\n\nTimeService\n\nThe time service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nTime\t.google.protobuf.Empty\tTimeResponse\t\nTimeCheck\tTimeRequest\tTimeResponse\t\nScalar Value Types\n.proto Type\tNotes\tC++\tJava\tPython\tGo\tC#\tPHP\tRuby\ndouble\t\tdouble\tdouble\tfloat\tfloat64\tdouble\tfloat\tFloat\nfloat\t\tfloat\tfloat\tfloat\tfloat32\tfloat\tfloat\tFloat\nint32\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nint64\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nuint32\tUses variable-length encoding.\tuint32\tint\tint/long\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nuint64\tUses variable-length encoding.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum or Fixnum (as required)\nsint32\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsint64\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nfixed32\tAlways four bytes. More efficient than uint32 if values are often greater than 2^28.\tuint32\tint\tint\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nfixed64\tAlways eight bytes. More efficient than uint64 if values are often greater than 2^56.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum\nsfixed32\tAlways four bytes.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsfixed64\tAlways eight bytes.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nbool\t\tbool\tboolean\tboolean\tbool\tbool\tboolean\tTrueClass/FalseClass\nstring\tA string must always contain UTF-8 encoded or 7-bit ASCII text.\tstring\tString\tstr/unicode\tstring\tstring\tstring\tString (UTF-8)\nbytes\tMay contain any arbitrary sequence of bytes.\tstring\tByteString\tstr\t[]byte\tByteString\tstring\tString (ASCII-8BIT)\n2 - CLI\nTalosctl CLI tool reference.\ntalosctl apply-config\n\nApply a new configuration to a node\n\ntalosctl apply-config [flags]\n\nOptions\n      --cert-fingerprint strings                                 list of server certificate fingeprints to accept (defaults to no check)\n  -p, --config-patch strings                                     the list of config patches to apply to the local config file before sending it to the node\n      --dry-run                                                  check how the config change will be applied in dry-run mode\n  -f, --file string                                              the filename of the updated configuration\n  -h, --help                                                     help for apply-config\n  -i, --insecure                                                 apply the config using the insecure (encrypted with no auth) maintenance service\n  -m, --mode auto, interactive, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --timeout duration                                         the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl bootstrap\n\nBootstrap the etcd cluster on the specified node.\n\nSynopsis\n\nWhen Talos cluster is created etcd service on control plane nodes enter the join loop waiting to join etcd peers from other control plane nodes. One node should be picked as the boostrap node. When boostrap command is issued, the node aborts join process and bootstraps etcd cluster as a single node cluster. Other control plane nodes will join etcd cluster once Kubernetes is boostrapped on the bootstrap node.\n\nThis command should not be used when “init” type node are used.\n\nTalos etcd cluster can be recovered from a known snapshot with ‘–recover-from=’ flag.\n\ntalosctl bootstrap [flags]\n\nOptions\n  -h, --help                      help for bootstrap\n      --recover-from string       recover etcd cluster from the snapshot\n      --recover-skip-hash-check   skip integrity check when recovering etcd (use when recovering from data directory copy)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create\n\nCreates a local docker-based or QEMU-based kubernetes cluster\n\ntalosctl cluster create [flags]\n\nOptions\n      --arch string                              cluster architecture (default \"amd64\")\n      --bad-rtc                                  launch VM with bad RTC state (QEMU only)\n      --cidr string                              CIDR of the cluster network (IPv4, ULA network for IPv6 is derived in automated way) (default \"10.5.0.0/24\")\n      --cni-bin-path strings                     search path for CNI binaries (VM only) (default [/home/user/.talos/cni/bin])\n      --cni-bundle-url string                    URL to download CNI bundle from (VM only) (default \"https://github.com/siderolabs/talos/releases/download/v1.6.0-alpha.2/talosctl-cni-bundle-${ARCH}.tar.gz\")\n      --cni-cache-dir string                     CNI cache directory path (VM only) (default \"/home/user/.talos/cni/cache\")\n      --cni-conf-dir string                      CNI config directory path (VM only) (default \"/home/user/.talos/cni/conf.d\")\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --control-plane-port int                   control plane port (load balancer and local API port) (default 6443)\n      --controlplanes int                        the number of controlplanes to create (default 1)\n      --cpus string                              the share of CPUs as fraction (each control plane/VM) (default \"2.0\")\n      --cpus-workers string                      the share of CPUs as fraction (each worker/VM) (default \"2.0\")\n      --crashdump                                print debug crashdump to stderr when cluster startup fails\n      --custom-cni-url string                    install custom CNI from the URL (Talos cluster)\n      --disable-dhcp-hostname                    skip announcing hostname via DHCP (QEMU only)\n      --disk int                                 default limit on disk size in MB (each VM) (default 6144)\n      --disk-encryption-key-types stringArray    encryption key types to use for disk encryption (uuid, kms) (default [uuid])\n      --disk-image-path string                   disk image to use\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n      --docker-disable-ipv6                      skip enabling IPv6 in containers (Docker only)\n      --docker-host-ip string                    Host IP to forward exposed ports to (Docker provisioner only) (default \"0.0.0.0\")\n      --encrypt-ephemeral                        enable ephemeral partition encryption\n      --encrypt-state                            enable state partition encryption\n      --endpoint string                          use endpoint instead of provider defaults\n  -p, --exposed-ports string                     Comma-separated list of ports/protocols to expose on init node. Ex -p <hostPort>:<containerPort>/<protocol (tcp or udp)> (Docker provisioner only)\n      --extra-boot-kernel-args string            add extra kernel args to the initial boot from vmlinuz and initramfs (QEMU only)\n      --extra-disks int                          number of extra disks to create for each worker VM\n      --extra-disks-size int                     default limit on disk size in MB (each VM) (default 5120)\n      --extra-uefi-search-paths strings          additional search paths for UEFI firmware (only applies when UEFI is enabled)\n  -h, --help                                     help for create\n      --image string                             the image to use (default \"ghcr.io/siderolabs/talos:latest\")\n      --init-node-as-endpoint                    use init node as endpoint instead of any load balancer endpoint\n      --initrd-path string                       initramfs image to use (default \"_out/initramfs-${ARCH}.xz\")\n  -i, --input-dir string                         location of pre-generated config files\n      --install-image string                     the installer image to use (default \"ghcr.io/siderolabs/installer:latest\")\n      --ipv4                                     enable IPv4 network in the cluster (default true)\n      --ipv6                                     enable IPv6 network in the cluster (QEMU provisioner only)\n      --ipxe-boot-script string                  iPXE boot script (URL) to use\n      --iso-path string                          the ISO path to use for the initial boot (VM only)\n      --kubeprism-port int                       KubePrism port (set to 0 to disable) (default 7445)\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n      --memory int                               the limit on memory usage in MB (each control plane/VM) (default 2048)\n      --memory-workers int                       the limit on memory usage in MB (each worker/VM) (default 2048)\n      --mtu int                                  MTU of the cluster network (default 1500)\n      --nameservers strings                      list of nameservers to use (default [8.8.8.8,1.1.1.1,2001:4860:4860::8888,2606:4700:4700::1111])\n      --registry-insecure-skip-verify strings    list of registry hostnames to skip TLS verification for\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --skip-boot-phase-finished-check           skip waiting for node to finish boot phase\n      --skip-injecting-config                    skip injecting config from embedded metadata server, write config files to current directory\n      --skip-kubeconfig                          skip merging kubeconfig from the created cluster\n      --talos-version string                     the desired Talos version to generate config for (if not set, defaults to image version)\n      --talosconfig string                       The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n      --use-vip                                  use a virtual IP for the controlplane endpoint instead of the loadbalancer\n      --user-disk strings                        list of disks to create for each VM in format: <mount_point1>:<size1>:<mount_point2>:<size2>\n      --vmlinuz-path string                      the compressed kernel image to use (default \"_out/vmlinuz-${ARCH}\")\n      --wait                                     wait for the cluster to be ready before returning (default true)\n      --wait-timeout duration                    timeout to wait for the cluster to be ready (default 20m0s)\n      --wireguard-cidr string                    CIDR of the wireguard network\n      --with-apply-config                        enable apply config when the VM is starting in maintenance mode\n      --with-bootloader                          enable bootloader to load kernel and initramfs from disk image after install (default true)\n      --with-cluster-discovery                   enable cluster discovery (default true)\n      --with-debug                               enable debug in Talos config to send service logs to the console\n      --with-firewall string                     inject firewall rules into the cluster, value is default policy - accept/block (QEMU only)\n      --with-init-node                           create the cluster with an init node\n      --with-kubespan                            enable KubeSpan system\n      --with-network-bandwidth int               specify bandwidth restriction (in kbps) on the bridge interface when creating a qemu cluster\n      --with-network-chaos                       enable to use network chaos parameters when creating a qemu cluster\n      --with-network-jitter duration             specify jitter on the bridge interface when creating a qemu cluster\n      --with-network-latency duration            specify latency on the bridge interface when creating a qemu cluster\n      --with-network-packet-corrupt float        specify percent of corrupt packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-loss float           specify percent of packet loss on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-reorder float        specify percent of reordered packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-tpm2                                enable TPM2 emulation support using swtpm\n      --with-uefi                                enable UEFI on x86_64 architecture (default true)\n      --workers int                              the number of workers to create (default 1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster destroy\n\nDestroys a local docker-based or firecracker-based kubernetes cluster\n\ntalosctl cluster destroy [flags]\n\nOptions\n  -f, --force   force deletion of cluster directory if there were errors\n  -h, --help    help for destroy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster show\n\nShows info about a local provisioned kubernetes cluster\n\ntalosctl cluster show [flags]\n\nOptions\n  -h, --help   help for show\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster\n\nA collection of commands for managing local docker-based or QEMU-based clusters\n\nOptions\n  -h, --help                 help for cluster\n      --name string          the name of the cluster (default \"talos-default\")\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create - Creates a local docker-based or QEMU-based kubernetes cluster\ntalosctl cluster destroy - Destroys a local docker-based or firecracker-based kubernetes cluster\ntalosctl cluster show - Shows info about a local provisioned kubernetes cluster\ntalosctl completion\n\nOutput shell completion code for the specified shell (bash, fish or zsh)\n\nSynopsis\n\nOutput shell completion code for the specified shell (bash, fish or zsh). The shell code must be evaluated to provide interactive completion of talosctl commands. This can be done by sourcing it from the .bash_profile.\n\nNote for zsh users: [1] zsh completions are only supported in versions of zsh >= 5.2\n\ntalosctl completion SHELL [flags]\n\nExamples\n# Installing bash completion on macOS using homebrew\n## If running Bash 3.2 included with macOS\n\tbrew install bash-completion\n## or, if running Bash 4.1+\n\tbrew install bash-completion@2\n## If talosctl is installed via homebrew, this should start working immediately.\n## If you've installed via other means, you may need add the completion to your completion directory\n\ttalosctl completion bash > $(brew --prefix)/etc/bash_completion.d/talosctl\n\n# Installing bash completion on Linux\n## If bash-completion is not installed on Linux, please install the 'bash-completion' package\n## via your distribution's package manager.\n## Load the talosctl completion code for bash into the current shell\n\tsource <(talosctl completion bash)\n## Write bash completion code to a file and source if from .bash_profile\n\ttalosctl completion bash > ~/.talos/completion.bash.inc\n\tprintf \"\n\t\t# talosctl shell completion\n\t\tsource '$HOME/.talos/completion.bash.inc'\n\t\t\" >> $HOME/.bash_profile\n\tsource $HOME/.bash_profile\n# Load the talosctl completion code for fish[1] into the current shell\n\ttalosctl completion fish | source\n# Set the talosctl completion code for fish[1] to autoload on startup\n    talosctl completion fish > ~/.config/fish/completions/talosctl.fish\n# Load the talosctl completion code for zsh[1] into the current shell\n\tsource <(talosctl completion zsh)\n# Set the talosctl completion code for zsh[1] to autoload on startup\n    talosctl completion zsh > \"${fpath[1]}/_talosctl\"\n\nOptions\n  -h, --help   help for completion\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add\n\nAdd a new context\n\ntalosctl config add <context> [flags]\n\nOptions\n      --ca string    the path to the CA certificate\n      --crt string   the path to the certificate\n  -h, --help         help for add\n      --key string   the path to the key\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config context\n\nSet the current context\n\ntalosctl config context <context> [flags]\n\nOptions\n  -h, --help   help for context\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config contexts\n\nList defined contexts\n\ntalosctl config contexts [flags]\n\nOptions\n  -h, --help   help for contexts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config endpoint\n\nSet the endpoint(s) for the current context\n\ntalosctl config endpoint <endpoint>... [flags]\n\nOptions\n  -h, --help   help for endpoint\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config info\n\nShow information about the current context\n\ntalosctl config info [flags]\n\nOptions\n  -h, --help            help for info\n  -o, --output string   output format (json|yaml|text). Default text. (default \"text\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config merge\n\nMerge additional contexts from another client configuration file\n\nSynopsis\n\nContexts with the same name are renamed while merging configs.\n\ntalosctl config merge <from> [flags]\n\nOptions\n  -h, --help   help for merge\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config new\n\nGenerate a new client configuration file\n\ntalosctl config new [<path>] [flags]\n\nOptions\n      --crt-ttl duration   certificate TTL (default 87600h0m0s)\n  -h, --help               help for new\n      --roles strings      roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config node\n\nSet the node(s) for the current context\n\ntalosctl config node <endpoint>... [flags]\n\nOptions\n  -h, --help   help for node\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config remove\n\nRemove contexts\n\ntalosctl config remove <context> [flags]\n\nOptions\n      --dry-run     dry run\n  -h, --help        help for remove\n  -y, --noconfirm   do not ask for confirmation\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config\n\nManage the client configuration file (talosconfig)\n\nOptions\n  -h, --help   help for config\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add - Add a new context\ntalosctl config context - Set the current context\ntalosctl config contexts - List defined contexts\ntalosctl config endpoint - Set the endpoint(s) for the current context\ntalosctl config info - Show information about the current context\ntalosctl config merge - Merge additional contexts from another client configuration file\ntalosctl config new - Generate a new client configuration file\ntalosctl config node - Set the node(s) for the current context\ntalosctl config remove - Remove contexts\ntalosctl conformance kubernetes\n\nRun Kubernetes conformance tests\n\ntalosctl conformance kubernetes [flags]\n\nOptions\n  -h, --help          help for kubernetes\n      --mode string   conformance test mode: [fast, certified] (default \"fast\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl conformance - Run conformance tests\ntalosctl conformance\n\nRun conformance tests\n\nOptions\n  -h, --help   help for conformance\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl conformance kubernetes - Run Kubernetes conformance tests\ntalosctl containers\n\nList containers\n\ntalosctl containers [flags]\n\nOptions\n  -h, --help         help for containers\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl copy\n\nCopy data out from the node\n\nSynopsis\n\nCreates an .tar.gz archive at the node starting at and streams it back to the client.\n\nIf ‘-’ is given for , archive is written to stdout. Otherwise archive is extracted to which should be an empty directory or talosctl creates a directory if doesn’t exist. Command doesn’t preserve ownership and access mode for the files in extract mode, while streamed .tar archive captures ownership and permission bits.\n\ntalosctl copy <src-path> -|<local-path> [flags]\n\nOptions\n  -h, --help   help for copy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dashboard\n\nCluster dashboard with node overview, logs and real-time metrics\n\nSynopsis\n\nProvide a text-based UI to navigate node overview, logs and real-time metrics.\n\nKeyboard shortcuts:\n\nh, <Left> - switch one node to the left\nl, <Right> - switch one node to the right\nj, <Down> - scroll logs/process list down\nk, <Up> - scroll logs/process list up\n<C-d> - scroll logs/process list half page down\n<C-u> - scroll logs/process list half page up\n<C-f> - scroll logs/process list one page down\n<C-b> - scroll logs/process list one page up\ntalosctl dashboard [flags]\n\nOptions\n  -h, --help                       help for dashboard\n  -d, --update-interval duration   interval between updates (default 3s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl disks\n\nGet the list of disks from /sys/block on the machine\n\ntalosctl disks [flags]\n\nOptions\n  -h, --help       help for disks\n  -i, --insecure   get disks using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dmesg\n\nRetrieve kernel logs\n\ntalosctl dmesg [flags]\n\nOptions\n  -f, --follow   specify if the kernel log should be streamed\n  -h, --help     help for dmesg\n      --tail     specify if only new messages should be sent (makes sense only when combined with --follow)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl edit\n\nEdit a resource from the default editor.\n\nSynopsis\n\nThe edit command allows you to directly edit any API resource you can retrieve via the command line tools.\n\nIt will open the editor defined by your TALOS_EDITOR, or EDITOR environment variables, or fall back to ‘vi’ for Linux or ’notepad’ for Windows.\n\ntalosctl edit <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     do not apply the change after editing and print the change summary instead\n  -h, --help                                        help for edit\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm disarm\n\nDisarm the etcd alarms for the node.\n\ntalosctl etcd alarm disarm [flags]\n\nOptions\n  -h, --help   help for disarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm list\n\nList the etcd alarms for the node.\n\ntalosctl etcd alarm list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm\n\nManage etcd alarms\n\nOptions\n  -h, --help   help for alarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd alarm disarm - Disarm the etcd alarms for the node.\ntalosctl etcd alarm list - List the etcd alarms for the node.\ntalosctl etcd defrag\n\nDefragment etcd database on the node\n\nSynopsis\n\nDefragmentation is a maintenance operation that releases unused space from the etcd database file. Defragmentation is a resource heavy operation and should be performed only when necessary on a single node at a time.\n\ntalosctl etcd defrag [flags]\n\nOptions\n  -h, --help   help for defrag\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd forfeit-leadership\n\nTell node to forfeit etcd cluster leadership\n\ntalosctl etcd forfeit-leadership [flags]\n\nOptions\n  -h, --help   help for forfeit-leadership\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd leave\n\nTell nodes to leave etcd cluster\n\ntalosctl etcd leave [flags]\n\nOptions\n  -h, --help   help for leave\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd members\n\nGet the list of etcd cluster members\n\ntalosctl etcd members [flags]\n\nOptions\n  -h, --help   help for members\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd remove-member\n\nRemove the node from etcd cluster\n\nSynopsis\n\nUse this command only if you want to remove a member which is in broken state. If there is no access to the node, or the node can’t access etcd to call etcd leave. Always prefer etcd leave over this command. It’s always better to use member ID than hostname, as hostname might not be set consistently.\n\ntalosctl etcd remove-member <member ID>|<hostname> [flags]\n\nOptions\n  -h, --help   help for remove-member\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd snapshot\n\nStream snapshot of the etcd node to the path.\n\ntalosctl etcd snapshot <path> [flags]\n\nOptions\n  -h, --help   help for snapshot\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd status\n\nGet the status of etcd cluster member\n\nSynopsis\n\nReturns the status of etcd member on the node, use multiple nodes to get status of all members.\n\ntalosctl etcd status [flags]\n\nOptions\n  -h, --help   help for status\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd\n\nManage etcd\n\nOptions\n  -h, --help   help for etcd\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd defrag - Defragment etcd database on the node\ntalosctl etcd forfeit-leadership - Tell node to forfeit etcd cluster leadership\ntalosctl etcd leave - Tell nodes to leave etcd cluster\ntalosctl etcd members - Get the list of etcd cluster members\ntalosctl etcd remove-member - Remove the node from etcd cluster\ntalosctl etcd snapshot - Stream snapshot of the etcd node to the path.\ntalosctl etcd status - Get the status of etcd cluster member\ntalosctl events\n\nStream runtime events\n\ntalosctl events [flags]\n\nOptions\n      --actor-id string     filter events by the specified actor ID (default is no filter)\n      --duration duration   show events for the past duration interval (one second resolution, default is to show no history)\n  -h, --help                help for events\n      --since string        show events after the specified event ID (default is to show no history)\n      --tail int32          show specified number of past events (use -1 to show full history, default is to show no history)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca\n\nGenerates a self-signed X.509 certificate authority\n\ntalosctl gen ca [flags]\n\nOptions\n  -h, --help                  help for ca\n      --hours int             the hours from now on which the certificate validity period ends (default 87600)\n      --organization string   X.509 distinguished name for the Organization\n      --rsa                   generate in RSA format\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen config\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl gen config <cluster name> <cluster endpoint> [flags]\n\nOptions\n      --additional-sans strings                  additional Subject-Alt-Names for the APIServer certificate\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n  -h, --help                                     help for config\n      --install-disk string                      the disk to install to (default \"/dev/sda\")\n      --install-image string                     the image used to perform an installation (default \"ghcr.io/siderolabs/installer:latest\")\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n  -o, --output string                            destination to output generated files. when multiple output types are specified, it must be a directory. for a single output type, it must either be a file path, or \"-\" for stdout\n  -t, --output-types strings                     types of outputs to be generated. valid types are: [\"controlplane\" \"worker\" \"talosconfig\"] (default [controlplane,worker,talosconfig])\n  -p, --persist                                  the desired persist value for configs (default true)\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --talos-version string                     the desired Talos version to generate config for (backwards compatibility, e.g. v0.8)\n      --version string                           the desired machine config version to generate (default \"v1alpha1\")\n      --with-cluster-discovery                   enable cluster discovery feature (default true)\n      --with-docs                                renders all machine configs adding the documentation for each field (default true)\n      --with-examples                            renders all machine configs with the commented examples (default true)\n      --with-kubespan                            enable KubeSpan feature\n      --with-secrets string                      use a secrets file generated using 'gen secrets'\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen crt\n\nGenerates an X.509 Ed25519 certificate\n\ntalosctl gen crt [flags]\n\nOptions\n      --ca string     path to the PEM encoded CERTIFICATE\n      --csr string    path to the PEM encoded CERTIFICATE REQUEST\n  -h, --help          help for crt\n      --hours int     the hours from now on which the certificate validity period ends (default 24)\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen csr\n\nGenerates a CSR using an Ed25519 private key\n\ntalosctl gen csr [flags]\n\nOptions\n  -h, --help            help for csr\n      --ip string       generate the certificate for this IP address\n      --key string      path to the PEM encoded EC or RSA PRIVATE KEY\n      --roles strings   roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen key\n\nGenerates an Ed25519 private key\n\ntalosctl gen key [flags]\n\nOptions\n  -h, --help          help for key\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen keypair\n\nGenerates an X.509 Ed25519 key pair\n\ntalosctl gen keypair [flags]\n\nOptions\n  -h, --help                  help for keypair\n      --ip string             generate the certificate for this IP address\n      --organization string   X.509 distinguished name for the Organization\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secrets\n\nGenerates a secrets bundle file which can later be used to generate a config\n\ntalosctl gen secrets [flags]\n\nOptions\n      --from-controlplane-config string     use the provided controlplane Talos machine configuration as input\n  -p, --from-kubernetes-pki string          use a Kubernetes PKI directory (e.g. /etc/kubernetes/pki) as input\n  -h, --help                                help for secrets\n  -t, --kubernetes-bootstrap-token string   use the provided bootstrap token as input\n  -o, --output-file string                  path of the output file (default \"secrets.yaml\")\n      --talos-version string                the desired Talos version to generate secrets bundle for (backwards compatibility, e.g. v0.8)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database\n\nGenerates a UEFI database to enroll the signing certificate\n\ntalosctl gen secureboot database [flags]\n\nOptions\n      --enrolled-certificate string   path to the certificate to enroll (default \"_out/uki-signing-cert.pem\")\n  -h, --help                          help for database\n      --signing-certificate string    path to the certificate used to sign the database (default \"_out/uki-signing-cert.pem\")\n      --signing-key string            path to the key used to sign the database (default \"_out/uki-signing-key.pem\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot pcr\n\nGenerates a key which is used to sign TPM PCR values\n\ntalosctl gen secureboot pcr [flags]\n\nOptions\n  -h, --help   help for pcr\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot uki\n\nGenerates a certificate which is used to sign boot assets (UKI)\n\ntalosctl gen secureboot uki [flags]\n\nOptions\n      --common-name string   common name for the certificate (default \"Test UKI Signing Key\")\n  -h, --help                 help for uki\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot\n\nGenerates secrets for the SecureBoot process\n\nOptions\n  -h, --help            help for secureboot\n  -o, --output string   path to the directory storing the generated files (default \"_out\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database - Generates a UEFI database to enroll the signing certificate\ntalosctl gen secureboot pcr - Generates a key which is used to sign TPM PCR values\ntalosctl gen secureboot uki - Generates a certificate which is used to sign boot assets (UKI)\ntalosctl gen\n\nGenerate CAs, certificates, and private keys\n\nOptions\n  -f, --force   will overwrite existing files\n  -h, --help    help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca - Generates a self-signed X.509 certificate authority\ntalosctl gen config - Generates a set of configuration files for Talos cluster\ntalosctl gen crt - Generates an X.509 Ed25519 certificate\ntalosctl gen csr - Generates a CSR using an Ed25519 private key\ntalosctl gen key - Generates an Ed25519 private key\ntalosctl gen keypair - Generates an X.509 Ed25519 key pair\ntalosctl gen secrets - Generates a secrets bundle file which can later be used to generate a config\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl get\n\nGet a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\n\nSynopsis\n\nSimilar to ‘kubectl get’, ’talosctl get’ returns a set of resources from the OS. To get a list of all available resource definitions, issue ’talosctl get rd’\n\ntalosctl get <type> [<id>] [flags]\n\nOptions\n  -h, --help               help for get\n  -i, --insecure           get resources using the insecure (encrypted with no auth) maintenance service\n      --namespace string   resource namespace (default is to use default namespace per resource)\n  -o, --output string      output mode (json, table, yaml, jsonpath) (default \"table\")\n  -w, --watch              watch resource changes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl health\n\nCheck cluster health\n\ntalosctl health [flags]\n\nOptions\n      --control-plane-nodes strings   specify IPs of control plane nodes\n  -h, --help                          help for health\n      --init-node string              specify IPs of init node\n      --k8s-endpoint string           use endpoint instead of kubeconfig default\n      --run-e2e                       run Kubernetes e2e test\n      --server                        run server-side check (default true)\n      --wait-timeout duration         timeout to wait for the cluster to be ready (default 20m0s)\n      --worker-nodes strings          specify IPs of worker nodes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default\n\nList the default images used by Talos\n\ntalosctl image default [flags]\n\nOptions\n  -h, --help   help for default\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image list\n\nList CRI images\n\ntalosctl image list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image pull\n\nPull an image into CRI\n\ntalosctl image pull [flags]\n\nOptions\n  -h, --help   help for pull\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image\n\nManage CRI containter images\n\nOptions\n  -h, --help               help for image\n      --namespace system   namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default - List the default images used by Talos\ntalosctl image list - List CRI images\ntalosctl image pull - Pull an image into CRI\ntalosctl inject serviceaccount\n\nInject Talos API ServiceAccount into Kubernetes manifests\n\ntalosctl inject serviceaccount [--roles='<ROLE_1>,<ROLE_2>'] -f <manifest.yaml> [flags]\n\nExamples\ntalosctl inject serviceaccount --roles=\"os:admin\" -f deployment.yaml > deployment-injected.yaml\n\nAlternatively, stdin can be piped to the command:\ncat deployment.yaml | talosctl inject serviceaccount --roles=\"os:admin\" -f - > deployment-injected.yaml\n\nOptions\n  -f, --file string     file with Kubernetes manifests to be injected with ServiceAccount\n  -h, --help            help for serviceaccount\n  -r, --roles strings   roles to add to the generated ServiceAccount manifests (default [os:reader])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inject\n\nInject Talos API resources into Kubernetes manifests\n\nOptions\n  -h, --help   help for inject\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inject serviceaccount - Inject Talos API ServiceAccount into Kubernetes manifests\ntalosctl inspect dependencies\n\nInspect controller-resource dependencies as graphviz graph.\n\nSynopsis\n\nInspect controller-resource dependencies as graphviz graph.\n\nPipe the output of the command through the “dot” program (part of graphviz package) to render the graph:\n\ntalosctl inspect dependencies | dot -Tpng > graph.png\n\ntalosctl inspect dependencies [flags]\n\nOptions\n  -h, --help             help for dependencies\n      --with-resources   display live resource information with dependencies\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inspect - Inspect internals of Talos\ntalosctl inspect\n\nInspect internals of Talos\n\nOptions\n  -h, --help   help for inspect\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inspect dependencies - Inspect controller-resource dependencies as graphviz graph.\ntalosctl kubeconfig\n\nDownload the admin kubeconfig from the node\n\nSynopsis\n\nDownload the admin kubeconfig from the node. If merge flag is defined, config will be merged with ~/.kube/config or [local-path] if specified. Otherwise kubeconfig will be written to PWD or [local-path] if specified.\n\ntalosctl kubeconfig [local-path] [flags]\n\nOptions\n  -f, --force                       Force overwrite of kubeconfig if already present, force overwrite on kubeconfig merge\n      --force-context-name string   Force context name for kubeconfig merge\n  -h, --help                        help for kubeconfig\n  -m, --merge                       Merge with existing kubeconfig (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl list\n\nRetrieve a directory listing\n\ntalosctl list [path] [flags]\n\nOptions\n  -d, --depth int32    maximum recursion depth (default 1)\n  -h, --help           help for list\n  -H, --humanize       humanize size and time in the output\n  -l, --long           display additional file details\n  -r, --recurse        recurse into subdirectories\n  -t, --type strings   filter by specified types:\n                       f\tregular file\n                       d\tdirectory\n                       l, L\tsymbolic link\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl logs\n\nRetrieve logs for a service\n\ntalosctl logs <service name> [flags]\n\nOptions\n  -f, --follow       specify if the logs should be streamed\n  -h, --help         help for logs\n  -k, --kubernetes   use the k8s.io containerd namespace\n      --tail int32   lines of log file to display (default is to show from the beginning) (default -1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl machineconfig gen <cluster name> <cluster endpoint> [flags]\n\nOptions\n  -h, --help   help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig patch\n\nPatch a machine config\n\ntalosctl machineconfig patch <machineconfig-file> [flags]\n\nOptions\n  -h, --help                help for patch\n  -o, --output string       output destination. if not specified, output will be printed to stdout\n  -p, --patch stringArray   patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig\n\nMachine config related commands\n\nOptions\n  -h, --help   help for machineconfig\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen - Generates a set of configuration files for Talos cluster\ntalosctl machineconfig patch - Patch a machine config\ntalosctl memory\n\nShow memory usage\n\ntalosctl memory [flags]\n\nOptions\n  -h, --help      help for memory\n  -v, --verbose   display extended memory statistics\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete\n\nDelete a key from the META partition.\n\ntalosctl meta delete key [flags]\n\nOptions\n  -h, --help   help for delete\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta write\n\nWrite a key-value pair to the META partition.\n\ntalosctl meta write key value [flags]\n\nOptions\n  -h, --help   help for write\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta\n\nWrite and delete keys in the META partition\n\nOptions\n  -h, --help       help for meta\n  -i, --insecure   write|delete meta using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete - Delete a key from the META partition.\ntalosctl meta write - Write a key-value pair to the META partition.\ntalosctl mounts\n\nList mounts\n\ntalosctl mounts [flags]\n\nOptions\n  -h, --help   help for mounts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl netstat\n\nShow network connections and sockets\n\nSynopsis\n\nShow network connections and sockets.\n\nYou can pass an optional argument to view a specific pod’s connections. To do this, format the argument as “namespace/pod”. Note that only pods with a pod network namespace are allowed. If you don’t pass an argument, the command will show host connections.\n\ntalosctl netstat [flags]\n\nOptions\n  -a, --all         display all sockets states (default: connected)\n  -x, --extend      show detailed socket information\n  -h, --help        help for netstat\n  -4, --ipv4        display only ipv4 sockets\n  -6, --ipv6        display only ipv6 sockets\n  -l, --listening   display listening server sockets\n  -k, --pods        show sockets used by Kubernetes pods\n  -p, --programs    show process using socket\n  -w, --raw         display only RAW sockets\n  -t, --tcp         display only TCP sockets\n  -o, --timers      display timers\n  -u, --udp         display only UDP sockets\n  -U, --udplite     display only UDPLite sockets\n  -v, --verbose     display sockets of all supported transport protocols\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl patch\n\nUpdate field(s) of a resource using a JSON patch.\n\ntalosctl patch <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     print the change summary and patch preview without applying the changes\n  -h, --help                                        help for patch\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n  -p, --patch stringArray                           the patch to be applied to the resource file, use @file to read a patch from file.\n      --patch-file string                           a file containing a patch to be applied to the resource.\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl pcap\n\nCapture the network packets from the node.\n\nSynopsis\n\nThe command launches packet capture on the node and streams back the packets as raw pcap file.\n\nDefault behavior is to decode the packets with internal decoder to stdout:\n\ntalosctl pcap -i eth0\n\n\nRaw pcap file can be saved with --output flag:\n\ntalosctl pcap -i eth0 --output eth0.pcap\n\n\nOutput can be piped to tcpdump:\n\ntalosctl pcap -i eth0 -o - | tcpdump -vvv -r -\n\n\nBPF filter can be applied, but it has to compiled to BPF instructions first using tcpdump. Correct link type should be specified for the tcpdump: EN10MB for Ethernet links and RAW for e.g. Wireguard tunnels:\n\ntalosctl pcap -i eth0 --bpf-filter \"$(tcpdump -dd -y EN10MB 'tcp and dst port 80')\"\n\ntalosctl pcap -i kubespan --bpf-filter \"$(tcpdump -dd -y RAW 'port 50000')\"\n\n\nAs packet capture is transmitted over the network, it is recommended to filter out the Talos API traffic, e.g. by excluding packets with the port 50000.\n\ntalosctl pcap [flags]\n\nOptions\n      --bpf-filter string   bpf filter to apply, tcpdump -dd format\n      --duration duration   duration of the capture\n  -h, --help                help for pcap\n  -i, --interface string    interface name to capture packets on (default \"eth0\")\n  -o, --output string       if not set, decode packets to stdout; if set write raw pcap data to a file, use '-' for stdout\n      --promiscuous         put interface into promiscuous mode\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl processes\n\nList running processes\n\ntalosctl processes [flags]\n\nOptions\n  -h, --help          help for processes\n  -s, --sort string   Column to sort output by. [rss|cpu] (default \"rss\")\n  -w, --watch         Stream running processes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl read\n\nRead a file on the machine\n\ntalosctl read <path> [flags]\n\nOptions\n  -h, --help   help for read\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reboot\n\nReboot a node\n\ntalosctl reboot [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n  -h, --help               help for reboot\n  -m, --mode string        select the reboot mode: \"default\", \"powercycle\" (skips kexec) (default \"default\")\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reset\n\nReset a node\n\ntalosctl reset [flags]\n\nOptions\n      --debug                                    debug operation from kernel logs. --wait is set to true when this flag is set\n      --graceful                                 if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n  -h, --help                                     help for reset\n      --insecure                                 reset using the insecure (encrypted with no auth) maintenance service\n      --reboot                                   if true, reboot the node after resetting instead of shutting down\n      --system-labels-to-wipe strings            if set, just wipe selected system disk partitions by label but keep other partitions intact\n      --timeout duration                         time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --user-disks-to-wipe strings               if set, wipes defined devices in the list\n      --wait                                     wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n      --wipe-mode all, system-disk, user-disks   disk reset mode (default all)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl restart\n\nRestart a process\n\ntalosctl restart <id> [flags]\n\nOptions\n  -h, --help         help for restart\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl rollback\n\nRollback a node to the previous installation\n\ntalosctl rollback [flags]\n\nOptions\n  -h, --help   help for rollback\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl service\n\nRetrieve the state of a service (or all services), control service state\n\nSynopsis\n\nService control command. If run without arguments, lists all the services and their state. If service ID is specified, default action ‘status’ is executed which shows status of a single list service. With actions ‘start’, ‘stop’, ‘restart’, service state is updated respectively.\n\ntalosctl service [<id> [start|stop|restart|status]] [flags]\n\nOptions\n  -h, --help   help for service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl shutdown\n\nShutdown a node\n\ntalosctl shutdown [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n      --force              if true, force a node to shutdown without a cordon/drain\n  -h, --help               help for shutdown\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl stats\n\nGet container stats\n\ntalosctl stats [flags]\n\nOptions\n  -h, --help         help for stats\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl support\n\nDump debug information about the cluster\n\nSynopsis\n\nGenerated bundle contains the following debug information:\n\nFor each node:\n\nKernel logs.\nAll Talos internal services logs.\nAll kube-system pods logs.\nTalos COSI resources without secrets.\nCOSI runtime state graph.\nProcesses snapshot.\nIO pressure snapshot.\nMounts list.\nPCI devices info.\nTalos version.\n\nFor the cluster:\n\nKubernetes nodes and kube-system pods manifests.\ntalosctl support [flags]\n\nOptions\n  -h, --help              help for support\n  -w, --num-workers int   number of workers per node (default 1)\n  -O, --output string     output file to write support archive to\n  -v, --verbose           verbose output\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl time\n\nGets current server time\n\ntalosctl time [--check server] [flags]\n\nOptions\n  -c, --check string   checks server time against specified ntp server\n  -h, --help           help for time\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade\n\nUpgrade Talos on the target node\n\ntalosctl upgrade [flags]\n\nOptions\n      --debug                debug operation from kernel logs. --wait is set to true when this flag is set\n  -f, --force                force the upgrade (skip checks on etcd health and members, might lead to data loss)\n  -h, --help                 help for upgrade\n  -i, --image string         the container image to use for performing the install (default \"ghcr.io/siderolabs/installer:v1.6.0-alpha.2\")\n      --insecure             upgrade using the insecure (encrypted with no auth) maintenance service\n  -p, --preserve             preserve data\n  -m, --reboot-mode string   select the reboot mode during upgrade. Mode \"powercycle\" bypasses kexec. Valid values are: [\"default\" \"powercycle\"]. (default \"default\")\n  -s, --stage                stage the upgrade to perform it after a reboot\n      --timeout duration     time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait                 wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade-k8s\n\nUpgrade Kubernetes control plane in the Talos cluster.\n\nSynopsis\n\nCommand runs upgrade of Kubernetes control plane components between specified versions.\n\ntalosctl upgrade-k8s [flags]\n\nOptions\n      --dry-run           skip the actual upgrade and show the upgrade plan instead\n      --endpoint string   the cluster control plane endpoint\n      --from string       the Kubernetes control plane version to upgrade from\n  -h, --help              help for upgrade-k8s\n      --pre-pull-images   pre-pull images before upgrade (default true)\n      --to string         the Kubernetes control plane version to upgrade to (default \"1.29.0\")\n      --upgrade-kubelet   upgrade kubelet service (default true)\n      --with-docs         patch all machine configs adding the documentation for each field (default true)\n      --with-examples     patch all machine configs with the commented examples (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl usage\n\nRetrieve a disk usage\n\ntalosctl usage [path1] [path2] ... [pathN] [flags]\n\nOptions\n  -a, --all             write counts for all files, not just directories\n  -d, --depth int32     maximum recursion depth\n  -h, --help            help for usage\n  -H, --humanize        humanize size and time in the output\n  -t, --threshold int   threshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl validate\n\nValidate config\n\ntalosctl validate [flags]\n\nOptions\n  -c, --config string   the path of the config file\n  -h, --help            help for validate\n  -m, --mode string     the mode to validate the config for (valid values are metal, cloud, and container)\n      --strict          treat validation warnings as errors\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl version\n\nPrints the version\n\ntalosctl version [flags]\n\nOptions\n      --client     Print client version only\n  -h, --help       help for version\n  -i, --insecure   use Talos maintenance mode API\n      --short      Print the short version\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl\n\nA CLI for out-of-band management of Kubernetes nodes created by Talos\n\nOptions\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -h, --help                 help for talosctl\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl apply-config - Apply a new configuration to a node\ntalosctl bootstrap - Bootstrap the etcd cluster on the specified node.\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl completion - Output shell completion code for the specified shell (bash, fish or zsh)\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl conformance - Run conformance tests\ntalosctl containers - List containers\ntalosctl copy - Copy data out from the node\ntalosctl dashboard - Cluster dashboard with node overview, logs and real-time metrics\ntalosctl disks - Get the list of disks from /sys/block on the machine\ntalosctl dmesg - Retrieve kernel logs\ntalosctl edit - Edit a resource from the default editor.\ntalosctl etcd - Manage etcd\ntalosctl events - Stream runtime events\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl get - Get a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\ntalosctl health - Check cluster health\ntalosctl image - Manage CRI containter images\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inspect - Inspect internals of Talos\ntalosctl kubeconfig - Download the admin kubeconfig from the node\ntalosctl list - Retrieve a directory listing\ntalosctl logs - Retrieve logs for a service\ntalosctl machineconfig - Machine config related commands\ntalosctl memory - Show memory usage\ntalosctl meta - Write and delete keys in the META partition\ntalosctl mounts - List mounts\ntalosctl netstat - Show network connections and sockets\ntalosctl patch - Update field(s) of a resource using a JSON patch.\ntalosctl pcap - Capture the network packets from the node.\ntalosctl processes - List running processes\ntalosctl read - Read a file on the machine\ntalosctl reboot - Reboot a node\ntalosctl reset - Reset a node\ntalosctl restart - Restart a process\ntalosctl rollback - Rollback a node to the previous installation\ntalosctl service - Retrieve the state of a service (or all services), control service state\ntalosctl shutdown - Shutdown a node\ntalosctl stats - Get container stats\ntalosctl support - Dump debug information about the cluster\ntalosctl time - Gets current server time\ntalosctl upgrade - Upgrade Talos on the target node\ntalosctl upgrade-k8s - Upgrade Kubernetes control plane in the Talos cluster.\ntalosctl usage - Retrieve a disk usage\ntalosctl validate - Validate config\ntalosctl version - Prints the version\n3 - Configuration\nTalos Linux machine configuration reference.\n\nTalos Linux machine is fully configured via a single YAML file called machine configuration.\n\nThe file might contain one or more configuration documents separated by --- (three dashes) lines. At the moment, majority of the configuration options are within the v1alpha1 document, so this is the only mandatory document in the configuration file.\n\nConfiguration documents might be named (contain a name: field) or unnamed. Unnamed documents can be supplied to the machine configuration file only once, while named documents can be supplied multiple times with unique names.\n\nThe v1alpha1 document has its own (legacy) structure, while every other document has the following set of fields:\n\napiVersion: v1alpha1 # version of the document\n\nkind: NetworkRuleConfig # type of document\n\nname: rule1 # only for named documents\n\n\nThis section contains the configuration reference, to learn more about Talos Linux machine configuration management, please see:\n\nquick guide to configuration generation\nconfiguration management in production\nconfiguration patches\nediting live machine configuration\n3.1 - network\nPackage network provides network machine configuration documents.\n3.1.1 - NetworkDefaultActionConfig\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: accept # Default action for all not explicitly configured ingress traffic: accept or block.\nField\tType\tDescription\tValue(s)\ningress\tDefaultAction\tDefault action for all not explicitly configured ingress traffic: accept or block.\taccept\nblock\n\n3.1.2 - NetworkRuleConfig\nNetworkRuleConfig is a network firewall rule config document.\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: ingress-apid # Name of the config document.\n\n# Port selector defines which ports and protocols on the host are affected by the rule.\n\nportSelector:\n\n    # Ports defines a list of port ranges or single ports.\n\n    ports:\n\n        - 50000\n\n    protocol: tcp # Protocol defines traffic protocol (e.g. TCP or UDP).\n\n# Ingress defines which source subnets are allowed to access the host ports/protocols defined by the `portSelector`.\n\ningress:\n\n    - subnet: 192.168.0.0/16 # Subnet defines a source subnet.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nportSelector\tRulePortSelector\tPort selector defines which ports and protocols on the host are affected by the rule.\t\ningress\t[]IngressRule\tIngress defines which source subnets are allowed to access the host ports/protocols defined by the portSelector.\t\nportSelector\n\nRulePortSelector is a port selector for the network rule.\n\nField\tType\tDescription\tValue(s)\nports\tPortRanges\t\nPorts defines a list of port ranges or single ports.\nShow example(s)\n\t\nprotocol\tProtocol\tProtocol defines traffic protocol (e.g. TCP or UDP).\ttcp\nudp\nicmp\nicmpv6\n\ningress[]\n\nIngressRule is a ingress rule.\n\nField\tType\tDescription\tValue(s)\nsubnet\tPrefix\tSubnet defines a source subnet.\nShow example(s)\n\t\nexcept\tPrefix\tExcept defines a source subnet to exclude from the rule, it gets excluded from the subnet.\t\n3.2 - runtime\nPackage runtime provides runtime machine configuration documents.\n3.2.1 - EventSinkConfig\nEventSinkConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: EventSinkConfig\n\nendpoint: 192.168.10.3:3247 # The endpoint for the event sink as 'host:port'.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tThe endpoint for the event sink as ‘host:port’.\nShow example(s)\n\t\n3.2.2 - KmsgLogConfig\nKmsgLogConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log # Name of the config document.\n\nurl: tcp://192.168.3.7:3478/ # The URL encodes the log destination.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nurl\tURL\t\nThe URL encodes the log destination.\nShow example(s)\n\t\n3.3 - siderolink\nPackage siderolink provides SideroLink machine configuration documents.\n3.3.1 - SideroLinkConfig\nSideroLinkConfig is a SideroLink connection machine configuration document.\napiVersion: v1alpha1\n\nkind: SideroLinkConfig\n\napiUrl: https://siderolink.api/join?token=secret # SideroLink API URL to connect to.\nField\tType\tDescription\tValue(s)\napiUrl\tURL\tSideroLink API URL to connect to.\nShow example(s)\n\t\n3.4 - v1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\n3.4.1 - Config\nConfig defines the v1alpha1.Config Talos machine configuration document.\nversion: v1alpha1\n\nmachine: # ...\n\ncluster: # ...\nField\tType\tDescription\tValue(s)\nversion\tstring\tIndicates the schema used to decode the contents.\tv1alpha1\n\ndebug\tbool\t\nEnable verbose logging to the console.\n\ttrue\nyes\nfalse\nno\n\nmachine\tMachineConfig\tProvides machine specific configuration options.\t\ncluster\tClusterConfig\tProvides cluster specific configuration options.\t\nmachine\n\nMachineConfig represents the machine-specific config values.\n\nmachine:\n\n    type: controlplane\n\n    # InstallConfig represents the installation options for preparing a node.\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ntype\tstring\t\nDefines the role of the machine within the cluster.\n\tcontrolplane\nworker\n\ntoken\tstring\t\nThe token is used by a machine to join the PKI of the cluster.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe root certificate authority of the PKI.\nShow example(s)\n\t\ncertSANs\t[]string\t\nExtra certificate subject alternative names for the machine’s certificate.\nShow example(s)\n\t\ncontrolPlane\tMachineControlPlaneConfig\tProvides machine specific control plane configuration options.\nShow example(s)\n\t\nkubelet\tKubeletConfig\tUsed to provide additional options to the kubelet.\nShow example(s)\n\t\npods\t[]Unstructured\t\nUsed to provide static pod definitions to be run by the kubelet directly bypassing the kube-apiserver.\nShow example(s)\n\t\nnetwork\tNetworkConfig\tProvides machine specific network configuration options.\nShow example(s)\n\t\ndisks\t[]MachineDisk\t\nUsed to partition, format and mount additional disks.\nShow example(s)\n\t\ninstall\tInstallConfig\t\nUsed to provide instructions for installations.\nShow example(s)\n\t\nfiles\t[]MachineFile\t\nAllows the addition of user specified files.\nShow example(s)\n\t\nenv\tEnv\t\nThe env field allows for the addition of environment variables.\nShow example(s)\n\tGRPC_GO_LOG_VERBOSITY_LEVEL\nGRPC_GO_LOG_SEVERITY_LEVEL\nhttp_proxy\nhttps_proxy\nno_proxy\n\ntime\tTimeConfig\tUsed to configure the machine’s time settings.\nShow example(s)\n\t\nsysctls\tmap[string]string\tUsed to configure the machine’s sysctls.\nShow example(s)\n\t\nsysfs\tmap[string]string\tUsed to configure the machine’s sysfs.\nShow example(s)\n\t\nregistries\tRegistriesConfig\t\nUsed to configure the machine’s container image registry mirrors.\nShow example(s)\n\t\nsystemDiskEncryption\tSystemDiskEncryptionConfig\t\nMachine system disk encryption configuration.\nShow example(s)\n\t\nfeatures\tFeaturesConfig\tFeatures describe individual Talos features that can be switched on or off.\nShow example(s)\n\t\nudev\tUdevConfig\tConfigures the udev system.\nShow example(s)\n\t\nlogging\tLoggingConfig\tConfigures the logging system.\nShow example(s)\n\t\nkernel\tKernelConfig\tConfigures the kernel.\nShow example(s)\n\t\nseccompProfiles\t[]MachineSeccompProfile\tConfigures the seccomp profiles for the machine.\nShow example(s)\n\t\nnodeLabels\tmap[string]string\tConfigures the node labels for the machine.\nShow example(s)\n\t\nnodeTaints\tmap[string]string\tConfigures the node taints for the machine. Effect is optional.\nShow example(s)\n\t\ncontrolPlane\n\nMachineControlPlaneConfig machine specific configuration options.\n\nmachine:\n\n    controlPlane:\n\n        # Controller manager machine specific configuration options.\n\n        controllerManager:\n\n            disabled: false # Disable kube-controller-manager on the node.\n\n        # Scheduler machine specific configuration options.\n\n        scheduler:\n\n            disabled: true # Disable kube-scheduler on the node.\nField\tType\tDescription\tValue(s)\ncontrollerManager\tMachineControllerManagerConfig\tController manager machine specific configuration options.\t\nscheduler\tMachineSchedulerConfig\tScheduler machine specific configuration options.\t\ncontrollerManager\n\nMachineControllerManagerConfig represents the machine specific ControllerManager config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-controller-manager on the node.\t\nscheduler\n\nMachineSchedulerConfig represents the machine specific Scheduler config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-scheduler on the node.\t\nkubelet\n\nKubeletConfig represents the kubelet config values.\n\nmachine:\n\n    kubelet:\n\n        image: ghcr.io/siderolabs/kubelet:v1.29.0 # The `image` field is an optional reference to an alternative kubelet image.\n\n        # The `extraArgs` field is used to provide additional flags to the kubelet.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n\n\n        # # The `ClusterDNS` field is an optional reference to an alternative kubelet clusterDNS ip list.\n\n        # clusterDNS:\n\n        #     - 10.96.0.10\n\n        #     - 169.254.2.53\n\n\n\n        # # The `extraMounts` field is used to add additional mounts to the kubelet container.\n\n        # extraMounts:\n\n        #     - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n        #       type: bind # Type specifies the mount kind.\n\n        #       source: /var/lib/example # Source specifies the source path of the mount.\n\n        #       # Options are fstab style mount options.\n\n        #       options:\n\n        #         - bind\n\n        #         - rshared\n\n        #         - rw\n\n\n\n        # # The `extraConfig` field is used to provide kubelet configuration overrides.\n\n        # extraConfig:\n\n        #     serverTLSBootstrap: true\n\n\n\n        # # The `KubeletCredentialProviderConfig` field is used to provide kubelet credential configuration.\n\n        # credentialProviderConfig:\n\n        #     apiVersion: kubelet.config.k8s.io/v1\n\n        #     kind: CredentialProviderConfig\n\n        #     providers:\n\n        #         - apiVersion: credentialprovider.kubelet.k8s.io/v1\n\n        #           defaultCacheDuration: 12h\n\n        #           matchImages:\n\n        #             - '*.dkr.ecr.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.*.amazonaws.com.cn'\n\n        #             - '*.dkr.ecr-fips.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.us-iso-east-1.c2s.ic.gov'\n\n        #             - '*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov'\n\n        #           name: ecr-credential-provider\n\n\n\n        # # The `nodeIP` field is used to configure `--node-ip` flag for the kubelet.\n\n        # nodeIP:\n\n        #     # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n        #     validSubnets:\n\n        #         - 10.0.0.0/8\n\n        #         - '!10.0.0.3/32'\n\n        #         - fdc7::/16\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe image field is an optional reference to an alternative kubelet image.\nShow example(s)\n\t\nclusterDNS\t[]string\tThe ClusterDNS field is an optional reference to an alternative kubelet clusterDNS ip list.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tThe extraArgs field is used to provide additional flags to the kubelet.\nShow example(s)\n\t\nextraMounts\t[]ExtraMount\t\nThe extraMounts field is used to add additional mounts to the kubelet container.\nShow example(s)\n\t\nextraConfig\tUnstructured\t\nThe extraConfig field is used to provide kubelet configuration overrides.\nShow example(s)\n\t\ncredentialProviderConfig\tUnstructured\tThe KubeletCredentialProviderConfig field is used to provide kubelet credential configuration.\nShow example(s)\n\t\ndefaultRuntimeSeccompProfileEnabled\tbool\tEnable container runtime default Seccomp profile.\ttrue\nyes\nfalse\nno\n\nregisterWithFQDN\tbool\t\nThe registerWithFQDN field is used to force kubelet to use the node FQDN for registration.\n\ttrue\nyes\nfalse\nno\n\nnodeIP\tKubeletNodeIPConfig\t\nThe nodeIP field is used to configure --node-ip flag for the kubelet.\nShow example(s)\n\t\nskipNodeRegistration\tbool\t\nThe skipNodeRegistration is used to run the kubelet without registering with the apiserver.\n\ttrue\nyes\nfalse\nno\n\ndisableManifestsDirectory\tbool\t\nThe disableManifestsDirectory field configures the kubelet to get static pod manifests from the /etc/kubernetes/manifests directory.\n\ttrue\nyes\nfalse\nno\n\nextraMounts[]\n\nExtraMount wraps OCI Mount specification.\n\nmachine:\n\n    kubelet:\n\n        extraMounts:\n\n            - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n              type: bind # Type specifies the mount kind.\n\n              source: /var/lib/example # Source specifies the source path of the mount.\n\n              # Options are fstab style mount options.\n\n              options:\n\n                - bind\n\n                - rshared\n\n                - rw\nField\tType\tDescription\tValue(s)\ndestination\tstring\tDestination is the absolute path where the mount will be placed in the container.\t\ntype\tstring\tType specifies the mount kind.\t\nsource\tstring\tSource specifies the source path of the mount.\t\noptions\t[]string\tOptions are fstab style mount options.\t\nuidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\ngidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\nuidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\ngidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\nnodeIP\n\nKubeletNodeIPConfig represents the kubelet node IP configuration.\n\nmachine:\n\n    kubelet:\n\n        nodeIP:\n\n            # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n            validSubnets:\n\n                - 10.0.0.0/8\n\n                - '!10.0.0.3/32'\n\n                - fdc7::/16\nField\tType\tDescription\tValue(s)\nvalidSubnets\t[]string\t\nThe validSubnets field configures the networks to pick kubelet node IP from.\n\t\nnetwork\n\nNetworkConfig represents the machine’s networking config values.\n\nmachine:\n\n    network:\n\n        hostname: worker-1 # Used to statically set the hostname for the machine.\n\n        # `interfaces` is used to define the network interface configuration.\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\n\n        # Used to statically set the nameservers for the machine.\n\n        nameservers:\n\n            - 9.8.7.6\n\n            - 8.7.6.5\n\n\n\n        # # Allows for extra entries to be added to the `/etc/hosts` file\n\n        # extraHostEntries:\n\n        #     - ip: 192.168.1.100 # The IP of the host.\n\n        #       # The host alias.\n\n        #       aliases:\n\n        #         - example\n\n        #         - example.domain.tld\n\n\n\n        # # Configures KubeSpan feature.\n\n        # kubespan:\n\n        #     enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nhostname\tstring\tUsed to statically set the hostname for the machine.\t\ninterfaces\t[]Device\t\ninterfaces is used to define the network interface configuration.\nShow example(s)\n\t\nnameservers\t[]string\t\nUsed to statically set the nameservers for the machine.\nShow example(s)\n\t\nextraHostEntries\t[]ExtraHost\tAllows for extra entries to be added to the /etc/hosts file\nShow example(s)\n\t\nkubespan\tNetworkKubeSpan\tConfigures KubeSpan feature.\nShow example(s)\n\t\ndisableSearchDomain\tbool\t\nDisable generating a default search domain in /etc/resolv.conf\n\ttrue\nyes\nfalse\nno\n\ninterfaces[]\n\nDevice represents a network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\ninterface\tstring\t\nThe interface name.\nShow example(s)\n\t\ndeviceSelector\tNetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\naddresses\t[]string\t\nAssigns static IP addresses to the interface.\nShow example(s)\n\t\nroutes\t[]Route\t\nA list of routes associated with the interface.\nShow example(s)\n\t\nbond\tBond\tBond specific options.\nShow example(s)\n\t\nbridge\tBridge\tBridge specific options.\nShow example(s)\n\t\nvlans\t[]Vlan\tVLAN specific options.\t\nmtu\tint\t\nThe interface’s MTU.\n\t\ndhcp\tbool\t\nIndicates if DHCP should be used to configure the interface.\nShow example(s)\n\t\nignore\tbool\tIndicates if the interface should be ignored (skips configuration).\t\ndummy\tbool\t\nIndicates if the interface is a dummy interface.\n\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\nShow example(s)\n\t\nwireguard\tDeviceWireguardConfig\t\nWireguard specific configuration.\nShow example(s)\n\t\nvip\tDeviceVIPConfig\tVirtual (shared) IP address configuration.\nShow example(s)\n\t\ndeviceSelector\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                  driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                - network: 10.2.0.0/16 # The route's network (destination).\n\n                  gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nbond\n\nBond contains the various options for configuring a bonded interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                # The interfaces that make up the bond.\n\n                interfaces:\n\n                    - enp2s0\n\n                    - enp2s1\n\n                mode: 802.3ad # A bond option.\n\n                lacpRate: fast # A bond option.\n\n\n\n                # # Picks a network device using the selector.\n\n\n\n                # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n                # deviceSelectors:\n\n                #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                #       driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bond.\t\ndeviceSelectors\t[]NetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\narpIPTarget\t[]string\t\nA bond option.\n\t\nmode\tstring\t\nA bond option.\n\t\nxmitHashPolicy\tstring\t\nA bond option.\n\t\nlacpRate\tstring\t\nA bond option.\n\t\nadActorSystem\tstring\t\nA bond option.\n\t\narpValidate\tstring\t\nA bond option.\n\t\narpAllTargets\tstring\t\nA bond option.\n\t\nprimary\tstring\t\nA bond option.\n\t\nprimaryReselect\tstring\t\nA bond option.\n\t\nfailOverMac\tstring\t\nA bond option.\n\t\nadSelect\tstring\t\nA bond option.\n\t\nmiimon\tuint32\t\nA bond option.\n\t\nupdelay\tuint32\t\nA bond option.\n\t\ndowndelay\tuint32\t\nA bond option.\n\t\narpInterval\tuint32\t\nA bond option.\n\t\nresendIgmp\tuint32\t\nA bond option.\n\t\nminLinks\tuint32\t\nA bond option.\n\t\nlpInterval\tuint32\t\nA bond option.\n\t\npacketsPerSlave\tuint32\t\nA bond option.\n\t\nnumPeerNotif\tuint8\t\nA bond option.\n\t\ntlbDynamicLb\tuint8\t\nA bond option.\n\t\nallSlavesActive\tuint8\t\nA bond option.\n\t\nuseCarrier\tbool\t\nA bond option.\n\t\nadActorSysPrio\tuint16\t\nA bond option.\n\t\nadUserPortKey\tuint16\t\nA bond option.\n\t\npeerNotifyDelay\tuint32\t\nA bond option.\n\t\ndeviceSelectors[]\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                    driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                    - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                      driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nbridge\n\nBridge contains the various options for configuring a bridge interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bridge:\n\n                # The interfaces that make up the bridge.\n\n                interfaces:\n\n                    - enxda4042ca9a51\n\n                    - enxae2a6774c259\n\n                # A bridge option.\n\n                stp:\n\n                    enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bridge.\t\nstp\tSTP\t\nA bridge option.\n\t\nstp\n\nSTP contains the various options for configuring the STP properties of a bridge interface.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tWhether Spanning Tree Protocol (STP) is enabled.\t\nvlans[]\n\nVlan represents vlan settings for a device.\n\nField\tType\tDescription\tValue(s)\naddresses\t[]string\tThe addresses in CIDR notation or as plain IPs to use.\t\nroutes\t[]Route\tA list of routes associated with the VLAN.\t\ndhcp\tbool\tIndicates if DHCP should be used.\t\nvlanId\tuint16\tThe VLAN’s ID.\t\nmtu\tuint32\tThe VLAN’s MTU.\t\nvip\tDeviceVIPConfig\tThe VLAN’s virtual IP address configuration.\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\n\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - routes:\n\n                    - network: 0.0.0.0/0 # The route's network (destination).\n\n                      gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                    - network: 10.2.0.0/16 # The route's network (destination).\n\n                      gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - vip:\n\n                    ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - dhcpOptions:\n\n                    routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - dhcpOptions:\n\n                routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\nwireguard\n\nDeviceWireguardConfig contains settings for configuring Wireguard network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                listenPort: 51111 # Specifies a device's listening port.\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n                      persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nField\tType\tDescription\tValue(s)\nprivateKey\tstring\t\nSpecifies a private key configuration (base64 encoded).\n\t\nlistenPort\tint\tSpecifies a device’s listening port.\t\nfirewallMark\tint\tSpecifies a device’s firewall mark.\t\npeers\t[]DeviceWireguardPeer\tSpecifies a list of peer configurations to apply to a device.\t\npeers[]\n\nDeviceWireguardPeer a WireGuard device peer configuration.\n\nField\tType\tDescription\tValue(s)\npublicKey\tstring\t\nSpecifies the public key of this peer.\n\t\nendpoint\tstring\tSpecifies the endpoint of this peer entry.\t\npersistentKeepaliveInterval\tDuration\t\nSpecifies the persistent keepalive interval for this peer.\n\t\nallowedIPs\t[]string\tAllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vip:\n\n                ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\nextraHostEntries[]\n\nExtraHost represents a host entry in /etc/hosts.\n\nmachine:\n\n    network:\n\n        extraHostEntries:\n\n            - ip: 192.168.1.100 # The IP of the host.\n\n              # The host alias.\n\n              aliases:\n\n                - example\n\n                - example.domain.tld\nField\tType\tDescription\tValue(s)\nip\tstring\tThe IP of the host.\t\naliases\t[]string\tThe host alias.\t\nkubespan\n\nNetworkKubeSpan struct describes KubeSpan configuration.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the KubeSpan feature.\n\t\nadvertiseKubernetesNetworks\tbool\t\nControl whether Kubernetes pod CIDRs are announced over KubeSpan from the node.\n\t\nallowDownPeerBypass\tbool\t\nSkip sending traffic via KubeSpan if the peer connection state is not up.\n\t\nharvestExtraEndpoints\tbool\t\nKubeSpan can collect and publish extra endpoints for each member of the cluster\n\t\nmtu\tuint32\t\nKubeSpan link MTU size.\n\t\nfilters\tKubeSpanFilters\t\nKubeSpan advanced filtering of network addresses .\n\t\nfilters\n\nKubeSpanFilters struct describes KubeSpan advanced network addresses filtering.\n\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nFilter node addresses which will be advertised as KubeSpan endpoints for peer-to-peer Wireguard connections.\nShow example(s)\n\t\ndisks[]\n\nMachineDisk represents the options available for partitioning, formatting, and mounting extra disks.\n\nmachine:\n\n    disks:\n\n        - device: /dev/sdb # The name of the disk to use.\n\n          # A list of partitions to create on the disk.\n\n          partitions:\n\n            - mountpoint: /var/mnt/extra # Where to mount the partition.\n\n\n\n              # # The size of partition: either bytes or human readable representation. If `size:` is omitted, the partition is sized to occupy the full disk.\n\n\n\n              # # Human readable representation.\n\n              # size: 100 MB\n\n              # # Precise value in bytes.\n\n              # size: 1073741824\nField\tType\tDescription\tValue(s)\ndevice\tstring\tThe name of the disk to use.\t\npartitions\t[]DiskPartition\tA list of partitions to create on the disk.\t\npartitions[]\n\nDiskPartition represents the options for a disk partition.\n\nField\tType\tDescription\tValue(s)\nsize\tDiskSize\tThe size of partition: either bytes or human readable representation. If size: is omitted, the partition is sized to occupy the full disk.\nShow example(s)\n\t\nmountpoint\tstring\tWhere to mount the partition.\t\ninstall\n\nInstallConfig represents the installation options for preparing a node.\n\nmachine:\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ndisk\tstring\tThe disk used for installations.\nShow example(s)\n\t\ndiskSelector\tInstallDiskSelector\t\nLook up disk using disk attributes like model, size, serial and others.\nShow example(s)\n\t\nextraKernelArgs\t[]string\t\nAllows for supplying extra kernel args via the bootloader.\nShow example(s)\n\t\nimage\tstring\t\nAllows for supplying the image used to perform the installation.\nShow example(s)\n\t\nextensions\t[]InstallExtensionConfig\tAllows for supplying additional system extension images to install on top of base Talos image.\nShow example(s)\n\t\nwipe\tbool\t\nIndicates if the installation disk should be wiped at installation time.\n\ttrue\nyes\nfalse\nno\n\nlegacyBIOSSupport\tbool\t\nIndicates if MBR partition should be marked as bootable (active).\n\t\ndiskSelector\n\nInstallDiskSelector represents a disk query parameters for the install disk lookup.\n\nmachine:\n\n    install:\n\n        diskSelector:\n\n            size: '>= 1TB' # Disk size.\n\n            model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n\n\n            # # Disk bus path.\n\n            # busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0\n\n            # busPath: /pci0000:00/*\nField\tType\tDescription\tValue(s)\nsize\tInstallDiskSizeMatcher\tDisk size.\nShow example(s)\n\t\nname\tstring\tDisk name /sys/block/<dev>/device/name.\t\nmodel\tstring\tDisk model /sys/block/<dev>/device/model.\t\nserial\tstring\tDisk serial number /sys/block/<dev>/serial.\t\nmodalias\tstring\tDisk modalias /sys/block/<dev>/device/modalias.\t\nuuid\tstring\tDisk UUID /sys/block/<dev>/uuid.\t\nwwid\tstring\tDisk WWID /sys/block/<dev>/wwid.\t\ntype\tInstallDiskType\tDisk Type.\tssd\nhdd\nnvme\nsd\n\nbusPath\tstring\tDisk bus path.\nShow example(s)\n\t\nextensions[]\n\nInstallExtensionConfig represents a configuration for a system extension.\n\nmachine:\n\n    install:\n\n        extensions:\n\n            - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\nimage\tstring\tSystem extension image.\t\nfiles[]\n\nMachineFile represents a file to write to disk.\n\nmachine:\n\n    files:\n\n        - content: '...' # The contents of the file.\n\n          permissions: 0o666 # The file's permissions in octal.\n\n          path: /tmp/file.txt # The path of the file.\n\n          op: append # The operation to use\nField\tType\tDescription\tValue(s)\ncontent\tstring\tThe contents of the file.\t\npermissions\tFileMode\tThe file’s permissions in octal.\t\npath\tstring\tThe path of the file.\t\nop\tstring\tThe operation to use\tcreate\nappend\noverwrite\n\ntime\n\nTimeConfig represents the options for configuring time on a machine.\n\nmachine:\n\n    time:\n\n        disabled: false # Indicates if the time service is disabled for the machine.\n\n        # Specifies time (NTP) servers to use for setting the system time.\n\n        servers:\n\n            - time.cloudflare.com\n\n        bootTimeout: 2m0s # Specifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\t\nIndicates if the time service is disabled for the machine.\n\t\nservers\t[]string\t\nSpecifies time (NTP) servers to use for setting the system time.\n\t\nbootTimeout\tDuration\t\nSpecifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\n\t\nregistries\n\nRegistriesConfig represents the image pull options.\n\nmachine:\n\n    registries:\n\n        # Specifies mirror configuration for each registry host namespace.\n\n        mirrors:\n\n            docker.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.local\n\n        # Specifies TLS & auth configuration for HTTPS image registries.\n\n        config:\n\n            registry.local:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n                # The auth configuration for this registry.\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nmirrors\tmap[string]RegistryMirrorConfig\t\nSpecifies mirror configuration for each registry host namespace.\nShow example(s)\n\t\nconfig\tmap[string]RegistryConfig\t\nSpecifies TLS & auth configuration for HTTPS image registries.\nShow example(s)\n\t\nmirrors.*\n\nRegistryMirrorConfig represents mirror configuration for a registry.\n\nmachine:\n\n    registries:\n\n        mirrors:\n\n            ghcr.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.insecure\n\n                    - https://ghcr.io/v2/\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nList of endpoints (URLs) for registry mirrors to use.\n\t\noverridePath\tbool\t\nUse the exact path specified for the endpoint (don’t append /v2/).\n\t\nconfig.*\n\nRegistryConfig specifies auth & TLS config per registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            registry.insecure:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n\n\n                # # The auth configuration for this registry.\n\n                # auth:\n\n                #     username: username # Optional registry authentication.\n\n                #     password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\ntls\tRegistryTLSConfig\tThe TLS configuration for the registry.\nShow example(s)\n\t\nauth\tRegistryAuthConfig\t\nThe auth configuration for this registry.\nShow example(s)\n\t\ntls\n\nRegistryTLSConfig specifies TLS config for HTTPS registries.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nField\tType\tDescription\tValue(s)\nclientIdentity\tPEMEncodedCertificateAndKey\t\nEnable mutual TLS authentication with the registry.\nShow example(s)\n\t\nca\tBase64Bytes\t\nCA registry certificate to add the list of trusted certificates.\n\t\ninsecureSkipVerify\tbool\tSkip TLS server certificate verification (not recommended).\t\nauth\n\nRegistryAuthConfig specifies authentication configuration for a registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nusername\tstring\t\nOptional registry authentication.\n\t\npassword\tstring\t\nOptional registry authentication.\n\t\nauth\tstring\t\nOptional registry authentication.\n\t\nidentityToken\tstring\t\nOptional registry authentication.\n\t\nsystemDiskEncryption\n\nSystemDiskEncryptionConfig specifies system disk partitions encryption settings.\n\nmachine:\n\n    systemDiskEncryption:\n\n        # Ephemeral partition encryption.\n\n        ephemeral:\n\n            provider: luks2 # Encryption provider to use for the encryption.\n\n            # Defines the encryption keys generation and storage method.\n\n            keys:\n\n                - # Deterministically generated key from the node UUID and PartitionLabel.\n\n                  nodeID: {}\n\n                  slot: 0 # Key slot number for LUKS2 encryption.\n\n\n\n                  # # KMS managed encryption key.\n\n                  # kms:\n\n                  #     endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\n\n\n\n            # # Cipher kind to use for the encryption. Depends on the encryption provider.\n\n            # cipher: aes-xts-plain64\n\n\n\n            # # Defines the encryption sector size.\n\n            # blockSize: 4096\n\n\n\n            # # Additional --perf parameters for the LUKS2 encryption.\n\n            # options:\n\n            #     - no_read_workqueue\n\n            #     - no_write_workqueue\nField\tType\tDescription\tValue(s)\nstate\tEncryptionConfig\tState partition encryption.\t\nephemeral\tEncryptionConfig\tEphemeral partition encryption.\t\nstate\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        state:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nephemeral\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        ephemeral:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nfeatures\n\nFeaturesConfig describes individual Talos features that can be switched on or off.\n\nmachine:\n\n    features:\n\n        rbac: true # Enable role-based access control (RBAC).\n\n\n\n        # # Configure Talos API access from Kubernetes pods.\n\n        # kubernetesTalosAPIAccess:\n\n        #     enabled: true # Enable Talos API access from Kubernetes pods.\n\n        #     # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n        #     allowedRoles:\n\n        #         - os:reader\n\n        #     # The list of Kubernetes namespaces Talos API access is available from.\n\n        #     allowedKubernetesNamespaces:\n\n        #         - kube-system\nField\tType\tDescription\tValue(s)\nrbac\tbool\tEnable role-based access control (RBAC).\t\nstableHostname\tbool\tEnable stable default hostname.\t\nkubernetesTalosAPIAccess\tKubernetesTalosAPIAccessConfig\t\nConfigure Talos API access from Kubernetes pods.\nShow example(s)\n\t\napidCheckExtKeyUsage\tbool\tEnable checks for extended key usage of client certificates in apid.\t\ndiskQuotaSupport\tbool\t\nEnable XFS project quota support for EPHEMERAL partition and user disks.\n\t\nkubePrism\tKubePrism\t\nKubePrism - local proxy/load balancer on defined port that will distribute\n\t\nkubernetesTalosAPIAccess\n\nKubernetesTalosAPIAccessConfig describes the configuration for the Talos API access from Kubernetes pods.\n\nmachine:\n\n    features:\n\n        kubernetesTalosAPIAccess:\n\n            enabled: true # Enable Talos API access from Kubernetes pods.\n\n            # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n            allowedRoles:\n\n                - os:reader\n\n            # The list of Kubernetes namespaces Talos API access is available from.\n\n            allowedKubernetesNamespaces:\n\n                - kube-system\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable Talos API access from Kubernetes pods.\t\nallowedRoles\t[]string\t\nThe list of Talos API roles which can be granted for access from Kubernetes pods.\n\t\nallowedKubernetesNamespaces\t[]string\tThe list of Kubernetes namespaces Talos API access is available from.\t\nkubePrism\n\nKubePrism describes the configuration for the KubePrism load balancer.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable KubePrism support - will start local load balacing proxy.\t\nport\tint\tKubePrism port.\t\nudev\n\nUdevConfig describes how the udev system should be configured.\n\nmachine:\n\n    udev:\n\n        # List of udev rules to apply to the udev system\n\n        rules:\n\n            - SUBSYSTEM==\"drm\", KERNEL==\"renderD*\", GROUP=\"44\", MODE=\"0660\"\nField\tType\tDescription\tValue(s)\nrules\t[]string\tList of udev rules to apply to the udev system\t\nlogging\n\nLoggingConfig struct configures Talos logging.\n\nmachine:\n\n    logging:\n\n        # Logging destination.\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345 # Where to send logs. Supported protocols are \"tcp\" and \"udp\".\n\n              format: json_lines # Logs format.\nField\tType\tDescription\tValue(s)\ndestinations\t[]LoggingDestination\tLogging destination.\t\ndestinations[]\n\nLoggingDestination struct configures Talos logging destination.\n\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\tWhere to send logs. Supported protocols are “tcp” and “udp”.\nShow example(s)\n\t\nformat\tstring\tLogs format.\tjson_lines\n\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://1.2.3.4:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://cluster1.internal:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: udp://127.0.0.1:12345\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nkernel\n\nKernelConfig struct configures Talos Linux kernel.\n\nmachine:\n\n    kernel:\n\n        # Kernel modules to load.\n\n        modules:\n\n            - name: brtfs # Module name.\nField\tType\tDescription\tValue(s)\nmodules\t[]KernelModuleConfig\tKernel modules to load.\t\nmodules[]\n\nKernelModuleConfig struct configures Linux kernel modules to load.\n\nField\tType\tDescription\tValue(s)\nname\tstring\tModule name.\t\nparameters\t[]string\tModule parameters, changes applied after reboot.\t\nseccompProfiles[]\n\nMachineSeccompProfile defines seccomp profiles for the machine.\n\nmachine:\n\n    seccompProfiles:\n\n        - name: audit.json # The `name` field is used to provide the file name of the seccomp profile.\n\n          # The `value` field is used to provide the seccomp profile.\n\n          value:\n\n            defaultAction: SCMP_ACT_LOG\nField\tType\tDescription\tValue(s)\nname\tstring\tThe name field is used to provide the file name of the seccomp profile.\t\nvalue\tUnstructured\tThe value field is used to provide the seccomp profile.\t\ncluster\n\nClusterConfig represents the cluster-wide config values.\n\ncluster:\n\n    # ControlPlaneConfig represents the control plane configuration options.\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\n\n    clusterName: talos.local\n\n    # ClusterNetworkConfig represents kube networking configuration options.\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\nid\tstring\tGlobally unique identifier for this cluster (base64 encoded random 32 bytes).\t\nsecret\tstring\t\nShared secret of cluster (base64 encoded random 32 bytes).\n\t\ncontrolPlane\tControlPlaneConfig\tProvides control plane specific configuration options.\nShow example(s)\n\t\nclusterName\tstring\tConfigures the cluster’s name.\t\nnetwork\tClusterNetworkConfig\tProvides cluster specific network configuration options.\nShow example(s)\n\t\ntoken\tstring\tThe bootstrap token used to join the cluster.\nShow example(s)\n\t\naescbcEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nsecretboxEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\tThe base64 encoded root certificate authority used by Kubernetes.\nShow example(s)\n\t\naggregatorCA\tPEMEncodedCertificateAndKey\t\nThe base64 encoded aggregator certificate authority used by Kubernetes for front-proxy certificate generation.\nShow example(s)\n\t\nserviceAccount\tPEMEncodedKey\tThe base64 encoded private key for service account token generation.\nShow example(s)\n\t\napiServer\tAPIServerConfig\tAPI server specific configuration options.\nShow example(s)\n\t\ncontrollerManager\tControllerManagerConfig\tController manager server specific configuration options.\nShow example(s)\n\t\nproxy\tProxyConfig\tKube-proxy server-specific configuration options\nShow example(s)\n\t\nscheduler\tSchedulerConfig\tScheduler server specific configuration options.\nShow example(s)\n\t\ndiscovery\tClusterDiscoveryConfig\tConfigures cluster member discovery.\nShow example(s)\n\t\netcd\tEtcdConfig\tEtcd specific configuration options.\nShow example(s)\n\t\ncoreDNS\tCoreDNS\tCore DNS specific configuration options.\nShow example(s)\n\t\nexternalCloudProvider\tExternalCloudProviderConfig\tExternal cloud provider configuration.\nShow example(s)\n\t\nextraManifests\t[]string\t\nA list of urls that point to additional manifests.\nShow example(s)\n\t\nextraManifestHeaders\tmap[string]string\tA map of key value pairs that will be added while fetching the extraManifests.\nShow example(s)\n\t\ninlineManifests\t[]ClusterInlineManifest\t\nA list of inline Kubernetes manifests.\nShow example(s)\n\t\nadminKubeconfig\tAdminKubeconfigConfig\t\nSettings for admin kubeconfig generation.\nShow example(s)\n\t\nallowSchedulingOnControlPlanes\tbool\tAllows running workload on control-plane nodes.\nShow example(s)\n\ttrue\nyes\nfalse\nno\n\ncontrolPlane\n\nControlPlaneConfig represents the control plane configuration options.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\t\nEndpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\nShow example(s)\n\t\nlocalAPIServerPort\tint\t\nThe port that the API server listens on internally.\n\t\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: https://cluster1.internal:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: udp://127.0.0.1:12345\ncluster:\n\n    controlPlane:\n\n        endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nnetwork\n\nClusterNetworkConfig represents kube networking configuration options.\n\ncluster:\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\ncni\tCNIConfig\t\nThe CNI used.\nShow example(s)\n\t\ndnsDomain\tstring\t\nThe domain used by Kubernetes DNS.\nShow example(s)\n\t\npodSubnets\t[]string\tThe pod subnet CIDR.\nShow example(s)\n\t\nserviceSubnets\t[]string\tThe service subnet CIDR.\nShow example(s)\n\t\ncni\n\nCNIConfig represents the CNI configuration options.\n\ncluster:\n\n    network:\n\n        cni:\n\n            name: custom # Name of CNI to use.\n\n            # URLs containing manifests to apply for the CNI.\n\n            urls:\n\n                - https://docs.projectcalico.org/archive/v3.20/manifests/canal.yaml\nField\tType\tDescription\tValue(s)\nname\tstring\tName of CNI to use.\tflannel\ncustom\nnone\n\nurls\t[]string\t\nURLs containing manifests to apply for the CNI.\n\t\nflannel\tFlannelCNIConfig\t\ndescription:\n\tFlannel configuration options.\n\nflannel\n\nFlannelCNIConfig represents the Flannel CNI configuration options.\n\nField\tType\tDescription\tValue(s)\nextraArgs\t[]string\tExtra arguments for ‘flanneld’.\nShow example(s)\n\t\napiServer\n\nAPIServerConfig represents the kube apiserver configuration options.\n\ncluster:\n\n    apiServer:\n\n        image: registry.k8s.io/kube-apiserver:v1.29.0 # The container image used in the API server manifest.\n\n        # Extra arguments to supply to the API server.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n            http2-max-streams-per-connection: \"32\"\n\n        # Extra certificate subject alternative names for the API server's certificate.\n\n        certSANs:\n\n            - 1.2.3.4\n\n            - 4.5.6.7\n\n\n\n        # # Configure the API server admission plugins.\n\n        # admissionControl:\n\n        #     - name: PodSecurity # Name is the name of the admission controller.\n\n        #       # Configuration is an embedded configuration object to be used as the plugin's\n\n        #       configuration:\n\n        #         apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n        #         defaults:\n\n        #             audit: restricted\n\n        #             audit-version: latest\n\n        #             enforce: baseline\n\n        #             enforce-version: latest\n\n        #             warn: restricted\n\n        #             warn-version: latest\n\n        #         exemptions:\n\n        #             namespaces:\n\n        #                 - kube-system\n\n        #             runtimeClasses: []\n\n        #             usernames: []\n\n        #         kind: PodSecurityConfiguration\n\n\n\n        # # Configure the API server audit policy.\n\n        # auditPolicy:\n\n        #     apiVersion: audit.k8s.io/v1\n\n        #     kind: Policy\n\n        #     rules:\n\n        #         - level: Metadata\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the API server manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the API server.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the API server static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\ncertSANs\t[]string\tExtra certificate subject alternative names for the API server’s certificate.\t\ndisablePodSecurityPolicy\tbool\tDisable PodSecurityPolicy in the API server and default manifests.\t\nadmissionControl\t[]AdmissionPluginConfig\tConfigure the API server admission plugins.\nShow example(s)\n\t\nauditPolicy\tUnstructured\tConfigure the API server audit policy.\nShow example(s)\n\t\nresources\tResourcesConfig\tConfigure the API server resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nadmissionControl[]\n\nAdmissionPluginConfig represents the API server admission plugin configuration.\n\ncluster:\n\n    apiServer:\n\n        admissionControl:\n\n            - name: PodSecurity # Name is the name of the admission controller.\n\n              # Configuration is an embedded configuration object to be used as the plugin's\n\n              configuration:\n\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n                defaults:\n\n                    audit: restricted\n\n                    audit-version: latest\n\n                    enforce: baseline\n\n                    enforce-version: latest\n\n                    warn: restricted\n\n                    warn-version: latest\n\n                exemptions:\n\n                    namespaces:\n\n                        - kube-system\n\n                    runtimeClasses: []\n\n                    usernames: []\n\n                kind: PodSecurityConfiguration\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName is the name of the admission controller.\n\t\nconfiguration\tUnstructured\t\nConfiguration is an embedded configuration object to be used as the plugin’s\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ncontrollerManager\n\nControllerManagerConfig represents the kube controller manager configuration options.\n\ncluster:\n\n    controllerManager:\n\n        image: registry.k8s.io/kube-controller-manager:v1.29.0 # The container image used in the controller manager manifest.\n\n        # Extra arguments to supply to the controller manager.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the controller manager manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the controller manager.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the controller manager static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the controller manager resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\nproxy\n\nProxyConfig represents the kube proxy configuration options.\n\ncluster:\n\n    proxy:\n\n        image: registry.k8s.io/kube-proxy:v1.29.0 # The container image used in the kube-proxy manifest.\n\n        mode: ipvs # proxy mode of kube-proxy.\n\n        # Extra arguments to supply to kube-proxy.\n\n        extraArgs:\n\n            proxy-mode: iptables\n\n\n\n        # # Disable kube-proxy deployment on cluster bootstrap.\n\n        # disabled: false\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-proxy deployment on cluster bootstrap.\nShow example(s)\n\t\nimage\tstring\tThe container image used in the kube-proxy manifest.\nShow example(s)\n\t\nmode\tstring\t\nproxy mode of kube-proxy.\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to kube-proxy.\t\nscheduler\n\nSchedulerConfig represents the kube scheduler configuration options.\n\ncluster:\n\n    scheduler:\n\n        image: registry.k8s.io/kube-scheduler:v1.29.0 # The container image used in the scheduler manifest.\n\n        # Extra arguments to supply to the scheduler.\n\n        extraArgs:\n\n            feature-gates: AllBeta=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the scheduler manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the scheduler.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the scheduler static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the scheduler resources.\t\nconfig\tUnstructured\tSpecify custom kube-scheduler configuration.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ndiscovery\n\nClusterDiscoveryConfig struct configures cluster membership discovery.\n\ncluster:\n\n    discovery:\n\n        enabled: true # Enable the cluster membership discovery feature.\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            # Kubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\n            kubernetes: {}\n\n            # Service registry is using an external service to push and pull information about cluster members.\n\n            service:\n\n                endpoint: https://discovery.talos.dev/ # External service endpoint.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the cluster membership discovery feature.\n\t\nregistries\tDiscoveryRegistriesConfig\tConfigure registries used for cluster member discovery.\t\nregistries\n\nDiscoveryRegistriesConfig struct configures cluster membership discovery.\n\nField\tType\tDescription\tValue(s)\nkubernetes\tRegistryKubernetesConfig\t\nKubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\t\nservice\tRegistryServiceConfig\tService registry is using an external service to push and pull information about cluster members.\t\nkubernetes\n\nRegistryKubernetesConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable Kubernetes discovery registry.\t\nservice\n\nRegistryServiceConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable external service discovery registry.\t\nendpoint\tstring\tExternal service endpoint.\nShow example(s)\n\t\netcd\n\nEtcdConfig represents the etcd configuration options.\n\ncluster:\n\n    etcd:\n\n        image: gcr.io/etcd-development/etcd:v3.5.11 # The container image used to create the etcd service.\n\n        # The `ca` is the root certificate authority of the PKI.\n\n        ca:\n\n            crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n            key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n        # Extra arguments to supply to etcd.\n\n        extraArgs:\n\n            election-timeout: \"5000\"\n\n\n\n        # # The `advertisedSubnets` field configures the networks to pick etcd advertised IP from.\n\n        # advertisedSubnets:\n\n        #     - 10.0.0.0/8\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used to create the etcd service.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe ca is the root certificate authority of the PKI.\nShow example(s)\n\t\nextraArgs\tmap[string]string\t\nExtra arguments to supply to etcd.\n\t\nadvertisedSubnets\t[]string\t\nThe advertisedSubnets field configures the networks to pick etcd advertised IP from.\nShow example(s)\n\t\nlistenSubnets\t[]string\t\nThe listenSubnets field configures the networks for the etcd to listen for peer and client connections.\n\t\ncoreDNS\n\nCoreDNS represents the CoreDNS config values.\n\ncluster:\n\n    coreDNS:\n\n        image: registry.k8s.io/coredns/coredns:v1.11.1 # The `image` field is an override to the default coredns image.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable coredns deployment on cluster bootstrap.\t\nimage\tstring\tThe image field is an override to the default coredns image.\t\nexternalCloudProvider\n\nExternalCloudProviderConfig contains external cloud provider configuration.\n\ncluster:\n\n    externalCloudProvider:\n\n        enabled: true # Enable external cloud provider.\n\n        # A list of urls that point to additional manifests for an external cloud provider.\n\n        manifests:\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/rbac.yaml\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/aws-cloud-controller-manager-daemonset.yaml\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable external cloud provider.\ttrue\nyes\nfalse\nno\n\nmanifests\t[]string\t\nA list of urls that point to additional manifests for an external cloud provider.\nShow example(s)\n\t\ninlineManifests[]\n\nClusterInlineManifest struct describes inline bootstrap manifests for the user.\n\ncluster:\n\n    inlineManifests:\n\n        - name: namespace-ci # Name of the manifest.\n\n          contents: |- # Manifest contents as a string.\n\n            apiVersion: v1\n\n            kind: Namespace\n\n            metadata:\n\n            \tname: ci\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName of the manifest.\nShow example(s)\n\t\ncontents\tstring\tManifest contents as a string.\nShow example(s)\n\t\nadminKubeconfig\n\nAdminKubeconfigConfig contains admin kubeconfig settings.\n\ncluster:\n\n    adminKubeconfig:\n\n        certLifetime: 1h0m0s # Admin kubeconfig certificate lifetime (default is 1 year).\nField\tType\tDescription\tValue(s)\ncertLifetime\tDuration\t\nAdmin kubeconfig certificate lifetime (default is 1 year).\n\t\n4 - Kernel\nLinux kernel reference.\nCommandline Parameters\n\nTalos supports a number of kernel commandline parameters. Some are required for it to operate. Others are optional and useful in certain circumstances.\n\nSeveral of these are enforced by the Kernel Self Protection Project KSPP.\n\nRequired parameters:\n\ntalos.platform: can be one of aws, azure, container, digitalocean, equinixMetal, gcp, hcloud, metal, nocloud, openstack, oracle, scaleway, upcloud, vmware or vultr\nslab_nomerge: required by KSPP\npti=on: required by KSPP\n\nRecommended parameters:\n\ninit_on_alloc=1: advised by KSPP, enabled by default in kernel config\ninit_on_free=1: advised by KSPP, enabled by default in kernel config\nAvailable Talos-specific parameters\nip\n\nInitial configuration of the interface, routes, DNS, NTP servers (multiple ip= kernel parameters are accepted).\n\nFull documentation is available in the Linux kernel docs.\n\nip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>\n\nTalos will use the configuration supplied via the kernel parameter as the initial network configuration. This parameter is useful in the environments where DHCP doesn’t provide IP addresses or when default DNS and NTP servers should be overridden before loading machine configuration. Partial configuration can be applied as well, e.g. ip=:::::::<dns0-ip>:<dns1-ip>:<ntp0-ip> sets only the DNS and NTP servers.\n\nIPv6 addresses can be specified by enclosing them in the square brackets, e.g. ip=[2001:db8::a]:[2001:db8::b]:[fe80::1]::controlplane1:eth1::[2001:4860:4860::6464]:[2001:4860:4860::64]:[2001:4860:4806::].\n\n<netmask> can use either an IP address notation (IPv4: 255.255.255.0, IPv6: [ffff:ffff:ffff:ffff::0]), or simply a number of one bits in the netmask (24).\n\n<device> can be traditional interface naming scheme eth0, eth1 or enx<MAC>, example: enx78e7d1ea46da\n\nDHCP can be enabled by setting <autoconf> to dhcp, example: ip=:::::eth0.3:dhcp. Alternative syntax is ip=eth0.3:dhcp.\n\nbond\n\nBond interface configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nbond=<bondname>:<bondslaves>:<options>:<mtu>\n\nTalos will use the bond= kernel parameter if supplied to set the initial bond configuration. This parameter is useful in environments where the switch ports are suspended if the machine doesn’t setup a LACP bond.\n\nIf only the bond name is supplied, the bond will be created with eth0 and eth1 as slaves and bond mode set as balance-rr\n\nAll these below configurations are equivalent:\n\nbond=bond0\nbond=bond0:\nbond=bond0::\nbond=bond0:::\nbond=bond0:eth0,eth1\nbond=bond0:eth0,eth1:balance-rr\n\nAn example of a bond configuration with all options specified:\n\nbond=bond1:eth3,eth4:mode=802.3ad,xmit_hash_policy=layer2+3:1450\n\nThis will create a bond interface named bond1 with eth3 and eth4 as slaves and set the bond mode to 802.3ad, the transmit hash policy to layer2+3 and bond interface MTU to 1450.\n\nvlan\n\nThe interface vlan configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nTalos will use the vlan= kernel parameter if supplied to set the initial vlan configuration. This parameter is useful in environments where the switch ports are VLAN tagged with no native VLAN.\n\nOnly one vlan can be configured at this stage.\n\nAn example of a vlan configuration including static ip configuration:\n\nvlan=eth0.100:eth0 ip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\nThis will create a vlan interface named eth0.100 with eth0 as the underlying interface and set the vlan id to 100 with static IP 172.20.0.2/24 and 172.20.0.1 as default gateway.\n\nnet.ifnames=0\n\nDisable the predictable network interface names by specifying net.ifnames=0 on the kernel command line.\n\npanic\n\nThe amount of time to wait after a panic before a reboot is issued.\n\nTalos will always reboot if it encounters an unrecoverable error. However, when collecting debug information, it may reboot too quickly for humans to read the logs. This option allows the user to delay the reboot to give time to collect debug information from the console screen.\n\nA value of 0 disables automatic rebooting entirely.\n\ntalos.config\n\nThe URL at which the machine configuration data may be found (only for metal platform, with the kernel parameter talos.platform=metal).\n\nThis parameter supports variable substitution inside URL query values for the following case-insensitive placeholders:\n\n${uuid} the SMBIOS UUID\n${serial} the SMBIOS Serial Number\n${mac} the MAC address of the first network interface attaining link state up\n${hostname} the hostname of the machine\n\nThe following example\n\nhttp://example.com/metadata?h=${hostname}&m=${mac}&s=${serial}&u=${uuid}\n\nmay translate to\n\nhttp://example.com/metadata?h=myTestHostname&m=52%3A2f%3Afd%3Adf%3Afc%3Ac0&s=0OCZJ19N65&u=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nFor backwards compatibility we insert the system UUID into the query parameter uuid if its value is empty. As in http://example.com/metadata?uuid= => http://example.com/metadata?uuid=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nmetal-iso\n\nWhen the kernel parameter talos.config=metal-iso is set, Talos will attempt to load the machine configuration from any block device with a filesystem label of metal-iso. Talos will look for a file named config.yaml in the root of the filesystem.\n\nFor example, such ISO filesystem can be created with:\n\nmkdir iso/\n\ncp config.yaml iso/\n\nmkisofs -joliet -rock -volid 'metal-iso' -output config.iso iso/\n\ntalos.config.auth.*\n\nKernel parameters prefixed with talos.config.auth. are used to configure OAuth2 authentication for the machine configuration.\n\ntalos.platform\n\nThe platform name on which Talos will run.\n\nValid options are:\n\naws\nazure\ncontainer\ndigitalocean\nequinixMetal\ngcp\nhcloud\nmetal\nnocloud\nopenstack\noracle\nscaleway\nupcloud\nvmware\nvultr\ntalos.board\n\nThe board name, if Talos is being used on an ARM64 SBC.\n\nSupported boards are:\n\nbananapi_m64: Banana Pi M64\nlibretech_all_h3_cc_h5: Libre Computer ALL-H3-CC\nrock64: Pine64 Rock64\n…\ntalos.hostname\n\nThe hostname to be used. The hostname is generally specified in the machine config. However, in some cases, the DHCP server needs to know the hostname before the machine configuration has been acquired.\n\nUnless specifically required, the machine configuration should be used instead.\n\ntalos.shutdown\n\nThe type of shutdown to use when Talos is told to shutdown.\n\nValid options are:\n\nhalt\npoweroff\ntalos.network.interface.ignore\n\nA network interface which should be ignored and not configured by Talos.\n\nBefore a configuration is applied (early on each boot), Talos attempts to configure each network interface by DHCP. If there are many network interfaces on the machine which have link but no DHCP server, this can add significant boot delays.\n\nThis option may be specified multiple times for multiple network interfaces.\n\ntalos.experimental.wipe\n\nResets the disk before starting up the system.\n\nValid options are:\n\nsystem resets system disk.\nsystem:EPHEMERAL,STATE resets ephemeral and state partitions. Doing this reverts Talos into maintenance mode.\ntalos.unified_cgroup_hierarchy\n\nTalos defaults to always using the unified cgroup hierarchy (cgroupsv2), but cgroupsv1 can be forced with talos.unified_cgroup_hierarchy=0.\n\nNote: cgroupsv1 is deprecated and it should be used only for compatibility with workloads which don’t support cgroupsv2 yet.\n\ntalos.dashboard.disabled\n\nBy default, Talos redirects kernel logs to virtual console /dev/tty1 and starts the dashboard on /dev/tty2, then switches to the dashboard tty.\n\nIf you set talos.dashboard.disabled=1, this behavior will be disabled. Kernel logs will be sent to the currently active console and the dashboard will not be started.\n\nIt is set to be 1 by default on SBCs.\n\ntalos.environment\n\nEach value of the argument sets a default environment variable. The expected format is key=value.\n\nExample:\n\ntalos.environment=http_proxy=http://proxy.example.com:8080 talos.environment=https_proxy=http://proxy.example.com:8080\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "siderolink | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/siderolink/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nsiderolink\nPackage siderolink provides SideroLink machine configuration documents.\n1: SideroLinkConfig\n1 - SideroLinkConfig\nSideroLinkConfig is a SideroLink connection machine configuration document.\napiVersion: v1alpha1\n\nkind: SideroLinkConfig\n\napiUrl: https://siderolink.api/join?token=secret # SideroLink API URL to connect to.\nField\tType\tDescription\tValue(s)\napiUrl\tURL\tSideroLink API URL to connect to.\nShow example(s)\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Welcome | Talos Linux",
    "url": "https://www.talos.dev/v1.6/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nWelcome\n1: Introduction\n1.1: What is Talos?\n1.2: Quickstart\n1.3: Getting Started\n1.4: Production Clusters\n1.5: System Requirements\n1.6: What's New in Talos 1.6.0\n1.7: Support Matrix\n1.8: Troubleshooting\n2: Talos Linux Guides\n2.1: Installation\n2.1.1: Bare Metal Platforms\n2.1.1.1: Digital Rebar\n2.1.1.2: Equinix Metal\n2.1.1.3: ISO\n2.1.1.4: Matchbox\n2.1.1.5: Network Configuration\n2.1.1.6: PXE\n2.1.1.7: SecureBoot\n2.1.2: Virtualized Platforms\n2.1.2.1: Hyper-V\n2.1.2.2: KVM\n2.1.2.3: Proxmox\n2.1.2.4: Vagrant & Libvirt\n2.1.2.5: VMware\n2.1.2.6: Xen\n2.1.3: Cloud Platforms\n2.1.3.1: AWS\n2.1.3.2: Azure\n2.1.3.3: DigitalOcean\n2.1.3.4: Exoscale\n2.1.3.5: GCP\n2.1.3.6: Hetzner\n2.1.3.7: Nocloud\n2.1.3.8: Openstack\n2.1.3.9: Oracle\n2.1.3.10: Scaleway\n2.1.3.11: UpCloud\n2.1.3.12: Vultr\n2.1.4: Local Platforms\n2.1.4.1: Docker\n2.1.4.2: QEMU\n2.1.4.3: VirtualBox\n2.1.5: Single Board Computers\n2.1.5.1: Banana Pi M64\n2.1.5.2: Friendlyelec Nano PI R4S\n2.1.5.3: Jetson Nano\n2.1.5.4: Libre Computer Board ALL-H3-CC\n2.1.5.5: Pine64\n2.1.5.6: Pine64 Rock64\n2.1.5.7: Radxa ROCK PI 4\n2.1.5.8: Radxa ROCK PI 4C\n2.1.5.9: Raspberry Pi Series\n2.1.6: Boot Assets\n2.1.7: Omni SaaS\n2.2: Configuration\n2.2.1: Configuration Patches\n2.2.2: Containerd\n2.2.3: Custom Certificate Authorities\n2.2.4: Disk Encryption\n2.2.5: Editing Machine Configuration\n2.2.6: Logging\n2.2.7: Managing Talos PKI\n2.2.8: NVIDIA Fabric Manager\n2.2.9: NVIDIA GPU (OSS drivers)\n2.2.10: NVIDIA GPU (Proprietary drivers)\n2.2.11: Pull Through Image Cache\n2.2.12: Role-based access control (RBAC)\n2.2.13: System Extensions\n2.3: How Tos\n2.3.1: How to enable workers on your control plane nodes\n2.3.2: How to manage certificate lifetimes with Talos Linux\n2.3.3: How to scale down a Talos cluster\n2.3.4: How to scale up a Talos cluster\n2.4: Network\n2.4.1: Corporate Proxies\n2.4.2: Ingress Firewall\n2.4.3: KubeSpan\n2.4.4: Network Device Selector\n2.4.5: Predictable Interface Names\n2.4.6: Virtual (shared) IP\n2.4.7: Wireguard Network\n2.5: Discovery Service\n2.6: Interactive Dashboard\n2.7: Resetting a Machine\n2.8: Upgrading Talos Linux\n3: Kubernetes Guides\n3.1: Configuration\n3.1.1: Ceph Storage cluster with Rook\n3.1.2: Deploying Metrics Server\n3.1.3: iSCSI Storage with Synology CSI\n3.1.4: KubePrism\n3.1.5: Local Storage\n3.1.6: Pod Security\n3.1.7: Replicated Local Storage\n3.1.8: Seccomp Profiles\n3.1.9: Storage\n3.2: Network\n3.2.1: Deploying Cilium CNI\n3.3: Upgrading Kubernetes\n4: Advanced Guides\n4.1: Advanced Networking\n4.2: Air-gapped Environments\n4.3: Building Custom Talos Images\n4.4: Customizing the Kernel\n4.5: Customizing the Root Filesystem\n4.6: Developing Talos\n4.7: Disaster Recovery\n4.8: etcd Maintenance\n4.9: Extension Services\n4.10: Machine Configuration OAuth2 Authentication\n4.11: Metal Network Configuration\n4.12: Migrating from Kubeadm\n4.13: Proprietary Kernel Modules\n4.14: Static Pods\n4.15: Talos API access from Kubernetes\n4.16: Verifying Images\n5: Reference\n5.1: API\n5.2: CLI\n5.3: Configuration\n5.3.1: network\n5.3.1.1: NetworkDefaultActionConfig\n5.3.1.2: NetworkRuleConfig\n5.3.2: runtime\n5.3.2.1: EventSinkConfig\n5.3.2.2: KmsgLogConfig\n5.3.3: siderolink\n5.3.3.1: SideroLinkConfig\n5.3.4: v1alpha1\n5.3.4.1: Config\n5.4: Kernel\n6: Learn More\n6.1: Philosophy\n6.2: Architecture\n6.3: Components\n6.4: Control Plane\n6.5: Image Factory\n6.6: Controllers and Resources\n6.7: Networking Resources\n6.8: Network Connectivity\n6.9: KubeSpan\n6.10: Process Capabilities\n6.11: talosctl\n6.12: FAQs\n6.13: Knowledge Base\nWelcome\n\nWelcome to the Talos documentation. If you are just getting familiar with Talos, we recommend starting here:\n\nWhat is Talos: a quick description of Talos\nQuickstart: the fastest way to get a Talos cluster up and running\nGetting Started: a long-form, guided tour of getting a full Talos cluster deployed\nOpen Source\nCommunity\nGitHub: repo\nSupport: Questions, bugs, feature requests GitHub Discussions\nCommunity Slack: Join our slack channel\nMatrix: Join our Matrix channels:\nCommunity: #talos:matrix.org\nCommunity Support: #talos-support:matrix.org\nForum: community\nTwitter: @SideroLabs\nEmail: info@SideroLabs.com\n\nIf you’re interested in this project and would like to help in engineering efforts, or have general usage questions, we are happy to have you! We hold a weekly meeting that all audiences are welcome to attend.\n\nWe would appreciate your feedback so that we can make Talos even better! To do so, you can take our survey.\n\nOffice Hours\nWhen: Mondays at 16:30 UTC.\nWhere: Google Meet.\n\nYou can subscribe to this meeting by joining the community forum above.\n\nEnterprise\n\nIf you are using Talos in a production setting, and need consulting services to get started or to integrate Talos into your existing environment, we can help. Sidero Labs, Inc. offers support contracts with SLA (Service Level Agreement)-bound terms for mission-critical environments.\n\nLearn More\n\n1 - Introduction\n1.1 - What is Talos?\nA quick introduction in to what Talos is and why it should be used.\n\nTalos is a container optimized Linux distro; a reimagining of Linux for distributed systems such as Kubernetes. Designed to be as minimal as possible while still maintaining practicality. For these reasons, Talos has a number of features unique to it:\n\nit is immutable\nit is atomic\nit is ephemeral\nit is minimal\nit is secure by default\nit is managed via a single declarative configuration file and gRPC API\n\nTalos can be deployed on container, cloud, virtualized, and bare metal platforms.\n\nWhy Talos\n\nIn having less, Talos offers more. Security. Efficiency. Resiliency. Consistency.\n\nAll of these areas are improved simply by having less.\n\n1.2 - Quickstart\nA short guide on setting up a simple Talos Linux cluster locally with Docker.\nLocal Docker Cluster\n\nThe easiest way to try Talos is by using the CLI (talosctl) to create a cluster on a machine with docker installed.\n\nPrerequisites\ntalosctl\n\nDownload talosctl:\n\ncurl -sL https://talos.dev/install | sh\n\nkubectl\n\nDownload kubectl via one of methods outlined in the documentation.\n\nCreate the Cluster\n\nNow run the following:\n\ntalosctl cluster create\n\n\nYou can explore using Talos API commands:\n\ntalosctl dashboard --nodes 10.5.0.2\n\n\nVerify that you can reach Kubernetes:\n\n$ kubectl get nodes -o wide\n\nNAME                     STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-default-controlplane-1   Ready    master   115s   v1.29.0   10.5.0.2      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\ntalos-default-worker-1   Ready    <none>   115s   v1.29.0   10.5.0.3      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\nDestroy the Cluster\n\nWhen you are all done, remove the cluster:\n\ntalosctl cluster destroy\n\n1.3 - Getting Started\nA guide to setting up a Talos Linux cluster.\n\nThis document will walk you through installing a simple Talos Cluster with a single control plane node and one or more worker nodes, explaining some of the concepts.\n\nIf this is your first use of Talos Linux, we recommend the Quickstart first, to quickly create a local virtual cluster in containers on your workstation.\n\nFor a production cluster, extra steps are needed - see Production Notes.\n\nRegardless of where you run Talos, the steps to create a Kubernetes cluster are:\n\nboot machines off the Talos Linux image\ndefine the endpoint for the Kubernetes API and generate your machine configurations\nconfigure Talos Linux by applying machine configurations to the machines\nconfigure talosctl\nbootstrap Kubernetes\nPrerequisites\ntalosctl\n\ntalosctl is a CLI tool which interfaces with the Talos API. Talos Linux has no SSH access: talosctl is the tool you use to interact with the operating system on the machines.\n\nInstall talosctl before continuing:\n\ncurl -sL https://talos.dev/install | sh\n\n\nNote: If you boot systems off the ISO, Talos on the ISO image runs in RAM and acts as an installer. The version of talosctl that is used to create the machine configurations controls the version of Talos Linux that is installed on the machines - NOT the image that the machines are initially booted off. For example, booting a machine off the Talos 1.3.7 ISO, but creating the initial configuration with talosctl binary of version 1.4.1, will result in a machine running Talos Linux version 1.4.1.\n\nIt is advisable to use the same version of talosctl as the version of the boot media used.\n\nNetwork access\n\nThis guide assumes that the systems being installed have outgoing access to the internet, allowing them to pull installer and container images, query NTP, etc. If needed, see the documentation on registry proxies, local registries, and airgapped installation.\n\nAcquire the Talos Linux image and boot machines\n\nThe most general way to install Talos Linux is to use the ISO image.\n\nThe latest ISO image can be found on the Github Releases page:\n\nX86: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\nARM64: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-arm64.iso\n\nWhen booted from the ISO, Talos will run in RAM and will not install to disk until provided a configuration. Thus, it is safe to boot any machine from the ISO.\n\nAt this point, you should:\n\nboot one machine off the ISO to be the control plane node\nboot one or more machines off the same ISO to be the workers\nAlternative Booting\n\nFor network booting and self-built media, see Production Notes. There are installation methods specific to specific platforms, such as pre-built AMIs for AWS - check the specific Installation Guides.)\n\nDefine the Kubernetes Endpoint\n\nIn order to configure Kubernetes, Talos needs to know what the endpoint of the Kubernetes API Server will be.\n\nBecause we are only creating a single control plane node in this guide, we can use the control plane node directly as the Kubernetes API endpoint.\n\nIdentify the IP address or DNS name of the control plane node that was booted above, and convert it to a fully-qualified HTTPS URL endpoint address for the Kubernetes API Server which (by default) runs on port 6443. The endpoint should be formatted like:\n\nhttps://192.168.0.2:6443\nhttps://kube.mycluster.mydomain.com:6443\n\nNOTE: For a production cluster, you should have three control plane nodes, and have the endpoint allocate traffic to all three - see Production Notes.\n\nAccessing the Talos API\n\nAdministrative tasks are performed by calling the Talos API (usually with talosctl) on Talos Linux control plane nodes - thus, ensure your control plane node is directly reachable on TCP port 50000 from the workstation where you run the talosctl client. This may require changing firewall rules or cloud provider access-lists.\n\nFor production configurations, see Production Notes.\n\nConfigure Talos Linux\n\nWhen Talos boots without a configuration, such as when booting off the Talos ISO, it enters maintenance mode and waits for a configuration to be provided.\n\nA configuration can be passed in on boot via kernel parameters or metadata servers. See Production Notes.\n\nUnlike traditional Linux, Talos Linux is not configured by SSHing to the server and issuing commands. Instead, the entire state of the machine is defined by a machine config file which is passed to the server. This allows machines to be managed in a declarative way, and lends itself to GitOps and modern operations paradigms. The state of a machine is completely defined by, and can be reproduced from, the machine configuration file.\n\nTo generate the machine configurations for a cluster, run this command on the workstation where you installed talosctl:\n\ntalosctl gen config <cluster-name> <cluster-endpoint>\n\n\ncluster-name is an arbitrary name, used as a label in your local client configuration. It should be unique in the configuration on your local workstation.\n\ncluster-endpoint is the Kubernetes Endpoint you constructed from the control plane node’s IP address or DNS name above. It should be a complete URL, with https:// and port.\n\nFor example:\n\n$ talosctl gen config mycluster https://192.168.0.2:6443\n\ngenerating PKI and tokens\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n\nWhen you run this command, three files are created in your current directory:\n\ncontrolplane.yaml\nworker.yaml\ntalosconfig\n\nThe .yaml files are Machine Configs: they describe everything from what disk Talos should be installed on, to network settings. The controlplane.yaml file also describes how Talos should form a Kubernetes cluster.\n\nThe talosconfig file is your local client configuration file, used to connect to and authenticate access to the cluster.\n\nControlplane and Worker\n\nThe two types of Machine Configs correspond to the two roles of Talos nodes, control plane nodes (which run both the Talos and Kubernetes control planes) and worker nodes (which run the workloads).\n\nThe main difference between Controlplane Machine Config files and Worker Machine Config files is that the former contains information about how to form the Kubernetes cluster.\n\nModifying the Machine configs\n\nThe generated Machine Configs have defaults that work for most cases. They use DHCP for interface configuration, and install to /dev/sda.\n\nSometimes, you will need to modify the generated files to work with your systems. A common case is needing to change the installation disk. If you try to to apply the machine config to a node, and get an error like the below, you need to specify a different installation disk:\n\n$ talosctl apply-config --insecure -n 192.168.0.2 --file controlplane.yaml\n\nerror applying new configuration: rpc error: code = InvalidArgument desc = configuration validation failed: 1 error occurred:\n\n    * specified install disk does not exist: \"/dev/sda\"\n\n\nYou can verify which disks your nodes have by using the talosctl disks --insecure command.\n\nInsecure mode is needed at this point as the PKI infrastructure has not yet been set up.\n\nFor example, the talosctl disks command below shows that the system has a vda drive, not an sda:\n\n$ talosctl -n 192.168.0.2 disks --insecure\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID  MODALIAS                    NAME   SIZE    BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      69 GB   /pci0000:00/0000:00:06.0/virtio2/\n\n\nIn this case, you would modify the controlplane.yaml and worker.yaml files and edit the line:\n\ninstall:\n\n  disk: /dev/sda # The disk used for installations.\n\n\nto reflect vda instead of sda.\n\nFor information on customizing your machine configurations (such as to specify the version of Kubernetes), using machine configuration patches, or customizing configurations for individual machines (such as setting static IP addresses), see the Production Notes.\n\nUnderstand talosctl, endpoints and nodes\n\nIt is important to understand the concept of endpoints and nodes. In short: endpoints are where talosctl sends commands to, but the command operates on the specified nodes. The endpoint will forward the command to the nodes, if needed.\n\nEndpoints\n\nEndpoints are the IP addresses of control plane nodes, to which the talosctl client directly talks.\n\nEndpoints automatically proxy requests destined to another node in the cluster. This means that you only need access to the control plane nodes in order to manage the rest of the cluster.\n\nYou can pass in --endpoints <Control Plane IP Address> or -e <Control Plane IP Address> to the current talosctl command.\n\nIn this tutorial setup, the endpoint will always be the single control plane node.\n\nNodes\n\nNodes are the target(s) you wish to perform the operation on.\n\nWhen specifying nodes, the IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied through the endpoints.\n\nYou may provide -n or --nodes to any talosctl command to supply the node or (comma-separated) nodes on which you wish to perform the operation.\n\nFor example, to see the containers running on node 192.168.0.200, by routing the containers command through the control plane endpoint 192.168.0.2:\n\ntalosctl -e 192.168.0.2 -n 192.168.0.200 containers\n\n\nTo see the etcd logs on both nodes 192.168.0.10 and 192.168.0.11:\n\ntalosctl -e 192.168.0.2 -n 192.168.0.10,192.168.0.11 logs etcd\n\n\nFor a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nApply Configuration\n\nTo apply the Machine Configs, you need to know the machines’ IP addresses.\n\nTalos prints the IP addresses of the machines on the console during the boot process:\n\n[4.605369] [talos] task loadConfig (1/1): this machine is reachable at:\n[4.607358] [talos] task loadConfig (1/1):   192.168.0.2\n\n\nIf you do not have console access, the IP address may also be discoverable from your DHCP server.\n\nOnce you have the IP address, you can then apply the correct configuration. Apply the controlplane.yaml file to the control plane node, and the worker.yaml file to all the worker node(s).\n\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --file controlplane.yaml\n\n\nThe --insecure flag is necessary because the PKI infrastructure has not yet been made available to the node. Note: the connection will be encrypted, but not authenticated.\n\nWhen using the --insecure flag, it is not necessary to specify an endpoint.\n\nDefault talosconfig configuration file\n\nYou reference which configuration file to use by the --talosconfig parameter:\n\ntalosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 -e 192.168.0.2 version\n\n\nNote that talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. See Production Notes for more information.\n\nWhile getting started, a common mistake is referencing a configuration context for a different cluster, resulting in authentication or connection failures. Thus it is recommended to explicitly pass in the configuration file while becoming familiar with Talos Linux.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster with Talos is as simple as calling talosctl bootstrap on your control plane node:\n\ntalosctl bootstrap --nodes 192.168.0.2 --endpoints 192.168.0.2 \\\n\n  --talosconfig=./talosconfig\n\n\nThe bootstrap operation should only be called ONCE on a SINGLE control plane node. (If you have multiple control plane nodes, it doesn’t matter which one you issue the bootstrap command against.)\n\nAt this point, Talos will form an etcd cluster, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\n  talosctl kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\n  talosctl kubeconfig alternative-kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 health \\\n\n   --talosconfig=./talosconfig\n\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 dashboard \\\n\n   --talosconfig=./talosconfig\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n1.4 - Production Clusters\nRecommendations for setting up a Talos Linux cluster in production.\n\nThis document explains recommendations for running Talos Linux in production.\n\nAcquire the installation image\nAlternative Booting\n\nFor network booting and self-built media, you can use the published kernel and initramfs images:\n\nX86: vmlinuz-amd64 initramfs-amd64.xz\nARM64: vmlinuz-arm64 initramfs-arm64.xz\n\nNote that to use alternate booting, there are a number of required kernel parameters. Please see the kernel docs for more information.\n\nControl plane nodes\n\nFor a production, highly available Kubernetes cluster, it is recommended to use three control plane nodes. Using five nodes can provide greater fault tolerance, but imposes more replication overhead and can result in worse performance.\n\nBoot all three control plane nodes at this point. They will boot Talos Linux, and come up in maintenance mode, awaiting a configuration.\n\nDecide the Kubernetes Endpoint\n\nThe Kubernetes API Server endpoint, in order to be highly available, should be configured in a way that uses all available control plane nodes. There are three common ways to do this: using a load-balancer, using Talos Linux’s built in VIP functionality, or using multiple DNS records.\n\nDedicated Load-balancer\n\nIf you are using a cloud provider or have your own load-balancer (such as HAProxy, Nginx reverse proxy, or an F5 load-balancer), a dedicated load balancer is a natural choice. Create an appropriate frontend for the endpoint, listening on TCP port 6443, and point the backends at the addresses of each of the Talos control plane nodes. Your Kubernetes endpoint will be the IP address or DNS name of the load balancer front end, with the port appended (e.g. https://myK8s.mydomain.io:6443).\n\nNote: an HTTP load balancer can’t be used, as Kubernetes API server does TLS termination and mutual TLS authentication.\n\nLayer 2 VIP Shared IP\n\nTalos has integrated support for serving Kubernetes from a shared/virtual IP address. This requires Layer 2 connectivity between control plane nodes.\n\nChoose an unused IP address on the same subnet as the control plane nodes for the VIP. For instance, if your control plane node IPs are:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nyou could choose the IP 192.168.0.15 as your VIP IP address. (Make sure that 192.168.0.15 is not used by any other machine and is excluded from DHCP ranges.)\n\nOnce chosen, form the full HTTPS URL from this IP:\n\nhttps://192.168.0.15:6443\n\n\nIf you create a DNS record for this IP, note you will need to use the IP address itself, not the DNS name, to configure the shared IP (machine.network.interfaces[].vip.ip) in the Talos configuration.\n\nAfter the machine configurations are generated, you will want to edit the controlplane.yaml file to activate the VIP:\n\nmachine:\n\n    network:\n\n     interfaces:\n\n      - interface: enp2s0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.0.15\n\n\nFor more information about using a shared IP, see the related Guide\n\nDNS records\n\nAdd multiple A or AAAA records (one for each control plane node) to a DNS name.\n\nFor instance, you could add:\n\nkube.cluster1.mydomain.com  IN  A  192.168.0.10\nkube.cluster1.mydomain.com  IN  A  192.168.0.11\nkube.cluster1.mydomain.com  IN  A  192.168.0.12\n\n\nwhere the IP addresses are those of the control plane nodes.\n\nThen, your endpoint would be:\n\nhttps://kube.cluster1.mydomain.com:6443\n\nMultihoming\n\nIf your machines are multihomed, i.e., they have more than one IPv4 and/or IPv6 addresss other than loopback, then additional configuration is required. A point to note is that the machines may become multihomed via privileged workloads.\n\nMultihoming and etcd\n\nThe etcd cluster needs to establish a mesh of connections among the members. It is done using the so-called advertised address - each node learns the others’ addresses as they are advertised. It is crucial that these IP addresses are stable, i.e., that each node always advertises the same IP address. Moreover, it is beneficial to control them to establish the correct routes between the members and, e.g., avoid congested paths. In Talos, these addresses are controlled using the cluster.etcd.advertisedSubnets configuration key.\n\nMultihoming and kubelets\n\nStable IP addressing for kubelets (i.e., nodeIP) is not strictly necessary but highly recommended as it ensures that, e.g., kube-proxy and CNI routing take the desired routes. Analogously to etcd, for kubelets this is controlled via machine.kubelet.nodeIP.validSubnets.\n\nExample\n\nLet’s assume that we have a cluster with two networks:\n\npublic network\nprivate network 192.168.0.0/16\n\nWe want to use the private network for etcd and kubelet communication:\n\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.0.0/16\n\n#...\n\ncluster:\n\n  etcd:\n\n    advertisedSubnets: # listenSubnets defaults to advertisedSubnets if not set explicitly\n\n      - 192.168.0.0/16\n\n\nThis way we ensure that the etcd cluster will use the private network for communication and the kubelets will use the private network for communication with the control plane.\n\nLoad balancing the Talos API\n\nThe talosctl tool provides built-in client-side load-balancing across control plane nodes, so usually you do not need to configure a load balancer for the Talos API.\n\nHowever, if the control plane nodes are not directly reachable from the workstation where you run talosctl, then configure a load balancer to forward TCP port 50000 to the control plane nodes.\n\nNote: Because the Talos Linux API uses gRPC and mutual TLS, it cannot be proxied by a HTTP/S proxy, but only by a TCP load balancer.\n\nIf you create a load balancer to forward the Talos API calls, the load balancer IP or hostname will be used as the endpoint for talosctl.\n\nAdd the load balancer IP or hostname to the .machine.certSANs field of the machine configuration file.\n\nDo not use Talos Linux’s built in VIP function for accessing the Talos API. In the event of an error in etcd, the VIP will not function, and you will not be able to access the Talos API to recover.\n\nConfigure Talos\n\nIn many installation methods, a configuration can be passed in on boot.\n\nFor example, Talos can be booted with the talos.config kernel argument set to an HTTP(s) URL from which it should receive its configuration. Where a PXE server is available, this is much more efficient than manually configuring each node. If you do use this method, note that Talos requires a number of other kernel commandline parameters. See required kernel parameters.\n\nSimilarly, if creating EC2 kubernetes clusters, the configuration file can be passed in as --user-data to the aws ec2 run-instances command. See generally the Installation Guide for the platform being deployed.\n\nSeparating out secrets\n\nWhen generating the configuration files for a Talos Linux cluster, it is recommended to start with generating a secrets bundle which should be saved in a secure location. This bundle can be used to generate machine or client configurations at any time:\n\ntalosctl gen secrets -o secrets.yaml\n\n\nThe secrets.yaml can also be extracted from the existing controlplane machine configuration with talosctl gen secrets --from-controlplane-config controlplane.yaml -o secrets.yaml command.\n\nNow, we can generate the machine configuration for each node:\n\ntalosctl gen config --with-secrets secrets.yaml <cluster-name> <cluster-endpoint>\n\n\nHere, cluster-name is an arbitrary name for the cluster, used in your local client configuration as a label. It should be unique in the configuration on your local workstation.\n\nThe cluster-endpoint is the Kubernetes Endpoint you selected from above. This is the Kubernetes API URL, and it should be a complete URL, with https:// and port. (The default port is 6443, but you may have configured your load balancer to forward a different port.) For example:\n\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ngenerating PKI and tokens\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCustomizing Machine Configuration\n\nThe generated machine configuration provides sane defaults for most cases, but can be modified to fit specific needs.\n\nSome machine configuration options are available as flags for the talosctl gen config command, for example setting a specific Kubernetes version:\n\ntalosctl gen config --with-secrets secrets.yaml --kubernetes-version 1.25.4 my-cluster https://192.168.64.15:6443\n\n\nOther modifications are done with machine configuration patches. Machine configuration patches can be applied with talosctl gen config command:\n\ntalosctl gen config --with-secrets secrets.yaml --config-patch-control-plane @cni.patch my-cluster https://192.168.64.15:6443\n\n\nNote: @cni.patch means that the patch is read from a file named cni.patch.\n\nMachine Configs as Templates\n\nIndividual machines may need different settings: for instance, each may have a different static IP address.\n\nWhen different files are needed for machines of the same type, there are two supported flows:\n\nUse the talosctl gen config command to generate a template, and then patch the template for each machine with talosctl machineconfig patch.\nGenerate each machine configuration file separately with talosctl gen config while applying patches.\n\nFor example, given a machine configuration patch which sets the static machine hostname:\n\n# worker1.patch\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nEither of the following commands will generate a worker machine configuration file with the hostname set to worker1:\n\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n$ talosctl machineconfig patch worker.yaml --patch @worker1.patch --output worker1.yaml\n\ntalosctl gen config --with-secrets secrets.yaml --config-patch-worker @worker1.patch --output-types worker -o worker1.yaml my-cluster https://192.168.64.15:6443\n\nApply Configuration while validating the node identity\n\nIf you have console access you can extract the server certificate fingerprint and use it for an additional layer of validation:\n\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --cert-fingerprint xA9a1t2dMxB0NJ0qH1pDzilWbA3+DK/DjVbFaJBYheE= \\\n\n    --file cp0.yaml\n\n\nUsing the fingerprint allows you to be sure you are sending the configuration to the correct machine, but is completely optional. After the configuration is applied to a node, it will reboot. Repeat this process for each of the nodes in your cluster.\n\nFurther details about talosctl, endpoints and nodes\nEndpoints\n\nWhen passed multiple endpoints, talosctl will automatically load balance requests to, and fail over between, all endpoints.\n\nYou can pass in --endpoints <IP Address1>,<IP Address2> as a comma separated list of IP/DNS addresses to the current talosctl command. You can also set the endpoints in your talosconfig, by calling talosctl config endpoint <IP Address1> <IP Address2>. Note: these are space separated, not comma separated.\n\nAs an example, if the IP addresses of our control plane nodes are:\n\n192.168.0.2\n192.168.0.3\n192.168.0.4\n\nWe would set those in the talosconfig with:\n\n  talosctl --talosconfig=./talosconfig \\\n\n    config endpoint 192.168.0.2 192.168.0.3 192.168.0.4\n\nNodes\n\nThe node is the target you wish to perform the API call on.\n\nIt is possible to set a default set of nodes in the talosconfig file, but our recommendation is to explicitly pass in the node or nodes to be operated on with each talosctl command. For a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nDefault configuration file\n\nYou can reference which configuration file to use directly with the --talosconfig parameter:\n\n  talosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 version\n\n\nHowever, talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. This is done with the merge option.\n\n  talosctl config merge ./talosconfig\n\n\nThis will merge your new talosconfig into the default configuration file ($XDG_CONFIG_HOME/talos/config.yaml), creating it if necessary. Like Kubernetes, the talosconfig configuration files has multiple “contexts” which correspond to multiple clusters. The <cluster-name> you chose above will be used as the context name.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster by simply calling the bootstrap command against any of your control plane nodes (or the loadbalancer, if used for the Talos API endpoint).:\n\n  talosctl bootstrap --nodes 192.168.0.2\n\n\nThe bootstrap operation should only be called ONCE and only on a SINGLE control plane node!\n\nAt this point, Talos will form an etcd cluster, generate all of the core Kubernetes assets, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\n  talosctl kubeconfig\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\n  talosctl kubeconfig alternative-kubeconfig\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\n  talosctl -n <NODEIP> dashboard\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n1.5 - System Requirements\nHardware requirements for running Talos Linux.\nMinimum Requirements\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t2 GiB\t2\t10 GiB\nWorker\t1 GiB\t1\t10 GiB\nRecommended\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t4 GiB\t4\t100 GiB\nWorker\t2 GiB\t2\t100 GiB\n\nThese requirements are similar to that of Kubernetes.\n\nStorage\n\nTalos Linux itself only requires less than 100 MB of disk space, but the EPHEMERAL partition is used to store pulled images, container work directories, and so on. Thus a minimum is 10 GiB of disk space is required. 100 GiB is desired. Note, however, that because Talos Linux assumes complete control of the disk it is installed on, so that it can control the partition table for image based upgrades, you cannot partition the rest of the disk for use by workloads.\n\nThus it is recommended to install Talos Linux on a small, dedicated disk - using a Terabyte sized SSD for the Talos install disk would be wasteful. Sidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\n1.6 - What's New in Talos 1.6.0\nList of new and shiny features in Talos Linux.\n\nSee also upgrade notes for important changes.\n\nBreaking Changes\nLinux Firmware\n\nStarting with Talos 1.6, Linux firmware is not included in the default initramfs.\n\nUsers that need Linux firmware can pull them as an extension during install time using the Image Factory service. If the initial boot requires firmware, a custom ISO can be built with the firmware included using the Image Factory service or using the imager. This also ensures that the linux-firmware is not tied to a specific Talos version.\n\nThe list of firmware packages which were removed from the default initramfs and are now available as extensions:\n\nbnx2 and bnx2x firmware (Broadcom NetXtreme II)\nIntel ICE firmware (Intel(R) Ethernet Controller 800 Series)\nNetwork Device Selectors\n\nPreviously, network device selectors only matched the first link, now the configuration is applied to all matching links.\n\ntalosctl images command\n\nThe command images deprecated in Talos 1.5 was removed, please use talosctl images default instead.\n\n.persist Machine Configuration Option\n\nThe option .persist deprecated in Talos 1.5 was removed, the machine configuration is always persisted.\n\nNew Features\nKubernetes n-5 Version Support\n\nTalos Linux starting with version 1.6 supports the latest Kubernetes n-5 versions, for release 1.6.0 this means support for Kubernetes versions 1.24-1.29. This allows users to make it easier to upgrade to new Talos Linux versions without having to upgrade Kubernetes at the same time.\n\nSee Kubernetes release support for the list of supported versions by Kubernetes project.\n\nOAuth2 Machine Config Flow\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow.\n\nIngress Firewall\n\nTalos Linux now supports configuring the ingress firewall rules.\n\nImprovements\nComponent Updates\nLinux: 6.1.67\nKubernetes: 1.29.0\ncontainerd: 1.7.10\nrunc: 1.1.10\netcd: 3.5.11\nCoreDNS: 1.11.1\nFlannel: 0.23.0\n\nTalos is built with Go 1.21.5.\n\nExtension Services\n\nTalos now starts Extension Services early in the boot process, this allows guest agents packaged as extension services to be started in maintenance mode.\n\nFlannel Configuration\n\nTalos Linux now supports customizing default Flannel manifest with extra arguments for flanneld:\n\ncluster:\n\n  network:\n\n    cni:\n\n      flannel:\n\n        extraArgs:\n\n          - --iface-can-reach=192.168.1.1\n\nKernel Arguments\n\nTalos and Imager now supports dropping kernel arguments specified in .machine.install.extraKernelArgs or as --extra-kernel-arg to imager. Any kernel argument that starts with a - is dropped. Kernel arguments to be dropped can be specified either as -<key> which would remove all arguments that start with <key> or as -<key>=<value> which would remove the exact argument.\n\nFor example, console=ttyS0 can be dropped by specifying -console=ttyS0 as an extra argument.\n\nkube-scheduler Configuration\n\nTalos now supports specifying the kube-scheduler configuration in the Talos configuration file. It can be set under cluster.scheduler.config and kube-scheduler will be automatically configured to with the correct flags.\n\nKubernetes Node Taint Configuration\n\nSimilar to machine.nodeLabels Talos Linux now provides machine.nodeTaints machine configuration field to configure Kubernetes Node taints.\n\nKubelet Credential Provider Configuration\n\nTalos now supports specifying the kubelet credential provider configuration in the Talos configuration file. It can be set under machine.kubelet.credentialProviderConfig and kubelet will be automatically configured to with the correct flags. The credential binaries are expected to be present under /usr/local/lib/kubelet/credentialproviders. Talos System Extensions can be used to install the credential binaries.\n\nKubePrism\n\nKubePrism is enabled by default on port 7445.\n\nSysctl\n\nTalos now handles sysctl/sysfs key names in line with sysctl.conf(5):\n\nif the first separator is ‘/’, no conversion is done\nif the first separator is ‘.’, dots and slashes are remapped\n\nExample (both sysctls are equivalent):\n\nmachine:\n\n  sysctls:\n\n    net/ipv6/conf/eth0.100/disable_ipv6: \"1\"\n\n    net.ipv6.conf.eth0/100.disable_ipv6: \"1\"\n\nUser Disks\n\nTalos Linux now supports specifying user disks in .machine.disks machine configuration links via udev symlinks, e.g. /dev/disk/by-id/XXXX.\n\nPacket Capture\n\nTalos Linux provides more performant implementation server-side for the packet capture API (talosctl pcap CLI).\n\nMemory Usage and Performance\n\nTalos Linux core components now use less memory and start faster.\n\n1.7 - Support Matrix\nTable of supported Talos Linux versions and respective platforms.\nTalos Version\t1.6\t1.5\nRelease Date\t2023-12-15\t2023-08-17 (1.5.0)\nEnd of Community Support\t1.7.0 release (2024-04-15, TBD)\t1.6.0 release (2023-12-15)\nEnterprise Support\toffered by Sidero Labs Inc.\toffered by Sidero Labs Inc.\nKubernetes\t1.29, 1.28, 1.27, 1.26, 1.25, 1.24\t1.28, 1.27, 1.26\nArchitecture\tamd64, arm64\tamd64, arm64\nPlatforms\t\t\n- cloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\n- bare metal\tx86: BIOS, UEFI, SecureBoot; arm64: UEFI, SecureBoot; boot: ISO, PXE, disk image\tx86: BIOS, UEFI; arm64: UEFI; boot: ISO, PXE, disk image\n- virtualized\tVMware, Hyper-V, KVM, Proxmox, Xen\tVMware, Hyper-V, KVM, Proxmox, Xen\n- SBCs\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\n- local\tDocker, QEMU\tDocker, QEMU\nCluster API\t\t\nCAPI Bootstrap Provider Talos\t>= 0.6.3\t>= 0.6.1\nCAPI Control Plane Provider Talos\t>= 0.5.4\t>= 0.5.2\nSidero\t>= 0.6.2\t>= 0.6.0\nPlatform Tiers\nTier 1: Automated tests, high-priority fixes.\nTier 2: Tested from time to time, medium-priority bugfixes.\nTier 3: Not tested by core Talos team, community tested.\nTier 1\nMetal\nAWS\nGCP\nTier 2\nAzure\nDigital Ocean\nOpenStack\nVMWare\nTier 3\nExoscale\nHetzner\nnocloud\nOracle Cloud\nScaleway\nVultr\nUpcloud\n1.8 - Troubleshooting\nTroubleshoot control plane and other failures for Talos Linux clusters.\n\nIn this guide we assume that Talos is configured with default features enabled, such as Discovery Service and KubePrism. If these features are disabled, some of the troubleshooting steps may not apply or may need to be adjusted.\n\nThis guide is structured so that it can be followed step-by-step, skip sections which are not relevant to your issue.\n\nNetwork Configuration\n\nAs Talos Linux is an API-based operating system, it is important to have networking configured so that the API can be accessed. Some information can be gathered from the Interactive Dashboard which is available on the machine console.\n\nWhen running in the cloud the networking should be configured automatically. Whereas when running on bare-metal it may need more specific configuration, see networking metal configuration guide.\n\nTalos API\n\nThe Talos API runs on port 50000. Control plane nodes should always serve the Talos API, while worker nodes require access to the control plane nodes to issue TLS certificates for the workers.\n\nFirewall Issues\n\nMake sure that the firewall is not blocking port 50000, and communication on ports 50000/50001 inside the cluster.\n\nClient Configuration Issues\n\nMake sure to use correct talosconfig client configuration file matching your cluster. See getting started for more information.\n\nThe most common issue is that talosctl gen config writes talosconfig to the file in the current directory, while talosctl by default picks up the configuration from the default location (~/.talos/config). The path to the configuration file can be specified with --talosconfig flag to talosctl.\n\nConflict on Kubernetes and Host Subnets\n\nIf talosctl returns an error saying that certificate IPs are empty, it might be due to a conflict between Kubernetes and host subnets. The Talos API runs on the host network, but it automatically excludes Kubernetes pod & network subnets from the useable set of addresses.\n\nTalos default machine configuration specifies the following Kubernetes pod and subnet IPv4 CIDRs: 10.244.0.0/16 and 10.96.0.0/12. If the host network is configured with one of these subnets, change the machine configuration to use a different subnet.\n\nWrong Endpoints\n\nThe talosctl CLI connects to the Talos API via the specified endpoints, which should be a list of control plane machine addresses. The client will automatically retry on other endpoints if there are unavailable endpoints.\n\nWorker nodes should not be used as the endpoint, as they are not able to forward request to other nodes.\n\nThe VIP should never be used as Talos API endpoint.\n\nTCP Loadbalancer\n\nWhen using a TCP loadbalancer, make sure the loadbalancer endpoint is included in the .machine.certSANs list in the machine configuration.\n\nSystem Requirements\n\nIf minimum system requirements are not met, this might manifest itself in various ways, such as random failures when starting services, or failures to pull images from the container registry.\n\nRunning Health Checks\n\nTalos Linux provides a set of basic health checks with talosctl health command which can be used to check the health of the cluster.\n\nIn the default mode, talosctl health uses information from the discovery to get the information about cluster members. This can be overridden with command line flags --control-plane-nodes and --worker-nodes.\n\nGathering Logs\n\nWhile the logs and state of the system can be queried via the Talos API, it is often useful to gather the logs from all nodes in the cluster, and analyze them offline. The talosctl support command can be used to gather logs and other information from the nodes specified with --nodes flag (multiple nodes are supported).\n\nDiscovery and Cluster Membership\n\nTalos Linux uses Discovery Service to discover other nodes in the cluster.\n\nThe list of members on each machine should be consistent: talosctl -n <IP> get members.\n\nSome Members are Missing\n\nEnsure connectivity to the discovery service (default is discovery.talos.dev:443), and that the discovery registry is not disabled.\n\nDuplicate Members\n\nDon’t use same base secrets to generate machine configuration for multiple clusters, as some secrets are used to identify members of the same cluster. So if the same machine configuration (or secrets) are used to repeatedly create and destroy clusters, the discovery service will see the same nodes as members of different clusters.\n\nRemoved Members are Still Present\n\nTalos Linux removes itself from the discovery service when it is reset. If the machine was not reset, it might show up as a member of the cluster for the maximum TTL of the discovery service (30 minutes), and after that it will be automatically removed.\n\netcd Issues\n\netcd is the distributed key-value store used by Kubernetes to store its state. Talos Linux provides automation to manage etcd members running on control plane nodes. If etcd is not healthy, the Kubernetes API server will not be able to function correctly.\n\nIt is always recommended to run an odd number of etcd members, as with 3 or more members it provides fault tolerance for less than quorum member failures.\n\nCommon troubleshooting steps:\n\ncheck etcd service state with talosctl -n IP service etcd for each control plane node\ncheck etcd membership on each control plane node with talosctl -n IP etcd member list\ncheck etcd logs with talosctl -n IP logs etcd\ncheck etcd alarms with talosctl -n IP etcd alarm list\nAll etcd Services are Stuck in Pre State\n\nMake sure that a single member was bootstrapped.\n\nCheck that the machine is able to pull the etcd container image, check talosctl dmesg for messages starting with retrying: prefix.\n\nSome etcd Services are Stuck in Pre State\n\nMake sure traffic is not blocked on port 2380 between controlplane nodes.\n\nCheck that etcd quorum is not lost.\n\nCheck that all control plane nodes are reported in talosctl get members output.\n\netcd Reports and Alarm\n\nSee etcd maintenance guide.\n\netcd Quorum is Lost\n\nSee disaster recovery guide.\n\nOther Issues\n\netcd will only run on control plane nodes. If a node is designated as a worker node, you should not expect etcd to be running on it.\n\nWhen a node boots for the first time, the etcd data directory (/var/lib/etcd) is empty, and it will only be populated when etcd is launched.\n\nIf the etcd service is crashing and restarting, check its logs with talosctl -n <IP> logs etcd. The most common reasons for crashes are:\n\nwrong arguments passed via extraArgs in the configuration;\nbooting Talos on non-empty disk with an existing Talos installation, /var/lib/etcd contains data from the old cluster.\nkubelet and Kubernetes Node Issues\n\nThe kubelet service should be running on all Talos nodes, and it is responsible for running Kubernetes pods, static pods (including control plane components), and registering the node with the Kubernetes API server.\n\nIf the kubelet doesn’t run on a control plane node, it will block the control plane components from starting.\n\nThe node will not be registered in Kubernetes until the Kubernetes API server is up and initial Kubernetes manifests are applied.\n\nkubelet is not running\n\nCheck that kubelet image is available (talosctl image ls --namespace system).\n\nCheck kubelet logs with talosctl -n IP logs kubelet for startup errors:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure kubelet extra arguments and extra configuration supplied with Talos machine configuration is valid\nTalos Complains about Node Not Found\n\nkubelet hasn’t yet registered the node with the Kubernetes API server, this is expected during initial cluster bootstrap, the error will go away. If the message persists, check Kubernetes API health.\n\nThe Kubernetes controller manager (kube-controller-manager) is responsible for monitoring the certificate signing requests (CSRs) and issuing certificates for each of them. The kubelet is responsible for generating and submitting the CSRs for its associated node.\n\nThe state of any CSRs can be checked with kubectl get csr:\n\n$ kubectl get csr\n\nNAME        AGE   SIGNERNAME                                    REQUESTOR                 CONDITION\n\ncsr-jcn9j   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-p6b9q   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-sw6rm   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-vlghg   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\nkubectl get nodes Reports Wrong Internal IP\n\nConfigure the correct internal IP address with .machine.kubelet.nodeIP\n\nkubectl get nodes Reports Wrong External IP\n\nTalos Linux doesn’t manage the external IP, it is managed with the Kubernetes Cloud Controller Manager.\n\nkubectl get nodes Reports Wrong Node Name\n\nBy default, the Kubernetes node name is derived from the hostname. Update the hostname using the machine configuration, cloud configuration, or via DHCP server.\n\nNode Is Not Ready\n\nA Node in Kubernetes is marked as Ready only once its CNI is up. It takes a minute or two for the CNI images to be pulled and for the CNI to start. If the node is stuck in this state for too long, check CNI pods and logs with kubectl. Usually, CNI-related resources are created in kube-system namespace.\n\nFor example, for the default Talos Flannel CNI:\n\n$ kubectl -n kube-system get pods\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\n...\n\nkube-flannel-25drx                               1/1     Running   0          23m\n\nkube-flannel-8lmb6                               1/1     Running   0          23m\n\nkube-flannel-gl7nx                               1/1     Running   0          23m\n\nkube-flannel-jknt9                               1/1     Running   0          23m\n\n...\n\nDuplicate/Stale Nodes\n\nTalos Linux doesn’t remove Kubernetes nodes automatically, so if a node is removed from the cluster, it will still be present in Kubernetes. Remove the node from Kubernetes with kubectl delete node <node-name>.\n\nTalos Complains about Certificate Errors on kubelet API\n\nThis error might appear during initial cluster bootstrap, and it will go away once the Kubernetes API server is up and the node is registered.\n\nBy default configuration, kubelet issues a self-signed server certificate, but when rotate-server-certificates feature is enabled, kubelet issues its certificate using kube-apiserver. Make sure the kubelet CSR is approved by the Kubernetes API server.\n\nIn either case, this error is not critical, as it only affects reporting of the pod status to Talos Linux.\n\nKubernetes Control Plane\n\nThe Kubernetes control plane consists of the following components:\n\nkube-apiserver - the Kubernetes API server\nkube-controller-manager - the Kubernetes controller manager\nkube-scheduler - the Kubernetes scheduler\n\nOptionally, kube-proxy runs as a DaemonSet to provide pod-to-service communication.\n\ncoredns provides name resolution for the cluster.\n\nCNI is not part of the control plane, but it is required for Kubernetes pods using pod networking.\n\nTroubleshooting should always start with kube-apiserver, and then proceed to other components.\n\nTalos Linux configures kube-apiserver to talk to the etcd running on the same node, so etcd must be healthy before kube-apiserver can start. The kube-controller-manager and kube-scheduler are configured to talk to the kube-apiserver on the same node, so they will not start until kube-apiserver is healthy.\n\nControl Plane Static Pods\n\nTalos should generate the static pod definitions for the Kubernetes control plane as resources:\n\n$ talosctl -n <IP> get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.2   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.2   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.2   k8s         StaticPod   kube-scheduler            1\n\n\nTalos should report that the static pod definitions are rendered for the kubelet:\n\n$ talosctl -n <IP> dmesg | grep 'rendered new'\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.550527204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-apiserver\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.552186204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-controller-manager\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.554607204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-scheduler\"}\n\n\nIf the static pod definitions are not rendered, check etcd and kubelet service health (see above) and the controller runtime logs (talosctl logs controller-runtime).\n\nControl Plane Pod Status\n\nInitially the kube-apiserver component will not be running, and it takes some time before it becomes fully up during bootstrap (image should be pulled from the Internet, etc.)\n\nThe status of the control plane components on each of the control plane nodes can be checked with talosctl containers -k:\n\n$ talosctl -n <IP> containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                            IMAGE                                               PID    STATUS\n\n172.20.0.2   k8s.io      kube-system/kube-apiserver-talos-default-controlplane-1                                       registry.k8s.io/pause:3.2                                2539   SANDBOX_READY\n\n172.20.0.2   k8s.io      └─ kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271        registry.k8s.io/kube-apiserver:v1.29.0 2572   CONTAINER_RUNNING\n\n\nThe logs of the control plane components can be checked with talosctl logs --kubernetes (or with -k as a shorthand):\n\ntalosctl -n <IP> logs -k kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271\n\n\nIf the control plane component reports error on startup, check that:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure extra arguments and extra configuration supplied with Talos machine configuration is valid\nKubernetes Bootstrap Manifests\n\nAs part of the bootstrap process, Talos injects bootstrap manifests into Kubernetes API server. There are two kinds of these manifests: system manifests built-in into Talos and extra manifests downloaded (custom CNI, extra manifests in the machine config):\n\n$ talosctl -n <IP> get manifests\n\nNODE         NAMESPACE      TYPE       ID                               VERSION\n\n172.20.0.2   controlplane   Manifest   00-kubelet-bootstrapping-token   1\n\n172.20.0.2   controlplane   Manifest   01-csr-approver-role-binding     1\n\n172.20.0.2   controlplane   Manifest   01-csr-node-bootstrap            1\n\n172.20.0.2   controlplane   Manifest   01-csr-renewal-role-binding      1\n\n172.20.0.2   controlplane   Manifest   02-kube-system-sa-role-binding   1\n\n172.20.0.2   controlplane   Manifest   03-default-pod-security-policy   1\n\n172.20.0.2   controlplane   Manifest   05-https://docs.projectcalico.org/manifests/calico.yaml   1\n\n172.20.0.2   controlplane   Manifest   10-kube-proxy                    1\n\n172.20.0.2   controlplane   Manifest   11-core-dns                      1\n\n172.20.0.2   controlplane   Manifest   11-core-dns-svc                  1\n\n172.20.0.2   controlplane   Manifest   11-kube-config-in-cluster        1\n\n\nDetails of each manifest can be queried by adding -o yaml:\n\n$ talosctl -n <IP> get manifests 01-csr-approver-role-binding --namespace=controlplane -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: Manifests.kubernetes.talos.dev\n\n    id: 01-csr-approver-role-binding\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    - apiVersion: rbac.authorization.k8s.io/v1\n\n      kind: ClusterRoleBinding\n\n      metadata:\n\n        name: system-bootstrap-approve-node-client-csr\n\n      roleRef:\n\n        apiGroup: rbac.authorization.k8s.io\n\n        kind: ClusterRole\n\n        name: system:certificates.k8s.io:certificatesigningrequests:nodeclient\n\n      subjects:\n\n        - apiGroup: rbac.authorization.k8s.io\n\n          kind: Group\n\n          name: system:bootstrappers\n\nOther Control Plane Components\n\nOnce the Kubernetes API server is up, other control plane components issues can be troubleshooted with kubectl:\n\nkubectl get nodes -o wide\n\nkubectl get pods -o wide --all-namespaces\n\nkubectl describe pod -n NAMESPACE POD\n\nkubectl logs -n NAMESPACE POD\n\nKubernetes API\n\nThe Kubernetes API client configuration (kubeconfig) can be retrieved using Talos API with talosctl -n <IP> kubeconfig command. Talos Linux mostly doesn’t depend on the Kubernetes API endpoint for the cluster, but Kubernetes API endpoint should be configured correctly for external access to the cluster.\n\nKubernetes Control Plane Endpoint\n\nThe Kubernetes control plane endpoint is the single canonical URL by which the Kubernetes API is accessed. Especially with high-availability (HA) control planes, this endpoint may point to a load balancer or a DNS name which may have multiple A and AAAA records.\n\nLike Talos’ own API, the Kubernetes API uses mutual TLS, client certs, and a common Certificate Authority (CA). Unlike general-purpose websites, there is no need for an upstream CA, so tools such as cert-manager, Let’s Encrypt, or products such as validated TLS certificates are not required. Encryption, however, is, and hence the URL scheme will always be https://.\n\nBy default, the Kubernetes API server in Talos runs on port 6443. As such, the control plane endpoint URLs for Talos will almost always be of the form https://endpoint:6443. (The port, since it is not the https default of 443 is required.) The endpoint above may be a DNS name or IP address, but it should be directed to the set of all controlplane nodes, as opposed to a single one.\n\nAs mentioned above, this can be achieved by a number of strategies, including:\n\nan external load balancer\nDNS records\nTalos-builtin shared IP (VIP)\nBGP peering of a shared IP (such as with kube-vip)\n\nUsing a DNS name here is a good idea, since it allows any other option, while offering a layer of abstraction. It allows the underlying IP addresses to change without impacting the canonical URL.\n\nUnlike most services in Kubernetes, the API server runs with host networking, meaning that it shares the network namespace with the host. This means you can use the IP address(es) of the host to refer to the Kubernetes API server.\n\nFor availability of the API, it is important that any load balancer be aware of the health of the backend API servers, to minimize disruptions during common node operations like reboots and upgrades.\n\nMiscellaneous\nChecking Controller Runtime Logs\n\nTalos runs a set of controllers which operate on resources to build and support machine operations.\n\nSome debugging information can be queried from the controller logs with talosctl logs controller-runtime:\n\ntalosctl -n <IP> logs controller-runtime\n\n\nControllers continuously run a reconcile loop, so at any time, they may be starting, failing, or restarting. This is expected behavior.\n\nIf there are no new messages in the controller-runtime log, it means that the controllers have successfully finished reconciling, and that the current system state is the desired system state.\n\n2 - Talos Linux Guides\nDocumentation on how to manage Talos Linux\n2.1 - Installation\nHow to install Talos Linux on various platforms\n2.1.1 - Bare Metal Platforms\nInstallation of Talos Linux on various bare-metal platforms.\n2.1.1.1 - Digital Rebar\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\nPrerequisites\n3 nodes (please see hardware requirements)\nLoadbalancer\nDigital Rebar Server\nTalosctl access (see talosctl setup)\nCreating a Cluster\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes. We assume an existing digital rebar deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe loadbalancer is used to distribute the load across multiple controlplane nodes. This isn’t covered in detail, because we assume some loadbalancing knowledge before hand. If you think this should be added to the docs, please create a issue.\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nDigital Rebar has a built-in fileserver, which means we can use this feature to expose the talos configuration files. We will place controlplane.yaml, and worker.yaml into Digital Rebar file server by using the drpcli tools.\n\nCopy the generated files from the step above into your Digital Rebar installation.\n\ndrpcli file upload <file>.yaml as <file>.yaml\n\n\nReplacing <file> with controlplane or worker.\n\nDownload the boot files\n\nDownload a recent version of boot.tar.gz from github.\n\nUpload to DRB:\n\n$ drpcli isos upload boot.tar.gz as talos.tar.gz\n\n{\n\n  \"Path\": \"talos.tar.gz\",\n\n  \"Size\": 96470072\n\n}\n\n\nWe have some Digital Rebar example files in the Git repo you can use to provision Digital Rebar with drpcli.\n\nTo apply these configs you need to create them, and then apply them as follow:\n\n$ drpcli bootenvs create talos\n\n{\n\n  \"Available\": true,\n\n  \"BootParams\": \"\",\n\n  \"Bundle\": \"\",\n\n  \"Description\": \"\",\n\n  \"Documentation\": \"\",\n\n  \"Endpoint\": \"\",\n\n  \"Errors\": [],\n\n  \"Initrds\": [],\n\n  \"Kernel\": \"\",\n\n  \"Meta\": {},\n\n  \"Name\": \"talos\",\n\n  \"OS\": {\n\n    \"Codename\": \"\",\n\n    \"Family\": \"\",\n\n    \"IsoFile\": \"\",\n\n    \"IsoSha256\": \"\",\n\n    \"IsoUrl\": \"\",\n\n    \"Name\": \"\",\n\n    \"SupportedArchitectures\": {},\n\n    \"Version\": \"\"\n\n  },\n\n  \"OnlyUnknown\": false,\n\n  \"OptionalParams\": [],\n\n  \"ReadOnly\": false,\n\n  \"RequiredParams\": [],\n\n  \"Templates\": [],\n\n  \"Validated\": true\n\n}\n\ndrpcli bootenvs update talos - < bootenv.yaml\n\n\nYou need to do this for all files in the example directory. If you don’t have access to the drpcli tools you can also use the webinterface.\n\nIt’s important to have a corresponding SHA256 hash matching the boot.tar.gz\n\nBootenv BootParams\n\nWe’re using some of Digital Rebar built in templating to make sure the machine gets the correct role assigned.\n\ntalos.platform=metal talos.config={{ .ProvisionerURL }}/files/{{.Param \\\"talos/role\\\"}}.yaml\"\n\nThis is why we also include a params.yaml in the example directory to make sure the role is set to one of the following:\n\ncontrolplane\nworker\n\nThe {{.Param \\\"talos/role\\\"}} then gets populated with one of the above roles.\n\nBoot the Machines\n\nIn the UI of Digital Rebar you need to select the machines you want to provision. Once selected, you need to assign to following:\n\nProfile\nWorkflow\n\nThis will provision the Stage and Bootenv with the talos values. Once this is done, you can boot the machine.\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.1.2 - Equinix Metal\nCreating Talos clusters with Equinix Metal.\n\nYou can create a Talos Linux cluster on Equinix Metal in a variety of ways, such as through the EM web UI, the metal command line too, or through PXE booting. Talos Linux is a supported OS install option on Equinix Metal, so it’s an easy process.\n\nRegardless of the method, the process is:\n\nCreate a DNS entry for your Kubernetes endpoint.\nGenerate the configurations using talosctl.\nProvision your machines on Equinix Metal.\nPush the configurations to your servers (if not done as part of the machine provisioning).\nconfigure your Kubernetes endpoint to point to the newly created control plane nodes\nbootstrap the cluster\nDefine the Kubernetes Endpoint\n\nThere are a variety of ways to create an HA endpoint for the Kubernetes cluster. Some of the ways are:\n\nDNS\nLoad Balancer\nBGP\n\nWhatever way is chosen, it should result in an IP address/DNS name that routes traffic to all the control plane nodes. We do not know the control plane node IP addresses at this stage, but we should define the endpoint DNS entry so that we can use it in creating the cluster configuration. After the nodes are provisioned, we can use their addresses to create the endpoint A records, or bind them to the load balancer, etc.\n\nCreate the Machine Configuration Files\nGenerating Configurations\n\nUsing the DNS name of the loadbalancer defined above, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-em-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe port used above should be 6443, unless your load balancer maps a different port to port 6443 on the control plane nodes.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode metal\n\ntalosctl validate --config worker.yaml --mode metal\n\n\nNote: Validation of the install disk could potentially fail as validation is performed on your local machine and the specified disk may not exist.\n\nPassing in the configuration as User Data\n\nYou can use the metadata service provide by Equinix Metal to pass in the machines configuration. It is required to add a shebang to the top of the configuration file.\n\nThe convention we use is #!talos.\n\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\n\nSimply select the location and type of machines in the Equinix Metal web interface. Select Talos as the Operating System, then select the number of servers to create, and name them (in lowercase only.) Under optional settings, you can optionally paste in the contents of controlplane.yaml that was generated, above (ensuring you add a first line of #!talos).\n\nYou can repeat this process to create machines of different types for control plane and worker nodes (although you would pass in worker.yaml for the worker nodes, as user data).\n\nIf you did not pass in the machine configuration as User Data, you need to provide it to each machine, with the following command:\n\ntalosctl apply-config --insecure --nodes <Node IP> --file ./controlplane.yaml\n\nCreating a Cluster via the Equinix Metal CLI\n\nThis guide assumes the user has a working API token,and the Equinix Metal CLI installed.\n\nBecause Talos Linux is a supported operating system, Talos Linux machines can be provisioned directly via the CLI, using the -O talos_v1 parameter (for Operating System).\n\nNote: Ensure you have prepended #!talos to the controlplane.yaml file.\n\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --operating-system \"talos_v1\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\ne.g. metal device create -p <projectID> -f da11 -O talos_v1 -P c3.small.x86 -H steve.test.11 --userdata-file ./controlplane.yaml\n\nRepeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nNetwork Booting via iPXE\n\nTalos Linux can be PXE-booted on Equinix Metal using Image Factory, using the equinixMetal platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/equinixMetal-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate the Control Plane Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\nNote: Repeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nCreate the Worker Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file worker.yaml\n\nUpdate the Kubernetes endpoint\n\nNow our control plane nodes have been created, and we know their IP addresses, we can associate them with the Kubernetes endpoint. Configure your load balancer to route traffic to these nodes, or add A records to your DNS entry for the endpoint, for each control plane node. e.g.\n\nhost endpoint.mydomain.com\n\nendpoint.mydomain.com has address 145.40.90.201\n\nendpoint.mydomain.com has address 147.75.109.71\n\nendpoint.mydomain.com has address 145.40.90.177\n\nBootstrap Etcd\n\nSet the endpoints and nodes for talosctl:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\n\nThis only needs to be issued to one control plane node.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.1.3 - ISO\nBooting Talos on bare-metal with ISO.\n\nTalos can be installed on bare-metal machine using an ISO image. ISO images for amd64 and arm64 architectures are available on the Talos releases page.\n\nTalos doesn’t install itself to disk when booted from an ISO until the machine configuration is applied.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from a Talos ISO. The boot order should prefer disk over ISO, or the ISO should be removed after the installation to make Talos boot from disk.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nThere are two flavors of ISO images available:\n\nmetal-<arch>.iso supports booting on BIOS and UEFI systems (for x86, UEFI only for arm64)\nmetal-<arch>-secureboot.iso supports booting on only UEFI systems in SecureBoot mode (via Image Factory)\n2.1.1.4 - Matchbox\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\nCreating a Cluster\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing load balancer, matchbox deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nIn bare-metal setups it is up to the user to provide the configuration files over HTTP(S). A special kernel parameter (talos.config) must be used to inform Talos about where it should retrieve its configuration file. To keep things simple we will place controlplane.yaml, and worker.yaml into Matchbox’s assets directory. This directory is automatically served by Matchbox.\n\nCreate the Matchbox Configuration Files\n\nThe profiles we will create will reference vmlinuz, and initramfs.xz. Download these files from the release of your choice, and place them in /var/lib/matchbox/assets.\n\nProfiles\nControl Plane Nodes\n{\n\n  \"id\": \"control-plane\",\n\n  \"name\": \"control-plane\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/controlplane.yaml\"\n\n    ]\n\n  }\n\n}\n\n\nNote: Be sure to change http://matchbox.talos.dev to the endpoint of your matchbox server.\n\nWorker Nodes\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/worker.yaml\"\n\n    ]\n\n  }\n\n}\n\nGroups\n\nNow, create the following groups, and ensure that the selectors are accurate for your specific setup.\n\n{\n\n  \"id\": \"control-plane-1\",\n\n  \"name\": \"control-plane-1\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-2\",\n\n  \"name\": \"control-plane-2\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-3\",\n\n  \"name\": \"control-plane-3\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"profile\": \"default\"\n\n}\n\nBoot the Machines\n\nNow that we have our configuration files in place, boot all the machines. Talos will come up on each machine, grab its configuration file, and bootstrap itself.\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.1.5 - Network Configuration\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nBy default, Talos will run DHCP client on all interfaces which have a link, and that might be enough for most of the cases. If some advanced network configuration is required, it can be done via the machine configuration file.\n\nBut sometimes it is required to apply network configuration even before the machine configuration can be fetched from the network.\n\nKernel Command Line\n\nTalos supports some kernel command line parameters to configure network before the machine configuration is fetched.\n\nNote: Kernel command line parameters are not persisted after Talos installation, so proper network configuration should be done via the machine configuration.\n\nAddress, default gateway and DNS servers can be configured via ip= kernel command line parameter:\n\nip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\n\nBonding can be configured via bond= kernel command line parameter:\n\nbond=bond0:eth0,eth1:balance-rr\n\n\nVLANs can be configured via vlan= kernel command line parameter:\n\nvlan=eth0.100:eth0\n\n\nSee kernel parameters reference for more details.\n\nPlatform Network Configuration\n\nSome platforms (e.g. AWS, Google Cloud, etc.) have their own network configuration mechanisms, which can be used to perform the initial network configuration. There is no such mechanism for bare-metal platforms, so Talos provides a way to use platform network config on the metal platform to submit the initial network configuration.\n\nThe platform network configuration is a YAML document which contains resource specifications for various network resources. For the metal platform, the interactive dashboard can be used to edit the platform network configuration, also the configuration can be created manually.\n\nThe current value of the platform network configuration can be retrieved using the MetaKeys resource (key 0xa):\n\ntalosctl get meta 0xa\n\n\nThe platform network configuration can be updated using the talosctl meta command for the running node:\n\ntalosctl meta write 0xa '{\"externalIPs\": [\"1.2.3.4\"]}'\n\ntalosctl meta delete 0xa\n\n\nThe initial platform network configuration for the metal platform can be also included into the generated Talos image:\n\ndocker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\ndocker run --rm -i --privileged ghcr.io/siderolabs/imager:v1.6.2 image --platform metal --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\n\nThe platform network configuration gets merged with other sources of network configuration, the details can be found in the network resources guide.\n\n2.1.1.6 - PXE\nBooting Talos over the network on bare-metal with PXE.\n\nTalos can be installed on bare-metal using PXE service. There are two more detailed guides for PXE booting using Matchbox and Digital Rebar.\n\nThis guide describes generic steps for PXE booting Talos on bare-metal.\n\nFirst, download the vmlinuz and initramfs assets from the Talos releases page. Set up the machines to PXE boot from the network (usually by setting the boot order in the BIOS). There might be options specific to the hardware being used, booting in BIOS or UEFI mode, using iPXE, etc.\n\nTalos requires the following kernel parameters to be set on the initial boot:\n\ntalos.platform=metal\nslab_nomerge\npti=on\n\nWhen booted from the network without machine configuration, Talos will start in maintenance mode.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from network. The boot order should prefer disk over network.\n\nTalos can automatically fetch the machine configuration from the network on the initial boot using talos.config kernel parameter. A metadata service (HTTP service) can be implemented to deliver customized configuration to each node for example by using the MAC address of the node:\n\ntalos.config=https://metadata.service/talos/config?mac=${mac}\n\n\nNote: The talos.config kernel parameter supports other substitution variables, see kernel parameters reference for the full list.\n\nPXE booting can be also performed via Image Factory.\n\n2.1.1.7 - SecureBoot\nBooting Talos in SecureBoot mode on UEFI platforms.\n\nTalos now supports booting on UEFI systems in SecureBoot mode. When combined with TPM-based disk encryption, this provides Trusted Boot experience.\n\nNote: SecureBoot is not supported on x86 platforms in BIOS mode.\n\nThe implementation is using systemd-boot as a boot menu implementation, while the Talos kernel, initramfs and cmdline arguments are combined into the Unified Kernel Image (UKI) format. UEFI firmware loads the systemd-boot bootloader, which then loads the UKI image. Both systemd-boot and Talos UKI image are signed with the key, which is enrolled into the UEFI firmware.\n\nAs Talos Linux is fully contained in the UKI image, the full operating system is verified and booted by the UEFI firmware.\n\nNote: There is no support at the moment to upgrade non-UKI (GRUB-based) Talos installation to use UKI/SecureBoot, so a fresh installation is required.\n\nSecureBoot with Sidero Labs Images\n\nSidero Labs provides Talos images signed with the Sidero Labs SecureBoot key via Image Factory.\n\nNote: The SecureBoot images are available for Talos releases starting from v1.5.0.\n\nThe easiest way to get started with SecureBoot is to download the ISO, and boot it on a UEFI-enabled system which has SecureBoot enabled in setup mode.\n\nThe ISO bootloader will enroll the keys in the UEFI firmware, and boot the Talos Linux in SecureBoot mode. The install should performed using SecureBoot installer (put it Talos machine configuration): factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2.\n\nNote: SecureBoot images can also be generated with custom keys.\n\nBooting Talos Linux in SecureBoot Mode\n\nIn this guide we will use the ISO image to boot Talos Linux in SecureBoot mode, followed by submitting machine configuration to the machine in maintenance mode. We will use one the ways to generate and submit machine configuration to the node, please refer to the Production Notes for the full guide.\n\nFirst, make sure SecureBoot is enabled in the UEFI firmware. For the first boot, the UEFI firmware should be in the setup mode, so that the keys can be enrolled into the UEFI firmware automatically. If the UEFI firmware does not support automatic enrollment, you may need to hit Esc to force the boot menu to appear, and select the Enroll Secure Boot keys: auto option.\n\nNote: There are other ways to enroll the keys into the UEFI firmware, but this is out of scope of this guide.\n\nOnce Talos is running in maintenance mode, verify that secure boot is enabled:\n\n$ talosctl -n <IP> get securitystate --insecure\n\nNODE   NAMESPACE   TYPE            ID              VERSION   SECUREBOOT\n\n       runtime     SecurityState   securitystate   1         true\n\n\nNow we will generate the machine configuration for the node supplying the installer-secureboot container image, and applying the patch to enable TPM-based disk encryption (requires TPM 2.0):\n\n# tpm-disk-encryption.yaml\n\nmachine:\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n\nGenerate machine configuration:\n\ntalosctl gen config <cluster-name> https://<endpoint>:6443 --install-image=factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2 --install-disk=/dev/sda --config-patch @tpm-disk-encryption.yaml\n\n\nApply machine configuration to the node:\n\ntalosctl -n <IP> apply-config --insecure -f controlplane.yaml\n\n\nTalos will perform the installation to the disk and reboot the node. Please make sure that the ISO image is not attached to the node anymore, otherwise the node will boot from the ISO image again.\n\nOnce the node is rebooted, verify that the node is running in secure boot mode:\n\ntalosctl -n <IP> --talosconfig=talosconfig get securitystate\n\nUpgrading Talos Linux\n\nAny change to the boot asset (kernel, initramfs, kernel command line) requires the UKI to be regenerated and the installer image to be rebuilt. Follow the steps above to generate new installer image updating the boot assets: use new Talos version, add a system extension, or modify the kernel command line. Once the new installer image is pushed to the registry, upgrade the node using the new installer image.\n\nIt is important to preserve the UKI signing key and the PCR signing key, otherwise the node will not be able to boot with the new UKI and unlock the encrypted partitions.\n\nDisk Encryption with TPM\n\nWhen encrypting the disk partition for the first time, Talos Linux generates a random disk encryption key and seals (encrypts) it with the TPM device. The TPM unlock policy is configured to trust the expected policy signed by the PCR signing key. This way TPM unlocking doesn’t depend on the exact PCR measurements, but rather on the expected policy signed by the PCR signing key and the state of SecureBoot (PCR 7 measurement, including secureboot status and the list of enrolled keys).\n\nWhen the UKI image is generated, the UKI is measured and expected measurements are combined into TPM unlock policy and signed with the PCR signing key. During the boot process, systemd-stub component of the UKI performs measurements of the UKI sections into the TPM device. Talos Linux during the boot appends to the PCR register the measurements of the boot phases, and once the boot reaches the point of mounting the encrypted disk partition, the expected signed policy from the UKI is matched against measured values to unlock the TPM, and TPM unseals the disk encryption key which is then used to unlock the disk partition.\n\nDuring the upgrade, as long as the new UKI is contains PCR policy signed with the same PCR signing key, and SecureBoot state has not changed the disk partition will be unlocked successfully.\n\nDisk encryption is also tied to the state of PCR register 7, so that it unlocks only if SecureBoot is enabled and the set of enrolled keys hasn’t changed.\n\nOther Boot Options\n\nUnified Kernel Image (UKI) is a UEFI-bootable image which can be booted directly from the UEFI firmware skipping the systemd-boot bootloader. In network boot mode, the UKI can be used directly as well, as it contains the full set of boot assets required to boot Talos Linux.\n\nWhen SecureBoot is enabled, the UKI image ignores any kernel command line arguments passed to it, but rather uses the kernel command line arguments embedded into the UKI image itself. If kernel command line arguments need to be changed, the UKI image needs to be rebuilt with the new kernel command line arguments.\n\nSecureBoot with Custom Keys\nGenerating the Keys\n\nTalos requires two set of keys to be used for the SecureBoot process:\n\nSecureBoot key is used to sign the boot assets and it is enrolled into the UEFI firmware.\nPCR Signing Key is used to sign the TPM policy, which is used to seal the disk encryption key.\n\nThe same key might be used for both, but it is recommended to use separate keys for each purpose.\n\nTalos provides a utility to generate the keys, but existing PKI infrastructure can be used as well:\n\n$ talosctl gen secureboot uki --common-name \"SecureBoot Key\"\n\nwriting _out/uki-signing-cert.pem\n\nwriting _out/uki-signing-cert.der\n\nwriting _out/uki-signing-key.pem\n\n\nThe generated certificate and private key are written to disk in PEM-encoded format (RSA 4096-bit key). The certificate is also written in DER format for the systems which expect the certificate in DER format.\n\nPCR signing key can be generated with:\n\n$ talosctl gen secureboot pcr\n\nwriting _out/pcr-signing-key.pem\n\n\nThe file containing the private key is written to disk in PEM-encoded format (RSA 2048-bit key).\n\nOptionally, UEFI automatic key enrollment database can be generated using the _out/uki-signing-* files as input:\n\n$ talosctl gen secureboot database\n\nwriting _out/db.auth\n\nwriting _out/KEK.auth\n\nwriting _out/PK.auth\n\n\nThese files can be used to enroll the keys into the UEFI firmware automatically when booting from a SecureBoot ISO while UEFI firmware is in the setup mode.\n\nGenerating the SecureBoot Assets\n\nOnce the keys are generated, they can be used to sign the Talos boot assets to generate required ISO images, PXE boot assets, disk images, installer containers, etc. In this guide we will generate a SecureBoot ISO image and an installer image.\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-iso\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.5.0-alpha.3-35-ge0f383598-dirty\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\nISO ready\n\noutput asset path: /out/metal-amd64-secureboot.iso\n\n\nNext, the installer image should be generated to install Talos to disk on a SecureBoot-enabled system:\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-installer\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: installer\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\ninstaller container image ready\n\noutput asset path: /out/installer-amd64-secureboot.tar\n\n\nThe generated container image should be pushed to some container registry which Talos can access during the installation, e.g.:\n\ncrane push _out/installer-amd64-secureboot.tar ghcr.io/<user>/installer-amd64-secureboot:v1.6.2\n\n\nThe generated ISO and installer images might be further customized with system extensions, extra kernel command line arguments, etc.\n\n2.1.2 - Virtualized Platforms\nInstallation of Talos Linux for virtualization platforms.\n2.1.2.1 - Hyper-V\nCreating a Talos Kubernetes cluster using Hyper-V.\nPre-requisities\nDownload the latest metal-amd64.iso ISO from github releases page\nCreate a New-TalosVM folder in any of your PS Module Path folders $env:PSModulePath -split ';' and save the New-TalosVM.psm1 there\nPlan Overview\n\nHere we will create a basic 3 node cluster with a single control-plane node and two worker nodes. The only difference between control plane and worker node is the amount of RAM and an additional storage VHD. This is personal preference and can be configured to your liking.\n\nWe are using a VMNamePrefix argument for a VM Name prefix and not the full hostname. This command will find any existing VM with that prefix and “+1” the highest suffix it finds. For example, if VMs talos-cp01 and talos-cp02 exist, this will create VMs starting from talos-cp03, depending on NumberOfVMs argument.\n\nSetup a Control Plane Node\n\nUse the following command to create a single control plane node:\n\nNew-TalosVM -VMNamePrefix talos-cp -CPUCount 2 -StartupMemory 4GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 1 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos'\n\n\nThis will create talos-cp01 VM and power it on.\n\nSetup Worker Nodes\n\nUse the following command to create 2 worker nodes:\n\nNew-TalosVM -VMNamePrefix talos-worker -CPUCount 4 -StartupMemory 8GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 2 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos' -StorageVHDSize 50GB\n\n\nThis will create two VMs: talos-worker01 and talos-wworker02 and attach an additional VHD of 50GB for storage (which in my case will be passed to Mayastor).\n\nPushing Config to the Nodes\n\nNow that our VMs are ready, find their IP addresses from console of VM. With that information, push config to the control plane node with:\n\n# set control plane IP variable\n\n$CONTROL_PLANE_IP='10.10.10.x'\n\n\n\n# Generate talos config\n\ntalosctl gen config talos-cluster https://$($CONTROL_PLANE_IP):6443 --output-dir .\n\n\n\n# Apply config to control plane node\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file .\\controlplane.yaml\n\nPushing Config to Worker Nodes\n\nSimilarly, for the workers:\n\ntalosctl apply-config --insecure --nodes 10.10.10.x --file .\\worker.yaml\n\n\nApply the config to both nodes.\n\nBootstrap Cluster\n\nNow that our nodes are ready, we are ready to bootstrap the Kubernetes cluster.\n\n# Use following command to set node and endpoint permanantly in config so you dont have to type it everytime\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\n\n\n# Bootstrap cluster\n\ntalosctl bootstrap\n\n\n\n# Generate kubeconfig\n\ntalosctl kubeconfig .\n\n\nThis will generate the kubeconfig file, you can use to connect to the cluster.\n\n2.1.2.2 - KVM\n\nTalos is known to work on KVM.\n\nWe don’t yet have a documented guide specific to KVM; however, you can have a look at our Vagrant & Libvirt guide which uses KVM for virtualization.\n\nIf you run into any issues, our community can probably help!\n\n2.1.2.3 - Proxmox\nCreating Talos Kubernetes cluster using Proxmox.\n\nIn this guide we will create a Kubernetes cluster using Proxmox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get Proxmox\n\nIt is assumed that you have already installed Proxmox onto the server you wish to create Talos VMs on. Visit the Proxmox downloads page if necessary.\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nIn order to install Talos in Proxmox, you will need the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nUpload ISO\n\nFrom the Proxmox UI, select the “local” storage and enter the “Content” section. Click the “Upload” button:\n\nSelect the ISO you downloaded previously, then hit “Upload”\n\nCreate VMs\n\nBefore starting, familiarise yourself with the system requirements for Talos and assign VM resources accordingly.\n\nCreate a new VM by clicking the “Create VM” button in the Proxmox UI:\n\nFill out a name for the new VM:\n\nIn the OS tab, select the ISO we uploaded earlier:\n\nKeep the defaults set in the “System” tab.\n\nKeep the defaults in the “Hard Disk” tab as well, only changing the size if desired.\n\nIn the “CPU” section, give at least 2 cores to the VM:\n\nNote: As of Talos v1.0 (which requires the x86-64-v2 microarchitecture), prior to Proxmox V8.0, booting with the default Processor Type kvm64 will not work. You can enable the required CPU features after creating the VM by adding the following line in the corresponding /etc/pve/qemu-server/<vmid>.conf file:\n\nargs: -cpu kvm64,+cx16,+lahf_lm,+popcnt,+sse3,+ssse3,+sse4.1,+sse4.2\n\n\nAlternatively, you can set the Processor Type to host if your Proxmox host supports these CPU features, this however prevents using live VM migration.\n\nVerify that the RAM is set to at least 2GB:\n\nKeep the default values for networking, verifying that the VM is set to come up on the bridge interface:\n\nFinish creating the VM by clicking through the “Confirm” tab and then “Finish”.\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nNote: Talos doesn’t support memory hot plugging, if creating the VM programmatically don’t enable memory hotplug on your Talos VM’s. Doing so will cause Talos to be unable to see all available memory and have insufficient memory to complete installation of the cluster.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”.\n\nWith DHCP server\n\nOnce the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nWithout DHCP server\n\nTo apply the machine configurations in maintenance mode, VM has to have IP on the network. So you can set it on boot time manually.\n\nPress e on the boot time. And set the IP parameters for the VM. Format is:\n\nip=<client-ip>:<srv-ip>:<gw-ip>:<netmask>:<host>:<device>:<autoconf>\n\n\nFor example $CONTROL_PLANE_IP will be 192.168.0.100 and gateway 192.168.0.1\n\nlinux /boot/vmlinuz init_on_alloc=1 slab_nomerge pti=on panic=0 consoleblank=0 printk.devkmsg=on earlyprintk=ttyS0 console=tty0 console=ttyS0 talos.platform=metal ip=192.168.0.100::192.168.0.1:255.255.255.0::eth0:off\n\n\nThen press Ctrl-x or F10\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nNote: The Talos config by default will install to /dev/sda. Depending on your setup the virtual disk may be mounted differently Eg: /dev/vda. You can check for disks running the following command:\n\ntalosctl disks --insecure --nodes $CONTROL_PLANE_IP\n\n\nUpdate controlplane.yaml and worker.yaml config files to point to the correct disk location.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the Proxmox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\ntalosctl bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig .\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the Proxmox UI.\n\n2.1.2.4 - Vagrant & Libvirt\nPre-requisities\nLinux OS\nVagrant installed\nvagrant-libvirt plugin installed\ntalosctl installed\nkubectl installed\nOverview\n\nWe will use Vagrant and its libvirt plugin to create a KVM-based cluster with 3 control plane nodes and 1 worker node.\n\nFor this, we will mount Talos ISO into the VMs using a virtual CD-ROM, and configure the VMs to attempt to boot from the disk first with the fallback to the CD-ROM.\n\nWe will also configure a virtual IP address on Talos to achieve high-availability on kube-apiserver.\n\nPreparing the environment\n\nFirst, we download the latest metal-amd64.iso ISO from GitHub releases into the /tmp directory.\n\nwget --timestamping https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -O /tmp/metal-amd64.iso\n\n\nCreate a Vagrantfile with the following contents:\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define \"control-plane-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-2\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-2.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-3\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-3.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"worker-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 1\n\n      domain.memory = 1024\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/worker-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\nend\n\nBring up the nodes\n\nCheck the status of vagrant VMs:\n\nvagrant status\n\n\nYou should see the VMs in “not created” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      not created (libvirt)\n\ncontrol-plane-node-2      not created (libvirt)\n\ncontrol-plane-node-3      not created (libvirt)\n\nworker-node-1             not created (libvirt)\n\n\nBring up the vagrant environment:\n\nvagrant up --provider=libvirt\n\n\nCheck the status again:\n\nvagrant status\n\n\nNow you should see the VMs in “running” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      running (libvirt)\n\ncontrol-plane-node-2      running (libvirt)\n\ncontrol-plane-node-3      running (libvirt)\n\nworker-node-1             running (libvirt)\n\n\nFind out the IP addresses assigned by the libvirt DHCP by running:\n\nvirsh list | grep vagrant | awk '{print $2}' | xargs -t -L1 virsh domifaddr\n\n\nOutput will look like the following:\n\nvirsh domifaddr vagrant_control-plane-node-2\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet0      52:54:00:f9:10:e5    ipv4         192.168.121.119/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet1      52:54:00:0f:ae:59    ipv4         192.168.121.203/24\n\n\n\nvirsh domifaddr vagrant_worker-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet2      52:54:00:6f:28:95    ipv4         192.168.121.69/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-3\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet3      52:54:00:03:45:10    ipv4         192.168.121.125/24\n\n\nOur control plane nodes have the IPs: 192.168.121.203, 192.168.121.119, 192.168.121.125 and the worker node has the IP 192.168.121.69.\n\nNow you should be able to interact with Talos nodes that are in maintenance mode:\n\ntalosctl -n 192.168.121.203 disks --insecure\n\n\nSample output:\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID   MODALIAS                    NAME   SIZE     BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      8.6 GB   /pci0000:00/0000:00:03.0/virtio0/\n\nInstalling Talos\n\nPick an endpoint IP in the vagrant-libvirt subnet but not used by any nodes, for example 192.168.121.100.\n\nGenerate a machine configuration:\n\ntalosctl gen config my-cluster https://192.168.121.100:6443 --install-disk /dev/vda\n\n\nEdit controlplane.yaml to add the virtual IP you picked to a network interface under .machine.network.interfaces, for example:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.121.100\n\n\nApply the configuration to the initial control plane node:\n\ntalosctl -n 192.168.121.203 apply-config --insecure --file controlplane.yaml\n\n\nYou can tail the logs of the node:\n\nsudo tail -f /tmp/control-plane-node-1.log\n\n\nSet up your shell to use the generated talosconfig and configure its endpoints (use the IPs of the control plane nodes):\n\nexport TALOSCONFIG=$(realpath ./talosconfig)\n\ntalosctl config endpoint 192.168.121.203 192.168.121.119 192.168.121.125\n\n\nBootstrap the Kubernetes cluster from the initial control plane node:\n\ntalosctl -n 192.168.121.203 bootstrap\n\n\nFinally, apply the machine configurations to the remaining nodes:\n\ntalosctl -n 192.168.121.119 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.125 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.69 apply-config --insecure --file worker.yaml\n\n\nAfter a while, you should see that all the members have joined:\n\ntalosctl -n 192.168.121.203 get members\n\n\nThe output will be like the following:\n\nNODE              NAMESPACE   TYPE     ID                      VERSION   HOSTNAME                MACHINE TYPE   OS               ADDRESSES\n\n192.168.121.203   cluster     Member   talos-192-168-121-119   1         talos-192-168-121-119   controlplane   Talos (v1.1.0)   [\"192.168.121.119\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-69    1         talos-192-168-121-69    worker         Talos (v1.1.0)   [\"192.168.121.69\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-203   6         talos-192-168-121-203   controlplane   Talos (v1.1.0)   [\"192.168.121.100\",\"192.168.121.203\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-125   1         talos-192-168-121-125   controlplane   Talos (v1.1.0)   [\"192.168.121.125\"]\n\nInteracting with Kubernetes cluster\n\nRetrieve the kubeconfig from the cluster:\n\ntalosctl -n 192.168.121.203 kubeconfig ./kubeconfig\n\n\nList the nodes in the cluster:\n\nkubectl --kubeconfig ./kubeconfig get node -owide\n\n\nYou will see an output similar to:\n\nNAME                    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-192-168-121-203   Ready    control-plane,master   3m10s   v1.24.2   192.168.121.203   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-69    Ready    <none>                 2m25s   v1.24.2   192.168.121.69    <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-119   Ready    control-plane,master   8m46s   v1.24.2   192.168.121.119   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-125   Ready    control-plane,master   3m11s   v1.24.2   192.168.121.125   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\n\nCongratulations, you have a highly-available Talos cluster running!\n\nCleanup\n\nYou can destroy the vagrant environment by running:\n\nvagrant destroy -f\n\n\nAnd remove the ISO image you downloaded:\n\nsudo rm -f /tmp/metal-amd64.iso\n\n2.1.2.5 - VMware\nCreating Talos Kubernetes cluster using VMware.\nCreating a Cluster via the govc CLI\n\nIn this guide we will create an HA Kubernetes cluster with 2 worker nodes. We will use the govc cli which can be downloaded here.\n\nPrereqs/Assumptions\n\nThis guide will use the virtual IP (“VIP”) functionality that is built into Talos in order to provide a stable, known IP for the Kubernetes control plane. This simply means the user should pick an IP on their “VM Network” to designate for this purpose and keep it handy for future steps.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the VIP chosen in the prereq steps, we will now generate the base configuration files for the Talos machines. This can be done with the talosctl gen config ... command. Take note that we will also use a JSON6902 patch when creating the configs so that the control plane nodes get some special information about the VIP we chose earlier, as well as a daemonset to install vmware tools on talos nodes.\n\nFirst, download cp.patch.yaml to your local machine and edit the VIP to match your chosen IP. You can do this by issuing: curl -fsSLO https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/cp.patch.yaml. It’s contents should look like the following:\n\n- op: add\n\n  path: /machine/network\n\n  value:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: <VIP>\n\n- op: replace\n\n  path: /cluster/extraManifests\n\n  value:\n\n    - \"https://raw.githubusercontent.com/mologie/talos-vmtoolsd/master/deploy/unstable.yaml\"\n\n\nWith the patch in hand, generate machine configs with:\n\n$ talosctl gen config vmware-test https://<VIP>:<port> --config-patch-control-plane @cp.patch.yaml\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking if needed. Optionally, you can specify additional patches by adding to the cp.patch.yaml file downloaded earlier, or create your own patch files.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nSet Environment Variables\n\ngovc makes use of the following environment variables\n\nexport GOVC_URL=<vCenter url>\n\nexport GOVC_USERNAME=<vCenter username>\n\nexport GOVC_PASSWORD=<vCenter password>\n\n\nNote: If your vCenter installation makes use of self signed certificates, you’ll want to export GOVC_INSECURE=true.\n\nThere are some additional variables that you may need to set:\n\nexport GOVC_DATACENTER=<vCenter datacenter>\n\nexport GOVC_RESOURCE_POOL=<vCenter resource pool>\n\nexport GOVC_DATASTORE=<vCenter datastore>\n\nexport GOVC_NETWORK=<vCenter network>\n\nChoose Install Approach\n\nAs part of this guide, we have a more automated install script that handles some of the complexity of importing OVAs and creating VMs. If you wish to use this script, we will detail that next. If you wish to carry out the manual approach, simply skip ahead to the “Manual Approach” section.\n\nScripted Install\n\nDownload the vmware.sh script to your local machine. You can do this by issuing curl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/vmware.sh\". This script has default variables for things like Talos version and cluster name that may be interesting to tweak before deploying.\n\nImport OVA\n\nTo create a content library and import the Talos OVA corresponding to the mentioned Talos version, simply issue:\n\n./vmware.sh upload_ova\n\nCreate Cluster\n\nWith the OVA uploaded to the content library, you can create a 5 node (by default) cluster with 3 control plane and 2 worker nodes:\n\n./vmware.sh create\n\n\nThis step will create a VM from the OVA, edit the settings based on the env variables used for VM size/specs, then power on the VMs.\n\nYou may now skip past the “Manual Approach” section down to “Bootstrap Cluster”.\n\nManual Approach\nImport the OVA into vCenter\n\nA talos.ova asset is published with each release. We will refer to the version of the release as $TALOS_VERSION below. It can be easily exported with export TALOS_VERSION=\"v0.3.0-alpha.10\" or similar.\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/$TALOS_VERSION/talos.ova\n\n\nCreate a content library (if needed) with:\n\ngovc library.create <library name>\n\n\nImport the OVA to the library with:\n\ngovc library.import -n talos-${TALOS_VERSION} <library name> /path/to/downloaded/talos.ova\n\nCreate the Bootstrap Node\n\nWe’ll clone the OVA to create the bootstrap node (our first control plane node).\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-1\n\n\nTalos makes use of the guestinfo facility of VMware to provide the machine/cluster configuration. This can be set using the govc vm.change command. To facilitate persistent storage using the vSphere cloud provider integration with Kubernetes, disk.enableUUID=1 is used.\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(cat controlplane.yaml | base64)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-1\n\nUpdate Hardware Resources for the Bootstrap Node\n-c is used to configure the number of cpus\n-m is used to configure the amount of memory (in MB)\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-1\n\n\nThe following can be used to adjust the EPHEMERAL disk size.\n\ngovc vm.disk.change -vm control-plane-1 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-1\n\nCreate the Remaining Control Plane Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-2\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-3\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-3\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-2\n\n\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-3\n\ngovc vm.disk.change -vm control-plane-2 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm control-plane-3 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-2\n\n\n\ngovc vm.power -on control-plane-3\n\nUpdate Settings for the Worker Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-1\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-1\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-2\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-1\n\n\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-2\n\ngovc vm.disk.change -vm worker-1 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm worker-2 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on worker-1\n\n\n\ngovc vm.power -on worker-2\n\nBootstrap Cluster\n\nIn the vSphere UI, open a console to one of the control plane nodes. You should see some output stating that etcd should be bootstrapped. This text should look like:\n\n\"etcd is waiting to join the cluster, if this node is the first node in the cluster, please run `talosctl bootstrap` against one of the following IPs:\n\n\nTake note of the IP mentioned here and issue:\n\ntalosctl --talosconfig talosconfig bootstrap -e <control plane IP> -n <control plane IP>\n\n\nKeep this IP handy for the following steps as well.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane IP>\n\ntalosctl --talosconfig talosconfig config node <control plane IP>\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nConfigure talos-vmtoolsd\n\nThe talos-vmtoolsd application was deployed as a daemonset as part of the cluster creation; however, we must now provide a talos credentials file for it to use.\n\nCreate a new talosconfig with:\n\ntalosctl --talosconfig talosconfig -n <control plane IP> config new vmtoolsd-secret.yaml --roles os:admin\n\n\nCreate a secret from the talosconfig:\n\nkubectl -n kube-system create secret generic talos-vmtoolsd-config \\\n\n  --from-file=talosconfig=./vmtoolsd-secret.yaml\n\n\nClean up the generated file from local system:\n\nrm vmtoolsd-secret.yaml\n\n\nOnce configured, you should now see these daemonset pods go into “Running” state and in vCenter, you will now see IPs and info from the Talos nodes present in the UI.\n\n2.1.2.6 - Xen\n\nTalos is known to work on Xen. We don’t yet have a documented guide specific to Xen; however, you can follow the General Getting Started Guide. If you run into any issues, our community can probably help!\n\n2.1.3 - Cloud Platforms\nInstallation of Talos Linux on many cloud platforms.\n2.1.3.1 - AWS\nCreating a cluster via the AWS CLI.\nCreating a Cluster via the AWS CLI\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing VPC, and some familiarity with AWS. If you need more information on AWS specifics, please see the official AWS documentation.\n\nSet the needed info\n\nChange to your desired region:\n\nREGION=\"us-west-2\"\n\naws ec2 describe-vpcs --region $REGION\n\n\n\nVPC=\"(the VpcId from the above command)\"\n\nCreate the Subnet\n\nUse a CIDR block that is present on the VPC specified above.\n\naws ec2 create-subnet \\\n\n    --region $REGION \\\n\n    --vpc-id $VPC \\\n\n    --cidr-block ${CIDR_BLOCK}\n\n\nNote the subnet ID that was returned, and assign it to a variable for ease of later use:\n\nSUBNET=\"(the subnet ID of the created subnet)\"\n\nOfficial AMI Images\n\nOfficial AMI image ID can be found in the cloud-images.json file attached to the Talos release:\n\nAMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.6.2/cloud-images.json | \\\n\n    jq -r '.[] | select(.region == \"'$REGION'\") | select (.arch == \"amd64\") | .id'`\n\necho $AMI\n\n\nReplace amd64 in the line above with the desired architecture. Note the AMI id that is returned is assigned to an environment variable: it will be used later when booting instances.\n\nIf using the official AMIs, you can skip to Creating the Security group\n\nCreate your own AMIs\n\nThe use of the official Talos AMIs are recommended, but if you wish to build your own AMIs, follow the procedure below.\n\nCreate the S3 Bucket\naws s3api create-bucket \\\n\n    --bucket $BUCKET \\\n\n    --create-bucket-configuration LocationConstraint=$REGION \\\n\n    --acl private\n\nCreate the vmimport Role\n\nIn order to create an AMI, ensure that the vmimport role exists as described in the official AWS documentation.\n\nNote that the role should be associated with the S3 bucket we created above.\n\nCreate the Image Snapshot\n\nFirst, download the AWS image from a Talos release:\n\ncurl -L https://github.com/siderolabs/talos/releases/download/v1.6.2/aws-amd64.raw.xz | xz -d > disk.raw\n\n\nCopy the RAW disk to S3 and import it as a snapshot:\n\naws s3 cp disk.raw s3://$BUCKET/talos-aws-tutorial.raw\n\naws ec2 import-snapshot \\\n\n    --region $REGION \\\n\n    --description \"Talos kubernetes tutorial\" \\\n\n    --disk-container \"Format=raw,UserBucket={S3Bucket=$BUCKET,S3Key=talos-aws-tutorial.raw}\"\n\n\nSave the SnapshotId, as we will need it once the import is done. To check on the status of the import, run:\n\naws ec2 describe-import-snapshot-tasks \\\n\n    --region $REGION \\\n\n    --import-task-ids\n\n\nOnce the SnapshotTaskDetail.Status indicates completed, we can register the image.\n\nRegister the Image\naws ec2 register-image \\\n\n    --region $REGION \\\n\n    --block-device-mappings \"DeviceName=/dev/xvda,VirtualName=talos,Ebs={DeleteOnTermination=true,SnapshotId=$SNAPSHOT,VolumeSize=4,VolumeType=gp2}\" \\\n\n    --root-device-name /dev/xvda \\\n\n    --virtualization-type hvm \\\n\n    --architecture x86_64 \\\n\n    --ena-support \\\n\n    --name talos-aws-tutorial-ami\n\n\nWe now have an AMI we can use to create our cluster. Save the AMI ID, as we will need it when we create EC2 instances.\n\nAMI=\"(AMI ID of the register image command)\"\n\nCreate a Security Group\naws ec2 create-security-group \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --description \"Security Group for EC2 instances to allow ports required by Talos\"\n\n\n\nSECURITY_GROUP=\"(security group id that is returned)\"\n\n\nUsing the security group from above, allow all internal traffic within the same security group:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol all \\\n\n    --port 0 \\\n\n    --source-group talos-aws-tutorial-sg\n\n\nand expose the Talos and Kubernetes APIs:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 6443 \\\n\n    --cidr 0.0.0.0/0\n\n\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 50000-50001 \\\n\n    --cidr 0.0.0.0/0\n\n\nIf you are using KubeSpan and will be adding workers outside of AWS, you need to allow inbound UDP for the Wireguard port:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol udp --port 51820 --cidr 0.0.0.0/0\n\nCreate a Load Balancer\naws elbv2 create-load-balancer \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-lb \\\n\n    --type network --subnets $SUBNET\n\n\nTake note of the DNS name and ARN. We will need these soon.\n\nLOAD_BALANCER_ARN=\"(arn of the load balancer)\"\n\naws elbv2 create-target-group \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-tg \\\n\n    --protocol TCP \\\n\n    --port 6443 \\\n\n    --target-type ip \\\n\n    --vpc-id $VPC\n\n\nAlso note the TargetGroupArn that is returned.\n\nTARGET_GROUP_ARN=\"(target group arn)\"\n\nCreate the Machine Configuration Files\n\nUsing the DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines.\n\nNote that the port used here is the externally accessible port configured on the load balancer - 443 - not the internal port of 6443:\n\n$ talosctl gen config talos-k8s-aws-tutorial https://<load balancer DNS>:<port> --with-examples=false --with-docs=false\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nNote that the generated configs are too long for AWS userdata field if the --with-examples and --with-docs flags are not passed.\n\nAt this point, you can modify the generated configs to your liking.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the EC2 Instances\n\nchange the instance type if desired. Note: There is a known issue that prevents Talos from running on T2 instance types. Please use T3 if you need burstable instance types.\n\nCreate the Control Plane Nodes\nCP_COUNT=1\n\nwhile [[ \"$CP_COUNT\" -lt 4 ]]; do\n\n  aws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 1 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://controlplane.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP \\\n\n    --associate-public-ip-address \\\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\"\n\n  ((CP_COUNT++))\n\ndone\n\n\nMake a note of the resulting PrivateIpAddress from the controlplane nodes for later use.\n\nCreate the Worker Nodes\naws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 3 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://worker.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\"\n\nConfigure the Load Balancer\n\nNow, using the load balancer target group’s ARN, and the PrivateIpAddress from the controlplane instances that you created :\n\naws elbv2 register-targets \\\n\n    --region $REGION \\\n\n    --target-group-arn $TARGET_GROUP_ARN \\\n\n    --targets Id=$CP_NODE_1_IP  Id=$CP_NODE_2_IP  Id=$CP_NODE_3_IP\n\n\nUsing the ARNs of the load balancer and target group from previous steps, create the listener:\n\naws elbv2 create-listener \\\n\n    --region $REGION \\\n\n    --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n    --protocol TCP \\\n\n    --port 443 \\\n\n    --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN\n\nBootstrap Etcd\n\nSet the endpoints (the control plane node to which talosctl commands are sent) and nodes (the nodes that the command operates on):\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 PUBLIC IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 PUBLIC IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nThe different control plane nodes should sendi/receive traffic via the load balancer, notice that one of the control plane has intiated the etcd cluster, and the others should join. You can now watch as your cluster bootstraps, by using\n\ntalosctl --talosconfig talosconfig  health\n\n\nYou can also watch the performance of a node, via:\n\ntalosctl  --talosconfig talosconfig dashboard\n\n\nAnd use standard kubectl commands.\n\n2.1.3.2 - Azure\nCreating a cluster via the CLI on Azure.\nCreating a Cluster via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node. We assume existing Blob Storage, and some familiarity with Azure. If you need more information on Azure specifics, please see the official Azure documentation.\n\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_ACCOUNT=\"StorageAccountName\"\n\n\n\n# Storage container to upload to\n\nexport STORAGE_CONTAINER=\"StorageContainerName\"\n\n\n\n# Resource group name\n\nexport GROUP=\"ResourceGroupName\"\n\n\n\n# Location\n\nexport LOCATION=\"centralus\"\n\n\n\n# Get storage account connection string based on info above\n\nexport CONNECTION=$(az storage account show-connection-string \\\n\n                    -n $STORAGE_ACCOUNT \\\n\n                    -g $GROUP \\\n\n                    -o tsv)\n\nChoose an Image\n\nThere are two methods of deployment in this tutorial.\n\nIf you would like to use the official Talos image uploaded to Azure Community Galleries by SideroLabs, you may skip ahead to setting up your network infrastructure.\n\nNetwork Infrastructure\n\nOtherwise, if you would like to upload your own image to Azure and use it to deploy Talos, continue to Creating an Image.\n\nCreate the Image\n\nFirst, download the Azure image from a Talos release. Once downloaded, untar with tar -xvf /path/to/azure-amd64.tar.gz\n\nUpload the VHD\n\nOnce you have pulled down the image, you can upload it to blob storage with:\n\naz storage blob upload \\\n\n  --connection-string $CONNECTION \\\n\n  --container-name $STORAGE_CONTAINER \\\n\n  -f /path/to/extracted/talos-azure.vhd \\\n\n  -n talos-azure.vhd\n\nRegister the Image\n\nNow that the image is present in our blob storage, we’ll register it.\n\naz image create \\\n\n  --name talos \\\n\n  --source https://$STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER/talos-azure.vhd \\\n\n  --os-type linux \\\n\n  -g $GROUP\n\nNetwork Infrastructure\nVirtual Networks and Security Groups\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a network security group and add rules to it.\n\n# Create vnet\n\naz network vnet create \\\n\n  --resource-group $GROUP \\\n\n  --location $LOCATION \\\n\n  --name talos-vnet \\\n\n  --subnet-name talos-subnet\n\n\n\n# Create network security group\n\naz network nsg create -g $GROUP -n talos-sg\n\n\n\n# Client -> apid\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n apid \\\n\n  --priority 1001 \\\n\n  --destination-port-ranges 50000 \\\n\n  --direction inbound\n\n\n\n# Trustd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n trustd \\\n\n  --priority 1002 \\\n\n  --destination-port-ranges 50001 \\\n\n  --direction inbound\n\n\n\n# etcd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n etcd \\\n\n  --priority 1003 \\\n\n  --destination-port-ranges 2379-2380 \\\n\n  --direction inbound\n\n\n\n# Kubernetes API Server\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n kube \\\n\n  --priority 1004 \\\n\n  --destination-port-ranges 6443 \\\n\n  --direction inbound\n\nLoad Balancer\n\nWe will create a public ip, load balancer, and a health check that we will use for our control plane.\n\n# Create public ip\n\naz network public-ip create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-public-ip \\\n\n  --allocation-method static\n\n\n\n# Create lb\n\naz network lb create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-lb \\\n\n  --public-ip-address talos-public-ip \\\n\n  --frontend-ip-name talos-fe \\\n\n  --backend-pool-name talos-be-pool\n\n\n\n# Create health check\n\naz network lb probe create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-lb-health \\\n\n  --protocol tcp \\\n\n  --port 6443\n\n\n\n# Create lb rule for 6443\n\naz network lb rule create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-6443 \\\n\n  --protocol tcp \\\n\n  --frontend-ip-name talos-fe \\\n\n  --frontend-port 6443 \\\n\n  --backend-pool-name talos-be-pool \\\n\n  --backend-port 6443 \\\n\n  --probe-name talos-lb-health\n\nNetwork Interfaces\n\nIn Azure, we have to pre-create the NICs for our control plane so that they can be associated with our load balancer.\n\nfor i in $( seq 0 1 2 ); do\n\n  # Create public IP for each nic\n\n  az network public-ip create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-public-ip-$i \\\n\n    --allocation-method static\n\n\n\n\n\n  # Create nic\n\n  az network nic create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-nic-$i \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --network-security-group talos-sg \\\n\n    --public-ip-address talos-controlplane-public-ip-$i\\\n\n    --lb-name talos-lb \\\n\n    --lb-address-pools talos-be-pool\n\ndone\n\n\n\n# NOTES:\n\n# Talos can detect PublicIPs automatically if PublicIP SKU is Basic.\n\n# Use `--sku Basic` to set SKU to Basic.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(az network public-ip show \\\n\n              --resource-group $GROUP \\\n\n              --name talos-public-ip \\\n\n              --query \"ipAddress\" \\\n\n              --output tsv)\n\n\n\ntalosctl gen config talos-k8s-azure-tutorial https://${LB_PUBLIC_IP}:6443\n\nCompute Creation\n\nWe are now ready to create our azure nodes. Azure allows you to pass Talos machine configuration to the virtual machine at bootstrap time via user-data or custom-data methods.\n\nTalos supports only custom-data method, machine configuration is available to the VM only on the first boot.\n\nUse the steps below depending on whether you have manually uploaded a Talos image or if you are using the Community Gallery image.\n\nManual Image Upload\nAzure Community Gallery Image\nManual Image Upload\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image talos \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image talos \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nAzure Community Gallery Image\n\nTalos is updated in Azure’s Community Galleries (Preview) on every release.\n\nTo use the Talos image for the current release create the following environment variables.\n\nEdit the variables below if you would like to use a different architecture or version.\n\n# The architecture you would like to use. Options are \"talos-x64\" or \"talos-arm64\"\n\nARCHITECTURE=\"talos-x64\"\n\n\n\n# This will use the latest version of Talos. The version must be \"latest\" or in the format Major(int).Minor(int).Patch(int), e.g. 1.5.0\n\nVERSION=\"latest\"\n\n\nCreate the Virtual Machines.\n\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(az network public-ip show \\\n\n                    --resource-group $GROUP \\\n\n                    --name talos-controlplane-public-ip-0 \\\n\n                    --query \"ipAddress\" \\\n\n                    --output tsv)\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.3.3 - DigitalOcean\nCreating a cluster via the CLI on DigitalOcean.\nCreating a Talos Linux Cluster on Digital Ocean via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node, in the NYC region. We assume an existing Space, and some familiarity with DigitalOcean. If you need more information on DigitalOcean specifics, please see the official DigitalOcean documentation.\n\nCreate the Image\n\nDownload the DigitalOcean image digital-ocean-amd64.raw.gz from the latest Talos release.\n\nNote: the minimum version of Talos required to support Digital Ocean is v1.3.3.\n\nUsing an upload method of your choice (doctl does not have Spaces support), upload the image to a space. (It’s easy to drag the image file to the space using DigitalOcean’s web console.)\n\nNote: Make sure you upload the file as public.\n\nNow, create an image using the URL of the uploaded image:\n\nexport REGION=nyc3\n\n\n\ndoctl compute image create \\\n\n    --region $REGION \\\n\n    --image-description talos-digital-ocean-tutorial \\\n\n    --image-url https://$SPACENAME.$REGION.digitaloceanspaces.com/digital-ocean-amd64.raw.gz \\\n\n    Talos\n\n\nSave the image ID. We will need it when creating droplets.\n\nCreate a Load Balancer\ndoctl compute load-balancer create \\\n\n    --region $REGION \\\n\n    --name talos-digital-ocean-tutorial-lb \\\n\n    --tag-name talos-digital-ocean-tutorial-control-plane \\\n\n    --health-check protocol:tcp,port:6443,check_interval_seconds:10,response_timeout_seconds:5,healthy_threshold:5,unhealthy_threshold:3 \\\n\n    --forwarding-rules entry_protocol:tcp,entry_port:443,target_protocol:tcp,target_port:6443\n\n\nNote the returned ID of the load balancer.\n\nWe will need the IP of the load balancer. Using the ID of the load balancer, run:\n\ndoctl compute load-balancer get --format IP <load balancer ID>\n\n\nNote that it may take a few minutes before the load balancer is provisioned, so repeat this command until it returns with the IP address.\n\nCreate the Machine Configuration Files\n\nUsing the IP address (or DNS name, if you have created one) of the loadbalancer, generate the base configuration files for the Talos machines. Also note that the load balancer forwards port 443 to port 6443 on the associated nodes, so we should use 443 as the port in the config definition:\n\n$ talosctl gen config talos-k8s-digital-ocean-tutorial https://<load balancer IP or DNS>:443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCreate the Droplets\nCreate a dummy SSH key\n\nAlthough SSH is not used by Talos, DigitalOcean requires that an SSH key be associated with a droplet during creation. We will create a dummy key that can be used to satisfy this requirement.\n\ndoctl compute ssh-key create --public-key \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDbl0I1s/yOETIKjFr7mDLp8LmJn6OIZ68ILjVCkoN6lzKmvZEqEm1YYeWoI0xgb80hQ1fKkl0usW6MkSqwrijoUENhGFd6L16WFL53va4aeJjj2pxrjOr3uBFm/4ATvIfFTNVs+VUzFZ0eGzTgu1yXydX8lZMWnT4JpsMraHD3/qPP+pgyNuI51LjOCG0gVCzjl8NoGaQuKnl8KqbSCARIpETg1mMw+tuYgaKcbqYCMbxggaEKA0ixJ2MpFC/kwm3PcksTGqVBzp3+iE5AlRe1tnbr6GhgT839KLhOB03j7lFl1K9j1bMTOEj5Io8z7xo/XeF2ZQKHFWygAJiAhmKJ dummy@dummy.local\" dummy\n\n\nNote the ssh key ID that is returned - we will use it in creating the droplets.\n\nCreate the Control Plane Nodes\n\nRun the following commands to create three control plane nodes:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-1\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-2\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-3\n\n\nNote the droplet ID returned for the first control plane node.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --user-data-file worker.yaml \\\n\n    --ssh-keys <ssh key ID>  \\\n\n    talos-worker-1\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\ndoctl compute droplet get --format PublicIPv4 <droplet ID>\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nWe can also watch the cluster bootstrap via:\n\ntalosctl --talosconfig talosconfig health\n\n2.1.3.4 - Exoscale\nCreating a cluster via the CLI using exoscale.com\n\nTalos is known to work on exoscale.com; however, it is currently undocumented.\n\n2.1.3.5 - GCP\nCreating a cluster via the CLI on Google Cloud Platform.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in GCP with 1 worker node. We will assume an existing Cloud Storage bucket, and some familiarity with Google Cloud. If you need more information on Google Cloud specifics, please see the official Google documentation.\n\njq and talosctl also needs to be installed\n\nManual Setup\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_BUCKET=\"StorageBucketName\"\n\n# Region\n\nexport REGION=\"us-central1\"\n\nCreate the Image\n\nFirst, download the Google Cloud image from a Talos release. These images are called gcp-$ARCH.tar.gz.\n\nUpload the Image\n\nOnce you have downloaded the image, you can upload it to your storage bucket with:\n\ngsutil cp /path/to/gcp-amd64.tar.gz gs://$STORAGE_BUCKET\n\nRegister the image\n\nNow that the image is present in our bucket, we’ll register it.\n\ngcloud compute images create talos \\\n\n --source-uri=gs://$STORAGE_BUCKET/gcp-amd64.tar.gz \\\n\n --guest-os-features=VIRTIO_SCSI_MULTIQUEUE\n\nNetwork Infrastructure\nLoad Balancers and Firewalls\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a firewall, load balancer, and their required components.\n\n130.211.0.0/22 and 35.191.0.0/16 are the GCP Load Balancer IP ranges\n\n# Create Instance Group\n\ngcloud compute instance-groups unmanaged create talos-ig \\\n\n  --zone $REGION-b\n\n\n\n# Create port for IG\n\ngcloud compute instance-groups set-named-ports talos-ig \\\n\n    --named-ports tcp6443:6443 \\\n\n    --zone $REGION-b\n\n\n\n# Create health check\n\ngcloud compute health-checks create tcp talos-health-check --port 6443\n\n\n\n# Create backend\n\ngcloud compute backend-services create talos-be \\\n\n    --global \\\n\n    --protocol TCP \\\n\n    --health-checks talos-health-check \\\n\n    --timeout 5m \\\n\n    --port-name tcp6443\n\n\n\n# Add instance group to backend\n\ngcloud compute backend-services add-backend talos-be \\\n\n    --global \\\n\n    --instance-group talos-ig \\\n\n    --instance-group-zone $REGION-b\n\n\n\n# Create tcp proxy\n\ngcloud compute target-tcp-proxies create talos-tcp-proxy \\\n\n    --backend-service talos-be \\\n\n    --proxy-header NONE\n\n\n\n# Create LB IP\n\ngcloud compute addresses create talos-lb-ip --global\n\n\n\n# Forward 443 from LB IP to tcp proxy\n\ngcloud compute forwarding-rules create talos-fwd-rule \\\n\n    --global \\\n\n    --ports 443 \\\n\n    --address talos-lb-ip \\\n\n    --target-tcp-proxy talos-tcp-proxy\n\n\n\n# Create firewall rule for health checks\n\ngcloud compute firewall-rules create talos-controlplane-firewall \\\n\n     --source-ranges 130.211.0.0/22,35.191.0.0/16 \\\n\n     --target-tags talos-controlplane \\\n\n     --allow tcp:6443\n\n\n\n# Create firewall rule to allow talosctl access\n\ngcloud compute firewall-rules create talos-controlplane-talosctl \\\n\n  --source-ranges 0.0.0.0/0 \\\n\n  --target-tags talos-controlplane \\\n\n  --allow tcp:50000\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(gcloud compute forwarding-rules describe talos-fwd-rule \\\n\n               --global \\\n\n               --format json \\\n\n               | jq -r .IPAddress)\n\n\n\ntalosctl gen config talos-k8s-gcp-tutorial https://${LB_PUBLIC_IP}:443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our GCP nodes.\n\n# Create the control plane nodes.\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instances create talos-controlplane-$i \\\n\n    --image talos \\\n\n    --zone $REGION-b \\\n\n    --tags talos-controlplane \\\n\n    --boot-disk-size 20GB \\\n\n    --metadata-from-file=user-data=./controlplane.yaml\n\n    --tags talos-controlplane-$i\n\ndone\n\n\n\n# Add control plane nodes to instance group\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instance-groups unmanaged add-instances talos-ig \\\n\n      --zone $REGION-b \\\n\n      --instances talos-controlplane-$i\n\ndone\n\n\n\n# Create worker\n\ngcloud compute instances create talos-worker-0 \\\n\n  --image talos \\\n\n  --zone $REGION-b \\\n\n  --boot-disk-size 20GB \\\n\n  --metadata-from-file=user-data=./worker.yaml\n\n  --tags talos-worker-$i\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(gcloud compute instances describe talos-controlplane-0 \\\n\n                     --zone $REGION-b \\\n\n                     --format json \\\n\n                     | jq -r '.networkInterfaces[0].accessConfigs[0].natIP')\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nCleanup\n# cleanup VM's\n\ngcloud compute instances delete \\\n\n  talos-worker-0 \\\n\n  talos-controlplane-0 \\\n\n  talos-controlplane-1 \\\n\n  talos-controlplane-2\n\n\n\n# cleanup firewall rules\n\ngcloud compute firewall-rules delete \\\n\n  talos-controlplane-talosctl \\\n\n  talos-controlplane-firewall\n\n\n\n# cleanup forwarding rules\n\ngcloud compute forwarding-rules delete \\\n\n  talos-fwd-rule\n\n\n\n# cleanup addresses\n\ngcloud compute addresses delete \\\n\n  talos-lb-ip\n\n\n\n# cleanup proxies\n\ngcloud compute target-tcp-proxies delete \\\n\n  talos-tcp-proxy\n\n\n\n# cleanup backend services\n\ngcloud compute backend-services delete \\\n\n  talos-be\n\n\n\n# cleanup health checks\n\ngcloud compute health-checks delete \\\n\n  talos-health-check\n\n\n\n# cleanup unmanaged instance groups\n\ngcloud compute instance-groups unmanaged delete \\\n\n  talos-ig\n\n\n\n# cleanup Talos image\n\ngcloud compute images delete \\\n\n  talos\n\nUsing GCP Deployment manager\n\nUsing GCP deployment manager automatically creates a Google Storage bucket and uploads the Talos image to it. Once the deployment is complete the generated talosconfig and kubeconfig files are uploaded to the bucket.\n\nBy default this setup creates a three node control plane and a single worker in us-west1-b\n\nFirst we need to create a folder to store our deployment manifests and perform all subsequent operations from that folder.\n\nmkdir -p talos-gcp-deployment\n\ncd talos-gcp-deployment\n\nGetting the deployment manifests\n\nWe need to download two deployment manifests for the deployment from the Talos github repository.\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/config.yaml\"\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/talos-ha.jinja\"\n\n# if using ccm\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/gcp-ccm.yaml\"\n\nUpdating the config\n\nNow we need to update the local config.yaml file with any required changes such as changing the default zone, Talos version, machine sizes, nodes count etc.\n\nAn example config.yaml file is shown below:\n\nimports:\n\n  - path: talos-ha.jinja\n\n\n\nresources:\n\n  - name: talos-ha\n\n    type: talos-ha.jinja\n\n    properties:\n\n      zone: us-west1-b\n\n      talosVersion: v1.6.2\n\n      externalCloudProvider: false\n\n      controlPlaneNodeCount: 5\n\n      controlPlaneNodeType: n1-standard-1\n\n      workerNodeCount: 3\n\n      workerNodeType: n1-standard-1\n\noutputs:\n\n  - name: bucketName\n\n    value: $(ref.talos-ha.bucketName)\n\nEnabling external cloud provider\n\nNote: The externalCloudProvider property is set to false by default. The manifest used for deploying the ccm (cloud controller manager) is currently using the GCP ccm provided by openshift since there are no public images for the ccm yet.\n\nSince the routes controller is disabled while deploying the CCM, the CNI pods needs to be restarted after the CCM deployment is complete to remove the node.kubernetes.io/network-unavailable taint. See Nodes network-unavailable taint not removed after installing ccm for more information\n\nUse a custom built image for the ccm deployment if required.\n\nCreating the deployment\n\nNow we are ready to create the deployment. Confirm with y for any prompts. Run the following command to create the deployment:\n\n# use a unique name for the deployment, resources are prefixed with the deployment name\n\nexport DEPLOYMENT_NAME=\"<deployment name>\"\n\ngcloud deployment-manager deployments create \"${DEPLOYMENT_NAME}\" --config config.yaml\n\nRetrieving the outputs\n\nFirst we need to get the deployment outputs.\n\n# first get the outputs\n\nOUTPUTS=$(gcloud deployment-manager deployments describe \"${DEPLOYMENT_NAME}\" --format json | jq '.outputs[]')\n\n\n\nBUCKET_NAME=$(jq -r '. | select(.name == \"bucketName\").finalValue' <<< \"${OUTPUTS}\")\n\n# used when cloud controller is enabled\n\nSERVICE_ACCOUNT=$(jq -r '. | select(.name == \"serviceAccount\").finalValue' <<< \"${OUTPUTS}\")\n\nPROJECT=$(jq -r '. | select(.name == \"project\").finalValue' <<< \"${OUTPUTS}\")\n\n\nNote: If cloud controller manager is enabled, the below command needs to be run to allow the controller custom role to access cloud resources\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\nDownloading talos and kube config\n\nIn addition to the talosconfig and kubeconfig files, the storage bucket contains the controlplane.yaml and worker.yaml files used to join additional nodes to the cluster.\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/talosconfig\" .\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/kubeconfig\" .\n\nDeploying the cloud controller manager\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  apply \\\n\n  --filename gcp-ccm.yaml\n\n#  wait for the ccm to be up\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset cloud-controller-manager\n\n\nIf the cloud controller manager is enabled, we need to restart the CNI pods to remove the node.kubernetes.io/network-unavailable taint.\n\n# restart the CNI pods, in this case flannel\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout restart \\\n\n  daemonset kube-flannel\n\n# wait for the pods to be restarted\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset kube-flannel\n\nCheck cluster status\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  get nodes\n\nCleanup deployment\n\nWarning: This will delete the deployment and all resources associated with it.\n\nRun below if cloud controller manager is enabled\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\n\nNow we can finally remove the deployment\n\n# delete the objects in the bucket first\n\ngsutil -m rm -r \"gs://${BUCKET_NAME}\"\n\ngcloud deployment-manager deployments delete \"${DEPLOYMENT_NAME}\" --quiet\n\n2.1.3.6 - Hetzner\nCreating a cluster via the CLI (hcloud) on Hetzner.\nUpload image\n\nHetzner Cloud does not support uploading custom images. You can email their support to get a Talos ISO uploaded by following issues:3599 or you can prepare image snapshot by yourself.\n\nThere are two options to upload your own.\n\nRun an instance in rescue mode and replace the system OS with the Talos image\nUse Hashicorp packer to prepare an image\nRescue mode\n\nCreate a new Server in the Hetzner console. Enable the Hetzner Rescue System for this server and reboot. Upon a reboot, the server will boot a special minimal Linux distribution designed for repair and reinstall. Once running, login to the server using ssh to prepare the system disk by doing the following:\n\n# Check that you in Rescue mode\n\ndf\n\n\n\n### Result is like:\n\n# udev                   987432         0    987432   0% /dev\n\n# 213.133.99.101:/nfs 308577696 247015616  45817536  85% /root/.oldroot/nfs\n\n# overlay                995672      8340    987332   1% /\n\n# tmpfs                  995672         0    995672   0% /dev/shm\n\n# tmpfs                  398272       572    397700   1% /run\n\n# tmpfs                    5120         0      5120   0% /run/lock\n\n# tmpfs                  199132         0    199132   0% /run/user/0\n\n\n\n# Download the Talos image\n\ncd /tmp\n\nwget -O /tmp/talos.raw.xz https://github.com/siderolabs/talos/releases/download/v1.6.2/hcloud-amd64.raw.xz\n\n# Replace system\n\nxz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\n\n# shutdown the instance\n\nshutdown -h now\n\n\nTo make sure disk content is consistent, it is recommended to shut the server down before taking an image (snapshot). Once shutdown, simply create an image (snapshot) from the console. You can now use this snapshot to run Talos on the cloud.\n\nPacker\n\nInstall packer to the local machine.\n\nCreate a config file for packer to use:\n\n# hcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    hcloud = {\n\n      source  = \"github.com/hetznercloud/hcloud\"\n\n      version = \"~> 1\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nvariable \"arch\" {\n\n  type    = string\n\n  default = \"amd64\"\n\n}\n\n\n\nvariable \"server_type\" {\n\n  type    = string\n\n  default = \"cx11\"\n\n}\n\n\n\nvariable \"server_location\" {\n\n  type    = string\n\n  default = \"hel1\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/hcloud-${var.arch}.raw.xz\"\n\n}\n\n\n\nsource \"hcloud\" \"talos\" {\n\n  rescue       = \"linux64\"\n\n  image        = \"debian-11\"\n\n  location     = \"${var.server_location}\"\n\n  server_type  = \"${var.server_type}\"\n\n  ssh_username = \"root\"\n\n\n\n  snapshot_name   = \"talos system disk - ${var.arch} - ${var.talos_version}\"\n\n  snapshot_labels = {\n\n    type    = \"infra\",\n\n    os      = \"talos\",\n\n    version = \"${var.talos_version}\",\n\n    arch    = \"${var.arch}\",\n\n  }\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.hcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget\",\n\n      \"wget -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\",\n\n    ]\n\n  }\n\n}\n\n\nAdditionally you could create a file containing\n\narch            = \"arm64\"\n\nserver_type     = \"cax11\"\n\nserver_location = \"fsn1\"\n\n\nand build the snapshot for arm64.\n\nCreate a new image by issuing the commands shown below. Note that to create a new API token for your Project, switch into the Hetzner Cloud Console choose a Project, go to Access → Security, and create a new token.\n\n# First you need set API Token\n\nexport HCLOUD_TOKEN=${TOKEN}\n\n\n\n# Upload image\n\npacker init .\n\npacker build .\n\n# Save the image ID\n\nexport IMAGE_ID=<image-id-in-packer-output>\n\n\nAfter doing this, you can find the snapshot in the console interface.\n\nCreating a Cluster via the CLI\n\nThis section assumes you have the hcloud console utility on your local machine.\n\n# Set hcloud context and api key\n\nhcloud context create talos-tutorial\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nhcloud load-balancer create --name controlplane --network-zone eu-central --type lb11 --label 'type=controlplane'\n\n\n\n### Result is like:\n\n# LoadBalancer 484487 created\n\n# IPv4: 49.12.X.X\n\n# IPv6: 2a01:4f8:X:X::1\n\n\n\nhcloud load-balancer add-service controlplane \\\n\n    --listen-port 6443 --destination-port 6443 --protocol tcp\n\nhcloud load-balancer add-target controlplane \\\n\n    --label-selector 'type=controlplane'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-hcloud-tutorial https://<load balancer IP or DNS>:6443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\n\nWe can now create our servers. Note that you can find IMAGE_ID in the snapshot section of the console: https://console.hetzner.cloud/projects/$PROJECT_ID/servers/snapshots.\n\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport IMAGE_ID=<your-image-id>\n\n\n\nhcloud server create --name talos-control-plane-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-2 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location fsn1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-3 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location nbg1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nhcloud server create --name talos-worker-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=worker' \\\n\n    --user-data-from-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nhcloud server list | grep talos-control-plane\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <control-plane-1-IP>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.3.7 - Nocloud\nCreating a cluster via the CLI using qemu.\n\nTalos supports nocloud data source implementation.\n\nThere are two ways to configure Talos server with nocloud platform:\n\nvia SMBIOS “serial number” option\nusing CDROM or USB-flash filesystem\n\nNote: This requires the nocloud image which can be found on the Github Releases page.\n\nSMBIOS Serial Number\n\nThis method requires the network connection to be up (e.g. via DHCP). Configuration is delivered from the HTTP server.\n\nds=nocloud-net;s=http://10.10.0.1/configs/;h=HOSTNAME\n\n\nAfter the network initialization is complete, Talos fetches:\n\nthe machine config from http://10.10.0.1/configs/user-data\nthe network config (if available) from http://10.10.0.1/configs/network-config\nSMBIOS: QEMU\n\nAdd the following flag to qemu command line when starting a VM:\n\nqemu-system-x86_64 \\\n\n  ...\\\n\n  -smbios type=1,serial=ds=nocloud-net;s=http://10.10.0.1/configs/\n\nSMBIOS: Proxmox\n\nSet the source machine config through the serial number on Proxmox GUI.\n\nThe Proxmox stores the VM config at /etc/pve/qemu-server/$ID.conf ($ID - VM ID number of virtual machine), you will see something like:\n\n...\nsmbios1: uuid=ceae4d10,serial=ZHM9bm9jbG91ZC1uZXQ7cz1odHRwOi8vMTAuMTAuMC4xL2NvbmZpZ3Mv,base64=1\n...\n\n\nWhere serial holds the base64-encoded string version of ds=nocloud-net;s=http://10.10.0.1/configs/.\n\nCDROM/USB\n\nTalos can also get machine config from local attached storage without any prior network connection being established.\n\nYou can provide configs to the server via files on a VFAT or ISO9660 filesystem. The filesystem volume label must be cidata or CIDATA.\n\nExample: QEMU\n\nCreate and prepare Talos machine config:\n\nexport CONTROL_PLANE_IP=192.168.1.10\n\n\n\ntalosctl gen config talos-nocloud https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nPrepare cloud-init configs:\n\nmkdir -p iso\n\nmv _out/controlplane.yaml iso/user-data\n\necho \"local-hostname: controlplane-1\" > iso/meta-data\n\ncat > iso/network-config << EOF\n\nversion: 1\n\nconfig:\n\n   - type: physical\n\n     name: eth0\n\n     mac_address: \"52:54:00:12:34:00\"\n\n     subnets:\n\n        - type: static\n\n          address: 192.168.1.10\n\n          netmask: 255.255.255.0\n\n          gateway: 192.168.1.254\n\nEOF\n\n\nCreate cloud-init iso image\n\ncd iso && genisoimage -output cidata.iso -V cidata -r -J user-data meta-data network-config\n\n\nStart the VM\n\nqemu-system-x86_64 \\\n\n    ...\n\n    -cdrom iso/cidata.iso \\\n\n    ...\n\nExample: Proxmox\n\nProxmox can create cloud-init disk for you. Edit the cloud-init config information in Proxmox as follows, substitute your own information as necessary:\n\nand then update cicustom param at /etc/pve/qemu-server/$ID.conf.\n\ncicustom: user=local:snippets/controlplane-1.yml\nipconfig0: ip=192.168.1.10/24,gw=192.168.10.254\nnameserver: 1.1.1.1\nsearchdomain: local\n\n\nNote: snippets/controlplane-1.yml is Talos machine config. It is usually located at /var/lib/vz/snippets/controlplane-1.yml. This file must be placed to this path manually, as Proxmox does not support snippet uploading via API/GUI.\n\nClick on Regenerate Image button after the above changes are made.\n\n2.1.3.8 - Openstack\nCreating a cluster via the CLI on Openstack.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in Openstack with 1 worker node. We will assume an existing some familiarity with Openstack. If you need more information on Openstack specifics, please see the official Openstack documentation.\n\nEnvironment Setup\n\nYou should have an existing openrc file. This file will provide environment variables necessary to talk to your Openstack cloud. See here for instructions on fetching this file.\n\nCreate the Image\n\nFirst, download the Openstack image from a Talos release. These images are called openstack-$ARCH.tar.gz. Untar this file with tar -xvf openstack-$ARCH.tar.gz. The resulting file will be called disk.raw.\n\nUpload the Image\n\nOnce you have the image, you can upload to Openstack with:\n\nopenstack image create --public --disk-format raw --file disk.raw talos\n\nNetwork Infrastructure\nLoad Balancer and Network Ports\n\nOnce the image is prepared, you will need to work through setting up the network. Issue the following to create a load balancer, the necessary network ports for each control plane node, and associations between the two.\n\nCreating loadbalancer:\n\n# Create load balancer, updating vip-subnet-id if necessary\n\nopenstack loadbalancer create --name talos-control-plane --vip-subnet-id public\n\n\n\n# Create listener\n\nopenstack loadbalancer listener create --name talos-control-plane-listener --protocol TCP --protocol-port 6443 talos-control-plane\n\n\n\n# Pool and health monitoring\n\nopenstack loadbalancer pool create --name talos-control-plane-pool --lb-algorithm ROUND_ROBIN --listener talos-control-plane-listener --protocol TCP\n\nopenstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP talos-control-plane-pool\n\n\nCreating ports:\n\n# Create ports for control plane nodes, updating network name if necessary\n\nopenstack port create --network shared talos-control-plane-1\n\nopenstack port create --network shared talos-control-plane-2\n\nopenstack port create --network shared talos-control-plane-3\n\n\n\n# Create floating IPs for the ports, so that you will have talosctl connectivity to each control plane\n\nopenstack floating ip create --port talos-control-plane-1 public\n\nopenstack floating ip create --port talos-control-plane-2 public\n\nopenstack floating ip create --port talos-control-plane-3 public\n\n\nNote: Take notice of the private and public IPs associated with each of these ports, as they will be used in the next step. Additionally, take node of the port ID, as it will be used in server creation.\n\nAssociate port’s private IPs to loadbalancer:\n\n# Create members for each port IP, updating subnet-id and address as necessary.\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-1 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-2 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-3 PORT> --protocol-port 6443 talos-control-plane-pool\n\nSecurity Groups\n\nThis example uses the default security group in Openstack. Ports have been opened to ensure that connectivity from both inside and outside the group is possible. You will want to allow, at a minimum, ports 6443 (Kubernetes API server) and 50000 (Talos API) from external sources. It is also recommended to allow communication over all ports from within the subnet.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(openstack loadbalancer show talos-control-plane -f json | jq -r .vip_address)\n\n\n\ntalosctl gen config talos-k8s-openstack-tutorial https://${LB_PUBLIC_IP}:6443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our Openstack nodes.\n\nCreate control plane:\n\n# Create control planes 2 and 3, substituting the same info.\n\nfor i in $( seq 1 3 ); do\n\n  openstack server create talos-control-plane-$i --flavor m1.small --nic port-id=talos-control-plane-$i --image talos --user-data /path/to/controlplane.yaml\n\ndone\n\n\nCreate worker:\n\n# Update network name as necessary.\n\nopenstack server create talos-worker-1 --flavor m1.small --network shared --image talos --user-data /path/to/worker.yaml\n\n\nNote: This step can be repeated to add more workers.\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will use one of the floating IPs we allocated earlier. It does not matter which one.\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.3.9 - Oracle\nCreating a cluster via the CLI (oci) on OracleCloud.com.\nUpload image\n\nOracle Cloud at the moment does not have a Talos official image. So you can use Bring Your Own Image (BYOI) approach.\n\nOnce the image is uploaded, set the Boot volume type to Paravirtualized mode.\n\nOracleCloud has highly available NTP service, it can be enabled in Talos machine config with:\n\nmachine:\n\n  time:\n\n    servers:\n\n      - 169.254.169.254\n\nCreating a Cluster via the CLI\n\nLogin to the console. And open the Cloud Shell.\n\nCreate a network\nexport cidr_block=10.0.0.0/16\n\nexport subnet_block=10.0.0.0/24\n\nexport compartment_id=<substitute-value-of-compartment_id> # https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/latest/oci_cli_docs/cmdref/network/vcn/create.html#cmdoption-compartment-id\n\n\n\nexport vcn_id=$(oci network vcn create --cidr-block $cidr_block --display-name talos-example --compartment-id $compartment_id --query data.id --raw-output)\n\nexport rt_id=$(oci network subnet create --cidr-block $subnet_block --display-name kubernetes --compartment-id $compartment_id --vcn-id $vcn_id --query data.route-table-id --raw-output)\n\nexport ig_id=$(oci network internet-gateway create --compartment-id $compartment_id --is-enabled true --vcn-id $vcn_id --query data.id --raw-output)\n\n\n\noci network route-table update --rt-id $rt_id --route-rules \"[{\\\"cidrBlock\\\":\\\"0.0.0.0/0\\\",\\\"networkEntityId\\\":\\\"$ig_id\\\"}]\" --force\n\n\n\n# disable firewall\n\nexport sl_id=$(oci network vcn list --compartment-id $compartment_id --query 'data[0].\"default-security-list-id\"' --raw-output)\n\n\n\noci network security-list update --security-list-id $sl_id --egress-security-rules '[{\"destination\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --ingress-security-rules '[{\"source\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --force\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer create --compartment-id $compartment_id --display-name controlplane-lb --subnet-id $subnet_id --is-preserve-source-destination false --is-private false --query data.id --raw-output)\n\n\n\ncat <<EOF > talos-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 50000,\n\n  \"protocol\": \"TCP\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://talos-health-checker.json --name talos --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name talos --name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --protocol TCP\n\n\n\ncat <<EOF > controlplane-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 6443,\n\n  \"protocol\": \"HTTPS\",\n\n  \"returnCode\": 401,\n\n  \"urlPath\": \"/readyz\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://controlplane-health-checker.json --name controlplane --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name controlplane --name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --protocol TCP\n\n\n\n# Save the external IP\n\noci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].\"ip-addresses\"'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-oracle-tutorial https://<load balancer IP or DNS>:6443 --additional-sans <load balancer IP or DNS>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport shape='VM.Standard.A1.Flex'\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --shape $shape --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].id' --raw-output)\n\n\n\ncat <<EOF > shape.json\n\n{\n\n  \"memoryInGBs\": 4,\n\n  \"ocpus\": 1\n\n}\n\nEOF\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-1 --private-ip 10.0.0.11 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-2 --private-ip 10.0.0.12 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-3 --private-ip 10.0.0.13 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport shape='VM.Standard.E2.1.Micro'\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-1 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-2 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-3 --assign-public-ip true --user-data-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nexport instance_id=$(oci compute instance list --compartment-id $compartment_id --display-name controlplane-1 --query 'data[0].id' --raw-output)\n\n\n\noci compute instance list-vnics --instance-id $instance_id --query 'data[0].\"private-ip\"' --raw-output\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <load balancer IP or DNS>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2.1.3.10 - Scaleway\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nTalos is known to work on scaleway.com; however, it is currently undocumented.\n\n2.1.3.11 - UpCloud\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nIn this guide we will create an HA Kubernetes cluster 3 control plane nodes and 1 worker node. We assume some familiarity with UpCloud. If you need more information on UpCloud specifics, please see the official UpCloud documentation.\n\nCreate the Image\n\nThe best way to create an image for UpCloud, is to build one using Hashicorp packer, with the upcloud-amd64.raw.xz image found on the Talos Releases. Using the general ISO is also possible, but the UpCloud image has some UpCloud specific features implemented, such as the fetching of metadata and user data to configure the nodes.\n\nTo create the cluster, you need a few things locally installed:\n\nUpCloud CLI\nHashicorp Packer\n\nNOTE: Make sure your account allows API connections. To do so, log into UpCloud control panel and go to People -> Account -> Permissions -> Allow API connections checkbox. It is recommended to create a separate subaccount for your API access and only set the API permission.\n\nTo use the UpCloud CLI, you need to create a config in $HOME/.config/upctl.yaml\n\nusername: your_upcloud_username\n\npassword: your_upcloud_password\n\n\nTo use the UpCloud packer plugin, you need to also export these credentials to your environment variables, by e.g. putting the following in your .bashrc or .zshrc\n\nexport UPCLOUD_USERNAME=\"<username>\"\n\nexport UPCLOUD_PASSWORD=\"<password>\"\n\n\nNext create a config file for packer to use:\n\n# upcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    upcloud = {\n\n      version = \">=v1.0.0\"\n\n      source  = \"github.com/UpCloudLtd/upcloud\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/upcloud-amd64.raw.xz\"\n\n}\n\n\n\nvariable \"username\" {\n\n  type        = string\n\n  description = \"UpCloud API username\"\n\n  default     = \"${env(\"UPCLOUD_USERNAME\")}\"\n\n}\n\n\n\nvariable \"password\" {\n\n  type        = string\n\n  description = \"UpCloud API password\"\n\n  default     = \"${env(\"UPCLOUD_PASSWORD\")}\"\n\n  sensitive   = true\n\n}\n\n\n\nsource \"upcloud\" \"talos\" {\n\n  username        = \"${var.username}\"\n\n  password        = \"${var.password}\"\n\n  zone            = \"us-nyc1\"\n\n  storage_name    = \"Debian GNU/Linux 11 (Bullseye)\"\n\n  template_name   = \"Talos (${var.talos_version})\"\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.upcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget xz-utils\",\n\n      \"wget -q -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/vda\",\n\n    ]\n\n  }\n\n\n\n  provisioner \"shell-local\" {\n\n      inline = [\n\n      \"upctl server stop --type hard custom\",\n\n      ]\n\n  }\n\n}\n\n\nNow create a new image by issuing the commands shown below.\n\npacker init .\n\npacker build .\n\n\nAfter doing this, you can find the custom image in the console interface under storage.\n\nCreating a Cluster via the CLI\nCreate an Endpoint\n\nTo communicate with the Talos cluster you will need a single endpoint that is used to access the cluster. This can either be a loadbalancer that will sit in front of all your control plane nodes, a DNS name with one or more A or AAAA records pointing to the control plane nodes, or directly the IP of a control plane node.\n\nWhich option is best for you will depend on your needs. Endpoint selection has been further documented here.\n\nAfter you decide on which endpoint to use, note down the domain name or IP, as we will need it in the next step.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the endpoint created earlier, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-upcloud-tutorial https://<load balancer IP or DNS>:<port> --install-disk /dev/vda\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Depending on the Kubernetes version you want to run, you might need to select a different Talos version, as not all versions are compatible. You can find the support matrix here.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch or yamlpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nRun the following to create three total control plane nodes:\n\nfor ID in $(seq 3); do\n\n    upctl server create \\\n\n      --zone us-nyc1 \\\n\n      --title talos-us-nyc1-master-$ID \\\n\n      --hostname talos-us-nyc1-master-$ID \\\n\n      --plan 2xCPU-4GB \\\n\n      --os \"Talos (v1.6.2)\" \\\n\n      --user-data \"$(cat controlplane.yaml)\" \\\n\n      --enable-metada\n\ndone\n\n\nNote: modify the zone and OS depending on your preferences. The OS should match the template name generated with packer in the previous step.\n\nNote the IP address of the first control plane node, as we will need it later.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nupctl server create \\\n\n  --zone us-nyc1 \\\n\n  --title talos-us-nyc1-worker-1 \\\n\n  --hostname talos-us-nyc1-worker-1 \\\n\n  --plan 2xCPU-4GB \\\n\n  --os \"Talos (v1.6.2)\" \\\n\n  --user-data \"$(cat worker.yaml)\" \\\n\n  --enable-metada\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP, as noted earlier. We only add one node IP, as that is the entry into our cluster against which our commands will be run. All requests to other nodes are proxied through the endpoint, and therefore not all nodes need to be manually added to the config. You don’t want to run your commands against all nodes, as this can destroy your cluster if you are not careful (further documentation).\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig\n\n\nIt will take a few minutes before Kubernetes has been fully bootstrapped, and is accessible.\n\nYou can check if the nodes are registered in Talos by running\n\ntalosctl --talosconfig talosconfig get members\n\n\nTo check if your nodes are ready, run\n\nkubectl get nodes\n\n2.1.3.12 - Vultr\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\nCreating a Cluster using the Vultr CLI\n\nThis guide will demonstrate how to create a highly-available Kubernetes cluster with one worker using the Vultr cloud provider.\n\nVultr have a very well documented REST API, and an open-source CLI tool to interact with the API which will be used in this guide. Make sure to follow installation and authentication instructions for the vultr-cli tool.\n\nBoot Options\nUpload an ISO Image\n\nFirst step is to make the Talos ISO available to Vultr by uploading the latest release of the ISO to the Vultr ISO server.\n\nvultr-cli iso create --url https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\n\n\nMake a note of the ID in the output, it will be needed later when creating the instances.\n\nPXE Booting via Image Factory\n\nTalos Linux can be PXE-booted on Vultr using Image Factory, using the vultr platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/vultr-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate a Load Balancer\n\nA load balancer is needed to serve as the Kubernetes endpoint for the cluster.\n\nvultr-cli load-balancer create \\\n\n   --region $REGION \\\n\n   --label \"Talos Kubernetes Endpoint\" \\\n\n   --port 6443 \\\n\n   --protocol tcp \\\n\n   --check-interval 10 \\\n\n   --response-timeout 5 \\\n\n   --healthy-threshold 5 \\\n\n   --unhealthy-threshold 3 \\\n\n   --forwarding-rules frontend_protocol:tcp,frontend_port:443,backend_protocol:tcp,backend_port:6443\n\n\nMake a note of the ID of the load balancer from the output of the above command, it will be needed after the control plane instances are created.\n\nvultr-cli load-balancer get $LOAD_BALANCER_ID | grep ^IP\n\n\nMake a note of the IP address, it will be needed later when generating the configuration.\n\nCreate the Machine Configuration\nGenerate Base Configuration\n\nUsing the IP address (or DNS name if one was created) of the load balancer created above, generate the machine configuration files for the new cluster.\n\ntalosctl gen config talos-kubernetes-vultr https://$LOAD_BALANCER_ADDRESS\n\n\nOnce generated, the machine configuration can be modified as necessary for the new cluster, for instance updating disk installation, or adding SANs for the certificates.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode cloud\n\ntalosctl validate --config worker.yaml --mode cloud\n\nCreate the Nodes\nCreate the Control Plane Nodes\n\nFirst a control plane needs to be created, with the example below creating 3 instances in a loop. The instance type (noted by the --plan vc2-2c-4gb argument) in the example is for a minimum-spec control plane node, and should be updated to suit the cluster being created.\n\nfor id in $(seq 3); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-2c-4gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-cp${id} \\\n\n        --label \"Talos Kubernetes Control Plane\" \\\n\n        --tags talos,kubernetes,control-plane\n\ndone\n\n\nMake a note of the instance IDs, as they are needed to attach to the load balancer created earlier.\n\nvultr-cli load-balancer update $LOAD_BALANCER_ID --instances $CONTROL_PLANE_1_ID,$CONTROL_PLANE_2_ID,$CONTROL_PLANE_3_ID\n\n\nOnce the nodes are booted and waiting in maintenance mode, the machine configuration can be applied to each one in turn.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_1_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_2_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_3_ADDRESS --file controlplane.yaml\n\nCreate the Worker Nodes\n\nNow worker nodes can be created and configured in a similar way to the control plane nodes, the difference being mainly in the machine configuration file. Note that like with the control plane nodes, the instance type (here set by --plan vc2-1-1gb) should be changed for the actual cluster requirements.\n\nfor id in $(seq 1); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-1c-1gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-worker${id} \\\n\n        --label \"Talos Kubernetes Worker\" \\\n\n        --tags talos,kubernetes,worker\n\ndone\n\n\nOnce the worker is booted and in maintenance mode, the machine configuration can be applied in the following manner.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $WORKER_1_ADDRESS --file worker.yaml\n\nBootstrap etcd\n\nOnce all the cluster nodes are correctly configured, the cluster can be bootstrapped to become functional. It is important that the talosctl bootstrap command be executed only once and against only a single control plane node.\n\ntalosctl --talosconfig talosconfig boostrap --endpoints $CONTROL_PLANE_1_ADDRESS --nodes $CONTROL_PLANE_1_ADDRESS\n\nConfigure Endpoints and Nodes\n\nWhile the cluster goes through the bootstrapping process and beings to self-manage, the talosconfig can be updated with the endpoints and nodes.\n\ntalosctl --talosconfig talosconfig config endpoints $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS\n\ntalosctl --talosconfig talosconfig config nodes $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS WORKER_1_ADDRESS\n\nRetrieve the kubeconfig\n\nFinally, with the cluster fully running, the administrative kubeconfig can be retrieved from the Talos API to be saved locally.\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nNow the kubeconfig can be used by any of the usual Kubernetes tools to interact with the Talos-based Kubernetes cluster as normal.\n\n2.1.4 - Local Platforms\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\n2.1.4.1 - Docker\nCreating Talos Kubernetes cluster using Docker.\n\nIn this guide we will create a Kubernetes cluster in Docker, using a containerized version of Talos.\n\nRunning Talos in Docker is intended to be used in CI pipelines, and local testing when you need a quick and easy cluster. Furthermore, if you are running Talos in production, it provides an excellent way for developers to develop against the same version of Talos.\n\nRequirements\n\nThe follow are requirements for running Talos in Docker:\n\nDocker 18.03 or greater\na recent version of talosctl\nCaveats\n\nDue to the fact that Talos will be running in a container, certain APIs are not available. For example upgrade, reset, and similar APIs don’t apply in container mode. Further, when running on a Mac in docker, due to networking limitations, VIPs are not supported.\n\nCreate the Cluster\n\nCreating a local cluster is as simple as:\n\ntalosctl cluster create --wait\n\n\nOnce the above finishes successfully, your talosconfig(~/.talos/config) will be configured to point to the new cluster.\n\nNote: Startup times can take up to a minute or more before the cluster is available.\n\nFinally, we just need to specify which nodes you want to communicate with using talosctl. Talosctl can operate on one or all the nodes in the cluster – this makes cluster wide commands much easier.\n\ntalosctl config nodes 10.5.0.2 10.5.0.3\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nCleaning Up\n\nTo cleanup, run:\n\ntalosctl cluster destroy\n\nRunning Talos in Docker Manually\n\nTo run Talos in a container manually, run:\n\ndocker run --rm -it \\\n\n  --name tutorial \\\n\n  --hostname talos-cp \\\n\n  --read-only \\\n\n  --privileged \\\n\n  --security-opt seccomp=unconfined \\\n\n  --mount type=tmpfs,destination=/run \\\n\n  --mount type=tmpfs,destination=/system \\\n\n  --mount type=tmpfs,destination=/tmp \\\n\n  --mount type=volume,destination=/system/state \\\n\n  --mount type=volume,destination=/var \\\n\n  --mount type=volume,destination=/etc/cni \\\n\n  --mount type=volume,destination=/etc/kubernetes \\\n\n  --mount type=volume,destination=/usr/libexec/kubernetes \\\n\n  --mount type=volume,destination=/usr/etc/udev \\\n\n  --mount type=volume,destination=/opt \\\n\n  -e PLATFORM=container \\\n\n  ghcr.io/siderolabs/talos:v1.6.2\n\n2.1.4.2 - QEMU\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nIn this guide we will create a Kubernetes cluster using QEMU.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\nLinux\na kernel with\nKVM enabled (/dev/kvm must exist)\nCONFIG_NET_SCH_NETEM enabled\nCONFIG_NET_SCH_INGRESS enabled\nat least CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities\nQEMU\nbridge, static and firewall CNI plugins from the standard CNI plugins, and tc-redirect-tap CNI plugin from the awslabs tc-redirect-tap installed to /opt/cni/bin (installed automatically by talosctl)\niptables\n/var/run/netns directory should exist\nInstallation\nHow to get QEMU\n\nInstall QEMU with your operating system package manager. For example, on Ubuntu for x86:\n\napt install qemu-system-x86 qemu-kvm\n\nInstall talosctl\n\nDownload talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nInstall Talos kernel and initramfs\n\nQEMU provisioner depends on Talos kernel (vmlinuz) and initramfs (initramfs.xz). These files can be downloaded from the Talos release:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/vmlinuz-<arch> -L -o _out/vmlinuz-<arch>\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/initramfs-<arch>.xz -L -o _out/initramfs-<arch>.xz\n\n\nFor example version v1.6.2:\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/vmlinuz-amd64 -L -o _out/vmlinuz-amd64\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/initramfs-amd64.xz -L -o _out/initramfs-amd64.xz\n\nCreate the Cluster\n\nFor the first time, create root state directory as your user so that you can inspect the logs as non-root user:\n\nmkdir -p ~/.talos/clusters\n\n\nCreate the cluster:\n\nsudo --preserve-env=HOME talosctl cluster create --provisioner qemu\n\n\nBefore the first cluster is created, talosctl will download the CNI bundle for the VM provisioning and install it to ~/.talos/cni directory.\n\nOnce the above finishes successfully, your talosconfig (~/.talos/config) will be configured to point to the new cluster, and kubeconfig will be downloaded and merged into default kubectl config location (~/.kube/config).\n\nCluster provisioning process can be optimized with registry pull-through caches.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl -n 10.5.0.2 containers for a list of containers in the system namespace, or talosctl -n 10.5.0.2 containers -k for the k8s.io namespace. To view the logs of a container, use talosctl -n 10.5.0.2 logs <container> or talosctl -n 10.5.0.2 logs -k <container>.\n\nA bridge interface will be created, and assigned the default IP 10.5.0.1. Each node will be directly accessible on the subnet specified at cluster creation time. A loadbalancer runs on 10.5.0.1 by default, which handles loadbalancing for the Kubernetes APIs.\n\nYou can see a summary of the cluster state by running:\n\n$ talosctl cluster show --provisioner qemu\n\nPROVISIONER       qemu\n\nNAME              talos-default\n\nNETWORK NAME      talos-default\n\nNETWORK CIDR      10.5.0.0/24\n\nNETWORK GATEWAY   10.5.0.1\n\nNETWORK MTU       1500\n\n\n\nNODES:\n\n\n\nNAME                           TYPE           IP         CPU    RAM      DISK\n\ntalos-default-controlplane-1   ControlPlane   10.5.0.2   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-2   ControlPlane   10.5.0.3   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-3   ControlPlane   10.5.0.4   1.00   1.6 GB   4.3 GB\n\ntalos-default-worker-1         Worker         10.5.0.5   1.00   1.6 GB   4.3 GB\n\nCleaning Up\n\nTo cleanup, run:\n\nsudo --preserve-env=HOME talosctl cluster destroy --provisioner qemu\n\n\nNote: In that case that the host machine is rebooted before destroying the cluster, you may need to manually remove ~/.talos/clusters/talos-default.\n\nManual Clean Up\n\nThe talosctl cluster destroy command depends heavily on the clusters state directory. It contains all related information of the cluster. The PIDs and network associated with the cluster nodes.\n\nIf you happened to have deleted the state folder by mistake or you would like to cleanup the environment, here are the steps how to do it manually:\n\nRemove VM Launchers\n\nFind the process of talosctl qemu-launch:\n\nps -elf | grep 'talosctl qemu-launch'\n\n\nTo remove the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 157615 and 157617\n\nps -elf | grep '[t]alosctl qemu-launch'\n\n0 S root      157615    2835  0  80   0 - 184934 -     07:53 ?        00:00:00 talosctl qemu-launch\n\n0 S root      157617    2835  0  80   0 - 185062 -     07:53 ?        00:00:00 talosctl qemu-launch\n\nsudo kill -s SIGTERM 157615\n\nsudo kill -s SIGTERM 157617\n\nStopping VMs\n\nFind the process of qemu-system:\n\nps -elf | grep 'qemu-system'\n\n\nTo stop the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 158065 and 158216\n\nps -elf | grep qemu-system\n\n2 S root     1061663 1061168 26  80   0 - 1786238 -    14:05 ?        01:53:56 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/home/username/.talos/clusters/talos-default/bootstrap-master.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=1e:86:c6:b4:7c:c4 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=7ec0a73c-826e-4eeb-afd1-39ff9f9160ca -machine q35,accel=kvm\n\n2 S root     1061663 1061170 67  80   0 - 621014 -     21:23 ?        00:00:07 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/homeusername/.talos/clusters/talos-default/pxe-1.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=36:f3:2f:c3:9f:06 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=ce12a0d0-29c8-490f-b935-f6073ab916a6 -machine q35,accel=kvm\n\nsudo kill -s SIGTERM 1061663\n\nsudo kill -s SIGTERM 1061663\n\nRemove load balancer\n\nFind the process of talosctl loadbalancer-launch:\n\nps -elf | grep 'talosctl loadbalancer-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl loadbalancer-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl loadbalancer-launch --loadbalancer-addr 10.5.0.1 --loadbalancer-upstreams 10.5.0.2\n\nsudo kill -s SIGTERM 157609\n\nRemove DHCP server\n\nFind the process of talosctl dhcpd-launch:\n\nps -elf | grep 'talosctl dhcpd-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl dhcpd-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl dhcpd-launch --state-path /home/username/.talos/clusters/talos-default --addr 10.5.0.1 --interface talosbd9c32bc\n\nsudo kill -s SIGTERM 157609\n\nRemove network\n\nThis is more tricky part as if you have already deleted the state folder. If you didn’t then it is written in the state.yaml in the ~/.talos/clusters/<cluster-name> directory.\n\nsudo cat ~/.talos/clusters/<cluster-name>/state.yaml | grep bridgename\n\nbridgename: talos<uuid>\n\n\nIf you only had one cluster, then it will be the interface with name talos<uuid>\n\n46: talos<uuid>: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n\n    link/ether a6:72:f4:0a:d3:9c brd ff:ff:ff:ff:ff:ff\n\n    inet 10.5.0.1/24 brd 10.5.0.255 scope global talos17c13299\n\n       valid_lft forever preferred_lft forever\n\n    inet6 fe80::a472:f4ff:fe0a:d39c/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n\nTo remove this interface:\n\nsudo ip link del talos<uuid>\n\nRemove state directory\n\nTo remove the state directory execute:\n\nsudo rm -Rf /home/$USER/.talos/clusters/<cluster-name>\n\nTroubleshooting\nLogs\n\nInspect logs directory\n\nsudo cat ~/.talos/clusters/<cluster-name>/*.log\n\n\nLogs are saved under <cluster-name>-<role>-<node-id>.log\n\nFor example in case of k8s cluster name:\n\nls -la ~/.talos/clusters/k8s | grep log\n\n-rw-r--r--. 1 root root      69415 Apr 26 20:58 k8s-master-1.log\n\n-rw-r--r--. 1 root root      68345 Apr 26 20:58 k8s-worker-1.log\n\n-rw-r--r--. 1 root root      24621 Apr 26 20:59 lb.log\n\n\nInspect logs during the installation\n\ntail -f ~/.talos/clusters/<cluster-name>/*.log\n\n2.1.4.3 - VirtualBox\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\nIn this guide we will create a Kubernetes cluster using VirtualBox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get VirtualBox\n\nInstall VirtualBox with your operating system package manager or from the website. For example, on Ubuntu for x86:\n\napt install virtualbox\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nDownload the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nCreate VMs\n\nStart by creating a new VM by clicking the “New” button in the VirtualBox UI:\n\nSupply a name for this VM, and specify the Type and Version:\n\nEdit the memory to supply at least 2GB of RAM for the VM:\n\nProceed through the disk settings, keeping the defaults. You can increase the disk space if desired.\n\nOnce created, select the VM and hit “Settings”:\n\nIn the “System” section, supply at least 2 CPUs:\n\nIn the “Network” section, switch the network “Attached To” section to “Bridged Adapter”:\n\nFinally, in the “Storage” section, select the optical drive and, on the right, select the ISO by browsing your filesystem:\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”. Once the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-vbox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the VirtualBox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig $TALOSCONFIG config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig $TALOSCONFIG config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig $TALOSCONFIG bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig $TALOSCONFIG kubeconfig .\n\n\nYou can then use kubectl in this fashion:\n\nkubectl get nodes\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the VirtualBox UI.\n\n2.1.5 - Single Board Computers\nInstallation of Talos Linux on single-board computers.\n2.1.5.1 - Banana Pi M64\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-bananapi_m64-arm64.raw.xz\n\nxz -d metal-bananapi_m64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-bananapi_m64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.2 - Friendlyelec Nano PI R4S\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-nanopi_r4s-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-nanopi_r4s-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.3 - Jetson Nano\nInstalling Talos on Jetson Nano SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card/USB drive\ncrane CLI\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nFlashing the firmware to on-board SPI flash\n\nFlashing the firmware only needs to be done once.\n\nWe will use the R32.7.2 release for the Jetson Nano. Most of the instructions is similar to this doc except that we’d be using a upstream version of u-boot with patches from NVIDIA u-boot so that USB boot also works.\n\nBefore flashing we need the following:\n\nA USB-A to micro USB cable\nA jumper wire to enable recovery mode\nA HDMI monitor to view the logs if the USB serial adapter is not available\nA USB to Serial adapter with 3.3V TTL (optional)\nA 5V DC barrel jack\n\nIf you’re planning to use the serial console follow the documentation here\n\nFirst start by downloading the Jetson Nano L4T release.\n\ncurl -SLO https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/t210/jetson-210_linux_r32.7.2_aarch64.tbz2\n\n\nNext we will extract the L4T release and replace the u-boot binary with the patched version.\n\ntar xf jetson-210_linux_r32.6.1_aarch64.tbz2\n\ncd Linux_for_Tegra\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C bootloader/t210ref/p3450-0000/ jetson_nano/u-boot.bin\n\n\nNext we will flash the firmware to the Jetson Nano SPI flash. In order to do that we need to put the Jetson Nano into Force Recovery Mode (FRC). We will use the instructions from here\n\nEnsure that the Jetson Nano is powered off. There is no need for the SD card/USB storage/network cable to be connected\nConnect the micro USB cable to the micro USB port on the Jetson Nano, don’t plug the other end to the PC yet\nEnable Force Recovery Mode (FRC) by placing a jumper across the FRC pins on the Jetson Nano\nFor board revision A02, these are pins 3 and 4 of header J40\nFor board revision B01, these are pins 9 and 10 of header J50\nPlace another jumper across J48 to enable power from the DC jack and connect the Jetson Nano to the DC jack J25\nNow connect the other end of the micro USB cable to the PC and remove the jumper wire from the FRC pins\n\nNow the Jetson Nano is in Force Recovery Mode (FRC) and can be confirmed by running the following command\n\nlsusb | grep -i \"nvidia\"\n\n\nNow we can move on the flashing the firmware.\n\nsudo ./flash p3448-0000-max-spi external\n\n\nThis will flash the firmware to the Jetson Nano SPI flash and you’ll see a lot of output. If you’ve connected the serial console you’ll also see the progress there. Once the flashing is done you can disconnect the USB cable and power off the Jetson Nano.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-jetson_nano-arm64.raw.xz\n\nxz -d metal-jetson_nano-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card/USB storage:\n\nsudo dd if=metal-jetson_nano-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M status=progress\n\n\n| Replace /dev/mmcblk0 with the name of your SD card/USB storage.\n\nBootstrapping the Node\n\nInsert the SD card/USB storage to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.4 - Libre Computer Board ALL-H3-CC\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nxz -d metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-libretech_all_h3_cc_h5-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.5 - Pine64\nInstalling Talos on a Pine64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-pine64-arm64.raw.xz\n\nxz -d metal-pine64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-pine64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.6 - Pine64 Rock64\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rock64-arm64.raw.xz\n\nxz -d metal-rock64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rock64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.7 - Radxa ROCK PI 4\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.8 - Radxa ROCK PI 4C\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4c-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4c/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4c-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2.1.5.9 - Raspberry Pi Series\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\nTalos disk image for the Raspberry Pi generic should in theory work for the boards supported by u-boot rpi_arm64_defconfig. This has only been officialy tested on the Raspberry Pi 4 and community tested on one variant of the Compute Module 4 using Super 6C boards. If you have tested this on other Raspberry Pi boards, please let us know.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nUpdating the EEPROM\n\nUse Raspberry Pi Imager to write an EEPROM update image to a spare SD card. Select Misc utility images under the Operating System tab.\n\nRemove the SD card from your local machine and insert it into the Raspberry Pi. Power the Raspberry Pi on, and wait at least 10 seconds. If successful, the green LED light will blink rapidly (forever), otherwise an error pattern will be displayed. If an HDMI display is attached to the port closest to the power/USB-C port, the screen will display green for success or red if a failure occurs. Power off the Raspberry Pi and remove the SD card from it.\n\nNote: Updating the bootloader only needs to be done once.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rpi_generic-arm64.raw.xz\n\nxz -d metal-rpi_generic-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rpi_generic-arm64.raw of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nNote: if you have an HDMI display attached and it shows only a rainbow splash, please use the other HDMI port, the one closest to the power/USB-C port.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\nTroubleshooting\n\nThe following table can be used to troubleshoot booting issues:\n\nLong Flashes\tShort Flashes\tStatus\n0\t3\tGeneric failure to boot\n0\t4\tstart*.elf not found\n0\t7\tKernel image not found\n0\t8\tSDRAM failure\n0\t9\tInsufficient SDRAM\n0\t10\tIn HALT state\n2\t1\tPartition not FAT\n2\t2\tFailed to read from partition\n2\t3\tExtended partition not FAT\n2\t4\tFile signature/hash mismatch - Pi 4\n4\t4\tUnsupported board type\n4\t5\tFatal firmware error\n4\t6\tPower failure type A\n4\t7\tPower failure type B\n2.1.6 - Boot Assets\nCreating customized Talos boot assets, disk images, ISO and installer images.\n\nTalos Linux provides a set of pre-built images on the release page, but these images can be customized further for a specific use case:\n\nadding system extensions\nupdating kernel command line arguments\nusing custom META contents, e.g. for metal network configuration\ngenerating SecureBoot images signed with a custom key\n\nThere are two ways to generate Talos boot assets:\n\nusing Image Factory service (recommended)\nmanually using imager container image (advanced)\n\nImage Factory is easier to use, but it only produces images for official Talos Linux releases and official Talos Linux system extensions. The imager container can be used to generate images from main branch, with local changes, or with custom system extensions.\n\nImage Factory\n\nImage Factory is a service that generates Talos boot assets on-demand. Image Factory allows to generate boot assets for the official Talos Linux releases and official Talos Linux system extensions.\n\nThe main concept of the Image Factory is a schematic which defines the customization of the boot asset. Once the schematic is configured, Image Factory can be used to pull various Talos Linux images, ISOs, installer images, PXE booting bare-metal machines across different architectures, versions of Talos and platforms.\n\nSidero Labs maintains a public Image Factory instance at https://factory.talos.dev. Image Factory provides a simple UI to prepare schematics and retrieve asset links.\n\nExample: Bare-metal with Image Factory\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument.\n\nFirst, let’s create the schematic file bare-metal.yaml:\n\n# bare-metal.yaml\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n      - siderolabs/intel-ucode\n\n\nThe schematic doesn’t contain system extension versions, Image Factory will pick the correct version matching Talos Linux release.\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @bare-metal.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176\"}\n\n\nThe returned schematic ID b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176 we will use to generate the boot assets.\n\nThe schematic ID is based on the schematic contents, so uploading the same schematic will return the same ID.\n\nNow we have two options to boot our bare-metal machine:\n\nusing ISO image: https://factory.talos.dev/image/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64.iso (download it and burn to a CD/DVD or USB stick)\nPXE booting via iPXE script: https://factory.talos.dev/pxe/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64\n\nThe Image Factory URL contains both schematic ID and Talos version, and both can be changed to generate different boot assets.\n\nOnce the bare-metal machine is booted up for the first time, it will require Talos Linux installer image to be installed on the disk. The installer image will be produced by the Image Factory as well:\n\n# Talos machine configuration patch\n\nmachine:\n\n  install:\n\n    image: factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:v1.6.2\n\n\nOnce installed, the machine can be upgraded to a new version of Talos by referencing new installer image:\n\ntalosctl upgrade --image factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:<new_version>\n\n\nSame way upgrade process can be used to transition to a new set of system extensions: generate new schematic with the new set of system extensions, and upgrade the machine to the new schematic ID:\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nExample: AWS with Image Factory\n\nTalos Linux is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required. Let’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s create the schematic file aws.yaml:\n\n# aws.yaml\n\ncustomization:\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @aws.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5\"}\n\n\nThe returned schematic ID d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5 we will use to generate the boot assets.\n\nNow we can download the AWS disk image from the Image Factory:\n\ncurl -LO https://factory.talos.dev/image/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5/v1.6.2/aws-amd64.raw.xz\n\n\nNow the aws-amd64.raw.xz file contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nOnce the AWS VM is created from the AMI, it can be upgraded to a different Talos version or a different schematic using talosctl upgrade:\n\n# upgrade to a new Talos version\n\ntalosctl upgrade --image factory.talos.dev/installer/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5:<new_version>\n\n# upgrade to a new schematic\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nImager\n\nA custom disk image, boot asset can be generated by using the Talos Linux imager container: ghcr.io/siderolabs/imager:v1.6.2. The imager container image can be checked by verifying its signature.\n\nThe generation process can be run with a simple docker run command:\n\ndocker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 <image-kind> [optional: customization]\n\n\nA quick guide to the flags used for docker run:\n\n--rm flag removes the container after the run (as it’s not going to be used anymore)\n-t attaches a terminal for colorized output, it can be removed if used in scripts\n-v $PWD/_out:/secureboot:ro mounts the SecureBoot keys into the container (can be skipped if not generating SecureBoot image)\n-v $PWD/_out:/out mounts the output directory (where the generated image will be placed) into the container\n-v /dev:/dev --privileged is required to generate disk images (loop devices are used), but not required for ISOs, installer container images\n\nThe <image-kind> argument to the imager defines the base profile to be used for the image generation. There are several built-in profiles:\n\niso builds a Talos ISO image (see ISO)\nsecureboot-iso builds a Talos ISO image with SecureBoot (see SecureBoot)\nmetal builds a generic disk image for bare-metal machines\nsecureboot-metal builds a generic disk image for bare-metal machines with SecureBoot\nsecureboot-installer builds an installer container image with SecureBoot (see SecureBoot)\naws, gcp, azure, etc. builds a disk image for a specific Talos platform\n\nThe base profile can be customized with the additional flags to the imager:\n\n--arch specifies the architecture of the image to be generated (default: host architecture)\n--meta allows to set initial META values\n--extra-kernel-arg allows to customize the kernel command line arguments. Default kernel arg can be removed by prefixing the argument with a -. For example -console removes all console=<value> arguments, whereas -console=tty0 removes the console=tty0 default argument.\n--system-extension-image allows to install a system extension into the image\nExtension Image Reference\n\nWhile Image Factory automatically resolves the extension name into a matching container image for a specific version of Talos, imager requires the full explicit container image reference. The imager also allows to install custom extensions which are not part of the official Talos Linux system extensions.\n\nTo get the official Talos Linux system extension container image reference matching a Talos release, use the following command:\n\ncrane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep EXTENSION-NAME\n\n\nNote: this command is using crane tool, but any other tool which allows to export the image contents can be used.\n\nFor each Talos release, the ghcr.io/siderolabs/extensions:VERSION image contains a pinned reference to each system extension container image.\n\nExample: Bare-metal with Imager\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument and replace the Talos default console arguments and add a custom console arg.\n\nFirst, let’s lookup extension images for Intel CPU microcode updates and gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep -E 'gvisor|intel-ucode'\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\nghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow we can generate the ISO image with the following command:\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d --extra-kernel-arg net.ifnames=0 --extra-kernel-arg=-console --extra-kernel-arg=console=ttyS1\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: false\n\nversion: v1.6.2\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  systemExtensions:\n\n    - imageRef: ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n    - imageRef: ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\ninitramfs ready\n\nkernel command line: talos.platform=metal console=ttyS1 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 net.ifnames=0\n\nISO ready\n\noutput asset path: /out/metal-amd64.iso\n\n\nNow the _out/metal-amd64.iso contains the customized Talos ISO image.\n\nIf the machine is going to be booted using PXE, we can instead generate kernel and initramfs images:\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind kernel\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind initramfs --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow the _out/kernel-amd64 and _out/initramfs-amd64 contain the customized Talos kernel and initramfs images.\n\nNote: the extra kernel args are not used now, as they are set via the PXE boot process, and can’t be embedded into the kernel or initramfs.\n\nAs the next step, we should generate a custom installer image which contains all required system extensions (kernel args can’t be specified with the installer image, but they are set in the machine configuration):\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 installer --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n...\n\noutput asset path: /out/metal-amd64-installer.tar\n\n\nThe installer container image should be pushed to the container registry:\n\ncrane push _out/metal-amd64-installer.tar ghcr.io/<username></username>/installer:v1.6.2\n\n\nNow we can use the customized installer image to install Talos on the bare-metal machine.\n\nWhen it’s time to upgrade a machine, a new installer image can be generated using the new version of imager, and updating the system extension images to the matching versions. The custom installer image can now be used to upgrade Talos machine.\n\nExample: AWS with Imager\n\nTalos is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required.\n\nLet’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s lookup extension images for the gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep gvisor\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n\nNext, let’s generate AWS disk image with that system extension:\n\n$ docker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 aws --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n...\n\noutput asset path: /out/aws-amd64.raw\n\ncompression done: /out/aws-amd64.raw.xz\n\n\nNow the _out/aws-amd64.raw.xz contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nIf the AWS machine is later going to be upgraded to a new version of Talos (or a new set of system extensions), generate a customized installer image following the steps above, and upgrade Talos to that installer image.\n\n2.1.7 - Omni SaaS\nOmni is a project created by the Talos team that has native support for Talos Linux.\n\nOmni allows you to start with bare metal, virtual machines or a cloud provider, and create clusters spanning all of your locations, with a few clicks.\n\nYou provide the machines – edge compute, bare metal, VMs, or in your cloud account. Boot from an Omni Talos Linux image. Click to allocate to a cluster. That’s it!\n\nVanilla Kubernetes, on your machines, under your control.\nElegant UI for management and operations\nSecurity taken care of – ties into your Enterprise ID provider\nHighly Available Kubernetes API end point built in\nFirewall friendly: manage Edge nodes securely\nFrom single-node clusters to the largest scale\nSupport for GPUs and most CSIs.\n\nThe Omni SaaS is available to run locally, to support air-gapped security and data sovereignty concerns.\n\nOmni handles the lifecycle of Talos Linux machines, provides unified access to the Talos and Kubernetes API tied to the identity provider of your choice, and provides a UI for cluster management and operations. Omni automates scaling the clusters up and down, and provides a unified view of the state of your clusters.\n\nSee more in the Omni documentation.\n\n2.2 - Configuration\nGuides on how to configure Talos Linux machines\n2.2.1 - Configuration Patches\nIn this guide, we’ll patch the generated machine configuration.\n\nTalos generates machine configuration for two types of machines: controlplane and worker machines. Many configuration options can be adjusted using talosctl gen config but not all of them. Configuration patching allows modifying machine configuration to fit it for the cluster or a specific machine.\n\nConfiguration Patch Formats\n\nTalos supports two configuration patch formats:\n\nstrategic merge patches\nRFC6902 (JSON patches)\n\nStrategic merge patches are the easiest to use, but JSON patches allow more precise configuration adjustments.\n\nNote: Talos 1.5+ supports multi-document machine configuration. JSON patches don’t support multi-document machine configuration, while strategic merge patches do.\n\nStrategic Merge patches\n\nStrategic merge patches look like incomplete machine configuration files:\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nWhen applied to the machine configuration, the patch gets merged with the respective section of the machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.2/24\n\n    hostname: worker1\n\n\nIn general, machine configuration contents are merged with the contents of the strategic merge patch, with strategic merge patch values overriding machine configuration values. There are some special rules:\n\nIf the field value is a list, the patch value is appended to the list, with the following exceptions:\nvalues of the fields cluster.network.podSubnets and cluster.network.serviceSubnets are overwritten on merge\nnetwork.interfaces section is merged with the value in the machine config if there is a match on interface: or deviceSelector: keys\nnetwork.interfaces.vlans section is merged with the value in the machine config if there is a match on the vlanId: key\ncluster.apiServer.auditPolicy value is replaced on merge\n\nWhen patching a multi-document machine configuration, following rules apply:\n\nfor each document in the patch, the document is merged with the respective document in the machine configuration (matching by kind, apiVersion and name for named documents)\nif the patch document doesn’t exist in the machine configuration, it is appended to the machine configuration\n\nThe strategic merge patch itself might be a multi-document YAML, and each document will be applied as a patch to the base machine configuration.\n\nRFC6902 (JSON Patches)\n\nJSON patches can be written either in JSON or YAML format. A proper JSON patch requires an op field that depends on the machine configuration contents: whether the path already exists or not.\n\nFor example, the strategic merge patch from the previous section can be written either as:\n\n- op: replace\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nor:\n\n- op: add\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nThe correct op depends on whether the /machine/network/hostname section exists already in the machine config or not.\n\nExamples\nMachine Network\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n\nThe goal is to add a virtual IP 192.168.10.50 to the eth0 interface and add another interface eth1 with DHCP enabled.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nPatched machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nCluster Network\n\nBase machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 10.244.0.0/16\n\n    serviceSubnets:\n\n      - 10.96.0.0/12\n\n\nThe goal is to update pod and service subnets and disable default CNI (Flannel).\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  network:\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nPatched machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nKubelet\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  kubelet: {}\n\n\nThe goal is to set the kubelet node IP to come from the subnet 192.168.10.0/24.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nPatched machine configuration:\n\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nAdmission Control: Pod Security Policy\n\nBase machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\n\nThe goal is to add an exemption for the namespace rook-ceph.\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          exemptions:\n\n            namespaces:\n\n              - rook-ceph\n\nPatched machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n              - rook-ceph\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\nConfiguration Patching with talosctl CLI\n\nSeveral talosctl commands accept config patches as command-line flags. Config patches might be passed either as an inline value or as a reference to a file with @file.patch syntax:\n\ntalosctl ... --patch '[{\"op\": \"add\", \"path\": \"/machine/network/hostname\", \"value\": \"worker1\"}]' --patch @file.patch\n\n\nIf multiple config patches are specified, they are applied in the order of appearance. The format of the patch (JSON patch or strategic merge patch) is detected automatically.\n\nTalos machine configuration can be patched at the moment of generation with talosctl gen config:\n\ntalosctl gen config test-cluster https://172.20.0.1:6443 --config-patch @all.yaml --config-patch-control-plane @cp.yaml --config-patch-worker @worker.yaml\n\n\nGenerated machine configuration can also be patched after the fact with talosctl machineconfig patch\n\ntalosctl machineconfig patch worker.yaml --patch @patch.yaml -o worker1.yaml\n\n\nMachine configuration on the running Talos node can be patched with talosctl patch:\n\ntalosctl patch mc --nodes 172.20.0.2 --patch @patch.yaml\n\n2.2.2 - Containerd\nCustomize Containerd Settings\n\nThe base containerd configuration expects to merge in any additional configs present in /etc/cri/conf.d/20-customization.part.\n\nExamples\nExposing Metrics\n\nPatch the machine config by adding the following:\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [metrics]\n\n          address = \"0.0.0.0:11234\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nOnce the server reboots, metrics are now available:\n\n$ curl ${IP}:11234/v1/metrics\n\n# HELP container_blkio_io_service_bytes_recursive_bytes The blkio io service bytes recursive\n\n# TYPE container_blkio_io_service_bytes_recursive_bytes gauge\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Async\"} 0\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Discard\"} 0\n\n...\n\n...\n\nPause Image\n\nThis change is often required for air-gapped environments, as containerd CRI plugin has a reference to the pause image which is used to create pods, and it can’t be controlled with Kubernetes pod definitions.\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            sandbox_image = \"registry.k8s.io/pause:3.8\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow the pause image is set to registry.k8s.io/pause:3.8:\n\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                      PID    STATUS\n\n172.20.0.5   k8s.io      kube-system/kube-flannel-6hfck                                  registry.k8s.io/pause:3.8                                  1773   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-cni:bc39fec3cbac      ghcr.io/siderolabs/install-cni:v1.3.0-alpha.0-2-gb155fa0   0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-config:5c3989353b98   ghcr.io/siderolabs/flannel:v0.20.1                         0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:kube-flannel:116c67b50da8     ghcr.io/siderolabs/flannel:v0.20.1                         2092   CONTAINER_RUNNING\n\n172.20.0.5   k8s.io      kube-system/kube-proxy-xp7jq                                    registry.k8s.io/pause:3.8                                  1780   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-proxy-xp7jq:kube-proxy:84fc77c59e17         registry.k8s.io/kube-proxy:v1.26.0-alpha.3                 1843   CONTAINER_RUNNING\n\n2.2.3 - Custom Certificate Authorities\nHow to supply custom certificate authorities\nAppending the Certificate Authority\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\n2.2.4 - Disk Encryption\nGuide on using system disk encryption\n\nIt is possible to enable encryption for system disks at the OS level. Currently, only STATE and EPHEMERAL partitions can be encrypted. STATE contains the most sensitive node data: secrets and certs. The EPHEMERAL partition may contain sensitive workload data. Data is encrypted using LUKS2, which is provided by the Linux kernel modules and cryptsetup utility. The operating system will run additional setup steps when encryption is enabled.\n\nIf the disk encryption is enabled for the STATE partition, the system will:\n\nSave STATE encryption config as JSON in the META partition.\nBefore mounting the STATE partition, load encryption configs either from the machine config or from the META partition. Note that the machine config is always preferred over the META one.\nBefore mounting the STATE partition, format and encrypt it. This occurs only if the STATE partition is empty and has no filesystem.\n\nIf the disk encryption is enabled for the EPHEMERAL partition, the system will:\n\nGet the encryption config from the machine config.\nBefore mounting the EPHEMERAL partition, encrypt and format it.\n\nThis occurs only if the EPHEMERAL partition is empty and has no filesystem.\n\nTalos Linux supports four encryption methods, which can be combined together for a single partition:\n\nstatic - encrypt with the static passphrase (weakest protection, for STATE partition encryption it means that the passphrase will be stored in the META partition).\nnodeID - encrypt with the key derived from the node UUID (weak, it is designed to protect against data being leaked or recovered from a drive that has been removed from a Talos Linux node).\nkms - encrypt using key sealed with network KMS (strong, but requires network access to decrypt the data.)\ntpm - encrypt with the key derived from the TPM (strong, when used with SecureBoot).\n\nNote: nodeID encryption is not designed to protect against attacks where physical access to the machine, including the drive, is available. It uses the hardware characteristics of the machine in order to decrypt the data, so drives that have been removed, or recycled from a cloud environment or attached to a different virtual machine, will maintain their protection and encryption.\n\nConfiguration\n\nDisk encryption is disabled by default. To enable disk encryption you should modify the machine configuration with the following options:\n\nmachine:\n\n  ...\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\nEncryption Keys\n\nNote: What the LUKS2 docs call “keys” are, in reality, a passphrase. When this passphrase is added, LUKS2 runs argon2 to create an actual key from that passphrase.\n\nLUKS2 supports up to 32 encryption keys and it is possible to specify all of them in the machine configuration. Talos always tries to sync the keys list defined in the machine config with the actual keys defined for the LUKS2 partition. So if you update the keys list, keep at least one key that is not changed to be used for key management.\n\nWhen you define a key you should specify the key kind and the slot:\n\nmachine:\n\n  ...\n\n  state:\n\n    keys:\n\n      - nodeID: {} # key kind\n\n        slot: 1\n\n\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: supersecret\n\n        slot: 0\n\n\nTake a note that key order does not play any role on which key slot is used. Every key must always have a slot defined.\n\nEncryption Key Kinds\n\nTalos supports two kinds of keys:\n\nnodeID which is generated using the node UUID and the partition label (note that if the node UUID is not really random it will fail the entropy check).\nstatic which you define right in the configuration.\nkms which is sealed with the network KMS.\ntpm which is sealed using the TPM and protected with SecureBoot.\n\nNote: Use static keys only if your STATE partition is encrypted and only for the EPHEMERAL partition. For the STATE partition it will be stored in the META partition, which is not encrypted.\n\nKey Rotation\n\nIn order to completely rotate keys, it is necessary to do talosctl apply-config a couple of times, since there is a need to always maintain a single working key while changing the other keys around it.\n\nSo, for example, first add a new key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: oldkey\n\n        slot: 0\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\n\nThen remove the old key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\nGoing from Unencrypted to Encrypted and Vice Versa\nEphemeral Partition\n\nThere is no in-place encryption support for the partitions right now, so to avoid losing data only empty partitions can be encrypted.\n\nAs such, migration from unencrypted to encrypted needs some additional handling, especially around explicitly wiping partitions.\n\napply-config should be called with --mode=staged.\nPartition should be wiped after apply-config, but before the reboot.\n\nEdit your machine config and add the encryption configuration:\n\nvim config.yaml\n\n\nApply the configuration with --mode=staged:\n\ntalosctl apply-config -f config.yaml -n <node ip> --mode=staged\n\n\nWipe the partition you’re going to encrypt:\n\ntalosctl reset --system-labels-to-wipe EPHEMERAL -n <node ip> --reboot=true\n\n\nThat’s it! After you run the last command, the partition will be wiped and the node will reboot. During the next boot the system will encrypt the partition.\n\nState Partition\n\nCalling wipe against the STATE partition will make the node lose the config, so the previous flow is not going to work.\n\nThe flow should be to first wipe the STATE partition:\n\ntalosctl reset  --system-labels-to-wipe STATE -n <node ip> --reboot=true\n\n\nNode will enter into maintenance mode, then run apply-config with --insecure flag:\n\ntalosctl apply-config --insecure -n <node ip> -f config.yaml\n\n\nAfter installation is complete the node should encrypt the STATE partition.\n\n2.2.5 - Editing Machine Configuration\nHow to edit and patch Talos machine configuration, with reboot, immediately, or stage update on reboot.\n\nTalos node state is fully defined by machine configuration. Initial configuration is delivered to the node at bootstrap time, but configuration can be updated while the node is running.\n\nThere are three talosctl commands which facilitate machine configuration updates:\n\ntalosctl apply-config to apply configuration from the file\ntalosctl edit machineconfig to launch an editor with existing node configuration, make changes and apply configuration back\ntalosctl patch machineconfig to apply automated machine configuration via JSON patch\n\nEach of these commands can operate in one of four modes:\n\napply change in automatic mode (default): reboot if the change can’t be applied without a reboot, otherwise apply the change immediately\napply change with a reboot (--mode=reboot): update configuration, reboot Talos node to apply configuration change\napply change immediately (--mode=no-reboot flag): change is applied immediately without a reboot, fails if the change contains any fields that can not be updated without a reboot\napply change on next reboot (--mode=staged): change is staged to be applied after a reboot, but node is not rebooted\napply change with automatic revert (--mode=try): change is applied immediately (if not possible, returns an error), and reverts it automatically in 1 minute if no configuration update is applied\napply change in the interactive mode (--mode=interactive; only for talosctl apply-config): launches TUI based interactive installer\n\nNote: applying change on next reboot (--mode=staged) doesn’t modify current node configuration, so next call to talosctl edit machineconfig --mode=staged will not see changes\n\nAdditionally, there is also talosctl get machineconfig -o yaml, which retrieves the current node configuration API resource and contains the machine configuration in the .spec field. It can be used to modify the configuration locally before being applied to the node.\n\nThe list of config changes allowed to be applied immediately in Talos v1.6.2:\n\n.debug\n.cluster\n.machine.time\n.machine.certCANs\n.machine.install (configuration is only applied during install/upgrade)\n.machine.network\n.machine.nodeLabels\n.machine.sysfs\n.machine.sysctls\n.machine.logging\n.machine.controlplane\n.machine.kubelet\n.machine.pods\n.machine.kernel\n.machine.registries (CRI containerd plugin will not pick up the registry authentication settings without a reboot)\n.machine.features.kubernetesTalosAPIAccess\ntalosctl apply-config\n\nThis command is traditionally used to submit initial machine configuration generated by talosctl gen config to the node.\n\nIt can also be used to apply configuration to running nodes. The initial YAML for this is typically obtained using talosctl get machineconfig -o yaml | yq eval .spec >machs.yaml. (We must use yq because for historical reasons, get returns the configuration as a full resource, while apply-config only accepts the raw machine config directly.)\n\nExample:\n\ntalosctl -n <IP> apply-config -f config.yaml\n\n\nCommand apply-config can also be invoked as apply machineconfig:\n\ntalosctl -n <IP> apply machineconfig -f config.yaml\n\n\nApplying machine configuration immediately (without a reboot):\n\ntalosctl -n IP apply machineconfig -f config.yaml --mode=no-reboot\n\n\nStarting the interactive installer:\n\ntalosctl -n IP apply machineconfig --mode=interactive\n\n\nNote: when a Talos node is running in the maintenance mode it’s necessary to provide --insecure (-i) flag to connect to the API and apply the config.\n\ntaloctl edit machineconfig\n\nCommand talosctl edit loads current machine configuration from the node and launches configured editor to modify the config. If config hasn’t been changed in the editor (or if updated config is empty), update is not applied.\n\nNote: Talos uses environment variables TALOS_EDITOR, EDITOR to pick up the editor preference. If environment variables are missing, vi editor is used by default.\n\nExample:\n\ntalosctl -n <IP> edit machineconfig\n\n\nConfiguration can be edited for multiple nodes if multiple IP addresses are specified:\n\ntalosctl -n <IP1>,<IP2>,... edit machineconfig\n\n\nApplying machine configuration change immediately (without a reboot):\n\ntalosctl -n <IP> edit machineconfig --mode=no-reboot\n\ntalosctl patch machineconfig\n\nCommand talosctl patch works similar to talosctl edit command - it loads current machine configuration, but instead of launching configured editor it applies a set of JSON patches to the configuration and writes the result back to the node.\n\nExample, updating kubelet version (in auto mode):\n\n$ talosctl -n <IP> patch machineconfig -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nUpdating kube-apiserver version in immediate mode (without a reboot):\n\n$ talosctl -n <IP> patch machineconfig --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nA patch might be applied to multiple nodes when multiple IPs are specified:\n\ntalosctl -n <IP1>,<IP2>,... patch machineconfig -p '[{...}]'\n\n\nPatches can also be sourced from files using @file syntax:\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.json -p @manifest-patch.json\n\n\nIt might be easier to store patches in YAML format vs. the default JSON format. Talos can detect file format automatically:\n\n# kubelet-patch.yaml\n\n- op: replace\n\n  path: /machine/kubelet/image\n\n  value: ghcr.io/siderolabs/kubelet:v1.29.0\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.yaml\n\nRecovering from Node Boot Failures\n\nIf a Talos node fails to boot because of wrong configuration (for example, control plane endpoint is incorrect), configuration can be updated to fix the issue.\n\n2.2.6 - Logging\nDealing with Talos Linux logs.\nViewing logs\n\nKernel messages can be retrieved with talosctl dmesg command:\n\n$ talosctl -n 172.20.1.2 dmesg\n\n\n\n172.20.1.2: kern:    info: [2021-11-10T10:09:37.662764956Z]: Command line: init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 console=ttyS0 reboot=k panic=1 talos.shutdown=halt talos.platform=metal talos.config=http://172.20.1.1:40101/config.yaml\n\n[...]\n\n\nService logs can be retrieved with talosctl logs command:\n\n$ talosctl -n 172.20.1.2 services\n\n\n\nNODE         SERVICE      STATE     HEALTH   LAST CHANGE   LAST EVENT\n\n172.20.1.2   apid         Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   containerd   Running   OK       19m29s ago    Health check successful\n\n172.20.1.2   cri          Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   etcd         Running   OK       19m22s ago    Health check successful\n\n172.20.1.2   kubelet      Running   OK       19m20s ago    Health check successful\n\n172.20.1.2   machined     Running   ?        19m30s ago    Service started as goroutine\n\n172.20.1.2   trustd       Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   udevd        Running   OK       19m28s ago    Health check successful\n\n\n\n$ talosctl -n 172.20.1.2 logs machined\n\n\n\n172.20.1.2: [talos] task setupLogger (1/1): done, 106.109µs\n\n172.20.1.2: [talos] phase logger (1/7): done, 564.476µs\n\n[...]\n\n\nContainer logs for Kubernetes pods can be retrieved with talosctl logs -k command:\n\n$ talosctl -n 172.20.1.2 containers -k\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                         PID    STATUS\n\n172.20.1.2   k8s.io      kube-system/kube-flannel-dk6d5                                  registry.k8s.io/pause:3.6                                     1329   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-cni:f1d4cf68feb9      ghcr.io/siderolabs/install-cni:v0.7.0-alpha.0-1-g2bb2efc      0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-config:bc39fec3cbac   quay.io/coreos/flannel:v0.13.0                                0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:kube-flannel:5c3989353b98     quay.io/coreos/flannel:v0.13.0                                1610   CONTAINER_RUNNING\n\n172.20.1.2   k8s.io      kube-system/kube-proxy-gfkqj                                    registry.k8s.io/pause:3.5                                     1311   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f         registry.k8s.io/kube-proxy:v1.29.0                            1379   CONTAINER_RUNNING\n\n\n\n$ talosctl -n 172.20.1.2 logs -k kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f\n\n172.20.1.2: 2021-11-30T19:13:20.567825192Z stderr F I1130 19:13:20.567737       1 server_others.go:138] \"Detected node IP\" address=\"172.20.0.3\"\n\n172.20.1.2: 2021-11-30T19:13:20.599684397Z stderr F I1130 19:13:20.599613       1 server_others.go:206] \"Using iptables Proxier\"\n\n[...]\n\nSending logs\nService logs\n\nYou can enable logs sendings in machine configuration:\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"udp://127.0.0.1:12345/\"\n\n        format: \"json_lines\"\n\n      - endpoint: \"tcp://host:5044/\"\n\n        format: \"json_lines\"\n\n\nSeveral destinations can be specified. Supported protocols are UDP and TCP. The only currently supported format is json_lines:\n\n{\n\n  \"msg\": \"[talos] apply config request: immediate true, on reboot false\",\n\n  \"talos-level\": \"info\",\n\n  \"talos-service\": \"machined\",\n\n  \"talos-time\": \"2021-11-10T10:48:49.294858021Z\"\n\n}\n\n\nMessages are newline-separated when sent over TCP. Over UDP messages are sent with one message per packet. msg, talos-level, talos-service, and talos-time fields are always present; there may be additional fields.\n\nKernel logs\n\nKernel log delivery can be enabled with the talos.logging.kernel kernel command line argument, which can be specified in the .machine.installer.extraKernelArgs:\n\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - talos.logging.kernel=tcp://host:5044/\n\n\nAlso kernel logs delivery can be configured using the document in machine configuration:\n\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log\n\nurl: tcp://host:5044/\n\n\nKernel log destination is specified in the same way as service log endpoint. The only supported format is json_lines.\n\nSample message:\n\n{\n\n  \"clock\":6252819, // time relative to the kernel boot time\n\n  \"facility\":\"user\",\n\n  \"msg\":\"[talos] task startAllServices (1/1): waiting for 6 services\\n\",\n\n  \"priority\":\"warning\",\n\n  \"seq\":711,\n\n  \"talos-level\":\"warn\", // Talos-translated `priority` into common logging level\n\n  \"talos-time\":\"2021-11-26T16:53:21.3258698Z\" // Talos-translated `clock` using current time\n\n}\n\n\nextraKernelArgs in the machine configuration are only applied on Talos upgrades, not just by applying the config. (Upgrading to the same version is fine).\n\nFilebeat example\n\nTo forward logs to other Log collection services, one way to do this is sending them to a Filebeat running in the cluster itself (in the host network), which takes care of forwarding it to other endpoints (and the necessary transformations).\n\nIf Elastic Cloud on Kubernetes is being used, the following Beat (custom resource) configuration might be helpful:\n\napiVersion: beat.k8s.elastic.co/v1beta1\n\nkind: Beat\n\nmetadata:\n\n  name: talos\n\nspec:\n\n  type: filebeat\n\n  version: 7.15.1\n\n  elasticsearchRef:\n\n    name: talos\n\n  config:\n\n    filebeat.inputs:\n\n      - type: \"udp\"\n\n        host: \"127.0.0.1:12345\"\n\n        processors:\n\n          - decode_json_fields:\n\n              fields: [\"message\"]\n\n              target: \"\"\n\n          - timestamp:\n\n              field: \"talos-time\"\n\n              layouts:\n\n                - \"2006-01-02T15:04:05.999999999Z07:00\"\n\n          - drop_fields:\n\n              fields: [\"message\", \"talos-time\"]\n\n          - rename:\n\n              fields:\n\n                - from: \"msg\"\n\n                  to: \"message\"\n\n\n\n  daemonSet:\n\n    updateStrategy:\n\n      rollingUpdate:\n\n        maxUnavailable: 100%\n\n    podTemplate:\n\n      spec:\n\n        dnsPolicy: ClusterFirstWithHostNet\n\n        hostNetwork: true\n\n        securityContext:\n\n          runAsUser: 0\n\n        containers:\n\n          - name: filebeat\n\n            ports:\n\n              - protocol: UDP\n\n                containerPort: 12345\n\n                hostPort: 12345\n\n\nThe input configuration ensures that messages and timestamps are extracted properly. Refer to the Filebeat documentation on how to forward logs to other outputs.\n\nAlso note the hostNetwork: true in the daemonSet configuration.\n\nThis ensures filebeat uses the host network, and listens on 127.0.0.1:12345 (UDP) on every machine, which can then be specified as a logging endpoint in the machine configuration.\n\nFluent-bit example\n\nFirst, we’ll create a value file for the fluentd-bit Helm chart.\n\n# fluentd-bit.yaml\n\n\n\npodAnnotations:\n\n  fluentbit.io/exclude: 'true'\n\n\n\nextraPorts:\n\n  - port: 12345\n\n    containerPort: 12345\n\n    protocol: TCP\n\n    name: talos\n\n\n\nconfig:\n\n  service: |\n\n    [SERVICE]\n\n      Flush         5\n\n      Daemon        Off\n\n      Log_Level     warn\n\n      Parsers_File  custom_parsers.conf    \n\n\n\n  inputs: |\n\n    [INPUT]\n\n      Name          tcp\n\n      Listen        0.0.0.0\n\n      Port          12345\n\n      Format        json\n\n      Tag           talos.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         kubernetes\n\n      Path          /var/log/containers/*.log\n\n      Parser        containerd\n\n      Tag           kubernetes.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         audit\n\n      Path          /var/log/audit/kube/*.log\n\n      Parser        audit\n\n      Tag           audit.*    \n\n\n\n  filters: |\n\n    [FILTER]\n\n      Name                kubernetes\n\n      Alias               kubernetes\n\n      Match               kubernetes.*\n\n      Kube_Tag_Prefix     kubernetes.var.log.containers.\n\n      Use_Kubelet         Off\n\n      Merge_Log           On\n\n      Merge_Log_Trim      On\n\n      Keep_Log            Off\n\n      K8S-Logging.Parser  Off\n\n      K8S-Logging.Exclude On\n\n      Annotations         Off\n\n      Labels              On\n\n\n\n    [FILTER]\n\n      Name          modify\n\n      Match         kubernetes.*\n\n      Add           source kubernetes\n\n      Remove        logtag    \n\n\n\n  customParsers: |\n\n    [PARSER]\n\n      Name          audit\n\n      Format        json\n\n      Time_Key      requestReceivedTimestamp\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z\n\n\n\n    [PARSER]\n\n      Name          containerd\n\n      Format        regex\n\n      Regex         ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$\n\n      Time_Key      time\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z    \n\n\n\n  outputs: |\n\n    [OUTPUT]\n\n      Name    stdout\n\n      Alias   stdout\n\n      Match   *\n\n      Format  json_lines    \n\n\n\n  # If you wish to ship directly to Loki from Fluentbit,\n\n  # Uncomment the following output, updating the Host with your Loki DNS/IP info as necessary.\n\n  # [OUTPUT]\n\n  # Name loki\n\n  # Match *\n\n  # Host loki.loki.svc\n\n  # Port 3100\n\n  # Labels job=fluentbit\n\n  # Auto_Kubernetes_Labels on\n\n\n\ndaemonSetVolumes:\n\n  - name: varlog\n\n    hostPath:\n\n      path: /var/log\n\n\n\ndaemonSetVolumeMounts:\n\n  - name: varlog\n\n    mountPath: /var/log\n\n\n\ntolerations:\n\n  - operator: Exists\n\n    effect: NoSchedule\n\n\nNext, we will add the helm repo for FluentBit, and deploy it to the cluster.\n\nhelm repo add fluent https://fluent.github.io/helm-charts\n\nhelm upgrade -i --namespace=kube-system -f fluentd-bit.yaml fluent-bit fluent/fluent-bit\n\n\nNow we need to find the service IP.\n\n$ kubectl -n kube-system get svc -l app.kubernetes.io/name=fluent-bit\n\n\n\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n\nfluent-bit   ClusterIP   10.200.0.138   <none>        2020/TCP,5170/TCP   108m\n\n\nFinally, we will change talos log destination with the command talosctl edit mc.\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"tcp://10.200.0.138:5170\"\n\n        format: \"json_lines\"\n\n\nThis example configuration was well tested with Cilium CNI, and it should work with iptables/ipvs based CNI plugins too.\n\nVector example\n\nVector is a lightweight observability pipeline ideal for a Kubernetes environment. It can ingest (source) logs from multiple sources, perform remapping on the logs (transform), and forward the resulting pipeline to multiple destinations (sinks). As it is an end to end platform, it can be run as a single-deployment ‘aggregator’ as well as a replicaSet of ‘Agents’ that run on each node.\n\nAs Talos can be set as above to send logs to a destination, we can run Vector as an Aggregator, and forward both kernel and service to a UDP socket in-cluster.\n\nBelow is an excerpt of a source/sink setup for Talos, with a ‘sink’ destination of an in-cluster Grafana Loki log aggregation service. As Loki can create labels from the log input, we have set up the Loki sink to create labels based on the host IP, service and facility of the inbound logs.\n\nNote that a method of exposing the Vector service will be required which may vary depending on your setup - a LoadBalancer is a good option.\n\nrole: \"Stateless-Aggregator\"\n\n\n\n# Sources\n\nsources:\n\n  talos_kernel_logs:\n\n    address: 0.0.0.0:6050\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n  talos_service_logs:\n\n    address: 0.0.0.0:6051\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n# Sinks\n\nsinks:\n\n  talos_kernel:\n\n    type: loki\n\n    inputs:\n\n      - talos_kernel_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 1048576\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      facility: >-\n\n                {{`{{ facility }}`}}\n\n\n\n  talos_service:\n\n    type: loki\n\n    inputs:\n\n      - talos_service_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 400000\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      service: >-\n\n                {{`{{ \"talos-service\" }}`}}\n\n2.2.7 - Managing Talos PKI\nHow to manage Public Key Infrastructure\nGenerating New Client Configuration\nUsing Controlplane Node\n\nIf you have a valid (not expired) talosconfig with os:admin role, a new client configuration file can be generated with talosctl config new against any controlplane node:\n\ntalosctl -n CP1 config new talosconfig-reader --roles os:reader --crt-ttl 24h\n\n\nA specific role and certificate lifetime can be specified.\n\nFrom Secrets Bundle\n\nIf a secrets bundle (secrets.yaml from talosctl gen secrets) was saved while generating machine configuration:\n\ntalosctl gen config --with-secrets secrets.yaml --output-types talosconfig -o talosconfig <cluster-name> https://<cluster-endpoint>\n\n\nNote: <cluster-name> and <cluster-endpoint> arguments don’t matter, as they are not used for talosconfig.\n\nFrom Control Plane Machine Configuration\n\nIn order to create a new key pair for client configuration, you will need the root Talos API CA. The base64 encoded CA can be found in the control plane node’s configuration file. Save the the CA public key, and CA private key as ca.crt, and ca.key respectively:\n\nyq eval .machine.ca.crt controlplane.yaml | base64 -d > ca.crt\n\nyq eval .machine.ca.key controlplane.yaml | base64 -d > ca.key\n\n\nNow, run the following commands to generate a certificate:\n\ntalosctl gen key --name admin\n\ntalosctl gen csr --key admin.key --ip 127.0.0.1\n\ntalosctl gen crt --ca ca --csr admin.csr --name admin\n\n\nPut the base64-encoded files to the respective location to the talosconfig:\n\ncontext: mycluster\n\ncontexts:\n\n    mycluster:\n\n        endpoints:\n\n            - CP1\n\n            - CP2\n\n        ca: <base64-encoded ca.crt>\n\n        crt: <base64-encoded admin.crt>\n\n        key: <base64-encoded admin.key>\n\nRenewing an Expired Administrator Certificate\n\nBy default admin talosconfig certificate is valid for 365 days, while cluster CAs are valid for 10 years. In order to prevent admin talosconfig from expiring, renew the client config before expiration using talosctl config new command described above.\n\nIf the talosconfig is expired or lost, you can still generate a new one using either the secrets.yaml secrets bundle or the control plane node’s configuration file using methods described above.\n\n2.2.8 - NVIDIA Fabric Manager\nIn this guide we’ll follow the procedure to enable NVIDIA Fabric Manager.\n\nNVIDIA GPUs that have nvlink support (for eg: A100) will need the nvidia-fabricmanager system extension also enabled in addition to the NVIDIA drivers. For more information on Fabric Manager refer https://docs.nvidia.com/datacenter/tesla/fabric-manager-user-guide/index.html\n\nThe published versions of the NVIDIA fabricmanager system extensions is available here\n\nThe nvidia-fabricmanager extension version has to match with the NVIDIA driver version in use.\n\nEnabling the NVIDIA fabricmanager system extension\n\nCreate the boot assets or a custom installer and perform a machine upgrade which include the following system extensions:\n\nghcr.io/siderolabs/nvidia-open-gpu-kernel-modules:535.129.03-v1.6.2\n\nghcr.io/siderolabs/nvidia-container-toolkit:535.129.03-v1.13.5\n\nghcr.io/siderolabs/nvidia-fabricmanager:535.129.03\n\n\nPatch the machine configuration to load the required modules:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n2.2.9 - NVIDIA GPU (OSS drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using OSS drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA OSS drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA OSS system extensions:\n\nnvidia-open-gpu-kernel-modules\nnvidia-container-toolkit\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nvidia-open-gpu-kernel-modules and nvidia-container-toolkit extensions. The nvidia-open-gpu-kernel-modules extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA OSS modules\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                           VERSION   NAME                             VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-container-toolkit-515.65.01-v1.10.0            1         nvidia-container-toolkit         515.65.01-v1.10.0\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-open-gpu-kernel-modules-515.65.01-v1.2.0       1         nvidia-open-gpu-kernel-modules   515.65.01-v1.2.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  515.65.01  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 12.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n2.2.10 - NVIDIA GPU (Proprietary drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using proprietary drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA system extensions:\n\nnonfree-kmod-nvidia\nnvidia-container-toolkit\n\nTo build a NVIDIA driver version not published by SideroLabs follow the instructions here\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nonfree-kmod-nvidia and nvidia-container-toolkit extensions. The nonfree-kmod-nvidia extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA modules and the system extension\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                 VERSION   NAME                       VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-frezbo-nvidia-container-toolkit-510.60.02-v1.9.0       1         nvidia-container-toolkit   510.60.02-v1.9.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  510.60.02  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 11.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n2.2.11 - Pull Through Image Cache\nHow to set up local transparent container images caches.\n\nIn this guide we will create a set of local caching Docker registry proxies to minimize local cluster startup time.\n\nWhen running Talos locally, pulling images from container registries might take a significant amount of time. We spin up local caching pass-through registries to cache images and configure a local Talos cluster to use those proxies. A similar approach might be used to run Talos in production in air-gapped environments. It can be also used to verify that all the images are available in local registries.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\n\nThe follow are requirements for creating the set of caching proxies:\n\nDocker 18.03 or greater\nLocal cluster requirements for either docker or QEMU.\nLaunch the Caching Docker Registry Proxies\n\nTalos pulls from docker.io, registry.k8s.io, gcr.io, and ghcr.io by default. If your configuration is different, you might need to modify the commands below:\n\ndocker run -d -p 5000:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \\\n\n    --restart always \\\n\n    --name registry-docker.io registry:2\n\n\n\ndocker run -d -p 5001:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry.k8s.io \\\n\n    --restart always \\\n\n    --name registry-registry.k8s.io registry:2\n\n\n\ndocker run -d -p 5003:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://gcr.io \\\n\n    --restart always \\\n\n    --name registry-gcr.io registry:2\n\n\n\ndocker run -d -p 5004:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \\\n\n    --restart always \\\n\n    --name registry-ghcr.io registry:2\n\n\nNote: Proxies are started as docker containers, and they’re automatically configured to start with Docker daemon.\n\nAs a registry container can only handle a single upstream Docker registry, we launch a container per upstream, each on its own host port (5000, 5001, 5002, 5003 and 5004).\n\nUsing Caching Registries with QEMU Local Cluster\n\nWith a QEMU local cluster, a bridge interface is created on the host. As registry containers expose their ports on the host, we can use bridge IP to direct proxy requests.\n\nsudo talosctl cluster create --provisioner qemu \\\n\n    --registry-mirror docker.io=http://10.5.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://10.5.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://10.5.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://10.5.0.1:5004\n\n\nThe Talos local cluster should now start pulling via caching registries. This can be verified via registry logs, e.g. docker logs -f registry-docker.io. The first time cluster boots, images are pulled and cached, so next cluster boot should be much faster.\n\nNote: 10.5.0.1 is a bridge IP with default network (10.5.0.0/24), if using custom --cidr, value should be adjusted accordingly.\n\nUsing Caching Registries with docker Local Cluster\n\nWith a docker local cluster we can use docker bridge IP, default value for that IP is 172.17.0.1. On Linux, the docker bridge address can be inspected with ip addr show docker0.\n\ntalosctl cluster create --provisioner docker \\\n\n    --registry-mirror docker.io=http://172.17.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.17.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://172.17.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.17.0.1:5004\n\nMachine Configuration\n\nThe caching registries can be configured via machine configuration patch, equivalent to the command line flags above:\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5000\n\n      gcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5003\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5004\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5001\n\nCleaning Up\n\nTo cleanup, run:\n\ndocker rm -f registry-docker.io\n\ndocker rm -f registry-registry.k8s.io\n\ndocker rm -f registry-gcr.io\n\ndocker rm -f registry-ghcr.io\n\n\nNote: Removing docker registry containers also removes the image cache. So if you plan to use caching registries, keep the containers running.\n\nUsing Harbor as a Caching Registry\n\nHarbor is an open source container registry that can be used as a caching proxy. Harbor supports configuring multiple upstream registries, so it can be used to cache multiple registries at once behind a single endpoint.\n\nAs Harbor puts a registry name in the pull image path, we need to set overridePath: true to prevent Talos and containerd from appending /v2 to the path.\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-docker.io\n\n        overridePath: true\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-ghcr.io\n\n        overridePath: true\n\n      gcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-gcr.io\n\n        overridePath: true\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-registry.k8s.io\n\n        overridePath: true\n\n\nThe Harbor external endpoint (http://harbor in this example) can be configured with authentication or custom TLS:\n\nmachine:\n\n  registries:\n\n    config:\n\n      harbor:\n\n        auth:\n\n          username: admin\n\n          password: password\n\n2.2.12 - Role-based access control (RBAC)\nSet up RBAC on the Talos Linux API.\n\nTalos v0.11 introduced initial support for role-based access control (RBAC). This guide will explain what that is and how to enable it without losing access to the cluster.\n\nRBAC in Talos\n\nTalos uses certificates to authorize users. The certificate subject’s organization field is used to encode user roles. There is a set of predefined roles that allow access to different API methods:\n\nos:admin grants access to all methods;\nos:operator grants everything os:reader role does, plus additional methods: rebooting, shutting down, etcd backup, etcd alarm management, and so on;\nos:reader grants access to “safe” methods (for example, that includes the ability to list files, but does not include the ability to read files content);\nos:etcd:backup grants access to /machine.MachineService/EtcdSnapshot method.\n\nRoles in the current talosconfig can be checked with the following command:\n\n$ talosctl config info\n\n\n\n[...]\n\nRoles:               os:admin\n\n[...]\n\n\nRBAC is enabled by default in new clusters created with talosctl v0.11+ and disabled otherwise.\n\nEnabling RBAC\n\nFirst, both the Talos cluster and talosctl tool should be upgraded. Then the talosctl config new command should be used to generate a new client configuration with the os:admin role. Additional configurations and certificates for different roles can be generated by passing --roles flag:\n\ntalosctl config new --roles=os:reader reader\n\n\nThat command will create a new client configuration file reader with a new certificate with os:reader role.\n\nAfter that, RBAC should be enabled in the machine configuration:\n\nmachine:\n\n  features:\n\n    rbac: true\n\n2.2.13 - System Extensions\nCustomizing the Talos Linux immutable root file system.\n\nSystem extensions allow extending the Talos root filesystem, which enables a variety of features, such as including custom container runtimes, loading additional firmware, etc.\n\nSystem extensions are only activated during the installation or upgrade of Talos Linux. With system extensions installed, the Talos root filesystem is still immutable and read-only.\n\nInstalling System Extensions\n\nNote: the way to install system extensions in the .machine.install section of the machine configuration is now deprecated.\n\nStarting with Talos v1.5.0, Talos supports generation of boot media with system extensions included, this removes the need to rebuild the initramfs.xz on the machine itself during the installation or upgrade.\n\nThere are two kinds of boot assets that Talos can generate:\n\ninitial boot assets (ISO, PXE, etc.) that are used to boot the machine\ndisk images that have Talos pre-installed\ninstaller container images that can be used to install or upgrade Talos on a machine (installation happens when booted from ISO or PXE)\n\nDepending on the nature of the system extension (e.g. network device driver or containerd plugin), it may be necessary to include the extension in both initial boot assets and disk images/installer, or just the installer.\n\nThe process of generating boot assets with extensions included is described in the boot assets guide.\n\nExample: Booting from an ISO\n\nLet’s assume NVIDIA extension is required on a bare metal machine which is going to be booted from an ISO. As NVIDIA extension is not required for the initial boot and install step, it is sufficient to include the extension in the installer image only.\n\nUse a generic Talos ISO to boot the machine.\nPrepare a custom installer container image with NVIDIA extension included, push the image to a registry.\nEnsure that machine configuration field .machine.install.image points to the custom installer image.\nBoot the machine using the ISO, apply the machine configuration.\nTalos pulls a custom installer image from the registry (containing NVIDIA extension), installs Talos on the machine, and reboots.\n\nWhen it’s time to upgrade Talos, generate a custom installer container for a new version of Talos, push it to a registry, and perform upgrade pointing to the custom installer image.\n\nExample: Disk Image\n\nLet’s assume NVIDIA extension is required on AWS VM.\n\nPrepare an AWS disk image with NVIDIA extension included.\nUpload the image to AWS, register it as an AMI.\nUse the AMI to launch a VM.\nTalos boots with NVIDIA extension included.\n\nWhen it’s time to upgrade Talos, either repeat steps 1-4 to replace the VM with a new AMI, or like in the previous example, generate a custom installer and use it to upgrade Talos in-place.\n\nAuthoring System Extensions\n\nA Talos system extension is a container image with the specific folder structure. System extensions can be built and managed using any tool that produces container images, e.g. docker build.\n\nSidero Labs maintains a repository of system extensions.\n\nResource Definitions\n\nUse talosctl get extensions to get a list of system extensions:\n\n$ talosctl get extensions\n\nNODE         NAMESPACE   TYPE              ID                                              VERSION   NAME          VERSION\n\n172.20.0.2   runtime     ExtensionStatus   000.ghcr.io-talos-systems-gvisor-54b831d        1         gvisor        20220117.0-v1.0.0\n\n172.20.0.2   runtime     ExtensionStatus   001.ghcr.io-talos-systems-intel-ucode-54b831d   1         intel-ucode   microcode-20210608-v1.0.0\n\n\nUse YAML or JSON format to see additional details about the extension:\n\n$ talosctl -n 172.20.0.2 get extensions 001.ghcr.io-talos-systems-intel-ucode-54b831d -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: runtime\n\n    type: ExtensionStatuses.runtime.talos.dev\n\n    id: 001.ghcr.io-talos-systems-intel-ucode-54b831d\n\n    version: 1\n\n    owner: runtime.ExtensionStatusController\n\n    phase: running\n\n    created: 2022-02-10T18:25:04Z\n\n    updated: 2022-02-10T18:25:04Z\n\nspec:\n\n    image: 001.ghcr.io-talos-systems-intel-ucode-54b831d.sqsh\n\n    metadata:\n\n        name: intel-ucode\n\n        version: microcode-20210608-v1.0.0\n\n        author: Spencer Smith\n\n        description: |\n\n            This system extension provides Intel microcode binaries.\n\n        compatibility:\n\n            talos:\n\n                version: '>= v1.0.0'\n\nExample: gVisor\n\nSee readme of the gVisor extension.\n\n2.3 - How Tos\nHow to guide for common tasks in Talos Linux\n2.3.1 - How to enable workers on your control plane nodes\nHow to enable workers on your control plane nodes.\n\nBy default, Talos Linux taints control plane nodes so that workloads are not schedulable on them.\n\nIn order to allow workloads to run on the control plane nodes (useful for single node clusters, or non-production clusters), follow the procedure below.\n\nModify the MachineConfig for the controlplane nodes to add allowSchedulingOnControlPlanes: true:\n\ncluster:\n\n    allowSchedulingOnControlPlanes: true\n\n\nThis may be done via editing the controlplane.yaml file before it is applied to the control plane nodes, by editing the machine config, or by patching the machine config.\n\n2.3.2 - How to manage certificate lifetimes with Talos Linux\n\nTalos Linux automatically manages and rotates all server side certs for etcd, Kubernetes, and the Talos API. Note however that the kubelet needs to be restarted at least once a year in order for the certificates to be rotated. Any upgrade/reboot of the node will suffice for this effect.\n\nClient certs (talosconfig and kubeconfig) are the user’s responsibility. Each time you download the kubeconfig file from a Talos Linux cluster, the client certificate is regenerated giving you a kubeconfig which is valid for a year.\n\nThe talosconfig file should be renewed at least once a year, using the talosctl config new command.\n\n2.3.3 - How to scale down a Talos cluster\nHow to remove nodes from a Talos Linux cluster.\n\nTo remove nodes from a Talos Linux cluster:\n\ntalosctl -n <IP.of.node.to.remove> reset\nkubectl delete node <nodename>\n\nThe command talosctl reset will cordon and drain the node, leaving etcd if required, and then erase its disks and power down the system.\n\nThis command will also remove the node from registration with the discovery service, so it will no longer show up in talosctl get members.\n\nIt is still necessary to remove the node from Kubernetes, as noted above.\n\n2.3.4 - How to scale up a Talos cluster\nHow to add more nodes to a Talos Linux cluster.\n\nTo add more nodes to a Talos Linux cluster, follow the same procedure as when initially creating the cluster:\n\nboot the new machines to install Talos Linux\napply the worker.yaml or controlplane.yaml configuration files to the new machines\n\nYou need the controlplane.yaml and worker.yaml that were created when you initially deployed your cluster. These contain the certificates that enable new machines to join.\n\nOnce you have the IP address, you can then apply the correct configuration for each machine you are adding, either worker or controlplane.\n\n  talosctl apply-config --insecure \\\n\n    --nodes [NODE IP] \\\n\n    --file controlplane.yaml\n\n\nThe insecure flag is necessary because the PKI infrastructure has not yet been made available to the node.\n\nYou do not need to bootstrap the new node. Regardless of whether you are adding a control plane or worker node, it will now join the cluster in its role.\n\n2.4 - Network\nSet up networking layers for Talos Linux\n2.4.1 - Corporate Proxies\nHow to configure Talos Linux to use proxies in a corporate environment\nAppending the Certificate Authority of MITM Proxies\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\nConfiguring a Machine to Use the Proxy\n\nTo make use of a proxy:\n\nmachine:\n\n  env:\n\n    http_proxy: <http proxy>\n\n    https_proxy: <https proxy>\n\n    no_proxy: <no proxy>\n\n\nAdditionally, configure the DNS nameservers, and NTP servers:\n\nmachine:\n\n  env:\n\n  ...\n\n  time:\n\n    servers:\n\n      - <server 1>\n\n      - <server ...>\n\n      - <server n>\n\n  ...\n\n  network:\n\n    nameservers:\n\n      - <ip 1>\n\n      - <ip ...>\n\n      - <ip n>\n\n\nIf a proxy is required before Talos machine configuration is applied, use kernel command line arguments:\n\ntalos.environment=http_proxy=<http-proxy> talos.environment=https_proxy=<https-proxy>\n\n2.4.2 - Ingress Firewall\nLearn to use Talos Linux Ingress Firewall to limit access to the host services.\n\nTalos Linux Ingress Firewall is a simple and effective way to limit access to the services running on the host, which includes both Talos standard services (e.g. apid and kubelet), and any additional workloads that may be running on the host. Talos Linux Ingress Firewall doesn’t affect the traffic between the Kubernetes pods/services, please use CNI Network Policies for that.\n\nConfiguration\n\nIngress rules are configured as extra documents NetworkDefaultActionConfig and NetworkRuleConfig in the Talos machine configuration:\n\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 172.20.0.0/24\n\n    except: 172.20.0.1/32\n\n\nThe first document configures the default action for the ingress traffic, which can be either accept or block, with the default being accept. If the default action is set to accept, then all the ingress traffic will be allowed, unless there is a matching rule that blocks it. If the default action is set to block, then all the ingress traffic will be blocked, unless there is a matching rule that allows it.\n\nWith either accept or block, the traffic is always allowed on the following network interfaces:\n\nlo\nsiderolink\nkubespan\n\nIn the block mode:\n\nICMP and ICMPv6 traffic is also allowed with a rate limit of 5 packets per second\ntraffic between Kubernetes pod/service subnets is allowed (for native routing CNIs)\n\nThe second document defines an ingress rule for a set of ports and protocols on the host. The NetworkRuleConfig might be repeated many times to define multiple rules, but each document must have a unique name.\n\nThe ports field accepts either a single port or a port range:\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n    - 10260\n\n    - 10300-10400\n\n\nThe protocol might be either tcp or udp.\n\nThe ingress specifies the list of subnets that are allowed to access the host services, with the optional except field to exclude a set of addresses from the subnet.\n\nNote: incorrect configuration of the ingress firewall might result in the host becoming inaccessible over Talos API. The configuration might be applied in --mode=try to make sure it gets reverted in case of a mistake.\n\nRecommended Rules\n\nThe following rules improve the security of the cluster and cover only standard Talos services. If there are additional services running with host networking in the cluster, they should be covered by additional rules.\n\nIn the block mode, the ingress firewall will also block encapsulated traffic (e.g. VXLAN) between the nodes, which needs to be explicitly allowed for the Kubernetes networking to function properly. Please refer to the CNI documentation for the specifics, some default configurations are listed below:\n\nFlannel, Calico: vxlan UDP port 4789\nCilium: vxlan UDP port 8472\n\nIn the examples we assume following template variables to describe the cluster:\n\n$CLUSTER_SUBNET, e.g. 172.20.0.0/24 - the subnet which covers all machines in the cluster\n$CP1, $CP2, $CP3 - the IP addresses of the controlplane nodes\n$VXLAN_PORT - the UDP port used by the CNI for encapsulated traffic\nControlplane\napid and Kubernetes API are wide open\nkubelet and trustd API is only accessible within the cluster\netcd API is limited to controlplane nodes\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: trustd-ingress\n\nportSelector:\n\n  ports:\n\n    - 50001\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubernetes-api-ingress\n\nportSelector:\n\n  ports:\n\n    - 6443\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: etcd-ingress\n\nportSelector:\n\n  ports:\n\n    - 2379-2380\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CP1/32\n\n  - subnet: $CP2/32\n\n  - subnet: $CP3/32\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nWorker\nkubelet and apid API is only accessible within the cluster\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nLearn More\n\nTalos Linux Ingress Firewall is using the nftables to perform the filtering.\n\nWith the default action set to accept, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy accept;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ip saddr != { 172.20.0.0/24 } tcp dport { 10250 } drop\n\n    meta nfproto ipv6 tcp dport { 10250 } drop\n\n  }\n\n}\n\n\nWith the default action set to block, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy drop;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ct state { established, related } accept\n\n    ct state invalid drop\n\n    meta l4proto icmp limit rate 5/second accept\n\n    meta l4proto ipv6-icmp limit rate 5/second accept\n\n    ip saddr { 172.20.0.0/24 } tcp dport { 10250 }  accept\n\n    meta nfproto ipv4 tcp dport { 50000 } accept\n\n    meta nfproto ipv6 tcp dport { 50000 } accept\n\n  }\n\n}\n\n2.4.3 - KubeSpan\nLearn to use KubeSpan to connect Talos Linux machines securely across networks.\n\nKubeSpan is a feature of Talos that automates the setup and maintenance of a full mesh WireGuard network for your cluster, giving you the ability to operate hybrid Kubernetes clusters that can span the edge, datacenter, and cloud. Management of keys and discovery of peers can be completely automated, making it simple and easy to create hybrid clusters.\n\nKubeSpan consists of client code in Talos Linux, as well as a discovery service that enables clients to securely find each other. Sidero Labs operates a free Discovery Service, but the discovery service may, with a commercial license, be operated by your organization and can be downloaded here.\n\nVideo Walkthrough\n\nTo see a live demo of KubeSpan, see one the videos below:\n\n \nNetwork Requirements\n\nKubeSpan uses UDP port 51820 to carry all KubeSpan encrypted traffic. Because UDP traversal of firewalls is often lenient, and the Discovery Service communicates the apparent IP address of all peers to all other peers, KubeSpan will often work automatically, even when each nodes is behind their own firewall. However, when both ends of a KubeSpan connection are behind firewalls, it is possible the connection may not be established correctly - it depends on each end sending out packets in a limited time window.\n\nThus best practice is to ensure that one end of all possible node-node communication allows UDP port 51820, inbound.\n\nFor example, if control plane nodes are running in a corporate data center, behind firewalls, KubeSpan connectivity will work correctly so long as worker nodes on the public Internet can receive packets on UDP port 51820. (Note the workers will also need to receive TCP port 50000 for initial configuration via talosctl).\n\nAn alternative topology would be to run control plane nodes in a public cloud, and allow inbound UDP port 51820 to the control plane nodes. Workers could be behind firewalls, and KubeSpan connectivity will be established. Note that if workers are in different locations, behind different firewalls, the KubeSpan connectivity between workers should be correctly established, but may require opening the KubeSpan UDP port on the local firewall also.\n\nCaveats\nKubernetes API Endpoint Limitations\n\nWhen the K8s endpoint is an IP address that is not part of Kubespan, but is an address that is forwarded on to the Kubespan address of a control plane node, without changing the source address, then worker nodes will fail to join the cluster. In such a case, the control plane node has no way to determine whether the packet arrived on the private Kubespan address, or the public IP address. If the source of the packet was a Kubespan member, the reply will be Kubespan encapsulated, and thus not translated to the public IP, and so the control plane will reply to the session with the wrong address.\n\nThis situation is seen, for example, when the Kubernetes API endpoint is the public IP of a VM in GCP or Azure for a single node control plane. The control plane will receive packets on the public IP, but will reply from it’s KubeSpan address. The workaround is to create a load balancer to terminate the Kubernetes API endpoint.\n\nDigital Ocean Limitations\n\nDigital Ocean assigns an “Anchor IP” address to each droplet. Talos Linux correctly identifies this as a link-local address, and configures KubeSpan correctly, but this address will often be selected by Flannel or other CNIs as a node’s private IP. Because this address is not routable, nor advertised via KubeSpan, it will break pod-pod communication between nodes. This can be worked-around by assigning a non-Anchor private IP:\n\nkubectl annotate node do-worker flannel.alpha.coreos.com/public-ip-overwrite=10.116.X.X\n\nThen restarting flannel: kubectl delete pods -n kube-system -l k8s-app=flannel\n\nEnabling\nCreating a New Cluster\n\nTo enable KubeSpan for a new cluster, we can use the --with-kubespan flag in talosctl gen config. This will enable peer discovery and KubeSpan.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\n\ncluster:\n\n    discovery:\n\n        enabled: true\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            kubernetes: # Kubernetes registry is problematic with KubeSpan, if the control plane endpoint is routeable itself via KubeSpan.\n\n              disabled: true\n\n            service: {}\n\n\nThe default discovery service is an external service hosted by Sidero Labs at https://discovery.talos.dev/. Contact Sidero Labs if you need to run this service privately.\n\nEnabling for an Existing Cluster\n\nIn order to enable KubeSpan on an existing cluster, enable kubespan and discovery settings in the machine config for each machine in the cluster (discovery is enabled by default):\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: true\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\nConfiguration\n\nKubeSpan will automatically discovery all cluster members, exchange Wireguard public keys and establish a full mesh network.\n\nThere are configuration options available which are not usually required:\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: false\n\n      advertiseKubernetesNetworks: false\n\n      allowDownPeerBypass: false\n\n      mtu: 1420\n\n      filters:\n\n        endpoints:\n\n          - 0.0.0.0/0\n\n          - ::/0\n\n\nThe setting advertiseKubernetesNetworks controls whether the node will advertise Kubernetes service and pod networks to other nodes in the cluster over KubeSpan. It defaults to being disabled, which means KubeSpan only controls the node-to-node traffic, while pod-to-pod traffic is routed and encapsulated by CNI. This setting should not be enabled with Calico and Cilium CNI plugins, as they do their own pod IP allocation which is not visible to KubeSpan.\n\nThe setting allowDownPeerBypass controls whether the node will allow traffic to bypass WireGuard if the destination is not connected over KubeSpan. If enabled, there is a risk that traffic will be routed unencrypted if the destination is not connected over KubeSpan, but it allows a workaround for the case where a node is not connected to the KubeSpan network, but still needs to access the cluster.\n\nThe mtu setting configures the Wireguard MTU, which defaults to 1420. This default value of 1420 is safe to use when the underlying network MTU is 1500, but if the underlying network MTU is smaller, the KubeSpanMTU should be adjusted accordingly: KubeSpanMTU = UnderlyingMTU - 80.\n\nThe filters setting allows hiding some endpoints from being advertised over KubeSpan. This is useful when some endpoints are known to be unreachable between the nodes, so that KubeSpan doesn’t try to establish a connection to them. Another use-case is hiding some endpoints if nodes can connect on multiple networks, and some of the networks are more preferable than others.\n\nResource Definitions\nKubeSpanIdentities\n\nA node’s WireGuard identities can be obtained with:\n\n$ talosctl get kubespanidentities -o yaml\n\n...\n\nspec:\n\n    address: fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94/128\n\n    subnet: fd83:b1f7:fcb5:2802::/64\n\n    privateKey: gNoasoKOJzl+/B+uXhvsBVxv81OcVLrlcmQ5jQwZO08=\n\n    publicKey: NzW8oeIH5rJyY5lefD9WRoHWWRr/Q6DwsDjMX+xKjT4=\n\n\nTalos automatically configures unique IPv6 address for each node in the cluster-specific IPv6 ULA prefix.\n\nThe Wireguard private key is generated and never leaves the node, while the public key is published through the cluster discovery.\n\nKubeSpanIdentity is persisted across reboots and upgrades in STATE partition in the file kubespan-identity.yaml.\n\nKubeSpanPeerSpecs\n\nA node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerspecs\n\nID                                             VERSION   LABEL                          ENDPOINTS\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   2         talos-default-controlplane-2   [\"172.20.0.3:51820\"]\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   2         talos-default-controlplane-3   [\"172.20.0.4:51820\"]\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   2         talos-default-worker-2         [\"172.20.0.6:51820\"]\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   2         talos-default-worker-1         [\"172.20.0.5:51820\"]\n\n\nThe peer ID is the Wireguard public key. KubeSpanPeerSpecs are built from the cluster discovery data.\n\nKubeSpanPeerStatuses\n\nThe status of a node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerstatuses\n\nID                                             VERSION   LABEL                          ENDPOINT           STATE   RX         TX\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   63        talos-default-controlplane-2   172.20.0.3:51820   up      15043220   17869488\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   62        talos-default-controlplane-3   172.20.0.4:51820   up      14573208   18157680\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   60        talos-default-worker-2         172.20.0.6:51820   up      130072     46888\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   60        talos-default-worker-1         172.20.0.5:51820   up      130044     46556\n\n\nKubeSpan peer status includes following information:\n\nthe actual endpoint used for peer communication\nlink state:\nunknown: the endpoint was just changed, link state is not known yet\nup: there is a recent handshake from the peer\ndown: there is no handshake from the peer\nnumber of bytes sent/received over the Wireguard link with the peer\n\nIf the connection state goes down, Talos will be cycling through the available endpoints until it finds the one which works.\n\nPeer status information is updated every 30 seconds.\n\nKubeSpanEndpoints\n\nA node’s WireGuard endpoints (peer addresses) can be obtained with:\n\n$ talosctl get kubespanendpoints\n\nID                                             VERSION   ENDPOINT           AFFILIATE ID\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   1         172.20.0.3:51820   2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   1         172.20.0.4:51820   b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   1         172.20.0.6:51820   NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   1         172.20.0.5:51820   6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA\n\n\nThe endpoint ID is the base64 encoded WireGuard public key.\n\nThe observed endpoints are submitted back to the discovery service (if enabled) so that other peers can try additional endpoints to establish the connection.\n\n2.4.4 - Network Device Selector\nHow to configure network devices by selecting them using hardware information\nConfiguring Network Device Using Device Selector\n\ndeviceSelector is an alternative method of configuring a network device:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          driver: virtio\n\n          hardwareAddr: \"00:00:*\"\n\n        address: 192.168.88.21\n\n\nSelector has the following traits:\n\nqualifiers match a device by reading the hardware information in /sys/class/net/...\nqualifiers are applied using logical AND\nmachine.network.interfaces.deviceConfig option is mutually exclusive with machine.network.interfaces.interface\nif the selector matches multiple devices, the controller will apply config to all of them\n\nThe available hardware information used in the selector can be observed in the LinkStatus resource (works in maintenance mode):\n\n# talosctl get links eth0 -o yaml\n\nspec:\n\n  ...\n\n  hardwareAddr: 4e:95:8e:8f:e4:47\n\n  busPath: 0000:06:00.0\n\n  driver: alx\n\n  pciID: 1969:E0B1\n\nUsing Device Selector for Bonding\n\nDevice selectors can be used to configure bonded interfaces:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        bond:\n\n          mode: balance-rr\n\n          deviceSelectors:\n\n            - hardwareAddr: '00:50:56:8e:8f:e4'\n\n            - hardwareAddr: '00:50:57:9c:2c:2d'\n\n\nIn this example, the bond0 interface will be created and bonded using two devices with the specified hardware addresses.\n\n2.4.5 - Predictable Interface Names\nHow to use predictable interface naming.\n\nStarting with version Talos 1.5, network interfaces are renamed to predictable names same way as systemd does that in other Linux distributions.\n\nThe naming schema enx78e7d1ea46da (based on MAC addresses) is enabled by default, the order of interface naming decisions is:\n\nfirmware/BIOS provided index numbers for on-board devices (example: eno1)\nfirmware/BIOS provided PCI Express hotplug slot index numbers (example: ens1)\nphysical/geographical location of the connector of the hardware (example: enp2s0)\ninterfaces’s MAC address (example: enx78e7d1ea46da)\n\nThe predictable network interface names features can be disabled by specifying net.ifnames=0 in the kernel command line.\n\nNote: Talos automatically adds the net.ifnames=0 kernel argument when upgrading from Talos versions before 1.5, so upgrades to 1.5 don’t require any manual intervention.\n\n“Cloud” platforms, like AWS, still use old eth0 naming scheme as Talos automatically adds net.ifnames=0 to the kernel command line.\n\nSingle Network Interface\n\nWhen running Talos on a machine with a single network interface, predictable interface names might be confusing, as it might come up as enxSOMETHING which is hard to address. There are two ways to solve this:\n\ndisable the feature by supplying net.ifnames=0 to the initial boot of Talos, Talos will persist net.ifnames=0 over installs/upgrades.\n\nuse device selectors:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n        # any configuration can follow, e.g:\n\n        addresses: [10.3.4.5/24]\n\n2.4.6 - Virtual (shared) IP\nUsing Talos Linux to set up a floating virtual IP address for cluster access.\n\nOne of the pain points when building a high-availability controlplane is giving clients a single IP or URL at which they can reach any of the controlplane nodes. The most common approaches - reverse proxy, load balancer, BGP, and DNS - all require external resources, and add complexity in setting up Kubernetes.\n\nTo simplify cluster creation, Talos Linux supports a “Virtual” IP (VIP) address to access the Kubernetes API server, providing high availability with no other resources required.\n\nWhat happens is that the controlplane machines vie for control of the shared IP address using etcd elections. There can be only one owner of the IP address at any given time. If that owner disappears or becomes non-responsive, another owner will be chosen, and it will take up the IP address.\n\nRequirements\n\nThe controlplane nodes must share a layer 2 network, and the virtual IP must be assigned from that shared network subnet. In practical terms, this means that they are all connected via a switch, with no router in between them. Note that the virtual IP election depends on etcd being up, as Talos uses etcd for elections and leadership (control) of the IP address.\n\nThe virtual IP is not restricted by ports - you can access any port that the control plane nodes are listening on, on that IP address. Thus it is possible to access the Talos API over the VIP, but it is not recommended, as you cannot access the VIP when etcd is down - and then you could not access the Talos API to recover etcd.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nChoose your Shared IP\n\nThe Virtual IP should be a reserved, unused IP address in the same subnet as your controlplane nodes. It should not be assigned or assignable by your DHCP server.\n\nFor our example, we will assume that the controlplane nodes have the following IP addresses:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nWe then choose our shared IP to be:\n\n192.168.0.15\nConfigure your Talos Machines\n\nThe shared IP setting is only valid for controlplane nodes.\n\nFor the example above, each of the controlplane nodes should have the following Machine Config snippet:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n\nVirtual IP’s can also be configured on a VLAN interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n      vlans:\n\n        - vlanId: 100\n\n          dhcp: true\n\n          vip:\n\n            ip: 192.168.1.15\n\n\nFor your own environment, the interface and the DHCP setting may differ, or you may use static addressing (adresses) instead of DHCP.\n\nWhen using predictable interface names, the interface name might not be eth0.\n\nIf the machine has a single network interface, it can be selected using a dummy device selector:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\nCaveats\n\nSince VIP functionality relies on etcd for elections, the shared IP will not come alive until after you have bootstrapped Kubernetes.\n\nDon’t use the VIP as the endpoint in the talosconfig, as the VIP is bound to etcd and kube-apiserver health, and you will not be able to recover from a failure of either of those components using Talos API.\n\n2.4.7 - Wireguard Network\nA guide on how to set up Wireguard network using Kernel module.\nConfiguring Wireguard Network\nQuick Start\n\nThe quickest way to try out Wireguard is to use talosctl cluster create command:\n\ntalosctl cluster create --wireguard-cidr 10.1.0.0/24\n\n\nIt will automatically generate Wireguard network configuration for each node with the following network topology:\n\nWhere all controlplane nodes will be used as Wireguard servers which listen on port 51111. All controlplanes and workers will connect to all controlplanes. It also sets PersistentKeepalive to 5 seconds to establish controlplanes to workers connection.\n\nAfter the cluster is deployed it should be possible to verify Wireguard network connectivity. It is possible to deploy a container with hostNetwork enabled, then do kubectl exec <container> /bin/bash and either do:\n\nping 10.1.0.2\n\n\nOr install wireguard-tools package and run:\n\nwg show\n\n\nWireguard show should output something like this:\n\ninterface: wg0\n\n  public key: OMhgEvNIaEN7zeCLijRh4c+0Hwh3erjknzdyvVlrkGM=\n\n  private key: (hidden)\n\n  listening port: 47946\n\n\n\npeer: 1EsxUygZo8/URWs18tqB5FW2cLVlaTA+lUisKIf8nh4=\n\n  endpoint: 10.5.0.2:51111\n\n  allowed ips: 10.1.0.0/24\n\n  latest handshake: 1 minute, 55 seconds ago\n\n  transfer: 3.17 KiB received, 3.55 KiB sent\n\n  persistent keepalive: every 5 seconds\n\n\nIt is also possible to use generated configuration as a reference by pulling generated config files using:\n\ntalosctl read -n 10.5.0.2 /system/state/config.yaml > controlplane.yaml\n\ntalosctl read -n 10.5.0.3 /system/state/config.yaml > worker.yaml\n\nManual Configuration\n\nAll Wireguard configuration can be done by changing Talos machine config files. As an example we will use this official Wireguard quick start tutorial.\n\nKey Generation\n\nThis part is exactly the same:\n\nwg genkey | tee privatekey | wg pubkey > publickey\n\nSetting up Device\n\nInline comments show relations between configs and wg quickstart tutorial commands:\n\n...\n\nnetwork:\n\n  interfaces:\n\n    ...\n\n      # ip link add dev wg0 type wireguard\n\n    - interface: wg0\n\n      mtu: 1500\n\n      # ip address add dev wg0 192.168.2.1/24\n\n      addresses:\n\n        - 192.168.2.1/24\n\n      # wg set wg0 listen-port 51820 private-key /path/to/private-key peer ABCDEF... allowed-ips 192.168.88.0/24 endpoint 209.202.254.14:8172\n\n      wireguard:\n\n        privateKey: <privatekey file contents>\n\n        listenPort: 51820\n\n        peers:\n\n          allowedIPs:\n\n            - 192.168.88.0/24\n\n          endpoint: 209.202.254.14.8172\n\n          publicKey: ABCDEF...\n\n...\n\n\nWhen networkd gets this configuration it will create the device, configure it and will bring it up (equivalent to ip link set up dev wg0).\n\nAll supported config parameters are described in the Machine Config Reference.\n\n2.5 - Discovery Service\nTalos Linux Node discovery services\n\nTalos Linux includes node-discovery capabilities that depend on a discovery registry. This allows you to see the members of your cluster, and the associated IP addresses of the nodes.\n\ntalosctl get members\n\nNODE       NAMESPACE   TYPE     ID                             VERSION   HOSTNAME                       MACHINE TYPE   OS               ADDRESSES\n\n10.5.0.2   cluster     Member   talos-default-controlplane-1   1         talos-default-controlplane-1   controlplane   Talos (v1.2.3)   [\"10.5.0.2\"]\n\n10.5.0.2   cluster     Member   talos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.2.3)   [\"10.5.0.3\"]\n\n\nThere are currently two supported discovery services: a Kubernetes registry (which stores data in the cluster’s etcd service) and an external registry service. Sidero Labs runs a public external registry service, which is enabled by default. The Kubernetes registry service is disabled by default. The advantage of the external registry service is that it is not dependent on etcd, and thus can inform you of cluster membership even when Kubernetes is down.\n\nVideo Walkthrough\n\nTo see a live demo of Cluster Discovery, see the video below:\n\nRegistries\n\nPeers are aggregated from enabled registries. By default, Talos will use the service registry, while the kubernetes registry is disabled. To disable a registry, set disabled to true (this option is the same for all registries): For example, to disable the service registry:\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\n    registries:\n\n      service:\n\n        disabled: true\n\n\nDisabling all registries effectively disables member discovery.\n\nNote: An enabled discovery service is required for KubeSpan to function correctly.\n\nThe Kubernetes registry uses Kubernetes Node resource data and additional Talos annotations:\n\n$ kubectl describe node <nodename>\n\nAnnotations:        cluster.talos.dev/node-id: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n                    networking.talos.dev/assigned-prefixes: 10.244.0.0/32,10.244.0.1/24\n\n                    networking.talos.dev/self-ips: 172.20.0.2,fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\n\n...\n\n\nThe Service registry by default uses a public external Discovery Service to exchange encrypted information about cluster members.\n\nNote: Talos supports operations when Discovery Service is disabled, but some features will rely on Kubernetes API availability to discover controlplane endpoints, so in case of a failure disabled Discovery Service makes troubleshooting much harder.\n\nDiscovery Service\n\nSidero Labs maintains a public discovery service at https://discovery.talos.dev/ whereby cluster members use a shared key that is globally unique to coordinate basic connection information (i.e. the set of possible “endpoints”, or IP:port pairs). We call this data “affiliate data.”\n\nNote: If KubeSpan is enabled the data has the addition of the WireGuard public key.\n\nData sent to the discovery service is encrypted with AES-GCM encryption and endpoint data is separately encrypted with AES in ECB mode so that endpoints coming from different sources can be deduplicated server-side. Each node submits its own data, plus the endpoints it sees from other peers, to the discovery service. The discovery service aggregates the data, deduplicates the endpoints, and sends updates to each connected peer. Each peer receives information back from the discovery service, decrypts it and uses it to drive KubeSpan and cluster discovery.\n\nData is stored in memory only. The cluster ID is used as a key to select the affiliates (so that different clusters see different affiliates).\n\nTo summarize, the discovery service knows the client version, cluster ID, the number of affiliates, some encrypted data for each affiliate, and a list of encrypted endpoints. The discovery service doesn’t see actual node information – it only stores and updates encrypted blobs. Discovery data is encrypted/decrypted by the clients – the cluster members. The discovery service does not have the encryption key.\n\nThe discovery service may, with a commercial license, be operated by your organization and can be downloaded here. In order for nodes to communicate to the discovery service, they must be able to connect to it on TCP port 443.\n\nResource Definitions\n\nTalos provides resources that can be used to introspect the discovery and KubeSpan features.\n\nDiscovery\nIdentities\n\nThe node’s unique identity (base62 encoded random 32 bytes) can be obtained with:\n\nNote: Using base62 allows the ID to be URL encoded without having to use the ambiguous URL-encoding version of base64.\n\n$ talosctl get identities -o yaml\n\n...\n\nspec:\n\n    nodeId: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n\nNode identity is used as the unique Affiliate identifier.\n\nNode identity resource is preserved in the STATE partition in node-identity.yaml file. Node identity is preserved across reboots and upgrades, but it is regenerated if the node is reset (wiped).\n\nAffiliates\n\nAn affiliate is a proposed member: the node has the same cluster ID and secret.\n\n$ talosctl get affiliates\n\nID                                             VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\n2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    2         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\n6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nNVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nUtoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd    4         talos-default-controlplane-1   controlplane   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\nb3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   2         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nOne of the Affiliates with the ID matching node identity is populated from the node data, other Affiliates are pulled from the registries. Enabled discovery registries run in parallel and discovered data is merged to build the list presented above.\n\nDetails about data coming from each registry can be queried from the cluster-raw namespace:\n\n$ talosctl get affiliates --namespace=cluster-raw\n\nID                                                     VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\nk8s/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF        3         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nk8s/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA       2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nk8s/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB       2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nk8s/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C       3         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\nservice/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    23        talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nservice/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   26        talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nservice/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   20        talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nservice/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   14        talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nEach Affiliate ID is prefixed with k8s/ for data coming from the Kubernetes registry and with service/ for data coming from the discovery service.\n\nMembers\n\nA member is an affiliate that has been approved to join the cluster. The members of the cluster can be obtained with:\n\n$ talosctl get members\n\nID                             VERSION   HOSTNAME                       MACHINE TYPE   OS                ADDRESSES\n\ntalos-default-controlplane-1   2         talos-default-controlplane-1   controlplane   Talos (v1.6.2)   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\ntalos-default-controlplane-2   1         talos-default-controlplane-2   controlplane   Talos (v1.6.2)   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\ntalos-default-controlplane-3   1         talos-default-controlplane-3   controlplane   Talos (v1.6.2)   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\ntalos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.6.2)   [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\ntalos-default-worker-2         1         talos-default-worker-2         worker         Talos (v1.6.2)   [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\n2.6 - Interactive Dashboard\nA tool to inspect the running Talos machine state on the physical video console.\n\nInteractive dashboard is enabled for all Talos platforms except for SBC images. The dashboard can be disabled with kernel parameter talos.dashboard.disabled=1.\n\nThe dashboard runs only on the physical video console (not serial console) on the 2nd virtual TTY. The first virtual TTY shows kernel logs same as in Talos <1.4.0. The virtual TTYs can be switched with <Alt+F1> and <Alt+F2> keys.\n\nKeys <F1> - <Fn> can be used to switch between different screens of the dashboard.\n\nThe dashboard is using either UEFI framebuffer or VGA/VESA framebuffer (for legacy BIOS boot). For legacy BIOS boot screen resolution can be controlled with the vga= kernel parameter.\n\nSummary Screen (F1)\n\nInteractive Dashboard Summary Screen\n\nThe header shows brief information about the node:\n\nhostname\nTalos version\nuptime\nCPU and memory hardware information\nCPU and memory load, number of processes\n\nTable view presents summary information about the machine:\n\nUUID (from SMBIOS data)\nCluster name (when the machine config is available)\nMachine stage: Installing, Upgrading, Booting, Maintenance, Running, Rebooting, Shutting down, etc.\nMachine stage readiness: checks Talos service status, static pod status, etc. (for Running stage)\nMachine type: controlplane/worker\nNumber of members discovered in the cluster\nKubernetes version\nStatus of Kubernetes components: kubelet and Kubernetes controlplane components (only on controlplane machines)\nNetwork information: Hostname, Addresses, Gateway, Connectivity, DNS and NTP servers\n\nBottom part of the screen shows kernel logs, same as on the virtual TTY 1.\n\nMonitor Screen (F2)\n\nInteractive Dashboard Monitor Screen\n\nMonitor screen provides live view of the machine resource usage: CPU, memory, disk, network and processes.\n\nNetwork Config Screen (F3)\n\nNote: network config screen is only available for metal platform.\n\nInteractive Dashboard Network Config Screen\n\nNetwork config screen provides editing capabilities for the metal platform network configuration.\n\nThe screen is split into three sections:\n\nthe leftmost section provides a way to enter network configuration: hostname, DNS and NTP servers, configure the network interface either via DHCP or static IP address, etc.\nthe middle section shows the current network configuration.\nthe rightmost section shows the network configuration which will be applied after pressing “Save” button.\n\nOnce the platform network configuration is saved, it is immediately applied to the machine.\n\n2.7 - Resetting a Machine\nSteps on how to reset a Talos Linux machine to a clean state.\n\nFrom time to time, it may be beneficial to reset a Talos machine to its “original” state. Bear in mind that this is a destructive action for the given machine. Doing this means removing the machine from Kubernetes, etcd (if applicable), and clears any data on the machine that would normally persist a reboot.\n\nCLI\n\nWARNING: Running a talosctl reset on cloud VM’s might result in the VM being unable to boot as this wipes the entire disk. It might be more useful to just wipe the STATE and EPHEMERAL partitions on a cloud VM if not booting via iPXE. talosctl reset --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL\n\nThe API command for doing this is talosctl reset. There are a couple of flags as part of this command:\n\nFlags:\n\n      --graceful                        if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n\n      --reboot                          if true, reboot the node after resetting instead of shutting down\n\n      --system-labels-to-wipe strings   if set, just wipe selected system disk partitions by label but keep other partitions intact keep other partitions intact\n\n\nThe graceful flag is especially important when considering HA vs. non-HA Talos clusters. If the machine is part of an HA cluster, a normal, graceful reset should work just fine right out of the box as long as the cluster is in a good state. However, if this is a single node cluster being used for testing purposes, a graceful reset is not an option since Etcd cannot be “left” if there is only a single member. In this case, reset should be used with --graceful=false to skip performing checks that would normally block the reset.\n\nKernel Parameter\n\nAnother way to reset a machine is to specify talos.experimental.wipe=system kernel parameter. If the machine got stuck in the boot loop and you access to the console you can use GRUB to specify this kernel argument. Then when Talos boots for the next time it will reset system disk and reboot.\n\nNext steps can be to install Talos either using PXE boot or by mounting an ISO.\n\n2.8 - Upgrading Talos Linux\nGuide to upgrading a Talos Linux machine.\n\nOS upgrades are effected by an API call, which can be sent via the talosctl CLI utility.\n\nThe upgrade API call passes a node the installer image to use to perform the upgrade. Each Talos version has a corresponding installer image, listed on the release page for the version, for example v1.6.2.\n\nUpgrades use an A-B image scheme in order to facilitate rollbacks. This scheme retains the previous Talos kernel and OS image following each upgrade. If an upgrade fails to boot, Talos will roll back to the previous version. Likewise, Talos may be manually rolled back via API (or talosctl rollback), which will update the boot reference and reboot.\n\nUnless explicitly told to preserve data, an upgrade will cause the node to wipe the EPHEMERAL partition, remove itself from the etcd cluster (if it is a controlplane node), and make itself as pristine as is possible. (This is the desired behavior except in specialised use cases such as single-node clusters.)\n\nNote An upgrade of the Talos Linux OS will not (since v1.0) apply an upgrade to the Kubernetes version by default. Kubernetes upgrades should be managed separately per upgrading kubernetes.\n\nSupported Upgrade Paths\n\nBecause Talos Linux is image based, an upgrade is almost the same as installing Talos, with the difference that the system has already been initialized with a configuration. The supported configuration may change between versions. The upgrade process should handle such changes transparently, but this migration is only tested between adjacent minor releases. Thus the recommended upgrade path is to always upgrade to the latest patch release of all intermediate minor releases.\n\nFor example, if upgrading from Talos 1.0 to Talos 1.2.4, the recommended upgrade path would be:\n\nupgrade from 1.0 to latest patch of 1.0 - to v1.0.6\nupgrade from v1.0.6 to latest patch of 1.1 - to v1.1.2\nupgrade from v1.1.2 to v1.2.4\nBefore Upgrade to v1.6.2\n\nThere are no specific actions to be taken before an upgrade.\n\nPlease review the release notes for any changes that may affect your cluster.\n\nVideo Walkthrough\n\nTo see a live demo of an upgrade of Talos Linux, see the video below:\n\nAfter Upgrade to v1.6.2\n\nThere are no specific actions to be taken after an upgrade.\n\ntalosctl upgrade\n\nTo upgrade a Talos node, specify the node’s IP address and the installer container image for the version of Talos to upgrade to.\n\nFor instance, if your Talos node has the IP address 10.20.30.40 and you want to install the current version, you would enter a command such as:\n\n  $ talosctl upgrade --nodes 10.20.30.40 \\\n\n      --image ghcr.io/siderolabs/installer:v1.6.2\n\n\nThere is an option to this command: --preserve, which will explicitly tell Talos to keep ephemeral data intact. In most cases, it is correct to let Talos perform its default action of erasing the ephemeral data. However, for a single-node control-plane, make sure that --preserve=true.\n\nRarely, an upgrade command will fail due to a process holding a file open on disk. In these cases, you can use the --stage flag. This puts the upgrade artifacts on disk, and adds some metadata to a disk partition that gets checked very early in the boot process, then reboots the node. On the reboot, Talos sees that it needs to apply an upgrade, and will do so immediately. Because this occurs in a just rebooted system, there will be no conflict with any files being held open. After the upgrade is applied, the node will reboot again, in order to boot into the new version. Note that because Talos Linux reboots via the kexec syscall, the extra reboot adds very little time.\n\nMachine Configuration Changes\n\nNew configuration documents:\n\nIngress Firewall configuration: NetworkRuleConfig and NetworkDefaultActionConfig.\n\nUpdates in v1alpha1 Config:\n\n.persist option was removed\n.machine.nodeTaints configures Kubernetes node taints\n.machine.kubelet.extraMounts supports new fields uidMappings and gidMappings\n.machine.kubelet.credendtialProviderConfig configures kubelet credential provider\n.machine.network.kubespan.harvestExtraEndpoints to disable harvesting extra endpoints\n.cluster.cni.flannel provides customization for the default Flannel CNI manifest\n.cluster.scheduler.config provides custom kube-scheduler configuration\nUpgrade Sequence\n\nWhen a Talos node receives the upgrade command, it cordons itself in Kubernetes, to avoid receiving any new workload. It then starts to drain its existing workload.\n\nNOTE: If any of your workloads are sensitive to being shut down ungracefully, be sure to use the lifecycle.preStop Pod spec.\n\nOnce all of the workload Pods are drained, Talos will start shutting down its internal processes. If it is a control node, this will include etcd. If preserve is not enabled, Talos will leave etcd membership. (Talos ensures the etcd cluster is healthy and will remain healthy after our node leaves the etcd cluster, before allowing a control plane node to be upgraded.)\n\nOnce all the processes are stopped and the services are shut down, the filesystems will be unmounted. This allows Talos to produce a very clean upgrade, as close as possible to a pristine system. We verify the disk and then perform the actual image upgrade. We set the bootloader to boot once with the new kernel and OS image, then we reboot.\n\nAfter the node comes back up and Talos verifies itself, it will make the bootloader change permanent, rejoin the cluster, and finally uncordon itself to receive new workloads.\n\nFAQs\n\nQ. What happens if an upgrade fails?\n\nA. Talos Linux attempts to safely handle upgrade failures.\n\nThe most common failure is an invalid installer image reference. In this case, Talos will fail to download the upgraded image and will abort the upgrade.\n\nSometimes, Talos is unable to successfully kill off all of the disk access points, in which case it cannot safely unmount all filesystems to effect the upgrade. In this case, it will abort the upgrade and reboot. (upgrade --stage can ensure that upgrades can occur even when the filesytems cannot be unmounted.)\n\nIt is possible (especially with test builds) that the upgraded Talos system will fail to start. In this case, the node will be rebooted, and the bootloader will automatically use the previous Talos kernel and image, thus effectively rolling back the upgrade.\n\nLastly, it is possible that Talos itself will upgrade successfully, start up, and rejoin the cluster but your workload will fail to run on it, for whatever reason. This is when you would use the talosctl rollback command to revert back to the previous Talos version.\n\nQ. Can upgrades be scheduled?\n\nA. Because the upgrade sequence is API-driven, you can easily tie it in to your own business logic to schedule and coordinate your upgrades.\n\nQ. Can the upgrade process be observed?\n\nA. Yes, using the talosctl dmesg -f command. You can also use talosctl upgrade --wait, and optionally talosctl upgrade --wait --debug to observe kernel logs\n\nQ. Are worker node upgrades handled differently from control plane node upgrades?\n\nA. Short answer: no.\n\nLong answer: Both node types follow the same set procedure. From the user’s standpoint, however, the processes are identical. However, since control plane nodes run additional services, such as etcd, there are some extra steps and checks performed on them. For instance, Talos will refuse to upgrade a control plane node if that upgrade would cause a loss of quorum for etcd. If multiple control plane nodes are asked to upgrade at the same time, Talos will protect the Kubernetes cluster by ensuring only one control plane node actively upgrades at any time, via checking etcd quorum. If running a single-node cluster, and you want to force an upgrade despite the loss of quorum, you can set preserve to true.\n\nQ. Can I break my cluster by upgrading everything at once?\n\nA. Possibly - it’s not recommended.\n\nNothing prevents the user from sending near-simultaneous upgrades to each node of the cluster - and while Talos Linux and Kubernetes can generally deal with this situation, other components of the cluster may not be able to recover from more than one node rebooting at a time. (e.g. any software that maintains a quorum or state across nodes, such as Rook/Ceph)\n\nQ. Which version of talosctl should I use to update a cluster?\n\nA. We recommend using the version that matches the current running version of the cluster.\n\n3 - Kubernetes Guides\nManagement of a Kubernetes Cluster hosted by Talos Linux\n3.1 - Configuration\nHow to configure components of the Kubernetes cluster itself.\n3.1.1 - Ceph Storage cluster with Rook\nGuide on how to create a simple Ceph storage cluster with Rook for Kubernetes\nPreparation\n\nTalos Linux reserves an entire disk for the OS installation, so machines with multiple available disks are needed for a reliable Ceph cluster with Rook and Talos Linux. Rook requires that the block devices or partitions used by Ceph have no partitions or formatted filesystems before use. Rook also requires a minimum Kubernetes version of v1.16 and Helm v3.0 for installation of charts. It is highly recommended that the Rook Ceph overview is read and understood before deploying a Ceph cluster with Rook.\n\nInstallation\n\nCreating a Ceph cluster with Rook requires two steps; first the Rook Operator needs to be installed which can be done with a Helm Chart. The example below installs the Rook Operator into the rook-ceph namespace, which is the default for a Ceph cluster with Rook.\n\n$ helm repo add rook-release https://charts.rook.io/release\n\n\"rook-release\" has been added to your repositories\n\n\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph\n\nW0327 17:52:44.277830   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nW0327 17:52:44.612243   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nNAME: rook-ceph\n\nLAST DEPLOYED: Sun Mar 27 17:52:42 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Rook Operator has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get pods -l \"app=rook-ceph-operator\"\n\n\n\nVisit https://rook.io/docs/rook/latest for instructions on how to create and configure Rook clusters\n\n\n\nImportant Notes:\n\n- You must customize the 'CephCluster' resource in the sample manifests for your cluster.\n\n- Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the namespace.\n\n- The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.\n\n- The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.\n\n- Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).\n\n\nOnce that is complete, the Ceph cluster can be installed with the official Helm Chart. The Chart can be installed with default values, which will attempt to use all nodes in the Kubernetes cluster, and all unused disks on each node for Ceph storage, and make available block storage, object storage, as well as a shared filesystem. Generally more specific node/device/cluster configuration is used, and the Rook documentation explains all the available options in detail. For this example the defaults will be adequate.\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster\n\nNAME: rook-ceph-cluster\n\nLAST DEPLOYED: Sun Mar 27 18:12:46 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Ceph Cluster has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get cephcluster\n\n\n\nVisit https://rook.github.io/docs/rook/latest/ceph-cluster-crd.html for more information about the Ceph CRD.\n\n\n\nImportant Notes:\n\n- You can only deploy a single cluster per namespace\n\n- If you wish to delete this cluster and start fresh, you will also have to wipe the OSD disks using `sfdisk`\n\n\nNow the Ceph cluster configuration has been created, the Rook operator needs time to install the Ceph cluster and bring all the components online. The progression of the Ceph cluster state can be followed with the following command.\n\n$ watch kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nEvery 2.0s: kubectl --namespace rook-ceph get cephcluster rook-ceph\n\n\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                 HEALTH   EXTERNAL\n\nrook-ceph   /var/lib/rook     3          57s   Progressing   Configuring Ceph Mons\n\n\nDepending on the size of the Ceph cluster and the availability of resources the Ceph cluster should become available, and with it the storage classes that can be used with Kubernetes Physical Volumes.\n\n$ kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          40m   Ready   Cluster created successfully   HEALTH_OK\n\n\n\n$ kubectl  get storageclass\n\nNAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n\nceph-block (default)   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   77m\n\nceph-bucket            rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  77m\n\nceph-filesystem        rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   77m\n\nTalos Linux Considerations\n\nIt is important to note that a Rook Ceph cluster saves cluster information directly onto the node (by default dataDirHostPath is set to /var/lib/rook). If running only a single mon instance, cluster management is little bit more involved, as any time a Talos Linux node is reconfigured or upgraded, the partition that stores the /var file system is wiped, but the --preserve option of talosctl upgrade will ensure that doesn’t happen.\n\nBy default, Rook configues Ceph to have 3 mon instances, in which case the data stored in dataDirHostPath can be regenerated from the other mon instances. So when performing maintenance on a Talos Linux node with a Rook Ceph cluster (e.g. upgrading the Talos Linux version), it is imperative that care be taken to maintain the health of the Ceph cluster. Before upgrading, you should always check the health status of the Ceph cluster to ensure that it is healthy.\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          98m   Ready   Cluster created successfully   HEALTH_OK\n\n\nIf it is, you can begin the upgrade process for the Talos Linux node, during which time the Ceph cluster will become unhealthy as the node is reconfigured. Before performing any other action on the Talos Linux nodes, the Ceph cluster must return to a healthy status.\n\n$ talosctl upgrade --nodes 172.20.15.5 --image ghcr.io/talos-systems/installer:v0.14.3\n\nNODE          ACK                        STARTED\n\n172.20.15.5   Upgrade request received   2022-03-27 20:29:55.292432887 +0200 CEST m=+10.050399758\n\n\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                   HEALTH        EXTERNAL\n\nrook-ceph   /var/lib/rook     3          99m   Progressing   Configuring Ceph Mgr(s)   HEALTH_WARN\n\n\n\n$ kubectl --namespace rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n\ncephcluster.ceph.rook.io/rook-ceph condition met\n\n\nThe above steps need to be performed for each Talos Linux node undergoing maintenance, one at a time.\n\nCleaning Up\nRook Ceph Cluster Removal\n\nRemoving a Rook Ceph cluster requires a few steps, starting with signalling to Rook that the Ceph cluster is really being destroyed. Then all Persistent Volumes (and Claims) backed by the Ceph cluster must be deleted, followed by the Storage Classes and the Ceph storage types.\n\n$ kubectl --namespace rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\ncephcluster.ceph.rook.io/rook-ceph patched\n\n\n\n$ kubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nstorageclass.storage.k8s.io \"ceph-block\" deleted\n\nstorageclass.storage.k8s.io \"ceph-bucket\" deleted\n\nstorageclass.storage.k8s.io \"ceph-filesystem\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephblockpools ceph-blockpool\n\ncephblockpool.ceph.rook.io \"ceph-blockpool\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephobjectstore ceph-objectstore\n\ncephobjectstore.ceph.rook.io \"ceph-objectstore\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephfilesystem ceph-filesystem\n\ncephfilesystem.ceph.rook.io \"ceph-filesystem\" deleted\n\n\nOnce that is complete, the Ceph cluster itself can be removed, along with the Rook Ceph cluster Helm chart installation.\n\n$ kubectl --namespace rook-ceph delete cephcluster rook-ceph\n\ncephcluster.ceph.rook.io \"rook-ceph\" deleted\n\n\n\n$ helm --namespace rook-ceph uninstall rook-ceph-cluster\n\nrelease \"rook-ceph-cluster\" uninstalled\n\n\nIf needed, the Rook Operator can also be removed along with all the Custom Resource Definitions that it created.\n\n$ helm --namespace rook-ceph uninstall rook-ceph\n\nW0328 12:41:14.998307  147203 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nThese resources were kept due to the resource policy:\n\n[CustomResourceDefinition] cephblockpools.ceph.rook.io\n\n[CustomResourceDefinition] cephbucketnotifications.ceph.rook.io\n\n[CustomResourceDefinition] cephbuckettopics.ceph.rook.io\n\n[CustomResourceDefinition] cephclients.ceph.rook.io\n\n[CustomResourceDefinition] cephclusters.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemmirrors.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystems.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemsubvolumegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephnfses.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectrealms.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstores.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstoreusers.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzonegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzones.ceph.rook.io\n\n[CustomResourceDefinition] cephrbdmirrors.ceph.rook.io\n\n[CustomResourceDefinition] objectbucketclaims.objectbucket.io\n\n[CustomResourceDefinition] objectbuckets.objectbucket.io\n\n\n\nrelease \"rook-ceph\" uninstalled\n\n\n\n$ kubectl delete crds cephblockpools.ceph.rook.io cephbucketnotifications.ceph.rook.io cephbuckettopics.ceph.rook.io \\\n\n                      cephclients.ceph.rook.io cephclusters.ceph.rook.io cephfilesystemmirrors.ceph.rook.io \\\n\n                      cephfilesystems.ceph.rook.io cephfilesystemsubvolumegroups.ceph.rook.io \\\n\n                      cephnfses.ceph.rook.io cephobjectrealms.ceph.rook.io cephobjectstores.ceph.rook.io \\\n\n                      cephobjectstoreusers.ceph.rook.io cephobjectzonegroups.ceph.rook.io cephobjectzones.ceph.rook.io \\\n\n                      cephrbdmirrors.ceph.rook.io objectbucketclaims.objectbucket.io objectbuckets.objectbucket.io\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephblockpools.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbucketnotifications.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbuckettopics.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclients.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclusters.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystems.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemsubvolumegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephnfses.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectrealms.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstores.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstoreusers.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzonegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzones.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephrbdmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbucketclaims.objectbucket.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbuckets.objectbucket.io\" deleted\n\nTalos Linux Rook Metadata Removal\n\nIf the Rook Operator is cleanly removed following the above process, the node metadata and disks should be clean and ready to be re-used. In the case of an unclean cluster removal, there may be still a few instances of metadata stored on the system disk, as well as the partition information on the storage disks. First the node metadata needs to be removed, make sure to update the nodeName with the actual name of a storage node that needs cleaning, and path with the Rook configuration dataDirHostPath set when installing the chart. The following will need to be repeated for each node used in the Rook Ceph cluster.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-clean\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  volumes:\n\n  - name: rook-data-dir\n\n    hostPath:\n\n      path: <dataDirHostPath>\n\n  containers:\n\n  - name: disk-clean\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    volumeMounts:\n\n    - name: rook-data-dir\n\n      mountPath: /node/rook-data\n\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\n\nEOF\n\npod/disk-clean created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\n\npod/disk-clean condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-clean\" deleted\n\n\nLastly, the disks themselves need the partition and filesystem data wiped before they can be reused. Again, the following as to be repeated for each node and disk used in the Rook Ceph cluster, updating nodeName and of= in the command as needed.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-wipe\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  containers:\n\n  - name: disk-wipe\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=<device>\"]\n\nEOF\n\npod/disk-wipe created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\n\npod/disk-wipe condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-wipe\" deleted\n\n3.1.2 - Deploying Metrics Server\nIn this guide you will learn how to set up metrics-server.\n\nMetrics Server enables use of the Horizontal Pod Autoscaler and Vertical Pod Autoscaler. It does this by gathering metrics data from the kubelets in a cluster. By default, the certificates in use by the kubelets will not be recognized by metrics-server. This can be solved by either configuring metrics-server to do no validation of the TLS certificates, or by modifying the kubelet configuration to rotate its certificates and use ones that will be recognized by metrics-server.\n\nNode Configuration\n\nTo enable kubelet certificate rotation, all nodes should have the following Machine Config snippet:\n\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      rotate-server-certificates: true\n\nInstall During Bootstrap\n\nWe will want to ensure that new certificates for the kubelets are approved automatically. This can easily be done with the Kubelet Serving Certificate Approver, which will automatically approve the Certificate Signing Requests generated by the kubelets.\n\nWe can have Kubelet Serving Certificate Approver and metrics-server installed on the cluster automatically during bootstrap by adding the following snippet to the Cluster Config of the node that will be handling the bootstrap process:\n\ncluster:\n\n  extraManifests:\n\n    - https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\n    - https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nInstall After Bootstrap\n\nIf you choose not to use extraManifests to install Kubelet Serving Certificate Approver and metrics-server during bootstrap, you can install them once the cluster is online using kubectl:\n\nkubectl apply -f https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n3.1.3 - iSCSI Storage with Synology CSI\nAutomatically provision iSCSI volumes on a Synology NAS with the synology-csi driver.\nBackground\n\nSynology is a company that specializes in Network Attached Storage (NAS) devices. They provide a number of features within a simple web OS, including an LDAP server, Docker support, and (perhaps most relevant to this guide) function as an iSCSI host. The focus of this guide is to allow a Kubernetes cluster running on Talos to provision Kubernetes storage (both dynamic or static) on a Synology NAS using a direct integration, rather than relying on an intermediary layer like Rook/Ceph or Maystor.\n\nThis guide assumes a very basic familiarity with iSCSI terminology (LUN, iSCSI target, etc.).\n\nPrerequisites\nSynology NAS running DSM 7.0 or above\nProvisioned Talos cluster running Kubernetes v1.20 or above\n(Optional) Both Volume Snapshot CRDs and the common snapshot controller must be installed in your Kubernetes cluster if you want to use the Snapshot feature\nSetting up the Synology user account\n\nThe synology-csi controller interacts with your NAS in two different ways: via the API and via the iSCSI protocol. Actions such as creating a new iSCSI target or deleting an old one are accomplished via the Synology API, and require administrator access. On the other hand, mounting the disk to a pod and reading from / writing to it will utilize iSCSI. Because you can only authenticate with one account per DSM configured, that account needs to have admin privileges. In order to minimize access in the case of these credentials being compromised, you should configure the account with the lease possible amount of access – explicitly specify “No Access” on all volumes when configuring the user permissions.\n\nSetting up the Synology CSI\n\nNote: this guide is paraphrased from the Synology CSI readme. Please consult the readme for more in-depth instructions and explanations.\n\nClone the git repository.\n\ngit clone https://github.com/zebernst/synology-csi-talos.git\n\n\nWhile Synology provides some automated scripts to deploy the CSI driver, they can be finicky especially when making changes to the source code. We will be configuring and deploying things manually in this guide.\n\nThe relevant files we will be touching are in the following locations:\n\n.\n\n├── Dockerfile\n\n├── Makefile\n\n├── config\n\n│   └── client-info-template.yml\n\n└── deploy\n\n    └── kubernetes\n\n        └── v1.20\n\n            ├── controller.yml\n\n            ├── csi-driver.yml\n\n            ├── namespace.yml\n\n            ├── node.yml\n\n            ├── snapshotter\n\n            │   ├── snapshotter.yaml\n\n            │   └── volume-snapshot-class.yml\n\n            └── storage-class.yml\n\nConfigure connection info\n\nUse config/client-info-template.yml as an example to configure the connection information for DSM. You can specify one or more storage systems on which the CSI volumes will be created. See below for an example:\n\n---\n\nclients:\n\n- host: 192.168.1.1   # ipv4 address or domain of the DSM\n\n  port: 5000          # port for connecting to the DSM\n\n  https: false        # set this true to use https. you need to specify the port to DSM HTTPS port as well\n\n  username: username  # username\n\n  password: password  # password\n\n\nCreate a Kubernetes secret using the client information config file.\n\nkubectl create secret -n synology-csi generic client-info-secret --from-file=config/client-info.yml\n\n\nNote that if you rename the secret to something other than client-info-secret, make sure you update the corresponding references in the deployment manifests as well.\n\nBuild the Talos-compatible image\n\nModify the Makefile so that the image is built and tagged under your GitHub Container Registry username:\n\nREGISTRY_NAME=ghcr.io/<username>\n\n\nWhen you run make docker-build or make docker-build-multiarch, it will push the resulting image to ghcr.io/<username>/synology-csi:v1.1.0. Ensure that you find and change any reference to synology/synology-csi:v1.1.0 to point to your newly-pushed image within the deployment manifests.\n\nConfigure the CSI driver\n\nBy default, the deployment manifests include one storage class and one volume snapshot class. See below for examples:\n\n---\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\n  name: syno-storage\n\nprovisioner: csi.san.synology.com\n\nparameters:\n\n  fsType: 'ext4'\n\n  dsm: '192.168.1.1'\n\n  location: '/volume1'\n\nreclaimPolicy: Retain\n\nallowVolumeExpansion: true\n\n---\n\napiVersion: snapshot.storage.k8s.io/v1\n\nkind: VolumeSnapshotClass\n\nmetadata:\n\n  name: syno-snapshot\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\ndriver: csi.san.synology.com\n\ndeletionPolicy: Delete\n\nparameters:\n\n  description: 'Kubernetes CSI'\n\n\nIt can be useful to configure multiple different StorageClasses. For example, a popular strategy is to create two nearly identical StorageClasses, with one configured with reclaimPolicy: Retain and the other with reclaimPolicy: Delete. Alternately, a workload may require a specific filesystem, such as ext4. If a Synology NAS is going to be the most common way to configure storage on your cluster, it can be convenient to add the storageclass.kubernetes.io/is-default-class: \"true\" annotation to one of your StorageClasses.\n\nThe following table details the configurable parameters for the Synology StorageClass.\n\nName\tType\tDescription\tDefault\tSupported protocols\ndsm\tstring\tThe IPv4 address of your DSM, which must be included in the client-info.yml for the CSI driver to log in to DSM\t-\tiSCSI, SMB\nlocation\tstring\tThe location (/volume1, /volume2, …) on DSM where the LUN for PersistentVolume will be created\t-\tiSCSI, SMB\nfsType\tstring\tThe formatting file system of the PersistentVolumes when you mount them on the pods. This parameter only works with iSCSI. For SMB, the fsType is always ‘cifs‘.\text4\tiSCSI\nprotocol\tstring\tThe backing storage protocol. Enter ‘iscsi’ to create LUNs or ‘smb‘ to create shared folders on DSM.\tiscsi\tiSCSI, SMB\ncsi.storage.k8s.io/node-stage-secret-name\tstring\tThe name of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\ncsi.storage.k8s.io/node-stage-secret-namespace\tstring\tThe namespace of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\n\nThe VolumeSnapshotClass can be similarly configured with the following parameters:\n\nName\tType\tDescription\tDefault\tSupported protocols\ndescription\tstring\tThe description of the snapshot on DSM\t-\tiSCSI\nis_locked\tstring\tWhether you want to lock the snapshot on DSM\tfalse\tiSCSI, SMB\nApply YAML manifests\n\nOnce you have created the desired StorageClass(es) and VolumeSnapshotClass(es), the final step is to apply the Kubernetes manifests against the cluster. The easiest way to apply them all at once is to create a kustomization.yaml file in the same directory as the manifests and use Kustomize to apply:\n\nkubectl apply -k path/to/manifest/directory\n\n\nAlternately, you can apply each manifest one-by-one:\n\nkubectl apply -f <file>\n\nRun performance tests\n\nIn order to test the provisioning, mounting, and performance of using a Synology NAS as Kubernetes persistent storage, use the following command:\n\nkubectl apply -f speedtest.yaml\n\n\nContent of speedtest.yaml (source)\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: test-claim\n\nspec:\n\n#  storageClassName: syno-storage\n\n  accessModes:\n\n  - ReadWriteMany\n\n  resources:\n\n    requests:\n\n      storage: 5G\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: read\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: read\n\n      labels:\n\n        app: speedtest\n\n        job: read\n\n    spec:\n\n      containers:\n\n      - name: read\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/mnt/pv/test.img\",\"of=/dev/null\",\"bs=8k\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: write\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: write\n\n      labels:\n\n        app: speedtest\n\n        job: write\n\n    spec:\n\n      containers:\n\n      - name: write\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/dev/zero\",\"of=/mnt/pv/test.img\",\"bs=1G\",\"count=1\",\"oflag=dsync\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n\nIf these two jobs complete successfully, use the following commands to get the results of the speed tests:\n\n# Pod logs for read test:\n\nkubectl logs -l app=speedtest,job=read\n\n\n\n# Pod logs for write test:\n\nkubectl logs -l app=speedtest,job=write\n\n\nWhen you’re satisfied with the results of the test, delete the artifacts created from the speedtest:\n\nkubectl delete -f speedtest.yaml\n\n3.1.4 - KubePrism\nEnabling in-cluster highly-available controlplane endpoint.\n\nKubernetes pods running in CNI mode can use the kubernetes.default.svc service endpoint to access the Kubernetes API server, while pods running in host networking mode can only use the external cluster endpoint to access the Kubernetes API server.\n\nKubernetes controlplane components run in host networking mode, and it is critical for them to be able to access the Kubernetes API server, same as CNI components (when CNI requires access to Kubernetes API).\n\nThe external cluster endpoint might be unavailable due to misconfiguration or network issues, or it might have higher latency than the internal endpoint. A failure to access the Kubernetes API server might cause a series of issues in the cluster: pods are not scheduled, service IPs stop working, etc.\n\nKubePrism feature solves this problem by enabling in-cluster highly-available controlplane endpoint on every node in the cluster.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nEnabling KubePrism\n\nAs of Talos 1.6, KubePrism is enabled by default with port 7445.\n\nNote: the port specified should be available on every node in the cluster.\n\nHow it works\n\nTalos spins up a TCP loadbalancer on every machine on the localhost on the specified port which automatically picks up one of the endpoints:\n\nthe external cluster endpoint as specified in the machine configuration\nfor controlplane machines: https://localhost:<api-server-local-port> (http://localhost:6443 in the default configuration)\nhttps://<controlplane-address>:<api-server-port> for every controlplane machine (based on the information from Cluster Discovery)\n\nKubePrism automatically filters out unhealthy (or unreachable) endpoints, and prefers lower-latency endpoints over higher-latency endpoints.\n\nTalos automatically reconfigures kubelet, kube-scheduler and kube-controller-manager to use the KubePrism endpoint. The kube-proxy manifest is also reconfigured to use the KubePrism endpoint by default, but when enabling KubePrism for a running cluster the manifest should be updated with talosctl upgrade-k8s command.\n\nWhen using CNI components that require access to the Kubernetes API server, the KubePrism endpoint should be passed to the CNI configuration (e.g. Cilium, Calico CNIs).\n\nNotes\n\nAs the list of endpoints for KubePrism includes the external cluster endpoint, KubePrism in the worst case scenario will behave the same as the external cluster endpoint. For controlplane nodes, the KubePrism should pick up the localhost endpoint of the kube-apiserver, minimizing the latency. Worker nodes might use direct address of the controlplane endpoint if the latency is lower than the latency of the external cluster endpoint.\n\nKubePrism listen endpoint is bound to localhost address, so it can’t be used outside the cluster.\n\n3.1.5 - Local Storage\nUsing local storage for Kubernetes workloads.\n\nUsing local storage for Kubernetes workloads implies that the pod will be bound to the node where the local storage is available. Local storage is not replicated, so in case of a machine failure contents of the local storage will be lost.\n\nNote: when using EPHEMERAL Talos partition (/var), make sure to use --preserve set while performing upgrades, otherwise you risk losing data.\n\nhostPath mounts\n\nThe simplest way to use local storage is to use hostPath mounts. When using hostPath mounts, make sure the root directory of the mount is mounted into the kubelet container:\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/mnt\n\n        type: bind\n\n        source: /var/mnt\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nBoth EPHEMERAL partition and user disks can be used for hostPath mounts.\n\nLocal Path Provisioner\n\nLocal Path Provisioner can be used to dynamically provision local storage. Make sure to update its configuration to use a path under /var, e.g. /var/local-path-provisioner as the root path for the local storage. (In Talos Linux default local path provisioner path /opt/local-path-provisioner is read-only).\n\nFor example, Local Path Provisioner can be installed using kustomize with the following configuration:\n\n# kustomization.yaml\n\napiVersion: kustomize.config.k8s.io/v1beta1\n\nkind: Kustomization\n\nresources:\n\n- github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26\n\npatches:\n\n- patch: |-\n\n    kind: ConfigMap\n\n    apiVersion: v1\n\n    metadata:\n\n      name: local-path-config\n\n      namespace: local-path-storage\n\n    data:\n\n      config.json: |-\n\n        {\n\n                \"nodePathMap\":[\n\n                {\n\n                        \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n\n                        \"paths\":[\"/var/local-path-provisioner\"]\n\n                }\n\n                ]\n\n        }    \n\n- patch: |-\n\n    apiVersion: storage.k8s.io/v1\n\n    kind: StorageClass\n\n    metadata:\n\n      name: local-path\n\n      annotations:\n\n        storageclass.kubernetes.io/is-default-class: \"true\"    \n\n- patch: |-\n\n    apiVersion: v1\n\n    kind: Namespace\n\n    metadata:\n\n      name: local-path-storage\n\n      labels:\n\n        pod-security.kubernetes.io/enforce: privileged    \n\n\nPut kustomization.yaml into a new directory, and run kustomize build | kubectl apply -f - to install Local Path Provisioner to a Talos Linux cluster. There are three patches applied:\n\nchange default /opt/local-path-provisioner path to /var/local-path-provisioner\nmake local-path storage class the default storage class (optional)\nlabel the local-path-storage namespace as privileged to allow privileged pods to be scheduled there\n3.1.6 - Pod Security\nEnabling Pod Security Admission plugin to configure Pod Security Standards.\n\nKubernetes deprecated Pod Security Policy as of v1.21, and it was removed in v1.25.\n\nPod Security Policy was replaced with Pod Security Admission, which is enabled by default starting with Kubernetes v1.23.\n\nTalos Linux by default enables and configures Pod Security Admission plugin to enforce Pod Security Standards with the baseline profile as the default enforced with the exception of kube-system namespace which enforces privileged profile.\n\nSome applications (e.g. Prometheus node exporter or storage solutions) require more relaxed Pod Security Standards, which can be configured by either updating the Pod Security Admission plugin configuration, or by using the pod-security.kubernetes.io/enforce label on the namespace level:\n\nkubectl label namespace NAMESPACE-NAME pod-security.kubernetes.io/enforce=privileged\n\nConfiguration\n\nTalos provides default Pod Security Admission in the machine configuration:\n\napiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\nkind: PodSecurityConfiguration\n\ndefaults:\n\n    enforce: \"baseline\"\n\n    enforce-version: \"latest\"\n\n    audit: \"restricted\"\n\n    audit-version: \"latest\"\n\n    warn: \"restricted\"\n\n    warn-version: \"latest\"\n\nexemptions:\n\n    usernames: []\n\n    runtimeClasses: []\n\n    namespaces: [kube-system]\n\n\nThis is a cluster-wide configuration for the Pod Security Admission plugin:\n\nby default baseline Pod Security Standard profile is enforced\nmore strict restricted profile is not enforced, but API server warns about found issues\n\nThis default policy can be modified by updating the generated machine configuration before the cluster is created or on the fly by using the talosctl CLI utility.\n\nVerify current admission plugin configuration with:\n\n$ talosctl get admissioncontrolconfigs.kubernetes.talos.dev admission-control -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: AdmissionControlConfigs.kubernetes.talos.dev\n\n    id: admission-control\n\n    version: 1\n\n    owner: config.K8sControlPlaneController\n\n    phase: running\n\n    created: 2022-02-22T20:28:21Z\n\n    updated: 2022-02-22T20:28:21Z\n\nspec:\n\n    config:\n\n        - name: PodSecurity\n\n          configuration:\n\n            apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n            defaults:\n\n                audit: restricted\n\n                audit-version: latest\n\n                enforce: baseline\n\n                enforce-version: latest\n\n                warn: restricted\n\n                warn-version: latest\n\n            exemptions:\n\n                namespaces:\n\n                    - kube-system\n\n                runtimeClasses: []\n\n                usernames: []\n\n            kind: PodSecurityConfiguration\n\nUsage\n\nCreate a deployment that satisfies the baseline policy but gives warnings on restricted policy:\n\n$ kubectl create deployment nginx --image=nginx\n\nWarning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndeployment.apps/nginx created\n\n$ kubectl get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nnginx-85b98978db-j68l8   1/1     Running   0          2m3s\n\n\nCreate a daemonset which fails to meet requirements of the baseline policy:\n\napiVersion: apps/v1\n\nkind: DaemonSet\n\nmetadata:\n\n  labels:\n\n    app: debug-container\n\n  name: debug-container\n\n  namespace: default\n\nspec:\n\n  revisionHistoryLimit: 10\n\n  selector:\n\n    matchLabels:\n\n      app: debug-container\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: debug-container\n\n    spec:\n\n      containers:\n\n      - args:\n\n        - \"360000\"\n\n        command:\n\n        - /bin/sleep\n\n        image: ubuntu:latest\n\n        imagePullPolicy: IfNotPresent\n\n        name: debug-container\n\n        resources: {}\n\n        securityContext:\n\n          privileged: true\n\n        terminationMessagePath: /dev/termination-log\n\n        terminationMessagePolicy: File\n\n      dnsPolicy: ClusterFirstWithHostNet\n\n      hostIPC: true\n\n      hostPID: true\n\n      hostNetwork: true\n\n      restartPolicy: Always\n\n      schedulerName: default-scheduler\n\n      securityContext: {}\n\n      terminationGracePeriodSeconds: 30\n\n  updateStrategy:\n\n    rollingUpdate:\n\n      maxSurge: 0\n\n      maxUnavailable: 1\n\n    type: RollingUpdate\n\n$ kubectl apply -f debug.yaml\n\nWarning: would violate PodSecurity \"restricted:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container \"debug-container\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"debug-container\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"debug-container\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"debug-container\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndaemonset.apps/debug-container created\n\n\nDaemonset debug-container gets created, but no pods are scheduled:\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   0         0         0       0            0           <none>          34s\n\n\nPod Security Admission plugin errors are in the daemonset events:\n\n$ kubectl describe ds debug-container\n\n...\n\n  Warning  FailedCreate  92s                daemonset-controller  Error creating: pods \"debug-container-kwzdj\" is forbidden: violates PodSecurity \"baseline:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true)\n\n\nPod Security Admission configuration can also be overridden on a namespace level:\n\n$ kubectl label ns default pod-security.kubernetes.io/enforce=privileged\n\nnamespace/default labeled\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   2         2         0       2            0           <none>          4s\n\n\nAs enforce policy was updated to the privileged for the default namespace, debug-container is now successfully running.\n\n3.1.7 - Replicated Local Storage\nUsing local storage with OpenEBS Jiva\n\nIf you want to use replicated storage leveraging disk space from a local disk with Talos Linux installed, OpenEBS Jiva is a great option. This requires installing the iscsi-tools system extension.\n\nSince OpenEBS Jiva is a replicated storage, it’s recommended to have at least three nodes where sufficient local disk space is available. The documentation will follow installing OpenEBS Jiva via the offical Helm chart. Since Talos is different from standard Operating Systems, the OpenEBS components need a little tweaking after the Helm installation. Refer to the OpenEBS Jiva documentation if you need further customization.\n\nNB: Also note that the Talos nodes need to be upgraded with --preserve set while running OpenEBS Jiva, otherwise you risk losing data. Even though it’s possible to recover data from other replicas if the node is wiped during an upgrade, this can require extra operational knowledge to recover, so it’s highly recommended to use --preserve to avoid data loss.\n\nPreparing the nodes\n\nCreate the boot assets which includes the iscsi-tools system extensions (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nCreate a machine config patch with the contents below and save as patch.yaml\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/openebs/local\n\n        type: bind\n\n        source: /var/openebs/local\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThe extension status can be verified by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get extensions\n\n\nAn output similar to below can be observed:\n\nNODE            NAMESPACE   TYPE              ID                                          VERSION   NAME          VERSION\n\n192.168.20.61   runtime     ExtensionStatus   000.ghcr.io-siderolabs-iscsi-tools-v0.1.1   1         iscsi-tools   v0.1.1\n\n\nThe service status can be checked by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> services\n\n\nYou should see that the ext-tgtd and the ext-iscsid services are running.\n\nNODE            SERVICE      STATE     HEALTH   LAST CHANGE     LAST EVENT\n\n192.168.20.51   apid         Running   OK       64h57m15s ago   Health check successful\n\n192.168.20.51   containerd   Running   OK       64h57m23s ago   Health check successful\n\n192.168.20.51   cri          Running   OK       64h57m20s ago   Health check successful\n\n192.168.20.51   etcd         Running   OK       64h55m29s ago   Health check successful\n\n192.168.20.51   ext-iscsid   Running   ?        64h57m19s ago   Started task ext-iscsid (PID 4040) for container ext-iscsid\n\n192.168.20.51   ext-tgtd     Running   ?        64h57m19s ago   Started task ext-tgtd (PID 3999) for container ext-tgtd\n\n192.168.20.51   kubelet      Running   OK       38h14m10s ago   Health check successful\n\n192.168.20.51   machined     Running   ?        64h57m29s ago   Service started as goroutine\n\n192.168.20.51   trustd       Running   OK       64h57m19s ago   Health check successful\n\n192.168.20.51   udevd        Running   OK       64h57m21s ago   Health check successful\n\nInstall OpenEBS Jiva\nhelm repo add openebs-jiva https://openebs.github.io/jiva-operator\n\nhelm repo update\n\nhelm upgrade --install --create-namespace --namespace openebs --version 3.2.0 openebs-jiva openebs-jiva/jiva\n\n\nThis will create a storage class named openebs-jiva-csi-default which can be used for workloads. The storage class named openebs-hostpath is used by jiva to create persistent volumes backed by local storage and then used for replicated storage by the jiva controller.\n\nPatching the Namespace\n\nwhen using the default Pod Security Admissions created by Talos you need the following labels on your namespace:\n\n    pod-security.kubernetes.io/audit: privileged\n\n    pod-security.kubernetes.io/enforce: privileged\n\n    pod-security.kubernetes.io/warn: privileged\n\n\nor via kubectl:\n\nkubectl label ns openebs pod-security.kubernetes.io/audit=privileged pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/warn=privileged\n\nNumber of Replicas\n\nBy Default Jiva uses 3 replicas if your cluster consists of lesser nodes consider setting defaultPolicy.replicas to the number of nodes in your cluster e.g. 2.\n\nPatching the jiva installation\n\nSince Jiva assumes iscisd to be running natively on the host and not as a Talos extension service, we need to modify the CSI node daemonset to enable it to find the PID of the iscsid service. The default config map used by Jiva also needs to be modified so that it can execute iscsiadm commands inside the PID namespace of the iscsid service.\n\nStart by creating a configmap definition named config.yaml as below:\n\napiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\n  labels:\n\n    app.kubernetes.io/managed-by: pulumi\n\n  name: openebs-jiva-csi-iscsiadm\n\n  namespace: openebs\n\ndata:\n\n  iscsiadm: |\n\n    #!/bin/sh\n\n    iscsid_pid=$(pgrep iscsid)\n\n\n\n    nsenter --mount=\"/proc/${iscsid_pid}/ns/mnt\" --net=\"/proc/${iscsid_pid}/ns/net\" -- /usr/local/sbin/iscsiadm \"$@\"    \n\n\nReplace the existing config map with the above config map by running the following command:\n\nkubectl --namespace openebs apply --filename config.yaml\n\n\nNow we need to update the jiva CSI daemonset to run with hostPID: true so it can find the PID of the iscsid service, by running the following command:\n\nkubectl --namespace openebs patch daemonset openebs-jiva-csi-node --type=json --patch '[{\"op\": \"add\", \"path\": \"/spec/template/spec/hostPID\", \"value\": true}]'\n\nTesting a simple workload\n\nIn order to test the Jiva installation, let’s first create a PVC referencing the openebs-jiva-csi-default storage class:\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: example-jiva-csi-pvc\n\nspec:\n\n  storageClassName: openebs-jiva-csi-default\n\n  accessModes:\n\n    - ReadWriteOnce\n\n  resources:\n\n    requests:\n\n      storage: 4Gi\n\n\nand then create a deployment using the above PVC:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: fio\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      name: fio\n\n  replicas: 1\n\n  strategy:\n\n    type: Recreate\n\n    rollingUpdate: null\n\n  template:\n\n    metadata:\n\n      labels:\n\n        name: fio\n\n    spec:\n\n      containers:\n\n      - name: perfrunner\n\n        image: openebs/tests-fio\n\n        command: [\"/bin/bash\"]\n\n        args: [\"-c\", \"while true ;do sleep 50; done\"]\n\n        volumeMounts:\n\n        - mountPath: /datadir\n\n          name: fio-vol\n\n      volumes:\n\n      - name: fio-vol\n\n        persistentVolumeClaim:\n\n          claimName: example-jiva-csi-pvc\n\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete deployment fio\n\nkubectl delete pvc example-jiva-csi-pvc\n\n3.1.8 - Seccomp Profiles\nUsing custom Seccomp Profiles with Kubernetes workloads.\n\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.\n\nRefer the Kubernetes Seccomp Guide for more details.\n\nIn this guide we are going to configure a custom Seccomp Profile that logs all syscalls made by the workload.\n\nPreparing the nodes\n\nCreate a machine config path with the contents below and save as patch.yaml\n\nmachine:\n\n  seccompProfiles:\n\n    - name: audit.json\n\n      value:\n\n        defaultAction: SCMP_ACT_LOG\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThis would create a seccomp profile name audit.json on the node at /var/lib/kubelet/seccomp/profiles.\n\nThe profiles can be used by Kubernetes pods by specfying the pod securityContext as below:\n\nspec:\n\n  securityContext:\n\n    seccompProfile:\n\n      type: Localhost\n\n      localhostProfile: profiles/audit.json\n\n\nNote that the localhostProfile uses the name of the profile created under profiles directory. So make sure to use path as profiles/<profile-name.json>\n\nThis can be verfied by running the below commands:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get seccompprofiles\n\n\nAn output similar to below can be observed:\n\nNODE       NAMESPACE   TYPE             ID           VERSION\n\n10.5.0.3   cri         SeccompProfile   audit.json   1\n\n\nThe content of the seccomp profile can be viewed by running the below command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> read /var/lib/kubelet/seccomp/profiles/audit.json\n\n\nAn output similar to below can be observed:\n\n{\"defaultAction\":\"SCMP_ACT_LOG\"}\n\nCreate a Kubernetes workload that uses the custom Seccomp Profile\n\nHere we’ll be using an example workload from the Kubernetes documentation.\n\nFirst open up a second terminal and run the following talosctl command so that we can view the Syscalls being logged in realtime:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> dmesg --follow --tail\n\n\nNow deploy the example workload from the Kubernetes documentation:\n\nkubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml\n\n\nOnce the pod starts running the terminal running talosctl dmesg command from above should log similar to below:\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.489473063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.490852063Z]: cni0: port 1(veth32488a86) entered disabled state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.492470063Z]: device veth32488a86 entered promiscuous mode\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503105063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503944063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): veth32488a86: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.504764063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.505423063Z]: cni0: port 1(veth32488a86) entered forwarding state\n\n10.5.0.3: kern: warning: [2022-07-28T11:49:44.873616063Z]: kauditd_printk_skb: 14 callbacks suppressed\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.873619063Z]: audit: type=1326 audit(1659008985.445:25): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.876609063Z]: audit: type=1326 audit(1659008985.445:26): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.878789063Z]: audit: type=1326 audit(1659008985.449:27): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=257 compat=0 ip=0x55ec0657bdaa code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.886693063Z]: audit: type=1326 audit(1659008985.461:28): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.888764063Z]: audit: type=1326 audit(1659008985.461:29): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.891009063Z]: audit: type=1326 audit(1659008985.461:30): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=1 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.893162063Z]: audit: type=1326 audit(1659008985.461:31): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.895365063Z]: audit: type=1326 audit(1659008985.461:32): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=39 compat=0 ip=0x55ec066eb68b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.898306063Z]: audit: type=1326 audit(1659008985.461:33): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=59 compat=0 ip=0x55ec0657be16 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.901518063Z]: audit: type=1326 audit(1659008985.473:34): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"http-echo\" exe=\"/http-echo\" sig=0 arch=c000003e syscall=158 compat=0 ip=0x455f35 code=0x7ffc0000\n\nCleanup\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete pod audit-pod\n\n3.1.9 - Storage\nSetting up storage for a Kubernetes cluster\n\nIn Kubernetes, using storage in the right way is well-facilitated by the API. However, unless you are running in a major public cloud, that API may not be hooked up to anything. This frequently sends users down a rabbit hole of researching all the various options for storage backends for their platform, for Kubernetes, and for their workloads. There are a lot of options out there, and it can be fairly bewildering.\n\nFor Talos, we try to limit the options somewhat to make the decision-making easier.\n\nPublic Cloud\n\nIf you are running on a major public cloud, use their block storage. It is easy and automatic.\n\nStorage Clusters\n\nSidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\nRedundancy, scaling capabilities, reliability, speed, maintenance load, and ease of use are all factors you must consider when managing your own storage.\n\nRunning a storage cluster can be a very good choice when managing your own storage, and there are two projects we recommend, depending on your situation.\n\nIf you need vast amounts of storage composed of more than a dozen or so disks, we recommend you use Rook to manage Ceph. Also, if you need both mount-once and mount-many capabilities, Ceph is your answer. Ceph also bundles in an S3-compatible object store. The down side of Ceph is that there are a lot of moving parts.\n\nPlease note that most people should never use mount-many semantics. NFS is pervasive because it is old and easy, not because it is a good idea. While it may seem like a convenience at first, there are all manner of locking, performance, change control, and reliability concerns inherent in any mount-many situation, so we strongly recommend you avoid this method.\n\nIf your storage needs are small enough to not need Ceph, use Mayastor.\n\nRook/Ceph\n\nCeph is the grandfather of open source storage clusters. It is big, has a lot of pieces, and will do just about anything. It scales better than almost any other system out there, open source or proprietary, being able to easily add and remove storage over time with no downtime, safely and easily. It comes bundled with RadosGW, an S3-compatible object store; CephFS, a NFS-like clustered filesystem; and RBD, a block storage system.\n\nWith the help of Rook, the vast majority of the complexity of Ceph is hidden away by a very robust operator, allowing you to control almost everything about your Ceph cluster from fairly simple Kubernetes CRDs.\n\nSo if Ceph is so great, why not use it for everything?\n\nCeph can be rather slow for small clusters. It relies heavily on CPUs and massive parallelisation to provide good cluster performance, so if you don’t have much of those dedicated to Ceph, it is not going to be well-optimised for you. Also, if your cluster is small, just running Ceph may eat up a significant amount of the resources you have available.\n\nTroubleshooting Ceph can be difficult if you do not understand its architecture. There are lots of acronyms and the documentation assumes a fair level of knowledge. There are very good tools for inspection and debugging, but this is still frequently seen as a concern.\n\nMayastor\n\nMayastor is an OpenEBS project built in Rust utilising the modern NVMEoF system. (Despite the name, Mayastor does not require you to have NVME drives.) It is fast and lean but still cluster-oriented and cloud native. Unlike most of the other OpenEBS project, it is not built on the ancient iSCSI system.\n\nUnlike Ceph, Mayastor is just a block store. It focuses on block storage and does it well. It is much less complicated to set up than Ceph, but you probably wouldn’t want to use it for more than a few dozen disks.\n\nMayastor is new, maybe too new. If you’re looking for something well-tested and battle-hardened, this is not it. However, if you’re looking for something lean, future-oriented, and simpler than Ceph, it might be a great choice.\n\nVideo Walkthrough\n\nTo see a live demo of this section, see the video below:\n\nPrep Nodes\n\nEither during initial cluster creation or on running worker nodes, several machine config values should be edited. (This information is gathered from the Mayastor documentation.) We need to set the vm.nr_hugepages sysctl and add openebs.io/engine=mayastor labels to the nodes which are meant to be storage nodes. This can be done with talosctl patch machineconfig or via config patches during talosctl gen config.\n\nSome examples are shown below: modify as needed.\n\nFirst create a config patch file named mayastor-patch.yaml with the following contents:\n\n- op: add\n\n  path: /machine/sysctls\n\n  value:\n\n    vm.nr_hugepages: \"1024\"\n\n- op: add\n\n  path: /machine/nodeLabels\n\n  value:\n\n    openebs.io/engine: mayastor\n\n\nUsing gen config\n\ntalosctl gen config my-cluster https://mycluster.local:6443 --config-patch @mayastor-patch.yaml\n\n\nPatching an existing node\n\ntalosctl patch --mode=no-reboot machineconfig -n <node ip> --patch @mayastor-patch.yaml\n\n\nNote: If you are adding/updating the vm.nr_hugepages on a node which already had the openebs.io/engine=mayastor label set, you’d need to restart kubelet so that it picks up the new value, by issuing the following command\n\ntalosctl -n <node ip> service kubelet restart\n\nDeploy Mayastor\n\nContinue setting up Mayastor using the official documentation.\n\nPiraeus / LINSTOR\nPiraeus-Operator\nLINSTOR\nDRBD Extension\nInstall Piraeus Operator V2\n\nThere is already a how-to for Talos: Link\n\nCreate first storage pool and PVC\n\nBefore proceeding, install linstor plugin for kubectl: https://github.com/piraeusdatastore/kubectl-linstor\n\nOr use krew: kubectl krew install linstor\n\n# Create device pool on a blank (no partitation table!) disk on node01\n\nkubectl linstor physical-storage create-device-pool --pool-name nvme_lvm_pool LVM node01 /dev/nvme0n1 --storage-pool nvme_pool\n\n\npiraeus-sc.yml\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  name: simple-nvme\n\nparameters:\n\n  csi.storage.k8s.io/fstype: xfs\n\n  linstor.csi.linbit.com/autoPlace: \"3\"\n\n  linstor.csi.linbit.com/storagePool: nvme_pool\n\nprovisioner: linstor.csi.linbit.com\n\nvolumeBindingMode: WaitForFirstConsumer\n\n# Create storage class\n\nkubectl apply -f piraeus-sc.yml\n\nNFS\n\nNFS is an old pack animal long past its prime. NFS is slow, has all kinds of bottlenecks involving contention, distributed locking, single points of service, and more. However, it is supported by a wide variety of systems. You don’t want to use it unless you have to, but unfortunately, that “have to” is too frequent.\n\nThe NFS client is part of the kubelet image maintained by the Talos team. This means that the version installed in your running kubelet is the version of NFS supported by Talos. You can reduce some of the contention problems by parceling Persistent Volumes from separate underlying directories.\n\nObject storage\n\nCeph comes with an S3-compatible object store, but there are other options, as well. These can often be built on top of other storage backends. For instance, you may have your block storage running with Mayastor but assign a Pod a large Persistent Volume to serve your object store.\n\nOne of the most popular open source add-on object stores is MinIO.\n\nOthers (iSCSI)\n\nThe most common remaining systems involve iSCSI in one form or another. These include the original OpenEBS, Rancher’s Longhorn, and many proprietary systems. iSCSI in Linux is facilitated by open-iscsi. This system was designed long before containers caught on, and it is not well suited to the task, especially when coupled with a read-only host operating system.\n\niSCSI support in Talos is now supported via the iscsi-tools system extension installed. The extension enables compatibility with OpenEBS Jiva - refer to the local storage installation guide for more information.\n\n3.2 - Network\nManaging the Kubernetes cluster networking\n3.2.1 - Deploying Cilium CNI\nIn this guide you will learn how to set up Cilium CNI on Talos.\n\nCilium can be installed either via the cilium cli or using helm.\n\nThis documentation will outline installing Cilium CNI v1.14.0 on Talos in six different ways. Adhering to Talos principles we’ll deploy Cilium with IPAM mode set to Kubernetes, and using the cgroupv2 and bpffs mount that talos already provides. As Talos does not allow loading kernel modules by Kubernetes workloads, SYS_MODULE capability needs to be dropped from the Cilium default set of values, this override can be seen in the helm/cilium cli install commands. Each method can either install Cilium using kube proxy (default) or without: Kubernetes Without kube-proxy\n\nIn this guide we assume that KubePrism is enabled and configured to use the port 7445.\n\nMachine config preparation\n\nWhen generating the machine config for a node set the CNI to none. For example using a config patch:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nOr if you want to deploy Cilium without kube-proxy, you also need to disable kube proxy:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\nInstallation using Cilium CLI\n\nNote: It is recommended to template the cilium manifest using helm and use it as part of Talos machine config, but if you want to install Cilium using the Cilium CLI, you can follow the steps below.\n\nInstall the Cilium CLI following the steps here.\n\nWith kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=disabled \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup\n\nWithout kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=true \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --helm-set=k8sServiceHost=localhost \\\n\n    --helm-set=k8sServicePort=7445\n\nInstallation using Helm\n\nRefer to Installing with Helm for more information.\n\nFirst we’ll need to add the helm repo for Cilium.\n\nhelm repo add cilium https://helm.cilium.io/\n\nhelm repo update\n\nMethod 1: Helm install\n\nAfter applying the machine config and bootstrapping Talos will appear to hang on phase 18/19 with the message: retrying error: node not ready. This happens because nodes in Kubernetes are only marked as ready once the CNI is up. As there is no CNI defined, the boot process is pending and will reboot the node to retry after 10 minutes, this is expected behavior.\n\nDuring this window you can install Cilium manually by running the following:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup\n\n\nOr if you want to deploy Cilium without kube-proxy, also set some extra paramaters:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445\n\n\nAfter Cilium is installed the boot process should continue and complete successfully.\n\nMethod 2: Helm manifests install\n\nInstead of directly installing Cilium you can instead first generate the manifest and then apply it:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\n\nWithout kube-proxy:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445 > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\nMethod 3: Helm manifests hosted install\n\nAfter generating cilium.yaml using helm template, instead of applying this manifest directly during the Talos boot window (before the reboot timeout). You can also host this file somewhere and patch the machine config to apply this manifest automatically during bootstrap. To do this patch your machine configuration to include this config instead of the above:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: custom\n\n      urls:\n\n        - https://server.yourdomain.tld/some/path/cilium.yaml\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nHowever, beware of the fact that the helm generated Cilium manifest contains sensitive key material. As such you should definitely not host this somewhere publicly accessible.\n\nMethod 4: Helm manifests inline install\n\nA more secure option would be to include the helm template output manifest inside the machine configuration. The machine config should be generated with CNI set to none\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nif deploying Cilium with kube-proxy disabled, you can also include the following:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\nmachine:\n\n  features:\n\n    kubePrism:\n\n      enabled: true\n\n      port: 7445\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nTo do so patch this into your machine configuration:\n\ninlineManifests:\n\n    - name: cilium\n\n      contents: |\n\n        --\n\n        # Source: cilium/templates/cilium-agent/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        metadata:\n\n          name: \"cilium\"\n\n          namespace: kube-system\n\n        ---\n\n        # Source: cilium/templates/cilium-operator/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        -> Your cilium.yaml file will be pretty long....        \n\n\nThis will install the Cilium manifests at just the right time during bootstrap.\n\nBeware though:\n\nChanging the namespace when templating with Helm does not generate a manifest containing the yaml to create that namespace. As the inline manifest is processed from top to bottom make sure to manually put the namespace yaml at the start of the inline manifest.\nOnly add the Cilium inline manifest to the control plane nodes machine configuration.\nMake sure all control plane nodes have an identical configuration.\nIf you delete any of the generated resources they will be restored whenever a control plane node reboots.\nAs a safety messure Talos only creates missing resources from inline manifests, it never deletes or updates anything.\nIf you need to update a manifest make sure to first edit all control plane machine configurations and then run talosctl upgrade-k8s as it will take care of updating inline manifests.\nKnown issues\nThere are some gotchas when using Talos and Cilium on the Google cloud platform when using internal load balancers. For more details: GCP ILB support / support scope local routes to be configured\nOther things to know\nTalos has full kernel module support for eBPF, See:\nCilium System Requirements\nTalos Kernel Config AMD64\nTalos Kernel Config ARM64\n3.3 - Upgrading Kubernetes\nGuide on how to upgrade the Kubernetes cluster from Talos Linux.\n\nThis guide covers upgrading Kubernetes on Talos Linux clusters.\n\nFor a list of Kubernetes versions compatible with each Talos release, see the Support Matrix.\n\nFor upgrading the Talos Linux operating system, see Upgrading Talos\n\nVideo Walkthrough\n\nTo see a demo of this process, watch this video:\n\nAutomated Kubernetes Upgrade\n\nThe recommended method to upgrade Kubernetes is to use the talosctl upgrade-k8s command. This will automatically update the components needed to upgrade Kubernetes safely. Upgrading Kubernetes is non-disruptive to the cluster workloads.\n\nTo trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to ugprade to, such as:\n\ntalosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nNote that the --nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded.\n\nTo check what will be upgraded you can run talosctl upgrade-k8s with the --dry-run flag:\n\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0 --dry-run\n\nWARNING: found resources which are going to be deprecated/migrated in the version 1.29.0\n\nRESOURCE                                                               COUNT\n\nvalidatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io   4\n\nmutatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io     3\n\ncustomresourcedefinitions.v1beta1.apiextensions.k8s.io                 25\n\napiservices.v1beta1.apiregistration.k8s.io                             54\n\nleases.v1beta1.coordination.k8s.io                                     4\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.4\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\nupdating \"kube-controller-manager\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-controller-manager: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n\n\n<snip>\n\n\n\nupdating manifests\n\n > apply manifest Secret bootstrap-token-3lb63t\n\n > apply skipped in dry run\n\n > apply manifest ClusterRoleBinding system-bootstrap-approve-node-client-csr\n\n > apply skipped in dry run\n\n<snip>\n\n\nTo upgrade Kubernetes from v1.28.3 to v1.29.0 run:\n\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > \"172.20.0.2\": machine configuration patched\n\n > \"172.20.0.2\": waiting for API server state pod update\n\n < \"172.20.0.2\": successfully updated\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n<snip>\n\n\nThis command runs in several phases:\n\nImages for new Kubernetes components are pre-pulled to the nodes to minimize downtime and test for image availability.\nEvery control plane node machine configuration is patched with the new image version for each control plane component. Talos renders new static pod definitions on the configuration update which is picked up by the kubelet. The command waits for the change to propagate to the API server state.\nThe command updates the kube-proxy daemonset with the new image version.\nOn every node in the cluster, the kubelet version is updated. The command then waits for the kubelet service to be restarted and become healthy. The update is verified by checking the Node resource state.\nKubernetes bootstrap manifests are re-applied to the cluster. Updated bootstrap manifests might come with a new Talos version (e.g. CoreDNS version update), or might be the result of machine configuration change.\n\nNote: The upgrade-k8s command never deletes any resources from the cluster: they should be deleted manually.\n\nIf the command fails for any reason, it can be safely restarted to continue the upgrade process from the moment of the failure.\n\nManual Kubernetes Upgrade\n\nKubernetes can be upgraded manually by following the steps outlined below. They are equivalent to the steps performed by the talosctl upgrade-k8s command.\n\nKubeconfig\n\nIn order to edit the control plane, you need a working kubectl config. If you don’t already have one, you can get one by running:\n\ntalosctl --nodes <controlplane node> kubeconfig\n\nAPI Server\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need to be adjusted if current machine configuration is missing .cluster.apiServer.image key.\n\nAlso the machine configuration can be edited manually with talosctl -n <IP> edit mc --mode=no-reboot.\n\nCapture the new version of kube-apiserver config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-apiserver -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-apiserver\n\n    version: 5\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-apiserver:v1.29.0\n\n    cloudProvider: \"\"\n\n    controlPlaneEndpoint: https://172.20.0.1:6443\n\n    etcdServers:\n\n        - https://127.0.0.1:2379\n\n    localPort: 6443\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, the new version is 5. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n5\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-apiserver-talos-default-controlplane-1   1/1     Running   0          16m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nController Manager\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/controllerManager/image\", \"value\": \"registry.k8s.io/kube-controller-manager:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need be adjusted if current machine configuration is missing .cluster.controllerManager.image key.\n\nCapture new version of kube-controller-manager config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-controller-manager -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-controller-manager\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-controller-manager:v1.29.0\n\n    cloudProvider: \"\"\n\n    podCIDR: 10.244.0.0/16\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\nkube-controller-manager-talos-default-controlplane-1   1/1     Running   0          35m\n\n\nRepeat this process for every control plane node, verifying that state propagated successfully between each node update.\n\nScheduler\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/scheduler/image\", \"value\": \"registry.k8s.io/kube-scheduler:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nJSON patch might need be adjusted if current machine configuration is missing .cluster.scheduler.image key.\n\nCapture new version of kube-scheduler config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-scheduler -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-scheduler\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-scheduler:v1.29.0\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-scheduler-talos-default-controlplane-1   1/1     Running   0          39m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nProxy\n\nIn the proxy’s DaemonSet, change:\n\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n\nto:\n\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n        - key: node-role.kubernetes.io/control-plane\n\n          operator: Exists\n\n          effect: NoSchedule\n\n\nTo edit the DaemonSet, run:\n\nkubectl edit daemonsets -n kube-system kube-proxy\n\nBootstrap Manifests\n\nBootstrap manifests can be retrieved in a format which works for kubectl with the following command:\n\ntalosctl -n <controlplane IP> get manifests -o yaml | yq eval-all '.spec | .[] | splitDoc' - > manifests.yaml\n\n\nDiff the manifests with the cluster:\n\nkubectl diff -f manifests.yaml\n\n\nApply the manifests:\n\nkubectl apply -f manifests.yaml\n\n\nNote: if some bootstrap resources were removed, they have to be removed from the cluster manually.\n\nkubelet\n\nFor every node, patch machine configuration with new kubelet version, wait for the kubelet to restart with new version:\n\n$ talosctl -n <IP> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nOnce kubelet restarts with the new configuration, confirm upgrade with kubectl get nodes <name>:\n\n$ kubectl get nodes talos-default-controlplane-1\n\nNAME                           STATUS   ROLES                  AGE    VERSION\n\ntalos-default-controlplane-1   Ready    control-plane          123m   v1.29.0\n\n4 - Advanced Guides\n4.1 - Advanced Networking\nHow to configure advanced networking options on Talos Linux.\nStatic Addressing\n\nStatic addressing is comprised of specifying addresses, routes ( remember to add your default gateway ), and interface. Most likely you’ll also want to define the nameservers so you have properly functioning DNS.\n\nmachine:\n\n  network:\n\n    hostname: talos\n\n    nameservers:\n\n      - 10.0.0.1\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.201/8\n\n        mtu: 8765\n\n        routes:\n\n          - network: 0.0.0.0/0\n\n            gateway: 10.0.0.1\n\n      - interface: eth1\n\n        ignore: true\n\n  time:\n\n    servers:\n\n      - time.cloudflare.com\n\nAdditional Addresses for an Interface\n\nIn some environments you may need to set additional addresses on an interface. In the following example, we set two additional addresses on the loopback interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: lo\n\n        addresses:\n\n          - 192.168.0.21/24\n\n          - 10.2.2.2/24\n\nBonding\n\nThe following example shows how to create a bonded interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        dhcp: true\n\n        bond:\n\n          mode: 802.3ad\n\n          lacpRate: fast\n\n          xmitHashPolicy: layer3+4\n\n          miimon: 100\n\n          updelay: 200\n\n          downdelay: 200\n\n          interfaces:\n\n            - eth0\n\n            - eth1\n\nSetting Up a Bridge\n\nThe following example shows how to set up a bridge between two interfaces with an assigned static address.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: br0\n\n        addresses:\n\n          - 192.168.0.42/24\n\n        bridge:\n\n          stp:\n\n            enabled: true\n\n          interfaces:\n\n              - eth0\n\n              - eth1\n\nVLANs\n\nTo setup vlans on a specific device use an array of VLANs to add. The master device may be configured without addressing by setting dhcp to false.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        vlans:\n\n          - vlanId: 100\n\n            addresses:\n\n              - \"192.168.2.10/28\"\n\n            routes:\n\n              - network: 0.0.0.0/0\n\n                gateway: 192.168.2.1\n\n4.2 - Air-gapped Environments\nSetting up Talos Linux to work in environments with no internet access.\n\nIn this guide we will create a Talos cluster running in an air-gapped environment with all the required images being pulled from an internal registry. We will use the QEMU provisioner available in talosctl to create a local cluster, but the same approach could be used to deploy Talos in bigger air-gapped networks.\n\nRequirements\n\nThe follow are requirements for this guide:\n\nDocker 18.03 or greater\nRequirements for the Talos QEMU cluster\nIdentifying Images\n\nIn air-gapped environments, access to the public Internet is restricted, so Talos can’t pull images from public Docker registries (docker.io, ghcr.io, etc.) We need to identify the images required to install and run Talos. The same strategy can be used for images required by custom workloads running on the cluster.\n\nThe talosctl image default command provides a list of default images used by the Talos cluster (with default configuration settings). To print the list of images, run:\n\ntalosctl image default\n\n\nThis list contains images required by a default deployment of Talos. There might be additional images required for the workloads running on this cluster, and those should be added to this list.\n\nPreparing the Internal Registry\n\nAs access to the public registries is restricted, we have to run an internal Docker registry. In this guide, we will launch the registry on the same machine using Docker:\n\n$ docker run -d -p 6000:5000 --restart always --name registry-airgapped registry:2\n\n1bf09802bee1476bc463d972c686f90a64640d87dacce1ac8485585de69c91a5\n\n\nThis registry will be accepting connections on port 6000 on the host IPs. The registry is empty by default, so we have fill it with the images required by Talos.\n\nFirst, we pull all the images to our local Docker daemon:\n\n$ for image in `talosctl image default`; do docker pull $image; done\n\nv0.15.1: Pulling from coreos/flannel\n\nDigest: sha256:9a296fbb67790659adc3701e287adde3c59803b7fcefe354f1fc482840cdb3d9\n\n...\n\n\nAll images are now stored in the Docker daemon store:\n\n$ docker images\n\nREPOSITORY                               TAG                                        IMAGE ID       CREATED         SIZE\n\ngcr.io/etcd-development/etcd             v3.5.3                                     604d4f022632   6 days ago      181MB\n\nghcr.io/siderolabs/install-cni           v1.0.0-2-gc5d3ab0                          4729e54f794d   6 days ago      76MB\n\n...\n\n\nNow we need to re-tag them so that we can push them to our local registry. We are going to replace the first component of the image name (before the first slash) with our registry endpoint 127.0.0.1:6000:\n\n$ for image in `talosctl image default`; do \\\n\n    docker tag $image `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nAs the next step, we push images to the internal registry:\n\n$ for image in `talosctl image default`; do \\\n\n    docker push `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nWe can now verify that the images are pushed to the registry:\n\n$ curl http://127.0.0.1:6000/v2/_catalog\n\n{\"repositories\":[\"coredns/coredns\",\"coreos/flannel\",\"etcd-development/etcd\",\"kube-apiserver\",\"kube-controller-manager\",\"kube-proxy\",\"kube-scheduler\",\"pause\",\"siderolabs/install-cni\",\"siderolabs/installer\",\"siderolabs/kubelet\"]}\n\n\nNote: images in the registry don’t have the registry endpoint prefix anymore.\n\nLaunching Talos in an Air-gapped Environment\n\nFor Talos to use the internal registry, we use the registry mirror feature to redirect all image pull requests to the internal registry. This means that the registry endpoint (as the first component of the image reference) gets ignored, and all pull requests are sent directly to the specified endpoint.\n\nWe are going to use a QEMU-based Talos cluster for this guide, but the same approach works with Docker-based clusters as well. As QEMU-based clusters go through the Talos install process, they can be used better to model a real air-gapped environment.\n\nIdentify all registry prefixes from talosctl image default, for example:\n\ndocker.io\ngcr.io\nghcr.io\nregistry.k8s.io\n\nThe talosctl cluster create command provides conveniences for common configuration options. The only required flag for this guide is --registry-mirror <endpoint>=http://10.5.0.1:6000 which redirects every pull request to the internal registry, this flag needs to be repeated for each of the identified registry prefixes above. The endpoint being used is 10.5.0.1, as this is the default bridge interface address which will be routable from the QEMU VMs (127.0.0.1 IP will be pointing to the VM itself).\n\n$ sudo --preserve-env=HOME talosctl cluster create --provisioner=qemu --install-image=ghcr.io/siderolabs/installer:v1.6.2 \\\n\n  --registry-mirror docker.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror gcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror ghcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror registry.k8s.io=http://10.5.0.1:6000 \\\n\nvalidating CIDR and reserving IPs\n\ngenerating PKI and tokens\n\ncreating state directory in \"/home/user/.talos/clusters/talos-default\"\n\ncreating network talos-default\n\ncreating load balancer\n\ncreating dhcpd\n\ncreating master nodes\n\ncreating worker nodes\n\nwaiting for API\n\n...\n\n\nNote: --install-image should match the image which was copied into the internal registry in the previous step.\n\nYou can be verify that the cluster is air-gapped by inspecting the registry logs: docker logs -f registry-airgapped.\n\nClosing Notes\n\nRunning in an air-gapped environment might require additional configuration changes, for example using custom settings for DNS and NTP servers.\n\nWhen scaling this guide to the bare-metal environment, following Talos config snippet could be used as an equivalent of the --registry-mirror flag above:\n\nmachine:\n\n  ...\n\n  registries:\n\n      mirrors:\n\n        docker.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        gcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        ghcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        registry.k8s.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n...\n\n\nOther implementations of Docker registry can be used in place of the Docker registry image used above to run the registry. If required, auth can be configured for the internal registry (and custom TLS certificates if needed).\n\nPlease see pull-through cache guide for an example using Harbor container registry with Talos.\n\n4.3 - Building Custom Talos Images\nHow to build a custom Talos image from source.\n\nThere might be several reasons to build Talos images from source:\n\nverifying the image integrity\nbuilding an image with custom configuration\nCheckout Talos Source\ngit clone https://github.com/siderolabs/talos.git\n\n\nIf building for a specific release, checkout the corresponding tag:\n\ngit checkout v1.6.2\n\nSet up the Build Environment\n\nSee Developing Talos for details on setting up the buildkit builder.\n\nArchitectures\n\nBy default, Talos builds for linux/amd64, but you can customize that by passing PLATFORM variable to make:\n\nmake <target> PLATFORM=linux/arm64 # build for arm64 only\n\nmake <target> PLATFORM=linux/arm64,linux/amd64 # build for arm64 and amd64, container images will be multi-arch\n\nCustomizations\n\nSome of the build parameters can be customized by passing environment variables to make, e.g. GOAMD64=v1 can be used to build Talos images compatible with old AMD64 CPUs:\n\nmake <target> GOAMD64=v1\n\nBuilding Kernel and Initramfs\n\nThe most basic boot assets can be built with:\n\nmake kernel initramfs\n\n\nBuild result will be stored as _out/vmlinuz-<arch> and _out/initramfs-<arch>.xz.\n\nBuilding Container Images\n\nTalos container images should be pushed to the registry as the result of the build process.\n\nThe default settings are:\n\nIMAGE_REGISTRY is set to ghcr.io\nUSERNAME is set to the siderolabs (or value of environment variable USERNAME if it is set)\n\nThe image can be pushed to any registry you have access to, but the access credentials should be stored in ~/.docker/config.json file (e.g. with docker login).\n\nBuilding and pushing the image can be done with:\n\nmake installer PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nmake imager PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nBuilding ISO\n\nThe ISO image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nmake iso\n\n\nThe ISO image will be stored as _out/talos-<arch>.iso.\n\nIf ISO image should be built with the custom imager image, it can be specified with IMAGE_REGISTRY/USERNAME variables:\n\nmake iso IMAGE_REGISTRY=docker.io USERNAME=<username>\n\nBuilding Disk Images\n\nThe disk image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nmake image-metal\n\n\nAvailable disk images are encoded in the image-% target, e.g. make image-aws. Same as with ISO image, the custom imager image can be specified with IMAGE_REGISTRY/USERNAME variables.\n\n4.4 - Customizing the Kernel\nGuide on how to customize the kernel used by Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nBuild and push your own kernel:\n\ngit clone https://github.com/talos-systems/pkgs.git\n\ncd pkgs\n\nmake kernel-menuconfig USERNAME=_your_github_user_name_\n\n\n\ndocker login ghcr.io --username _your_github_user_name_\n\nmake kernel USERNAME=_your_github_user_name_ PUSH=true\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nFROM scratch AS customization\n\n# this is needed so that Talos copies base kernel modules info and default modules shipped with Talos\n\nCOPY --from=<custom kernel image> /lib/modules /kernel/lib/modules\n\n# this copies over the custom modules\n\nCOPY --from=<custom kernel image> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\nCOPY --from=<custom kernel image> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nTo build the image, run:\n\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t installer:kernel .\n\n\nNote: buildkit has a bug #816, to disable it use DOCKER_BUILDKIT=0\n\nNow that we have a custom installer we can build Talos for the specific platform we wish to deploy to.\n\n4.5 - Customizing the Root Filesystem\nHow to add your own content to the immutable root file system of Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nFor example, say we have an image that contains the contents of a library we wish to add to the Talos rootfs. We need to define a stage with the name customization:\n\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nNote: <dest> is the path relative to the rootfs that you wish to place the contents of <src>.\n\nTo build the image, run:\n\ndocker build --squash -t <organization>/installer:latest .\n\n\nIn the case that you need to perform some cleanup before adding additional files to the rootfs, you can specify the RM build-time variable:\n\ndocker build --squash --build-arg RM=\"[<path> ...]\" -t <organization>/installer:latest .\n\n\nThis will perform a rm -rf on the specified paths relative to the rootfs.\n\nNote: RM must be a whitespace delimited list.\n\nThe resulting image can be used to:\n\ngenerate an image for any of the supported providers\nperform bare-metall installs\nperform upgrades\n\nWe will step through common customizations in the remainder of this section.\n\n4.6 - Developing Talos\nLearn how to set up a development environment for local testing and hacking on Talos itself!\n\nThis guide outlines steps and tricks to develop Talos operating systems and related components. The guide assumes Linux operating system on the development host. Some steps might work under Mac OS X, but using Linux is highly advised.\n\nPrepare\n\nCheck out the Talos repository.\n\nTry running make help to see available make commands. You would need Docker and buildx installed on the host.\n\nNote: Usually it is better to install up to date Docker from Docker apt repositories, e.g. Ubuntu instructions.\n\nIf buildx plugin is not available with OS docker packages, it can be installed as a plugin from GitHub releases.\n\nSet up a builder with access to the host network:\n\n docker buildx create --driver docker-container  --driver-opt network=host --name local1 --buildkitd-flags '--allow-insecure-entitlement security.insecure' --use\n\n\nNote: network=host allows buildx builder to access host network, so that it can push to a local container registry (see below).\n\nMake sure the following steps work:\n\nmake talosctl\nmake initramfs kernel\n\nSet up a local docker registry:\n\ndocker run -d -p 5005:5000 \\\n\n    --restart always \\\n\n    --name local registry:2\n\n\nTry to build and push to local registry an installer image:\n\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRecord the image name output in the step above.\n\nNote: it is also possible to force a stable image tag by using TAG variable: make installer IMAGE_REGISTRY=127.0.0.1:5005 TAG=v1.0.0-alpha.1 PUSH=true.\n\nRunning Talos cluster\n\nSet up local caching docker registries (this speeds up Talos cluster boot a lot), script is in the Talos repo:\n\nbash hack/start-registry-proxies.sh\n\n\nStart your local cluster with:\n\nsudo --preserve-env=HOME _out/talosctl-linux-amd64 cluster create \\\n\n    --provisioner=qemu \\\n\n    --cidr=172.20.0.0/24 \\\n\n    --registry-mirror docker.io=http://172.20.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.20.0.1:5001  \\\n\n    --registry-mirror gcr.io=http://172.20.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.20.0.1:5004 \\\n\n    --registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005 \\\n\n    --install-image=127.0.0.1:5005/siderolabs/installer:<RECORDED HASH from the build step> \\\n\n    --controlplanes 3 \\\n\n    --workers 2 \\\n\n    --with-bootloader=false\n\n--provisioner selects QEMU vs. default Docker\ncustom --cidr to make QEMU cluster use different network than default Docker setup (optional)\n--registry-mirror uses the caching proxies set up above to speed up boot time a lot, last one adds your local registry (installer image was pushed to it)\n--install-image is the image you built with make installer above\n--controlplanes & --workers configure cluster size, choose to match your resources; 3 controlplanes give you HA control plane; 1 controlplane is enough, never do 2 controlplanes\n--with-bootloader=false disables boot from disk (Talos will always boot from _out/vmlinuz-amd64 and _out/initramfs-amd64.xz). This speeds up development cycle a lot - no need to rebuild installer and perform install, rebooting is enough to get new code.\n\nNote: as boot loader is not used, it’s not necessary to rebuild installer each time (old image is fine), but sometimes it’s needed (when configuration changes are done and old installer doesn’t validate the config).\n\ntalosctl cluster create derives Talos machine configuration version from the install image tag, so sometimes early in the development cycle (when new minor tag is not released yet), machine config version can be overridden with --talos-version=v1.6.\n\nIf the --with-bootloader=false flag is not enabled, for Talos cluster to pick up new changes to the code (in initramfs), it will require a Talos upgrade (so new installer should be built). With --with-bootloader=false flag, Talos always boots from initramfs in _out/ directory, so simple reboot is enough to pick up new code changes.\n\nIf the installation flow needs to be tested, --with-bootloader=false shouldn’t be used.\n\nConsole Logs\n\nWatching console logs is easy with tail:\n\ntail -F ~/.talos/clusters/talos-default/talos-default-*.log\n\nInteracting with Talos\n\nOnce talosctl cluster create finishes successfully, talosconfig and kubeconfig will be set up automatically to point to your cluster.\n\nStart playing with talosctl:\n\ntalosctl -n 172.20.0.2 version\n\ntalosctl -n 172.20.0.3,172.20.0.4 dashboard\n\ntalosctl -n 172.20.0.4 get members\n\n\nSame with kubectl:\n\nkubectl get nodes -o wide\n\n\nYou can deploy some Kubernetes workloads to the cluster.\n\nYou can edit machine config on the fly with talosctl edit mc --immediate, config patches can be applied via --config-patch flags, also many features have specific flags in talosctl cluster create.\n\nQuick Reboot\n\nTo reboot whole cluster quickly (e.g. to pick up a change made in the code):\n\nfor socket in ~/.talos/clusters/talos-default/talos-default-*.monitor; do echo \"q\" | sudo socat - unix-connect:$socket; done\n\n\nSending q to a single socket allows to reboot a single node.\n\nNote: This command performs immediate reboot (as if the machine was powered down and immediately powered back up), for normal Talos reboot use talosctl reboot.\n\nDevelopment Cycle\n\nFast development cycle:\n\nbring up a cluster\nmake code changes\nrebuild initramfs with make initramfs\nreboot a node to pick new initramfs\nverify code changes\nmore code changes…\n\nSome aspects of Talos development require to enable bootloader (when working on installer itself), in that case quick development cycle is no longer possible, and cluster should be destroyed and recreated each time.\n\nRunning Integration Tests\n\nIf integration tests were changed (or when running them for the first time), first rebuild the integration test binary:\n\nrm -f  _out/integration-test-linux-amd64; make _out/integration-test-linux-amd64\n\n\nRunning short tests against QEMU provisioned cluster:\n\n_out/integration-test-linux-amd64 \\\n\n    -talos.provisioner=qemu \\\n\n    -test.v \\\n\n    -talos.crashdump=false \\\n\n    -test.short \\\n\n    -talos.talosctlpath=$PWD/_out/talosctl-linux-amd64\n\n\nWhole test suite can be run removing -test.short flag.\n\nSpecfic tests can be run with -test.run=TestIntegration/api.ResetSuite.\n\nBuild Flavors\n\nmake <something> WITH_RACE=1 enables Go race detector, Talos runs slower and uses more memory, but memory races are detected.\n\nmake <something> WITH_DEBUG=1 enables Go profiling and other debug features, useful for local development.\n\nDestroying Cluster\nsudo --preserve-env=HOME ../talos/_out/talosctl-linux-amd64 cluster destroy --provisioner=qemu\n\n\nThis command stops QEMU and helper processes, tears down bridged network on the host, and cleans up cluster state in ~/.talos/clusters.\n\nNote: if the host machine is rebooted, QEMU instances and helpers processes won’t be started back. In that case it’s required to clean up files in ~/.talos/clusters/<cluster-name> directory manually.\n\nOptional\n\nSet up cross-build environment with:\n\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n\n\nNote: the static qemu binaries which come with Ubuntu 21.10 seem to be broken.\n\nUnit tests\n\nUnit tests can be run in buildx with make unit-tests, on Ubuntu systems some tests using loop devices will fail because Ubuntu uses low-index loop devices for snaps.\n\nMost of the unit-tests can be run standalone as well, with regular go test, or using IDE integration:\n\ngo test -v ./internal/pkg/circular/\n\n\nThis provides much faster feedback loop, but some tests require either elevated privileges (running as root) or additional binaries available only in Talos rootfs (containerd tests).\n\nRunning tests as root can be done with -exec flag to go test, but this is risky, as test code has root access and can potentially make undesired changes:\n\ngo test -exec sudo  -v ./internal/app/machined/pkg/controllers/network/...\n\nGo Profiling\n\nBuild initramfs with debug enabled: make initramfs WITH_DEBUG=1.\n\nLaunch Talos cluster with bootloader disabled, and use go tool pprof to capture the profile and show the output in your browser:\n\ngo tool pprof http://172.20.0.2:9982/debug/pprof/heap\n\n\nThe IP address 172.20.0.2 is the address of the Talos node, and port :9982 depends on the Go application to profile:\n\n9981: apid\n9982: machined\n9983: trustd\nTesting Air-gapped Environments\n\nThere is a hidden talosctl debug air-gapped command which launches two components:\n\nHTTP proxy capable of proxying HTTP and HTTPS requests\nHTTPS server with a self-signed certificate\n\nThe command also writes down Talos machine configuration patch to enable the HTTP proxy and add a self-signed certificate to the list of trusted certificates:\n\n$ talosctl debug air-gapped --advertised-address 172.20.0.1\n\n2022/08/04 16:43:14 writing config patch to air-gapped-patch.yaml\n\n2022/08/04 16:43:14 starting HTTP proxy on :8002\n\n2022/08/04 16:43:14 starting HTTPS server with self-signed cert on :8001\n\n\nThe --advertised-address should match the bridge IP of the Talos node.\n\nGenerated machine configuration patch looks like:\n\nmachine:\n\n    files:\n\n        - content: |\n\n            -----BEGIN CERTIFICATE-----\n\n            MIIBijCCAS+gAwIBAgIBATAKBggqhkjOPQQDAjAUMRIwEAYDVQQKEwlUZXN0IE9u\n\n            bHkwHhcNMjIwODA0MTI0MzE0WhcNMjIwODA1MTI0MzE0WjAUMRIwEAYDVQQKEwlU\n\n            ZXN0IE9ubHkwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQfOJdaOFSOI1I+EeP1\n\n            RlMpsDZJaXjFdoo5zYM5VYs3UkLyTAXAmdTi7JodydgLhty0pwLEWG4NUQAEvip6\n\n            EmzTo3IwcDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsG\n\n            AQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFCwxL+BjG0pDwaH8QgKW\n\n            Ex0J2mVXMA8GA1UdEQQIMAaHBKwUAAEwCgYIKoZIzj0EAwIDSQAwRgIhAJoW0z0D\n\n            JwpjFcgCmj4zT1SbBFhRBUX64PHJpAE8J+LgAiEAvfozZG8Or6hL21+Xuf1x9oh4\n\n            /4Hx3jozbSjgDyHOLk4=\n\n            -----END CERTIFICATE-----            \n\n          permissions: 0o644\n\n          path: /etc/ssl/certs/ca-certificates\n\n          op: append\n\n    env:\n\n        http_proxy: http://172.20.0.1:8002\n\n        https_proxy: http://172.20.0.1:8002\n\n        no_proxy: 172.20.0.1/24\n\ncluster:\n\n    extraManifests:\n\n        - https://172.20.0.1:8001/debug.yaml\n\n\nThe first section appends a self-signed certificate of the HTTPS server to the list of trusted certificates, followed by the HTTP proxy setup (in-cluster traffic is excluded from the proxy). The last section adds an extra Kubernetes manifest hosted on the HTTPS server.\n\nThe machine configuration patch can now be used to launch a test Talos cluster:\n\ntalosctl cluster create ... --config-patch @air-gapped-patch.yaml\n\n\nThe following lines should appear in the output of the talosctl debug air-gapped command:\n\nCONNECT discovery.talos.dev:443: the HTTP proxy is used to talk to the discovery service\nhttp: TLS handshake error from 172.20.0.2:53512: remote error: tls: bad certificate: an expected error on Talos side, as self-signed cert is not written yet to the file\nGET /debug.yaml: Talos successfully fetches the extra manifest successfully\n\nThere might be more output depending on the registry caches being used or not.\n\nRunning Upgrade Integration Tests\n\nTalos has a separate set of provision upgrade tests, which create a cluster on older versions of Talos, perform an upgrade, and verify that the cluster is still functional.\n\nBuild the test binary:\n\nrm -f  _out/integration-test-provision-linux-amd64; make _out/integration-test-provision-linux-amd64\n\n\nPrepare the test artifacts for the upgrade test:\n\nmake release-artifacts\n\n\nBuild and push an installer image for the development version of Talos:\n\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRun the tests (the tests will create the cluster on the older version of Talos, perform an upgrade, and verify that the cluster is still functional):\n\nsudo --preserve-env=HOME _out/integration-test-provision-linux-amd64 \\\n\n    -test.v \\\n\n    -talos.talosctlpath _out/talosctl-linux-amd64 \\\n\n    -talos.provision.target-installer-registry=127.0.0.1:5005 \\\n\n    -talos.provision.registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005,docker.io=http://172.20.0.1:5000,registry.k8s.io=http://172.20.0.1:5001,quay.io=http://172.20.0.1:5002,gcr.io=http://172.20.0.1:5003,ghcr.io=http://172.20.0.1:5004 \\\n\n    -talos.provision.cidr 172.20.0.0/24\n\n4.7 - Disaster Recovery\nProcedure for snapshotting etcd database and recovering from catastrophic control plane failure.\n\netcd database backs Kubernetes control plane state, so if the etcd service is unavailable, the Kubernetes control plane goes down, and the cluster is not recoverable until etcd is recovered. etcd builds around the consensus protocol Raft, so highly-available control plane clusters can tolerate the loss of nodes so long as more than half of the members are running and reachable. For a three control plane node Talos cluster, this means that the cluster tolerates a failure of any single node, but losing more than one node at the same time leads to complete loss of service. Because of that, it is important to take routine backups of etcd state to have a snapshot to recover the cluster from in case of catastrophic failure.\n\nBackup\nSnapshotting etcd Database\n\nCreate a consistent snapshot of etcd database with talosctl etcd snapshot command:\n\n$ talosctl -n <IP> etcd snapshot db.snapshot\n\netcd snapshot saved to \"db.snapshot\" (2015264 bytes)\n\nsnapshot info: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: filename db.snapshot is arbitrary.\n\nThis database snapshot can be taken on any healthy control plane node (with IP address <IP> in the example above), as all etcd instances contain exactly same data. It is recommended to configure etcd snapshots to be created on some schedule to allow point-in-time recovery using the latest snapshot.\n\nDisaster Database Snapshot\n\nIf the etcd cluster is not healthy (for example, if quorum has already been lost), the talosctl etcd snapshot command might fail. In that case, copy the database snapshot directly from the control plane node:\n\ntalosctl -n <IP> cp /var/lib/etcd/member/snap/db .\n\n\nThis snapshot might not be fully consistent (if the etcd process is running), but it allows for disaster recovery when latest regular snapshot is not available.\n\nMachine Configuration\n\nMachine configuration might be required to recover the node after hardware failure. Backup Talos node machine configuration with the command:\n\ntalosctl -n IP get mc v1alpha1 -o yaml | yq eval '.spec' -\n\nRecovery\n\nBefore starting a disaster recovery procedure, make sure that etcd cluster can’t be recovered:\n\nget etcd cluster member list on all healthy control plane nodes with talosctl -n IP etcd members command and compare across all members.\nquery etcd health across control plane nodes with talosctl -n IP service etcd.\n\nIf the quorum can be restored, restoring quorum might be a better strategy than performing full disaster recovery procedure.\n\nLatest Etcd Snapshot\n\nGet hold of the latest etcd database snapshot. If a snapshot is not fresh enough, create a database snapshot (see above), even if the etcd cluster is unhealthy.\n\nInit Node\n\nMake sure that there are no control plane nodes with machine type init:\n\n$ talosctl -n <IP1>,<IP2>,... get machinetype\n\nNODE         NAMESPACE   TYPE          ID             VERSION   TYPE\n\n172.20.0.2   config      MachineType   machine-type   2         controlplane\n\n172.20.0.4   config      MachineType   machine-type   2         controlplane\n\n172.20.0.3   config      MachineType   machine-type   2         controlplane\n\n\nInit node type is deprecated, and are incompatible with etcd recovery procedure. init node can be converted to controlplane type with talosctl edit mc --mode=staged command followed by node reboot with talosctl reboot command.\n\nPreparing Control Plane Nodes\n\nIf some control plane nodes experienced hardware failure, replace them with new nodes.\n\nUse machine configuration backup to re-create the nodes with the same secret material and control plane settings to allow workers to join the recovered control plane.\n\nIf a control plane node is up but etcd isn’t, wipe the node’s EPHEMERAL partition to remove the etcd data directory (make sure a database snapshot is taken before doing this):\n\ntalosctl -n <IP> reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL\n\n\nAt this point, all control plane nodes should boot up, and etcd service should be in the Preparing state.\n\nThe Kubernetes control plane endpoint should be pointed to the new control plane nodes if there were changes to the node addresses.\n\nRecovering from the Backup\n\nMake sure all etcd service instances are in Preparing state:\n\n$ talosctl -n <IP> service etcd\n\nNODE     172.20.0.2\n\nID       etcd\n\nSTATE    Preparing\n\nHEALTH   ?\n\nEVENTS   [Preparing]: Running pre state (17s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", time sync (18s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", service \"networkd\" to be \"up\", time sync (20s ago)\n\n\nExecute the bootstrap command against any control plane node passing the path to the etcd database snapshot:\n\n$ talosctl -n <IP> bootstrap --recover-from=./db.snapshot\n\nrecovering from snapshot \"./db.snapshot\": hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: if database snapshot was copied out directly from the etcd data directory using talosctl cp, add flag --recover-skip-hash-check to skip integrity check on restore.\n\nTalos node should print matching information in the kernel log:\n\nrecovering etcd from snapshot: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n{\"level\":\"info\",\"msg\":\"restoring snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/li}\n{\"level\":\"info\",\"msg\":\"restored last compact revision\",\"meta-bucket-name\":\"meta\",\"meta-bucket-name-key\":\"finishedCompactRev\",\"restored-compact-revision\":3360}\n{\"level\":\"info\",\"msg\":\"added member\",\"cluster-id\":\"a3390e43eb5274e2\",\"local-member-id\":\"0\",\"added-peer-id\":\"eb4f6f534361855e\",\"added-peer-peer-urls\":[\"https:/}\n{\"level\":\"info\",\"msg\":\"restored snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/lib/etcd/member/snap\"}\n\n\nNow etcd service should become healthy on the bootstrap node, Kubernetes control plane components should start and control plane endpoint should become available. Remaining control plane nodes join etcd cluster once control plane endpoint is up.\n\nSingle Control Plane Node Cluster\n\nThis guide applies to the single control plane clusters as well. In fact, it is much more important to take regular snapshots of the etcd database in single control plane node case, as loss of the control plane node might render the whole cluster irrecoverable without a backup.\n\n4.8 - etcd Maintenance\nOperational instructions for etcd database.\n\netcd database backs Kubernetes control plane state, so etcd health is critical for Kubernetes availability.\n\nSpace Quota\n\netcd default database space quota is set to 2 GiB by default. If the database size exceeds the quota, etcd will stop operations until the issue is resolved.\n\nThis condition can be checked with talosctl etcd alarm list command:\n\n$ talosctl -n <IP> etcd alarm list\n\nNODE         MEMBER             ALARM\n\n172.20.0.2   a49c021e76e707db   NOSPACE\n\n\nIf the Kubernetes database contains lots of resources, space quota can be increased to match the actual usage. The recommended maximum size is 8 GiB.\n\nTo increase the space quota, edit the etcd section in the machine configuration:\n\nmachine:\n\n  etcd:\n\n    extraArgs:\n\n      quota-backend-bytes: 4294967296 # 4 GiB\n\n\nOnce the node is rebooted with the new configuration, use talosctl etcd alarm disarm to clear the NOSPACE alarm.\n\nDefragmentation\n\netcd database can become fragmented over time if there are lots of writes and deletes. Kubernetes API server performs automatic compaction of the etcd database, which marks deleted space as free and ready to be reused. However, the space is not actually freed until the database is defragmented.\n\nIf the database is heavily fragmented (in use/db size ratio is less than 0.5), defragmentation might increase the performance. If the database runs over the space quota (see above), but the actual in use database size is small, defragmentation is required to bring the on-disk database size below the limit.\n\nCurrent database size can be checked with talosctl etcd status command:\n\n$ talosctl -n <CP1>,<CP2>,<CP3> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE            LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.3   ecebb05b59a776f1   21 MB     6.0 MB (29.08%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.2   a49c021e76e707db   17 MB     4.5 MB (26.10%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.4   eb47fb33e59bf0e2   20 MB     5.9 MB (28.96%)   ecebb05b59a776f1   53391        4           53391                false\n\n\nIf any of the nodes are over database size quota, alarms will be printed in the ERRORS column.\n\nTo defragment the database, run talosctl etcd defrag command:\n\ntalosctl -n <CP1> etcd defrag\n\n\nNote: defragmentation is a resource-intensive operation, so it is recommended to run it on a single node at a time. Defragmentation to a live member blocks the system from reading and writing data while rebuilding its state.\n\nOnce the defragmentation is complete, the database size will match closely to the in use size:\n\n$ talosctl -n <CP1> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE             LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.2   a49c021e76e707db   4.5 MB    4.5 MB (100.00%)   ecebb05b59a776f1   56065        4           56065                false\n\nSnapshotting\n\nRegular backups of etcd database should be performed to ensure that the cluster can be restored in case of a failure. This procedure is described in the disaster recovery guide.\n\n4.9 - Extension Services\nUse extension services in Talos Linux.\n\nTalos provides a way to run additional system services early in the Talos boot process. Extension services should be included into the Talos root filesystem (e.g. using system extensions). Extension services run as privileged containers with ephemeral root filesystem located in the Talos root filesystem.\n\nExtension services can be used to use extend core features of Talos in a way that is not possible via static pods or Kubernetes DaemonSets.\n\nPotential extension services use-cases:\n\nstorage: Open iSCSI, software RAID, etc.\nnetworking: BGP FRR, etc.\nplatform integration: VMWare open VM tools, etc.\nConfiguration\n\nTalos on boot scans directory /usr/local/etc/containers for *.yaml files describing the extension services to run. Format of the extension service config:\n\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello-world\n\n  # an optional path to a file containing environment variables\n\n  environmentFile: /var/etc/hello-world/env\n\n  environment:\n\n    - XDG_RUNTIME_DIR=/run\n\n  args:\n\n     - -f\n\n  mounts:\n\n     - # OCI Mount Spec\n\ndepends:\n\n   - service: cri\n\n   - path: /run/machined/machined.sock\n\n   - network:\n\n       - addresses\n\n       - connectivity\n\n       - hostname\n\n       - etcfiles\n\n   - time: true\n\nrestart: never|always|untilSuccess\n\nname\n\nField name sets the service name, valid names are [a-z0-9-_]+. The service container root filesystem path is derived from the name: /usr/local/lib/containers/<name>. The extension service will be registered as a Talos service under an ext-<name> identifier.\n\ncontainer\nentrypoint defines the container entrypoint relative to the container root filesystem (/usr/local/lib/containers/<name>)\nenvironmentFile defines the path to a file containing environment variables, the service waits for the file to exist before starting\nenvironment defines the container environment variables, overrides the variables from environmentFile\nargs defines the additional arguments to pass to the entrypoint\nmounts defines the volumes to be mounted into the container root\ncontainer.mounts\n\nThe section mounts uses the standard OCI spec:\n\n- source: /var/log/audit\n\n  destination: /var/log/audit\n\n  type: bind\n\n  options:\n\n    - rshared\n\n    - bind\n\n    - ro\n\n\nAll requested directories will be mounted into the extension service container mount namespace. If the source directory doesn’t exist in the host filesystem, it will be created (only for writable paths in the Talos root filesystem).\n\ncontainer.security\n\nThe section security follows this example:\n\nmaskedPaths:\n\n  - \"/should/be/masked\"\n\nreadonlyPaths:\n\n  - \"/path/that/should/be/readonly\"\n\n  - \"/another/readonly/path\"\n\nwriteableRootfs: true\n\nwriteableSysfs: true\n\nrootfsPropagation: shared\n\nThe rootfs is readonly by default unless writeableRootfs: true is set.\nThe sysfs is readonly by default unless writeableSysfs: true is set.\nMasked paths if not set defaults to containerd defaults. Masked paths will be mounted to /dev/null. To set empty masked paths use:\ncontainer:\n\n  security:\n\n    maskedPaths: []\n\nRead Only paths if not set defaults to containerd defaults. Read-only paths will be mounted to /dev/null. To set empty read only paths use:\ncontainer:\n\n  security:\n\n    readonlyPaths: []\n\nRootfs propagation is not set by default (container mounts are private).\ndepends\n\nThe depends section describes extension service start dependencies: the service will not be started until all dependencies are met.\n\nAvailable dependencies:\n\nservice: <name>: wait for the service <name> to be running and healthy\npath: <path>: wait for the <path> to exist\nnetwork: [addresses, connectivity, hostname, etcfiles]: wait for the specified network readiness checks to succeed\ntime: true: wait for the NTP time sync\nrestart\n\nField restart defines the service restart policy, it allows to either configure an always running service or a one-shot service:\n\nalways: restart service always\nnever: start service only once and never restart\nuntilSuccess: restart failing service, stop restarting on successful run\nExample\n\nExample layout of the Talos root filesystem contents for the extension service:\n\n/\n\n└── usr\n\n    └── local\n\n        ├── etc\n\n        │   └── containers\n\n        │       └── hello-world.yaml\n\n        └── lib\n\n            └── containers\n\n                └── hello-world\n\n                    ├── hello\n\n                    └── config.ini\n\n\nTalos discovers the extension service configuration in /usr/local/etc/containers/hello-world.yaml:\n\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello\n\n  args:\n\n    - --config\n\n    - config.ini\n\ndepends:\n\n  - network:\n\n    - addresses\n\nrestart: always\n\n\nTalos starts the container for the extension service with container root filesystem at /usr/local/lib/containers/hello-world:\n\n/\n\n├── hello\n\n└── config.ini\n\n\nExtension service is registered as ext-hello-world in talosctl services:\n\n$ talosctl service ext-hello-world\n\nNODE     172.20.0.5\n\nID       ext-hello-world\n\nSTATE    Running\n\nHEALTH   ?\n\nEVENTS   [Running]: Started task ext-hello-world (PID 1100) for container ext-hello-world (2m47s ago)\n\n         [Preparing]: Creating service runner (2m47s ago)\n\n         [Preparing]: Running pre state (2m47s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\" (2m48s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\", network (2m49s ago)\n\n\nAn extension service can be started, restarted and stopped using talosctl service ext-hello-world start|restart|stop. Use talosctl logs ext-hello-world to get the logs of the service.\n\nComplete example of the extension service can be found in the extensions repository.\n\n4.10 - Machine Configuration OAuth2 Authentication\nHow to authenticate Talos machine configuration download (talos.config=) on metal platform using OAuth.\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow. The machine configuration is fetched from the URL specified with talos.config kernel argument, and by default this HTTP request is not authenticated. When the OAuth2 authentication is enabled, Talos will authenticate the request using OAuth device flow first, and then pass the token to the machine configuration download endpoint.\n\nPrerequisites\n\nObtain the following information:\n\nOAuth client ID (mandatory)\nOAuth client secret (optional)\nOAuth device endpoint\nOAuth token endpoint\nOAuth scopes, audience (optional)\nOAuth client secret (optional)\nextra Talos variables to send to the device auth endpoint (optional)\nConfiguration\n\nSet the following kernel parameters on the initial Talos boot to enable the OAuth flow:\n\ntalos.config set to the URL of the machine configuration endpoint (which will be authenticated using OAuth)\ntalos.config.oauth.client_id set to the OAuth client ID (required)\ntalos.config.oauth.client_secret set to the OAuth client secret (optional)\ntalos.config.oauth.scope set to the OAuth scopes (optional, repeat the parameter for multiple scopes)\ntalos.config.oauth.audience set to the OAuth audience (optional)\ntalos.config.oauth.device_auth_url set to the OAuth device endpoint (if not set defaults to talos.config URL with the path /device/code)\ntalos.config.oauth.token_url set to the OAuth token endpoint (if not set defaults to talos.config URL with the path /token)\ntalos.config.oauth.extra_variable set to the extra Talos variables to send to the device auth endpoint (optional, repeat the parameter for multiple variables)\n\nThe list of variables supported by the talos.config.oauth.extra_variable parameter is same as the list of variables supported by the talos.config parameter.\n\nFlow\n\nOn the initial Talos boot, when machine configuration is not available, Talos will print the following messages:\n\n[talos] downloading config {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"platform\": \"metal\"}\n\n[talos] waiting for network to be ready\n\n[talos] [OAuth] starting the authentication device flow with the following settings:\n\n[talos] [OAuth]  - client ID: \"<REDACTED>\"\n\n[talos] [OAuth]  - device auth URL: \"https://oauth2.googleapis.com/device/code\"\n\n[talos] [OAuth]  - token URL: \"https://oauth2.googleapis.com/token\"\n\n[talos] [OAuth]  - extra variables: [\"uuid\" \"mac\"]\n\n[talos] waiting for variables: [uuid mac]\n\n[talos] waiting for variables: [mac]\n\n[talos] [OAuth] please visit the URL https://www.google.com/device and enter the code <REDACTED>\n\n[talos] [OAuth] waiting for the device to be authorized (expires at 14:46:55)...\n\n\nIf the OAuth service provides the complete verification URL, the QR code to scan is also printed to the console:\n\n[talos] [OAuth] or scan the following QR code:\n\n█████████████████████████████████\n\n█████████████████████████████████\n\n████ ▄▄▄▄▄ ██▄▀▀    ▀█ ▄▄▄▄▄ ████\n\n████ █   █ █▄  ▀▄██▄██ █   █ ████\n\n████ █▄▄▄█ ██▀▄██▄  ▀█ █▄▄▄█ ████\n\n████▄▄▄▄▄▄▄█ ▀ █ ▀ █▄█▄▄▄▄▄▄▄████\n\n████   ▀ ▄▄ ▄█  ██▄█   ███▄█▀████\n\n████▀█▄  ▄▄▀▄▄█▀█▄██ ▄▀▄██▄ ▄████\n\n████▄██▀█▄▄▄███▀ ▀█▄▄  ██ █▄ ████\n\n████▄▀▄▄▄ ▄███ ▄ ▀ ▀▀▄▀▄▀█▄ ▄████\n\n████▄█████▄█  █ ██ ▀ ▄▄▄  █▀▀████\n\n████ ▄▄▄▄▄ █ █ ▀█▄█▄ █▄█  █▄ ████\n\n████ █   █ █▄ ▄▀ ▀█▀▄▄▄   ▀█▄████\n\n████ █▄▄▄█ █ ██▄ ▀  ▀███ ▀█▀▄████\n\n████▄▄▄▄▄▄▄█▄▄█▄██▄▄▄▄█▄███▄▄████\n\n█████████████████████████████████\n\n\nOnce the authentication flow is complete on the OAuth provider side, Talos will print the following message:\n\n[talos] [OAuth] device authorized\n\n[talos] fetching machine config from: \"http://example.com/config.yaml\"\n\n[talos] machine config loaded successfully {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"sources\": [\"metal\"]}\n\n4.11 - Metal Network Configuration\nHow to use META-based network configuration on Talos metal platform.\n\nNote: This is an advanced feature which requires deep understanding of Talos and Linux network configuration.\n\nTalos Linux when running on a cloud platform (e.g. AWS or Azure), uses the platform-provided metadata server to provide initial network configuration to the node. When running on bare-metal, there is no metadata server, so there are several options to provide initial network configuration (before machine configuration is acquired):\n\nuse automatic network configuration via DHCP (Talos default)\nuse initial boot kernel command line parameters to configure networking\nuse automatic network configuration via DHCP just enough to fetch machine configuration and then use machine configuration to set desired advanced configuration.\n\nIf DHCP option is available, it is by far the easiest way to configure networking. The initial boot kernel command line parameters are not very flexible, and they are not persisted after initial Talos installation.\n\nTalos starting with version 1.4.0 offers a new option to configure networking on bare-metal: META-based network configuration.\n\nNote: META-based network configuration is only available on Talos Linux metal platform.\n\nTalos dashboard provides a way to configure META-based network configuration for a machine using the console, but it doesn’t support all kinds of network configuration.\n\nNetwork Configuration Format\n\nTalos META-based network configuration is a YAML file with the following format:\n\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 10.68.182.1/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nlinks:\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      mtu: 0\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\nroutes:\n\n    - family: inet4\n\n      gateway: 147.75.61.42\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 1024\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nhostnames:\n\n    - hostname: ci-blue-worker-amd64-2\n\n      layer: platform\n\nresolvers: []\n\ntimeServers: []\n\n\nEvery section is optional, so you can configure only the parts you need. The format of each section matches the respective network *Spec resource .spec part, e.g the addresses: section matches the .spec of AddressSpec resource:\n\n# talosctl get addressspecs bond0/10.68.182.1/31 -o yaml | yq .spec\n\naddress: 10.68.182.1/31\n\nlinkName: bond0\n\nfamily: inet4\n\nscope: global\n\nflags: permanent\n\nlayer: platform\n\n\nSo one way to prepare the network configuration file is to boot Talos Linux, apply necessary network configuration using Talos machine configuration, and grab the resulting resources from the running Talos instance.\n\nIn this guide we will briefly cover the most common examples of the network configuration.\n\nAddresses\n\nThe addresses configured are usually routable IP addresses assigned to the machine, so the scope: should be set to global and flags: to permanent. Additionally, family: should be set to either inet4 or init6 depending on the address family.\n\nThe linkName: property should match the name of the link the address is assigned to, it might be a physical link, e.g. en9sp0, or the name of a logical link, e.g. bond0, created in the links: section.\n\nExample, IPv4 address:\n\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n\nExample, IPv6 address:\n\naddresses:\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nLinks\n\nFor physical network interfaces (links), the most usual configuration is to bring the link up:\n\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      layer: platform\n\n\nThis will bring the link up, and it will also disable Talos auto-configuration (disables running DHCP on the link).\n\nAnother common case is to set a custom MTU:\n\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      mtu: 9000\n\n      layer: platform\n\n\nThe order of the links in the links: section is not important.\n\nBonds\n\nFor bonded links, there should be a link resource for the bond itself, and a link resource for each enslaved link:\n\nlinks:\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n\nThe name of the bond can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: bond - this is a bonded link\ntype: ether - this is an Ethernet link\nbondMaster: - defines bond configuration, please see Linux documentation on the available options\n\nFor each enslaved link, the following properties are important:\n\nmasterName: bond0 - the name of the bond this link is enslaved to\nslaveIndex: 0 - the index of the enslaved link, starting from 0, controls the order of bond slaves\nVLANs\n\nVLANs are logical links which have a parent link, and a VLAN ID and protocol:\n\nlinks:\n\n    - name: bond0.35\n\n      logical: true\n\n      up: true\n\n      kind: vlan\n\n      type: ether\n\n      parentName: bond0\n\n      vlan:\n\n        vlanID: 35\n\n        vlanProtocol: 802.1ad\n\n\nThe name of the VLAN link can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: vlan - this is a VLAN link\ntype: ether - this is an Ethernet link\nparentName: bond0 - the name of the parent link\nvlan: - defines VLAN configuration: vlanID and vlanProtocol\nRoutes\n\nFor route configuration, most of the time table: main, scope: global, type: unicast and protocol: static are used.\n\nThe route most important fields are:\n\ndst: defines the destination network, if left empty means “default gateway”\ngateway: defines the gateway address\npriority: defines the route priority (metric), lower values are preferred for the same dst: network\noutLinkName: defines the name of the link the route is associated with\nsrc: sets the source address for the route (optional)\n\nAdditionally, family: should be set to either inet4 or init6 depending on the address family.\n\nExample, IPv6 default gateway:\n\nroutes:\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n\nExample, IPv4 route to 10/8 via 10.68.182.0 gateway:\n\nroutes:\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nHostnames\n\nEven though the section supports multiple hostnames, only a single one should be used:\n\nhostnames:\n\n    - hostname: host\n\n      domainname: some.org\n\n      layer: platform\n\n\nThe domainname: is optional.\n\nIf the hostname is not set, Talos will use default generated hostname.\n\nResolvers\n\nThe resolvers: section is used to configure DNS resolvers, only single entry should be used:\n\nresolvers:\n\n    - dnsServers:\n\n        - 8.8.8.8\n\n        - 1.1.1.1\n\n      layer: platform\n\n\nIf the dnsServers: is not set, Talos will use default DNS servers.\n\nTime Servers\n\nThe timeServers: section is used to configure NTP time servers, only single entry should be used:\n\ntimeServers:\n\n    - timeServers:\n\n        - 169.254.169.254\n\n      layer: platform\n\n\nIf the timeServers: is not set, Talos will use default NTP servers.\n\nSupplying META Network Configuration\n\nOnce the network configuration YAML document is ready, it can be supplied to Talos in one of the following ways:\n\nfor a running Talos machine, using Talos API (requires already established network connectivity)\nfor Talos disk images, it can be embedded into the image\nfor ISO/PXE boot methods, it can be supplied via kernel command line parameters as an environment variable\n\nThe metal network configuration is stored in Talos META partition under the key 0xa (decimal 10).\n\nIn this guide we will assume that the prepared network configuration is stored in the file network.yaml.\n\nNote: as JSON is a subset of YAML, the network configuration can be also supplied as a JSON document.\n\nSupplying Network Configuration to a Running Talos Machine\n\nUse the talosctl to write a network configuration to a running Talos machine:\n\ntalosctl meta write 0xa \"$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos Disk Image\n\nFollowing the boot assets guide, create a disk image passing the network configuration as a --meta flag:\n\ndocker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 metal --meta \"0xa=$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos ISO/PXE Boot\n\nAs there is no META partition created yet before Talos Linux is installed, META values can be set as an environment variable INSTALLER_META_BASE64 passed to the initial boot of Talos. The supplied value will be used immediately, and also it will be written to the META partition once Talos is installed.\n\nWhen using imager to create the ISO, the INSTALLER_META_BASE64 environment variable will be automatically generated from the --meta flag:\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --meta \"0xa=$(cat network.yaml)\"\n\n...\n\nkernel command line: ... talos.environment=INSTALLER_META_BASE64=MHhhPWZvbw==\n\n\nWhen PXE booting, the value of INSTALLER_META_BASE64 should be set manually:\n\necho -n \"0xa=$(cat network.yaml)\" | base64\n\n\nThe resulting base64 string should be passed as an environment variable INSTALLER_META_BASE64 to the initial boot of Talos: talos.environment=INSTALLER_META_BASE64=<base64-encoded value>.\n\nGetting Current META Network Configuration\n\nTalos exports META keys as resources:\n\n# talosctl get meta 0x0a -o yaml\n\n...\n\nspec:\n\n    value: '{\"addresses\": ...}'\n\n4.12 - Migrating from Kubeadm\nMigrating Kubeadm-based clusters to Talos.\n\nIt is possible to migrate Talos from a cluster that is created using kubeadm to Talos.\n\nHigh-level steps are the following:\n\nCollect CA certificates and a bootstrap token from a control plane node.\nCreate a Talos machine config with the CA certificates with the ones you collected.\nUpdate control plane endpoint in the machine config to point to the existing control plane (i.e. your load balancer address).\nBoot a new Talos machine and apply the machine config.\nVerify that the new control plane node is ready.\nRemove one of the old control plane nodes.\nRepeat the same steps for all control plane nodes.\nVerify that all control plane nodes are ready.\nRepeat the same steps for all worker nodes, using the machine config generated for the workers.\nRemarks on kube-apiserver load balancer\n\nWhile migrating to Talos, you need to make sure that your kube-apiserver load balancer is in place and keeps pointing to the correct set of control plane nodes.\n\nThis process depends on your load balancer setup.\n\nIf you are using an LB that is external to the control plane nodes (e.g. cloud provider LB, F5 BIG-IP, etc.), you need to make sure that you update the backend IPs of the load balancer to point to the control plane nodes as you add Talos nodes and remove kubeadm-based ones.\n\nIf your load balancing is done on the control plane nodes (e.g. keepalived + haproxy on the control plane nodes), you can do the following:\n\nAdd Talos nodes and remove kubeadm-based ones while updating the haproxy backends to point to the newly added nodes except the last kubeadm-based control plane node.\nTurn off keepalived to drop the virtual IP used by the kubeadm-based nodes (introduces kube-apiserver downtime).\nSet up a virtual-IP based new load balancer on the new set of Talos control plane nodes. Use the previous LB IP as the LB virtual IP.\nVerify apiserver connectivity over the Talos-managed virtual IP.\nMigrate the last control-plane node.\nPrerequisites\nAdmin access to the kubeadm-based cluster\nAccess to the /etc/kubernetes/pki directory (e.g. SSH & root permissions) on the control plane nodes of the kubeadm-based cluster\nAccess to kube-apiserver load-balancer configuration\nStep-by-step guide\n\nDownload /etc/kubernetes/pki directory from a control plane node of the kubeadm-based cluster.\n\nCreate a new join token for the new control plane nodes:\n\n# inside a control plane node\n\nkubeadm token create --ttl 0\n\n\nCreate Talos secrets from the PKI directory you downloaded on step 1 and the token you generated on step 2:\n\ntalosctl gen secrets --kubernetes-bootstrap-token <TOKEN> --from-kubernetes-pki <PKI_DIR>\n\n\nCreate a new Talos config from the secrets:\n\ntalosctl gen config --with-secrets secrets.yaml <CLUSTER_NAME> https://<EXISTING_CLUSTER_LB_IP>\n\n\nCollect the information about the kubeadm-based cluster from the kubeadm configmap:\n\nkubectl get configmap -n kube-system kubeadm-config -oyaml\n\n\nTake note of the following information in the ClusterConfiguration:\n\n.controlPlaneEndpoint\n.networking.dnsDomain\n.networking.podSubnet\n.networking.serviceSubnet\n\nReplace the following information in the generated controlplane.yaml:\n\n.cluster.network.cni.name with none\n.cluster.network.podSubnets[0] with the value of the networking.podSubnet from the previous step\n.cluster.network.serviceSubnets[0] with the value of the networking.serviceSubnet from the previous step\n.cluster.network.dnsDomain with the value of the networking.dnsDomain from the previous step\n\nGo through the rest of controlplane.yaml and worker.yaml to customize them according to your needs, especially :\n\n.cluster.secretboxEncryptionSecret should be either removed if you don’t currently use EncryptionConfig on your kube-apiserver or set to the correct value\n\nMake sure that, on your current Kubeadm cluster, the first --service-account-issuer= parameter in /etc/kubernetes/manifests/kube-apiserver.yaml is equal to the value of .cluster.controlPlane.endpoint in controlplane.yaml. If it’s not, add a new --service-account-issuer= parameter with the correct value before your current one in /etc/kubernetes/manifests/kube-apiserver.yaml on all of your control planes nodes, and restart the kube-apiserver containers.\n\nBring up a Talos node to be the initial Talos control plane node.\n\nApply the generated controlplane.yaml to the Talos control plane node:\n\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file controlplane.yaml\n\n\nWait until the new control plane node joins the cluster and is ready.\n\nkubectl get node -owide --watch\n\n\nUpdate your load balancer to point to the new control plane node.\n\nDrain the old control plane node you are replacing:\n\nkubectl drain <OLD_NODE> --delete-emptydir-data --force --ignore-daemonsets --timeout=10m\n\n\nRemove the old control plane node from the cluster:\n\nkubectl delete node <OLD_NODE>\n\n\nDestroy the old node:\n\n# inside the node\n\nsudo kubeadm reset --force\n\n\nRepeat the same steps, starting from step 7, for all control plane nodes.\n\nRepeat the same steps, starting from step 7, for all worker nodes while applying the worker.yaml instead and skipping the LB step:\n\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file worker.yaml\n\n\nYour kubeadm kube-proxy configuration may not be compatible with the one generated by Talos, which will make the Talos Kubernetes upgrades impossible (labels may not be the same, and selector.matchLabels is an immutable field). To be sure, export your current kube-proxy daemonset manifest, check the labels, they have to be:\n\ntier: node\n\nk8s-app: kube-proxy\n\n\nIf the are not, modify all the labels fields, save the file, delete your current kube-proxy daemonset, and apply the one you modified.\n\n4.13 - Proprietary Kernel Modules\nAdding a proprietary kernel module to Talos Linux\n\nPatching and building the kernel image\n\nClone the pkgs repository from Github and check out the revision corresponding to your version of Talos Linux\n\ngit clone https://github.com/talos-systems/pkgs pkgs && cd pkgs\n\ngit checkout v0.8.0\n\n\nClone the Linux kernel and check out the revision that pkgs uses (this can be found in kernel/kernel-prepare/pkg.yaml and it will be something like the following: https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-x.xx.x.tar.xz)\n\ngit clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git && cd linux\n\ngit checkout v5.15\n\n\nYour module will need to be converted to be in-tree. The steps for this are different depending on the complexity of the module to port, but generally it would involve moving the module source code into the drivers tree and creating a new Makefile and Kconfig.\n\nStage your changes in Git with git add -A.\n\nRun git diff --cached --no-prefix > foobar.patch to generate a patch from your changes.\n\nCopy this patch to kernel/kernel/patches in the pkgs repo.\n\nAdd a patch line in the prepare segment of kernel/kernel/pkg.yaml:\n\npatch -p0 < /pkg/patches/foobar.patch\n\n\nBuild the kernel image. Make sure you are logged in to ghcr.io before running this command, and you can change or omit PLATFORM depending on what you want to target.\n\nmake kernel PLATFORM=linux/amd64 USERNAME=your-username PUSH=true\n\n\nMake a note of the image name the make command outputs.\n\nBuilding the installer image\n\nCopy the following into a new Dockerfile:\n\nFROM scratch AS customization\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:<talos version>\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nRun to build and push the installer:\n\nINSTALLER_VERSION=<talos version>\n\nIMAGE_NAME=\"ghcr.io/your-username/talos-installer:$INSTALLER_VERSION\"\n\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t \"$IMAGE_NAME\" . && docker push \"$IMAGE_NAME\"\n\n\nDeploying to your cluster\n\ntalosctl upgrade --image ghcr.io/your-username/talos-installer:<talos version> --preserve=true\n\n4.14 - Static Pods\nUsing Talos Linux to set up static pods in Kubernetes.\nStatic Pods\n\nStatic pods are run directly by the kubelet bypassing the Kubernetes API server checks and validations. Most of the time DaemonSet is a better alternative to static pods, but some workloads need to run before the Kubernetes API server is available or might need to bypass security restrictions imposed by the API server.\n\nSee Kubernetes documentation for more information on static pods.\n\nConfiguration\n\nStatic pod definitions are specified in the Talos machine configuration:\n\nmachine:\n\n  pods:\n\n    - apiVersion: v1\n\n       kind: Pod\n\n       metadata:\n\n         name: nginx\n\n       spec:\n\n         containers:\n\n           - name: nginx\n\n             image: nginx\n\n\nTalos renders static pod definitions to the kubelet manifest directory (/etc/kubernetes/manifests), kubelet picks up the definition and launches the pod.\n\nTalos accepts changes to the static pod configuration without a reboot.\n\nUsage\n\nKubelet mirrors pod definition to the API server state, so static pods can be inspected with kubectl get pods, logs can be retrieved with kubectl logs, etc.\n\n$ kubectl get pods\n\nNAME                           READY   STATUS    RESTARTS   AGE\n\nnginx-talos-default-controlplane-2   1/1     Running   0          17s\n\n\nIf the API server is not available, status of the static pod can also be inspected with talosctl containers --kubernetes:\n\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                      IMAGE                                                   PID    STATUS\n\n172.20.0.3   k8s.io      default/nginx-talos-default-controlplane-2                                              registry.k8s.io/pause:3.6                               4886   SANDBOX_READY\n\n172.20.0.3   k8s.io      └─ default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771                        docker.io/library/nginx:latest\n\n...\n\n\nLogs of static pods can be retrieved with talosctl logs --kubernetes:\n\n$ talosctl logs --kubernetes default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771\n\n172.20.0.3: 2022-02-10T15:26:01.289208227Z stderr F 2022/02/10 15:26:01 [notice] 1#1: using the \"epoll\" event method\n\n172.20.0.3: 2022-02-10T15:26:01.2892466Z stderr F 2022/02/10 15:26:01 [notice] 1#1: nginx/1.21.6\n\n172.20.0.3: 2022-02-10T15:26:01.28925723Z stderr F 2022/02/10 15:26:01 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)\n\nTroubleshooting\n\nTalos doesn’t perform any validation on the static pod definitions. If the pod isn’t running, use kubelet logs (talosctl logs kubelet) to find the problem:\n\n$ talosctl logs kubelet\n\n172.20.0.2: {\"ts\":1644505520281.427,\"caller\":\"config/file.go:187\",\"msg\":\"Could not process manifest file\",\"path\":\"/etc/kubernetes/manifests/talos-default-nginx-gvisor.yaml\",\"err\":\"invalid pod: [spec.containers: Required value]\"}\n\nResource Definitions\n\nStatic pod definitions are available as StaticPod resources combined with Talos-generated control plane static pods:\n\n$ talosctl get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.3   k8s         StaticPod   default-nginx             1\n\n172.20.0.3   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.3   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.3   k8s         StaticPod   kube-scheduler            1\n\n\nTalos assigns ID <namespace>-<name> to the static pods specified in the machine configuration.\n\nOn control plane nodes status of the running static pods is available in the StaticPodStatus resource:\n\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE   TYPE              ID                                                           VERSION   READY\n\n172.20.0.3   k8s         StaticPodStatus   default/nginx-talos-default-controlplane-2                         2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-2            2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-2   3         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-2            3         True\n\n4.15 - Talos API access from Kubernetes\nHow to access Talos API from within Kubernetes.\n\nIn this guide, we will enable the Talos feature to access the Talos API from within Kubernetes.\n\nEnabling the Feature\n\nEdit the machine configuration to enable the feature, specifying the Kubernetes namespaces from which Talos API can be accessed and the allowed Talos API roles.\n\ntalosctl -n 172.20.0.2 edit machineconfig\n\n\nConfigure the kubernetesTalosAPIAccess like the following:\n\nspec:\n\n  machine:\n\n    features:\n\n      kubernetesTalosAPIAccess:\n\n        enabled: true\n\n        allowedRoles:\n\n          - os:reader\n\n        allowedKubernetesNamespaces:\n\n          - default\n\nInjecting Talos ServiceAccount into manifests\n\nCreate the following manifest file deployment.yaml:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  template:\n\n    metadata:\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n        - name: talos-api-access\n\n          image: alpine:3\n\n          command:\n\n            - sh\n\n            - -c\n\n            - |\n\n              wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n              chmod +x /usr/local/bin/talosctl\n\n              while true; talosctl -n 172.20.0.2 version; do sleep 1; done              \n\n\nNote: make sure that you replace the IP 172.20.0.2 with a valid Talos node IP.\n\nUse talosctl inject serviceaccount command to inject the Talos ServiceAccount into the manifest.\n\ntalosctl inject serviceaccount -f deployment.yaml > deployment-injected.yaml\n\n\nInspect the generated manifest:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  creationTimestamp: null\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  strategy: {}\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n      - command:\n\n        - sh\n\n        - -c\n\n        - |\n\n          wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n          chmod +x /usr/local/bin/talosctl\n\n          while true; talosctl -n 172.20.0.2 version; do sleep 1; done          \n\n        image: alpine:3\n\n        name: talos-api-access\n\n        resources: {}\n\n        volumeMounts:\n\n        - mountPath: /var/run/secrets/talos.dev\n\n          name: talos-secrets\n\n      tolerations:\n\n      - operator: Exists\n\n      volumes:\n\n      - name: talos-secrets\n\n        secret:\n\n          secretName: talos-api-access-talos-secrets\n\nstatus: {}\n\n---\n\napiVersion: talos.dev/v1alpha1\n\nkind: ServiceAccount\n\nmetadata:\n\n    name: talos-api-access-talos-secrets\n\nspec:\n\n    roles:\n\n        - os:reader\n\n---\n\n\nAs you can notice, your deployment manifest is now injected with the Talos ServiceAccount.\n\nTesting API Access\n\nApply the new manifest into default namespace:\n\nkubectl apply -n default -f deployment-injected.yaml\n\n\nFollow the logs of the pods belong to the deployment:\n\nkubectl logs -n default -f -l app=talos-api-access\n\n\nYou’ll see a repeating output similar to the following:\n\nClient:\n\n    Tag:         <talos version>\n\n    SHA:         ....\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\nServer:\n\n    NODE:        172.20.0.2\n\n    Tag:         <talos version>\n\n    SHA:         ...\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\n    Enabled:     RBAC\n\n\nThis means that the pod can talk to Talos API of node 172.20.0.2 successfully.\n\n4.16 - Verifying Images\nVerifying Talos container image signatures.\n\nSidero Labs signs the container images generated for the Talos release with cosign:\n\nghcr.io/siderolabs/installer (Talos installer)\nghcr.io/siderolabs/talos (Talos image for container runtime)\nghcr.io/siderolabs/talosctl (talosctl client packaged as a container image)\nghcr.io/siderolabs/imager (Talos install image generator)\nall system extension images\nVerifying Container Image Signatures\n\nThe cosign tool can be used to verify the signatures of the Talos container images:\n\n$ cosign verify --certificate-identity-regexp '@siderolabs\\.com$' --certificate-oidc-issuer https://accounts.google.com ghcr.io/siderolabs/installer:v1.4.0\n\n\n\nVerification for ghcr.io/siderolabs/installer:v1.4.0 --\n\nThe following checks were performed on each of these signatures:\n\n  - The cosign claims were validated\n\n  - Existence of the claims in the transparency log was verified offline\n\n  - The code-signing certificate was verified using trusted certificate authority certificates\n\n\n\n[{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/siderolabs/installer\"},\"image\":{\"docker-manifest-digest\":\"sha256:f41795cc88f40eb1bc6b3c638c4a3123f6ef3c90627bfc35c04ebab82581e3ee\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://accounts.google.com\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEQCIERkQpgEnPWnfjUHIWO9QxC9Ute3/xJOc7TO5GUnu59xAiBKcFvrDWHoUYChT0/+gaazTrI+r0/GWSbi+Q+sEQ5AKA==\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1681843022,\"logIndex\":18304044,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://accounts.google.com\",\"Subject\":\"andrey.smirnov@siderolabs.com\"}}]\n\n\nThe image should be signed using cosing keyless flow by a Sidero Labs employee with and email from siderolabs.com domain.\n\nReproducible Builds\n\nTalos builds for kernel, initramfs, talosctl, ISO image, and container images are reproducible. So you can verify that the build is the same as the one as provided on GitHub releases page.\n\nSee building Talos images for more details.\n\n5 - Reference\n5.1 - API\nTalos gRPC API reference.\nTable of Contents\n\ncommon/common.proto\n\nData\n\nDataResponse\n\nEmpty\n\nEmptyResponse\n\nError\n\nMetadata\n\nNetIP\n\nNetIPPort\n\nNetIPPrefix\n\nPEMEncodedCertificateAndKey\n\nPEMEncodedKey\n\nURL\n\nCode\n\nContainerDriver\n\nContainerdNamespace\n\nFile-level Extensions\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\nConfigSpec\nControlPlane\nIdentitySpec\nInfoSpec\nKubeSpanAffiliateSpec\nMemberSpec\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\nMachineType\nNethelpersADSelect\nNethelpersARPAllTargets\nNethelpersARPValidate\nNethelpersAddressFlag\nNethelpersBondMode\nNethelpersBondXmitHashPolicy\nNethelpersConntrackState\nNethelpersDuplex\nNethelpersFailOverMAC\nNethelpersFamily\nNethelpersLACPRate\nNethelpersLinkType\nNethelpersMatchOperator\nNethelpersNfTablesChainHook\nNethelpersNfTablesChainPriority\nNethelpersNfTablesVerdict\nNethelpersOperationalState\nNethelpersPort\nNethelpersPrimaryReselect\nNethelpersProtocol\nNethelpersRouteFlag\nNethelpersRouteProtocol\nNethelpersRouteType\nNethelpersRoutingTable\nNethelpersScope\nNethelpersVLANProtocol\nNetworkConfigLayer\nNetworkOperator\nRuntimeMachineStage\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\nConfigSpec.ExtraArgsEntry\nMemberSpec\nPKIStatusSpec\nSpecSpec\nSpecSpec.ExtraArgsEntry\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\nConstraint\nLayer\nMetadata\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\nEtcFileStatusSpec\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\nProcessorSpec\nSystemInformationSpec\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\nAPIServerConfigSpec.EnvironmentVariablesEntry\nAPIServerConfigSpec.ExtraArgsEntry\nAdmissionControlConfigSpec\nAdmissionPluginSpec\nAuditPolicyConfigSpec\nBootstrapManifestsConfigSpec\nConfigStatusSpec\nControllerManagerConfigSpec\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nControllerManagerConfigSpec.ExtraArgsEntry\nEndpointSpec\nExtraManifest\nExtraManifest.ExtraHeadersEntry\nExtraManifestsConfigSpec\nExtraVolume\nKubePrismConfigSpec\nKubePrismEndpoint\nKubePrismEndpointsSpec\nKubePrismStatusesSpec\nKubeletConfigSpec\nKubeletConfigSpec.ExtraArgsEntry\nKubeletSpecSpec\nManifestSpec\nManifestStatusSpec\nNodeIPConfigSpec\nNodeIPSpec\nNodeLabelSpecSpec\nNodeStatusSpec\nNodeStatusSpec.AnnotationsEntry\nNodeStatusSpec.LabelsEntry\nNodeTaintSpecSpec\nNodenameSpec\nResources\nResources.LimitsEntry\nResources.RequestsEntry\nSchedulerConfigSpec\nSchedulerConfigSpec.EnvironmentVariablesEntry\nSchedulerConfigSpec.ExtraArgsEntry\nSecretsStatusSpec\nSingleManifest\nStaticPodServerStatusSpec\nStaticPodSpec\nStaticPodStatusSpec\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\nEndpointSpec\nIdentitySpec\nPeerSpecSpec\nPeerStatusSpec\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\nAddressStatusSpec\nBondMasterSpec\nBondSlave\nBridgeMasterSpec\nBridgeSlave\nDHCP4OperatorSpec\nDHCP6OperatorSpec\nHardwareAddrSpec\nHostnameSpecSpec\nHostnameStatusSpec\nLinkRefreshSpec\nLinkSpecSpec\nLinkStatusSpec\nNfTablesAddressMatch\nNfTablesChainSpec\nNfTablesClampMSS\nNfTablesConntrackStateMatch\nNfTablesIfNameMatch\nNfTablesLayer4Match\nNfTablesLimitMatch\nNfTablesMark\nNfTablesPortMatch\nNfTablesRule\nNodeAddressFilterSpec\nNodeAddressSpec\nOperatorSpecSpec\nPortRange\nProbeSpecSpec\nProbeStatusSpec\nResolverSpecSpec\nResolverStatusSpec\nRouteSpecSpec\nRouteStatusSpec\nSTPSpec\nStatusSpec\nTCPProbeSpec\nTimeServerSpecSpec\nTimeServerStatusSpec\nVIPEquinixMetalSpec\nVIPHCloudSpec\nVIPOperatorSpec\nVLANSpec\nWireguardPeer\nWireguardSpec\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\nCPUStat\nMemorySpec\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\nMount\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\nEventSinkConfigSpec\nKernelModuleSpecSpec\nKernelParamSpecSpec\nKernelParamStatusSpec\nKmsgLogConfigSpec\nMachineStatusSpec\nMachineStatusStatus\nMaintenanceServiceConfigSpec\nMetaKeySpec\nMetaLoadedSpec\nMountStatusSpec\nPlatformMetadataSpec\nSecurityStateSpec\nUniqueMachineTokenSpec\nUnmetCondition\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\nCertSANSpec\nEtcdCertsSpec\nEtcdRootSpec\nKubeletSpec\nKubernetesCertsSpec\nKubernetesDynamicCertsSpec\nKubernetesRootSpec\nMaintenanceRootSpec\nMaintenanceServiceCertsSpec\nOSRootSpec\nTrustdCertsSpec\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\nStatusSpec\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\ninspect/inspect.proto\n\nControllerDependencyEdge\n\nControllerRuntimeDependenciesResponse\n\nControllerRuntimeDependency\n\nDependencyEdgeType\n\nInspectService\n\nmachine/machine.proto\n\nAddressEvent\n\nApplyConfiguration\n\nApplyConfigurationRequest\n\nApplyConfigurationResponse\n\nBPFInstruction\n\nBootstrap\n\nBootstrapRequest\n\nBootstrapResponse\n\nCNIConfig\n\nCPUInfo\n\nCPUInfoResponse\n\nCPUStat\n\nCPUsInfo\n\nClusterConfig\n\nClusterNetworkConfig\n\nConfigLoadErrorEvent\n\nConfigValidationErrorEvent\n\nConnectRecord\n\nConnectRecord.Process\n\nContainer\n\nContainerInfo\n\nContainersRequest\n\nContainersResponse\n\nControlPlaneConfig\n\nCopyRequest\n\nDHCPOptionsConfig\n\nDiskStat\n\nDiskStats\n\nDiskStatsResponse\n\nDiskUsageInfo\n\nDiskUsageRequest\n\nDmesgRequest\n\nEtcdAlarm\n\nEtcdAlarmDisarm\n\nEtcdAlarmDisarmResponse\n\nEtcdAlarmListResponse\n\nEtcdDefragment\n\nEtcdDefragmentResponse\n\nEtcdForfeitLeadership\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\n\nEtcdLeaveCluster\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\n\nEtcdMember\n\nEtcdMemberAlarm\n\nEtcdMemberListRequest\n\nEtcdMemberListResponse\n\nEtcdMemberStatus\n\nEtcdMembers\n\nEtcdRecover\n\nEtcdRecoverResponse\n\nEtcdRemoveMember\n\nEtcdRemoveMemberByID\n\nEtcdRemoveMemberByIDRequest\n\nEtcdRemoveMemberByIDResponse\n\nEtcdRemoveMemberRequest\n\nEtcdRemoveMemberResponse\n\nEtcdSnapshotRequest\n\nEtcdStatus\n\nEtcdStatusResponse\n\nEvent\n\nEventsRequest\n\nFeaturesInfo\n\nFileInfo\n\nGenerateClientConfiguration\n\nGenerateClientConfigurationRequest\n\nGenerateClientConfigurationResponse\n\nGenerateConfiguration\n\nGenerateConfigurationRequest\n\nGenerateConfigurationResponse\n\nHostname\n\nHostnameResponse\n\nImageListRequest\n\nImageListResponse\n\nImagePull\n\nImagePullRequest\n\nImagePullResponse\n\nInstallConfig\n\nListRequest\n\nLoadAvg\n\nLoadAvgResponse\n\nLogsRequest\n\nMachineConfig\n\nMachineStatusEvent\n\nMachineStatusEvent.MachineStatus\n\nMachineStatusEvent.MachineStatus.UnmetCondition\n\nMemInfo\n\nMemory\n\nMemoryResponse\n\nMetaDelete\n\nMetaDeleteRequest\n\nMetaDeleteResponse\n\nMetaWrite\n\nMetaWriteRequest\n\nMetaWriteResponse\n\nMountStat\n\nMounts\n\nMountsResponse\n\nNetDev\n\nNetstat\n\nNetstatRequest\n\nNetstatRequest.Feature\n\nNetstatRequest.L4proto\n\nNetstatRequest.NetNS\n\nNetstatResponse\n\nNetworkConfig\n\nNetworkDeviceConfig\n\nNetworkDeviceStats\n\nNetworkDeviceStatsResponse\n\nPacketCaptureRequest\n\nPhaseEvent\n\nPlatformInfo\n\nProcess\n\nProcessInfo\n\nProcessesResponse\n\nReadRequest\n\nReboot\n\nRebootRequest\n\nRebootResponse\n\nReset\n\nResetPartitionSpec\n\nResetRequest\n\nResetResponse\n\nRestart\n\nRestartEvent\n\nRestartRequest\n\nRestartResponse\n\nRollback\n\nRollbackRequest\n\nRollbackResponse\n\nRouteConfig\n\nSequenceEvent\n\nServiceEvent\n\nServiceEvents\n\nServiceHealth\n\nServiceInfo\n\nServiceList\n\nServiceListResponse\n\nServiceRestart\n\nServiceRestartRequest\n\nServiceRestartResponse\n\nServiceStart\n\nServiceStartRequest\n\nServiceStartResponse\n\nServiceStateEvent\n\nServiceStop\n\nServiceStopRequest\n\nServiceStopResponse\n\nShutdown\n\nShutdownRequest\n\nShutdownResponse\n\nSoftIRQStat\n\nStat\n\nStats\n\nStatsRequest\n\nStatsResponse\n\nSystemStat\n\nSystemStatResponse\n\nTaskEvent\n\nUpgrade\n\nUpgradeRequest\n\nUpgradeResponse\n\nVersion\n\nVersionInfo\n\nVersionResponse\n\nApplyConfigurationRequest.Mode\n\nConnectRecord.State\n\nConnectRecord.TimerActive\n\nEtcdMemberAlarm.AlarmType\n\nListRequest.Type\n\nMachineConfig.MachineType\n\nMachineStatusEvent.MachineStage\n\nNetstatRequest.Filter\n\nPhaseEvent.Action\n\nRebootRequest.Mode\n\nResetRequest.WipeMode\n\nSequenceEvent.Action\n\nServiceStateEvent.Action\n\nTaskEvent.Action\n\nUpgradeRequest.RebootMode\n\nMachineService\n\nsecurity/security.proto\n\nCertificateRequest\n\nCertificateResponse\n\nSecurityService\n\nstorage/storage.proto\n\nDisk\n\nDisks\n\nDisksResponse\n\nDisk.DiskType\n\nStorageService\n\ntime/time.proto\n\nTime\n\nTimeRequest\n\nTimeResponse\n\nTimeService\n\nScalar Value Types\n\nTop\n\ncommon/common.proto\n\nData\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\nbytes\tbytes\t\t\n\nDataResponse\nField\tType\tLabel\tDescription\nmessages\tData\trepeated\t\n\nEmpty\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\n\nEmptyResponse\nField\tType\tLabel\tDescription\nmessages\tEmpty\trepeated\t\n\nError\nField\tType\tLabel\tDescription\ncode\tCode\t\t\nmessage\tstring\t\t\ndetails\tgoogle.protobuf.Any\trepeated\t\n\nMetadata\n\nCommon metadata message nested in all reply message types\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\thostname of the server response comes from (injected by proxy)\nerror\tstring\t\terror is set if request failed to the upstream (rest of response is undefined)\nstatus\tgoogle.rpc.Status\t\terror as gRPC Status\n\nNetIP\nField\tType\tLabel\tDescription\nip\tbytes\t\t\n\nNetIPPort\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nport\tint32\t\t\n\nNetIPPrefix\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nprefix_length\tint32\t\t\n\nPEMEncodedCertificateAndKey\nField\tType\tLabel\tDescription\ncrt\tbytes\t\t\nkey\tbytes\t\t\n\nPEMEncodedKey\nField\tType\tLabel\tDescription\nkey\tbytes\t\t\n\nURL\nField\tType\tLabel\tDescription\nfull_path\tstring\t\t\n\nCode\nName\tNumber\tDescription\nFATAL\t0\t\nLOCKED\t1\t\nCANCELED\t2\t\n\nContainerDriver\nName\tNumber\tDescription\nCONTAINERD\t0\t\nCRI\t1\t\n\nContainerdNamespace\nName\tNumber\tDescription\nNS_UNKNOWN\t0\t\nNS_SYSTEM\t1\t\nNS_CRI\t2\t\n\nFile-level Extensions\nExtension\tType\tBase\tNumber\tDescription\nremove_deprecated_enum\tstring\t.google.protobuf.EnumOptions\t93117\tIndicates the Talos version when this deprecated enum will be removed from API.\nremove_deprecated_enum_value\tstring\t.google.protobuf.EnumValueOptions\t93117\tIndicates the Talos version when this deprecated enum value will be removed from API.\nremove_deprecated_field\tstring\t.google.protobuf.FieldOptions\t93117\tIndicates the Talos version when this deprecated filed will be removed from API.\nremove_deprecated_message\tstring\t.google.protobuf.MessageOptions\t93117\tIndicates the Talos version when this deprecated message will be removed from API.\nremove_deprecated_method\tstring\t.google.protobuf.MethodOptions\t93117\tIndicates the Talos version when this deprecated method will be removed from API.\nremove_deprecated_service\tstring\t.google.protobuf.ServiceOptions\t93117\tIndicates the Talos version when this deprecated service will be removed from API.\n\nTop\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\n\nAffiliateSpec describes Affiliate state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nnodename\tstring\t\t\noperating_system\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\nkube_span\tKubeSpanAffiliateSpec\t\t\ncontrol_plane\tControlPlane\t\t\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration.\n\nField\tType\tLabel\tDescription\ndiscovery_enabled\tbool\t\t\nregistry_kubernetes_enabled\tbool\t\t\nregistry_service_enabled\tbool\t\t\nservice_endpoint\tstring\t\t\nservice_endpoint_insecure\tbool\t\t\nservice_encryption_key\tbytes\t\t\nservice_cluster_id\tstring\t\t\n\nControlPlane\n\nControlPlane describes ControlPlane data if any.\n\nField\tType\tLabel\tDescription\napi_server_port\tint64\t\t\n\nIdentitySpec\n\nIdentitySpec describes status of rendered secrets.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\n\nInfoSpec\n\nInfoSpec describes cluster information.\n\nField\tType\tLabel\tDescription\ncluster_id\tstring\t\t\ncluster_name\tstring\t\t\n\nKubeSpanAffiliateSpec\n\nKubeSpanAffiliateSpec describes additional information specific for the KubeSpan.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\naddress\tcommon.NetIP\t\t\nadditional_addresses\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\n\nMemberSpec\n\nMemberSpec describes Member state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\noperating_system\tstring\t\t\ncontrol_plane\tControlPlane\t\t\n\nTop\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nSeccompProfileSpec represents the SeccompProfile.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nvalue\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\n\nKubespanPeerState is KubeSpan peer current state.\n\nName\tNumber\tDescription\nPEER_STATE_UNKNOWN\t0\t\nPEER_STATE_UP\t1\t\nPEER_STATE_DOWN\t2\t\n\nMachineType\n\nMachineType represents a machine type.\n\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\tTypeUnknown represents undefined node type, when there is no machine configuration yet.\nTYPE_INIT\t1\tTypeInit type designates the first control plane node to come up. You can think of it like a bootstrap node. This node will perform the initial steps to bootstrap the cluster – generation of TLS assets, starting of the control plane, etc.\nTYPE_CONTROL_PLANE\t2\tTypeControlPlane designates the node as a control plane member. This means it will host etcd along with the Kubernetes controlplane components such as API Server, Controller Manager, Scheduler.\nTYPE_WORKER\t3\tTypeWorker designates the node as a worker node. This means it will be an available compute node for scheduling workloads.\n\nNethelpersADSelect\n\nNethelpersADSelect is ADSelect.\n\nName\tNumber\tDescription\nAD_SELECT_STABLE\t0\t\nAD_SELECT_BANDWIDTH\t1\t\nAD_SELECT_COUNT\t2\t\n\nNethelpersARPAllTargets\n\nNethelpersARPAllTargets is an ARP targets mode.\n\nName\tNumber\tDescription\nARP_ALL_TARGETS_ANY\t0\t\nARP_ALL_TARGETS_ALL\t1\t\n\nNethelpersARPValidate\n\nNethelpersARPValidate is an ARP Validation mode.\n\nName\tNumber\tDescription\nARP_VALIDATE_NONE\t0\t\nARP_VALIDATE_ACTIVE\t1\t\nARP_VALIDATE_BACKUP\t2\t\nARP_VALIDATE_ALL\t3\t\n\nNethelpersAddressFlag\n\nNethelpersAddressFlag wraps IFF_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ADDRESSFLAG_UNSPECIFIED\t0\t\nADDRESS_TEMPORARY\t1\t\nADDRESS_NO_DAD\t2\t\nADDRESS_OPTIMISTIC\t4\t\nADDRESS_DAD_FAILED\t8\t\nADDRESS_HOME\t16\t\nADDRESS_DEPRECATED\t32\t\nADDRESS_TENTATIVE\t64\t\nADDRESS_PERMANENT\t128\t\nADDRESS_MANAGEMENT_TEMP\t256\t\nADDRESS_NO_PREFIX_ROUTE\t512\t\nADDRESS_MC_AUTO_JOIN\t1024\t\nADDRESS_STABLE_PRIVACY\t2048\t\n\nNethelpersBondMode\n\nNethelpersBondMode is a bond mode.\n\nName\tNumber\tDescription\nBOND_MODE_ROUNDROBIN\t0\t\nBOND_MODE_ACTIVE_BACKUP\t1\t\nBOND_MODE_XOR\t2\t\nBOND_MODE_BROADCAST\t3\t\nBOND_MODE8023_AD\t4\t\nBOND_MODE_TLB\t5\t\nBOND_MODE_ALB\t6\t\n\nNethelpersBondXmitHashPolicy\n\nNethelpersBondXmitHashPolicy is a bond hash policy.\n\nName\tNumber\tDescription\nBOND_XMIT_POLICY_LAYER2\t0\t\nBOND_XMIT_POLICY_LAYER34\t1\t\nBOND_XMIT_POLICY_LAYER23\t2\t\nBOND_XMIT_POLICY_ENCAP23\t3\t\nBOND_XMIT_POLICY_ENCAP34\t4\t\n\nNethelpersConntrackState\n\nNethelpersConntrackState is a conntrack state.\n\nName\tNumber\tDescription\nNETHELPERS_CONNTRACKSTATE_UNSPECIFIED\t0\t\nCONNTRACK_STATE_NEW\t8\t\nCONNTRACK_STATE_RELATED\t4\t\nCONNTRACK_STATE_ESTABLISHED\t2\t\nCONNTRACK_STATE_INVALID\t1\t\n\nNethelpersDuplex\n\nNethelpersDuplex wraps ethtool.Duplex for YAML marshaling.\n\nName\tNumber\tDescription\nHALF\t0\t\nFULL\t1\t\nUNKNOWN\t255\t\n\nNethelpersFailOverMAC\n\nNethelpersFailOverMAC is a MAC failover mode.\n\nName\tNumber\tDescription\nFAIL_OVER_MAC_NONE\t0\t\nFAIL_OVER_MAC_ACTIVE\t1\t\nFAIL_OVER_MAC_FOLLOW\t2\t\n\nNethelpersFamily\n\nNethelpersFamily is a network family.\n\nName\tNumber\tDescription\nNETHELPERS_FAMILY_UNSPECIFIED\t0\t\nFAMILY_INET4\t2\t\nFAMILY_INET6\t10\t\n\nNethelpersLACPRate\n\nNethelpersLACPRate is a LACP rate.\n\nName\tNumber\tDescription\nLACP_RATE_SLOW\t0\t\nLACP_RATE_FAST\t1\t\n\nNethelpersLinkType\n\nNethelpersLinkType is a link type.\n\nName\tNumber\tDescription\nLINK_NETROM\t0\t\nLINK_ETHER\t1\t\nLINK_EETHER\t2\t\nLINK_AX25\t3\t\nLINK_PRONET\t4\t\nLINK_CHAOS\t5\t\nLINK_IEE802\t6\t\nLINK_ARCNET\t7\t\nLINK_ATALK\t8\t\nLINK_DLCI\t15\t\nLINK_ATM\t19\t\nLINK_METRICOM\t23\t\nLINK_IEEE1394\t24\t\nLINK_EUI64\t27\t\nLINK_INFINIBAND\t32\t\nLINK_SLIP\t256\t\nLINK_CSLIP\t257\t\nLINK_SLIP6\t258\t\nLINK_CSLIP6\t259\t\nLINK_RSRVD\t260\t\nLINK_ADAPT\t264\t\nLINK_ROSE\t270\t\nLINK_X25\t271\t\nLINK_HWX25\t272\t\nLINK_CAN\t280\t\nLINK_PPP\t512\t\nLINK_CISCO\t513\t\nLINK_HDLC\t513\t\nLINK_LAPB\t516\t\nLINK_DDCMP\t517\t\nLINK_RAWHDLC\t518\t\nLINK_TUNNEL\t768\t\nLINK_TUNNEL6\t769\t\nLINK_FRAD\t770\t\nLINK_SKIP\t771\t\nLINK_LOOPBCK\t772\t\nLINK_LOCALTLK\t773\t\nLINK_FDDI\t774\t\nLINK_BIF\t775\t\nLINK_SIT\t776\t\nLINK_IPDDP\t777\t\nLINK_IPGRE\t778\t\nLINK_PIMREG\t779\t\nLINK_HIPPI\t780\t\nLINK_ASH\t781\t\nLINK_ECONET\t782\t\nLINK_IRDA\t783\t\nLINK_FCPP\t784\t\nLINK_FCAL\t785\t\nLINK_FCPL\t786\t\nLINK_FCFABRIC\t787\t\nLINK_FCFABRIC1\t788\t\nLINK_FCFABRIC2\t789\t\nLINK_FCFABRIC3\t790\t\nLINK_FCFABRIC4\t791\t\nLINK_FCFABRIC5\t792\t\nLINK_FCFABRIC6\t793\t\nLINK_FCFABRIC7\t794\t\nLINK_FCFABRIC8\t795\t\nLINK_FCFABRIC9\t796\t\nLINK_FCFABRIC10\t797\t\nLINK_FCFABRIC11\t798\t\nLINK_FCFABRIC12\t799\t\nLINK_IEE802TR\t800\t\nLINK_IEE80211\t801\t\nLINK_IEE80211PRISM\t802\t\nLINK_IEE80211_RADIOTAP\t803\t\nLINK_IEE8021154\t804\t\nLINK_IEE8021154MONITOR\t805\t\nLINK_PHONET\t820\t\nLINK_PHONETPIPE\t821\t\nLINK_CAIF\t822\t\nLINK_IP6GRE\t823\t\nLINK_NETLINK\t824\t\nLINK6_LOWPAN\t825\t\nLINK_VOID\t65535\t\nLINK_NONE\t65534\t\n\nNethelpersMatchOperator\n\nNethelpersMatchOperator is a netfilter match operator.\n\nName\tNumber\tDescription\nOPERATOR_EQUAL\t0\t\nOPERATOR_NOT_EQUAL\t1\t\n\nNethelpersNfTablesChainHook\n\nNethelpersNfTablesChainHook wraps nftables.ChainHook for YAML marshaling.\n\nName\tNumber\tDescription\nCHAIN_HOOK_PREROUTING\t0\t\nCHAIN_HOOK_INPUT\t1\t\nCHAIN_HOOK_FORWARD\t2\t\nCHAIN_HOOK_OUTPUT\t3\t\nCHAIN_HOOK_POSTROUTING\t4\t\n\nNethelpersNfTablesChainPriority\n\nNethelpersNfTablesChainPriority wraps nftables.ChainPriority for YAML marshaling.\n\nName\tNumber\tDescription\nNETHELPERS_NFTABLESCHAINPRIORITY_UNSPECIFIED\t0\t\nCHAIN_PRIORITY_FIRST\t-2147483648\t\nCHAIN_PRIORITY_CONNTRACK_DEFRAG\t-400\t\nCHAIN_PRIORITY_RAW\t-300\t\nCHAIN_PRIORITY_SE_LINUX_FIRST\t-225\t\nCHAIN_PRIORITY_CONNTRACK\t-200\t\nCHAIN_PRIORITY_MANGLE\t-150\t\nCHAIN_PRIORITY_NAT_DEST\t-100\t\nCHAIN_PRIORITY_FILTER\t0\t\nCHAIN_PRIORITY_SECURITY\t50\t\nCHAIN_PRIORITY_NAT_SOURCE\t100\t\nCHAIN_PRIORITY_SE_LINUX_LAST\t225\t\nCHAIN_PRIORITY_CONNTRACK_HELPER\t300\t\nCHAIN_PRIORITY_LAST\t2147483647\t\n\nNethelpersNfTablesVerdict\n\nNethelpersNfTablesVerdict wraps nftables.Verdict for YAML marshaling.\n\nName\tNumber\tDescription\nVERDICT_DROP\t0\t\nVERDICT_ACCEPT\t1\t\n\nNethelpersOperationalState\n\nNethelpersOperationalState wraps rtnetlink.OperationalState for YAML marshaling.\n\nName\tNumber\tDescription\nOPER_STATE_UNKNOWN\t0\t\nOPER_STATE_NOT_PRESENT\t1\t\nOPER_STATE_DOWN\t2\t\nOPER_STATE_LOWER_LAYER_DOWN\t3\t\nOPER_STATE_TESTING\t4\t\nOPER_STATE_DORMANT\t5\t\nOPER_STATE_UP\t6\t\n\nNethelpersPort\n\nNethelpersPort wraps ethtool.Port for YAML marshaling.\n\nName\tNumber\tDescription\nTWISTED_PAIR\t0\t\nAUI\t1\t\nMII\t2\t\nFIBRE\t3\t\nBNC\t4\t\nDIRECT_ATTACH\t5\t\nNONE\t239\t\nOTHER\t255\t\n\nNethelpersPrimaryReselect\n\nNethelpersPrimaryReselect is an ARP targets mode.\n\nName\tNumber\tDescription\nPRIMARY_RESELECT_ALWAYS\t0\t\nPRIMARY_RESELECT_BETTER\t1\t\nPRIMARY_RESELECT_FAILURE\t2\t\n\nNethelpersProtocol\n\nNethelpersProtocol is a inet protocol.\n\nName\tNumber\tDescription\nNETHELPERS_PROTOCOL_UNSPECIFIED\t0\t\nPROTOCOL_ICMP\t1\t\nPROTOCOL_TCP\t6\t\nPROTOCOL_UDP\t17\t\nPROTOCOL_ICM_PV6\t58\t\n\nNethelpersRouteFlag\n\nNethelpersRouteFlag wraps RTM_F_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ROUTEFLAG_UNSPECIFIED\t0\t\nROUTE_NOTIFY\t256\t\nROUTE_CLONED\t512\t\nROUTE_EQUALIZE\t1024\t\nROUTE_PREFIX\t2048\t\nROUTE_LOOKUP_TABLE\t4096\t\nROUTE_FIB_MATCH\t8192\t\nROUTE_OFFLOAD\t16384\t\nROUTE_TRAP\t32768\t\n\nNethelpersRouteProtocol\n\nNethelpersRouteProtocol is a routing protocol.\n\nName\tNumber\tDescription\nPROTOCOL_UNSPEC\t0\t\nPROTOCOL_REDIRECT\t1\t\nPROTOCOL_KERNEL\t2\t\nPROTOCOL_BOOT\t3\t\nPROTOCOL_STATIC\t4\t\nPROTOCOL_RA\t9\t\nPROTOCOL_MRT\t10\t\nPROTOCOL_ZEBRA\t11\t\nPROTOCOL_BIRD\t12\t\nPROTOCOL_DNROUTED\t13\t\nPROTOCOL_XORP\t14\t\nPROTOCOL_NTK\t15\t\nPROTOCOL_DHCP\t16\t\nPROTOCOL_MRTD\t17\t\nPROTOCOL_KEEPALIVED\t18\t\nPROTOCOL_BABEL\t42\t\nPROTOCOL_OPENR\t99\t\nPROTOCOL_BGP\t186\t\nPROTOCOL_ISIS\t187\t\nPROTOCOL_OSPF\t188\t\nPROTOCOL_RIP\t189\t\nPROTOCOL_EIGRP\t192\t\n\nNethelpersRouteType\n\nNethelpersRouteType is a route type.\n\nName\tNumber\tDescription\nTYPE_UNSPEC\t0\t\nTYPE_UNICAST\t1\t\nTYPE_LOCAL\t2\t\nTYPE_BROADCAST\t3\t\nTYPE_ANYCAST\t4\t\nTYPE_MULTICAST\t5\t\nTYPE_BLACKHOLE\t6\t\nTYPE_UNREACHABLE\t7\t\nTYPE_PROHIBIT\t8\t\nTYPE_THROW\t9\t\nTYPE_NAT\t10\t\nTYPE_X_RESOLVE\t11\t\n\nNethelpersRoutingTable\n\nNethelpersRoutingTable is a routing table ID.\n\nName\tNumber\tDescription\nTABLE_UNSPEC\t0\t\nTABLE_DEFAULT\t253\t\nTABLE_MAIN\t254\t\nTABLE_LOCAL\t255\t\n\nNethelpersScope\n\nNethelpersScope is an address scope.\n\nName\tNumber\tDescription\nSCOPE_GLOBAL\t0\t\nSCOPE_SITE\t200\t\nSCOPE_LINK\t253\t\nSCOPE_HOST\t254\t\nSCOPE_NOWHERE\t255\t\n\nNethelpersVLANProtocol\n\nNethelpersVLANProtocol is a VLAN protocol.\n\nName\tNumber\tDescription\nNETHELPERS_VLANPROTOCOL_UNSPECIFIED\t0\t\nVLAN_PROTOCOL8021_Q\t33024\t\nVLAN_PROTOCOL8021_AD\t34984\t\n\nNetworkConfigLayer\n\nNetworkConfigLayer describes network configuration layers, with lowest priority first.\n\nName\tNumber\tDescription\nCONFIG_DEFAULT\t0\t\nCONFIG_CMDLINE\t1\t\nCONFIG_PLATFORM\t2\t\nCONFIG_OPERATOR\t3\t\nCONFIG_MACHINE_CONFIGURATION\t4\t\n\nNetworkOperator\n\nNetworkOperator enumerates Talos network operators.\n\nName\tNumber\tDescription\nOPERATOR_DHCP4\t0\t\nOPERATOR_DHCP6\t1\t\nOPERATOR_VIP\t2\t\n\nRuntimeMachineStage\n\nRuntimeMachineStage describes the stage of the machine boot/run process.\n\nName\tNumber\tDescription\nMACHINE_STAGE_UNKNOWN\t0\t\nMACHINE_STAGE_BOOTING\t1\t\nMACHINE_STAGE_INSTALLING\t2\t\nMACHINE_STAGE_MAINTENANCE\t3\t\nMACHINE_STAGE_RUNNING\t4\t\nMACHINE_STAGE_REBOOTING\t5\t\nMACHINE_STAGE_SHUTTING_DOWN\t6\t\nMACHINE_STAGE_RESETTING\t7\t\nMACHINE_STAGE_UPGRADING\t8\t\n\nTop\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\n\nConfigSpec describes (some) configuration settings of etcd.\n\nField\tType\tLabel\tDescription\nadvertise_valid_subnets\tstring\trepeated\t\nadvertise_exclude_subnets\tstring\trepeated\t\nimage\tstring\t\t\nextra_args\tConfigSpec.ExtraArgsEntry\trepeated\t\nlisten_valid_subnets\tstring\trepeated\t\nlisten_exclude_subnets\tstring\trepeated\t\n\nConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nMemberSpec\n\nMemberSpec holds information about an etcd member.\n\nField\tType\tLabel\tDescription\nmember_id\tstring\t\t\n\nPKIStatusSpec\n\nPKIStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSpecSpec\n\nSpecSpec describes (some) Specuration settings of etcd.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nadvertised_addresses\tcommon.NetIP\trepeated\t\nimage\tstring\t\t\nextra_args\tSpecSpec.ExtraArgsEntry\trepeated\t\nlisten_peer_addresses\tcommon.NetIP\trepeated\t\nlisten_client_addresses\tcommon.NetIP\trepeated\t\n\nSpecSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nTop\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\n\nCompatibility describes extension compatibility.\n\nField\tType\tLabel\tDescription\ntalos\tConstraint\t\t\n\nConstraint\n\nConstraint describes compatibility constraint.\n\nField\tType\tLabel\tDescription\nversion\tstring\t\t\n\nLayer\n\nLayer defines overlay mount layer.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nmetadata\tMetadata\t\t\n\nMetadata\n\nMetadata describes base extension metadata.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nversion\tstring\t\t\nauthor\tstring\t\t\ndescription\tstring\t\t\ncompatibility\tCompatibility\t\t\n\nTop\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\n\nEtcFileSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ncontents\tbytes\t\t\nmode\tuint32\t\t\n\nEtcFileStatusSpec\n\nEtcFileStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nspec_version\tstring\t\t\n\nTop\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\n\nMemoryModuleSpec represents a single Memory.\n\nField\tType\tLabel\tDescription\nsize\tuint32\t\t\ndevice_locator\tstring\t\t\nbank_locator\tstring\t\t\nspeed\tuint32\t\t\nmanufacturer\tstring\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\nproduct_name\tstring\t\t\n\nProcessorSpec\n\nProcessorSpec represents a single processor.\n\nField\tType\tLabel\tDescription\nsocket\tstring\t\t\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nmax_speed\tuint32\t\t\nboot_speed\tuint32\t\t\nstatus\tuint32\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\npart_number\tstring\t\t\ncore_count\tuint32\t\t\ncore_enabled\tuint32\t\t\nthread_count\tuint32\t\t\n\nSystemInformationSpec\n\nSystemInformationSpec represents the system information obtained from smbios.\n\nField\tType\tLabel\tDescription\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nversion\tstring\t\t\nserial_number\tstring\t\t\nuuid\tstring\t\t\nwake_up_type\tstring\t\t\nsku_number\tstring\t\t\n\nTop\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\n\nAPIServerConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncloud_provider\tstring\t\t\ncontrol_plane_endpoint\tstring\t\t\netcd_servers\tstring\trepeated\t\nlocal_port\tint64\t\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tAPIServerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tAPIServerConfigSpec.EnvironmentVariablesEntry\trepeated\t\npod_security_policy_enabled\tbool\t\t\nadvertised_address\tstring\t\t\nresources\tResources\t\t\n\nAPIServerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAPIServerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAdmissionControlConfigSpec\n\nAdmissionControlConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tAdmissionPluginSpec\trepeated\t\n\nAdmissionPluginSpec\n\nAdmissionPluginSpec is a single admission plugin configuration Admission Control plugins.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nconfiguration\tgoogle.protobuf.Struct\t\t\n\nAuditPolicyConfigSpec\n\nAuditPolicyConfigSpec is audit policy configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tgoogle.protobuf.Struct\t\t\n\nBootstrapManifestsConfigSpec\n\nBootstrapManifestsConfigSpec is configuration for bootstrap manifests.\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\ncluster_domain\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nproxy_enabled\tbool\t\t\nproxy_image\tstring\t\t\nproxy_args\tstring\trepeated\t\ncore_dns_enabled\tbool\t\t\ncore_dns_image\tstring\t\t\ndns_service_ip\tstring\t\t\ndns_service_i_pv6\tstring\t\t\nflannel_enabled\tbool\t\t\nflannel_image\tstring\t\t\nflannel_cni_image\tstring\t\t\npod_security_policy_enabled\tbool\t\t\ntalos_api_service_enabled\tbool\t\t\nflannel_extra_args\tstring\trepeated\t\n\nConfigStatusSpec\n\nConfigStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nControllerManagerConfigSpec\n\nControllerManagerConfigSpec is configuration for kube-controller-manager.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\ncloud_provider\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tControllerManagerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tControllerManagerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\n\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nControllerManagerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nEndpointSpec\n\nEndpointSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nExtraManifest\n\nExtraManifest defines a single extra manifest to download.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurl\tstring\t\t\npriority\tstring\t\t\nextra_headers\tExtraManifest.ExtraHeadersEntry\trepeated\t\ninline_manifest\tstring\t\t\n\nExtraManifest.ExtraHeadersEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nExtraManifestsConfigSpec\n\nExtraManifestsConfigSpec is configuration for extra bootstrap manifests.\n\nField\tType\tLabel\tDescription\nextra_manifests\tExtraManifest\trepeated\t\n\nExtraVolume\n\nExtraVolume is a configuration of extra volume.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhost_path\tstring\t\t\nmount_path\tstring\t\t\nread_only\tbool\t\t\n\nKubePrismConfigSpec\n\nKubePrismConfigSpec describes KubePrismConfig data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tint64\t\t\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismEndpoint\n\nKubePrismEndpoint holds data for control plane endpoint.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tuint32\t\t\n\nKubePrismEndpointsSpec\n\nKubePrismEndpointsSpec describes KubePrismEndpoints configuration.\n\nField\tType\tLabel\tDescription\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismStatusesSpec\n\nKubePrismStatusesSpec describes KubePrismStatuses data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nhealthy\tbool\t\t\n\nKubeletConfigSpec\n\nKubeletConfigSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncluster_dns\tstring\trepeated\t\ncluster_domain\tstring\t\t\nextra_args\tKubeletConfigSpec.ExtraArgsEntry\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nextra_config\tgoogle.protobuf.Struct\t\t\ncloud_provider_external\tbool\t\t\ndefault_runtime_seccomp_enabled\tbool\t\t\nskip_node_registration\tbool\t\t\nstatic_pod_list_url\tstring\t\t\ndisable_manifests_directory\tbool\t\t\nenable_fs_quota_monitoring\tbool\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nKubeletConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nKubeletSpecSpec\n\nKubeletSpecSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nargs\tstring\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nexpected_nodename\tstring\t\t\nconfig\tgoogle.protobuf.Struct\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nManifestSpec\n\nManifestSpec holds the Kubernetes resources spec.\n\nField\tType\tLabel\tDescription\nitems\tSingleManifest\trepeated\t\n\nManifestStatusSpec\n\nManifestStatusSpec describes manifest application status.\n\nField\tType\tLabel\tDescription\nmanifests_applied\tstring\trepeated\t\n\nNodeIPConfigSpec\n\nNodeIPConfigSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\nvalid_subnets\tstring\trepeated\t\nexclude_subnets\tstring\trepeated\t\n\nNodeIPSpec\n\nNodeIPSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nNodeLabelSpecSpec\n\nNodeLabelSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec\n\nNodeStatusSpec describes Kubernetes NodeStatus.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nnode_ready\tbool\t\t\nunschedulable\tbool\t\t\nlabels\tNodeStatusSpec.LabelsEntry\trepeated\t\nannotations\tNodeStatusSpec.AnnotationsEntry\trepeated\t\n\nNodeStatusSpec.AnnotationsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec.LabelsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeTaintSpecSpec\n\nNodeTaintSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\neffect\tstring\t\t\nvalue\tstring\t\t\n\nNodenameSpec\n\nNodenameSpec describes Kubernetes nodename.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nhostname_version\tstring\t\t\nskip_node_registration\tbool\t\t\n\nResources\n\nResources is a configuration of cpu and memory resources.\n\nField\tType\tLabel\tDescription\nrequests\tResources.RequestsEntry\trepeated\t\nlimits\tResources.LimitsEntry\trepeated\t\n\nResources.LimitsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nResources.RequestsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec\n\nSchedulerConfigSpec is configuration for kube-scheduler.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\nextra_args\tSchedulerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tSchedulerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\nconfig\tgoogle.protobuf.Struct\t\t\n\nSchedulerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSecretsStatusSpec\n\nSecretsStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSingleManifest\n\nSingleManifest is a single manifest.\n\nField\tType\tLabel\tDescription\nobject\tgoogle.protobuf.Struct\t\t\n\nStaticPodServerStatusSpec\n\nStaticPodServerStatusSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\nurl\tstring\t\t\n\nStaticPodSpec\n\nStaticPodSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\npod\tgoogle.protobuf.Struct\t\t\n\nStaticPodStatusSpec\n\nStaticPodStatusSpec describes kubelet static pod status.\n\nField\tType\tLabel\tDescription\npod_status\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nallowed_api_roles\tstring\trepeated\t\nallowed_kubernetes_namespaces\tstring\trepeated\t\n\nTop\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\ncluster_id\tstring\t\t\nshared_secret\tstring\t\t\nforce_routing\tbool\t\t\nadvertise_kubernetes_networks\tbool\t\t\nmtu\tuint32\t\t\nendpoint_filters\tstring\trepeated\t\nharvest_extra_endpoints\tbool\t\t\n\nEndpointSpec\n\nEndpointSpec describes Endpoint state.\n\nField\tType\tLabel\tDescription\naffiliate_id\tstring\t\t\nendpoint\tcommon.NetIPPort\t\t\n\nIdentitySpec\n\nIdentitySpec describes KubeSpan keys and address.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nsubnet\tcommon.NetIPPrefix\t\t\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\n\nPeerSpecSpec\n\nPeerSpecSpec describes PeerSpec state.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIP\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\nlabel\tstring\t\t\n\nPeerStatusSpec\n\nPeerStatusSpec describes PeerStatus state.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.NetIPPort\t\t\nlabel\tstring\t\t\nstate\ttalos.resource.definitions.enums.KubespanPeerState\t\t\nreceive_bytes\tint64\t\t\ntransmit_bytes\tint64\t\t\nlast_handshake_time\tgoogle.protobuf.Timestamp\t\t\nlast_used_endpoint\tcommon.NetIPPort\t\t\nlast_endpoint_change\tgoogle.protobuf.Timestamp\t\t\n\nTop\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\n\nAddressSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\nannounce_with_arp\tbool\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nAddressStatusSpec\n\nAddressStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlocal\tcommon.NetIP\t\t\nbroadcast\tcommon.NetIP\t\t\nanycast\tcommon.NetIP\t\t\nmulticast\tcommon.NetIP\t\t\nlink_index\tuint32\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\n\nBondMasterSpec\n\nBondMasterSpec describes bond settings if Kind == “bond”.\n\nField\tType\tLabel\tDescription\nmode\ttalos.resource.definitions.enums.NethelpersBondMode\t\t\nhash_policy\ttalos.resource.definitions.enums.NethelpersBondXmitHashPolicy\t\t\nlacp_rate\ttalos.resource.definitions.enums.NethelpersLACPRate\t\t\narp_validate\ttalos.resource.definitions.enums.NethelpersARPValidate\t\t\narp_all_targets\ttalos.resource.definitions.enums.NethelpersARPAllTargets\t\t\nprimary_index\tuint32\t\t\nprimary_reselect\ttalos.resource.definitions.enums.NethelpersPrimaryReselect\t\t\nfail_over_mac\ttalos.resource.definitions.enums.NethelpersFailOverMAC\t\t\nad_select\ttalos.resource.definitions.enums.NethelpersADSelect\t\t\nmii_mon\tuint32\t\t\nup_delay\tuint32\t\t\ndown_delay\tuint32\t\t\narp_interval\tuint32\t\t\nresend_igmp\tuint32\t\t\nmin_links\tuint32\t\t\nlp_interval\tuint32\t\t\npackets_per_slave\tuint32\t\t\nnum_peer_notif\tfixed32\t\t\ntlb_dynamic_lb\tfixed32\t\t\nall_slaves_active\tfixed32\t\t\nuse_carrier\tbool\t\t\nad_actor_sys_prio\tfixed32\t\t\nad_user_port_key\tfixed32\t\t\npeer_notify_delay\tuint32\t\t\n\nBondSlave\n\nBondSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\nslave_index\tint64\t\t\n\nBridgeMasterSpec\n\nBridgeMasterSpec describes bridge settings if Kind == “bridge”.\n\nField\tType\tLabel\tDescription\nstp\tSTPSpec\t\t\n\nBridgeSlave\n\nBridgeSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\n\nDHCP4OperatorSpec\n\nDHCP4OperatorSpec describes DHCP4 operator options.\n\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nDHCP6OperatorSpec\n\nDHCP6OperatorSpec describes DHCP6 operator options.\n\nField\tType\tLabel\tDescription\nduid\tstring\t\t\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nHardwareAddrSpec\n\nHardwareAddrSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhardware_addr\tbytes\t\t\n\nHostnameSpecSpec\n\nHostnameSpecSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nHostnameStatusSpec\n\nHostnameStatusSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\n\nLinkRefreshSpec\n\nLinkRefreshSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ngeneration\tint64\t\t\n\nLinkSpecSpec\n\nLinkSpecSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nlogical\tbool\t\t\nup\tbool\t\t\nmtu\tuint32\t\t\nkind\tstring\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nparent_name\tstring\t\t\nbond_slave\tBondSlave\t\t\nbridge_slave\tBridgeSlave\t\t\nvlan\tVLANSpec\t\t\nbond_master\tBondMasterSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nLinkStatusSpec\n\nLinkStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nindex\tuint32\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nlink_index\tuint32\t\t\nflags\tuint32\t\t\nhardware_addr\tbytes\t\t\nbroadcast_addr\tbytes\t\t\nmtu\tuint32\t\t\nqueue_disc\tstring\t\t\nmaster_index\tuint32\t\t\noperational_state\ttalos.resource.definitions.enums.NethelpersOperationalState\t\t\nkind\tstring\t\t\nslave_kind\tstring\t\t\nbus_path\tstring\t\t\npciid\tstring\t\t\ndriver\tstring\t\t\ndriver_version\tstring\t\t\nfirmware_version\tstring\t\t\nproduct_id\tstring\t\t\nvendor_id\tstring\t\t\nproduct\tstring\t\t\nvendor\tstring\t\t\nlink_state\tbool\t\t\nspeed_megabits\tint64\t\t\nport\ttalos.resource.definitions.enums.NethelpersPort\t\t\nduplex\ttalos.resource.definitions.enums.NethelpersDuplex\t\t\nvlan\tVLANSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nbond_master\tBondMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\npermanent_addr\tbytes\t\t\n\nNfTablesAddressMatch\n\nNfTablesAddressMatch describes the match on the IP address.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\ninvert\tbool\t\t\n\nNfTablesChainSpec\n\nNfTablesChainSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ntype\tstring\t\t\nhook\ttalos.resource.definitions.enums.NethelpersNfTablesChainHook\t\t\npriority\ttalos.resource.definitions.enums.NethelpersNfTablesChainPriority\t\t\nrules\tNfTablesRule\trepeated\t\npolicy\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\n\nNfTablesClampMSS\n\nNfTablesClampMSS describes the TCP MSS clamping operation.\n\nMSS is limited by the MaxMTU so that:\n\nIPv4: MSS = MaxMTU - 40\nIPv6: MSS = MaxMTU - 60.\nField\tType\tLabel\tDescription\nmtu\tfixed32\t\t\n\nNfTablesConntrackStateMatch\n\nNfTablesConntrackStateMatch describes the match on the connection tracking state.\n\nField\tType\tLabel\tDescription\nstates\ttalos.resource.definitions.enums.NethelpersConntrackState\trepeated\t\n\nNfTablesIfNameMatch\n\nNfTablesIfNameMatch describes the match on the interface name.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NethelpersMatchOperator\t\t\ninterface_names\tstring\trepeated\t\n\nNfTablesLayer4Match\n\nNfTablesLayer4Match describes the match on the transport layer protocol.\n\nField\tType\tLabel\tDescription\nprotocol\ttalos.resource.definitions.enums.NethelpersProtocol\t\t\nmatch_source_port\tNfTablesPortMatch\t\t\nmatch_destination_port\tNfTablesPortMatch\t\t\n\nNfTablesLimitMatch\n\nNfTablesLimitMatch describes the match on the packet rate.\n\nField\tType\tLabel\tDescription\npacket_rate_per_second\tuint64\t\t\n\nNfTablesMark\n\nNfTablesMark encodes packet mark match/update operation.\n\nWhen used as a match computes the following condition: (mark & mask) ^ xor == value\n\nWhen used as an update computes the following operation: mark = (mark & mask) ^ xor.\n\nField\tType\tLabel\tDescription\nmask\tuint32\t\t\nxor\tuint32\t\t\nvalue\tuint32\t\t\n\nNfTablesPortMatch\n\nNfTablesPortMatch describes the match on the transport layer port.\n\nField\tType\tLabel\tDescription\nranges\tPortRange\trepeated\t\n\nNfTablesRule\n\nNfTablesRule describes a single rule in the nftables chain.\n\nField\tType\tLabel\tDescription\nmatch_o_if_name\tNfTablesIfNameMatch\t\t\nverdict\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\nmatch_mark\tNfTablesMark\t\t\nset_mark\tNfTablesMark\t\t\nmatch_source_address\tNfTablesAddressMatch\t\t\nmatch_destination_address\tNfTablesAddressMatch\t\t\nmatch_layer4\tNfTablesLayer4Match\t\t\nmatch_i_if_name\tNfTablesIfNameMatch\t\t\nclamp_mss\tNfTablesClampMSS\t\t\nmatch_limit\tNfTablesLimitMatch\t\t\nmatch_conntrack_state\tNfTablesConntrackStateMatch\t\t\nanon_counter\tbool\t\t\n\nNodeAddressFilterSpec\n\nNodeAddressFilterSpec describes a filter for NodeAddresses.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\n\nNodeAddressSpec\n\nNodeAddressSpec describes a set of node addresses.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIPPrefix\trepeated\t\n\nOperatorSpecSpec\n\nOperatorSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NetworkOperator\t\t\nlink_name\tstring\t\t\nrequire_up\tbool\t\t\ndhcp4\tDHCP4OperatorSpec\t\t\ndhcp6\tDHCP6OperatorSpec\t\t\nvip\tVIPOperatorSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nPortRange\n\nPortRange describes a range of ports.\n\nRange is [lo, hi].\n\nField\tType\tLabel\tDescription\nlo\tfixed32\t\t\nhi\tfixed32\t\t\n\nProbeSpecSpec\n\nProbeSpecSpec describes the Probe.\n\nField\tType\tLabel\tDescription\ninterval\tgoogle.protobuf.Duration\t\t\nfailure_threshold\tint64\t\t\ntcp\tTCPProbeSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nProbeStatusSpec\n\nProbeStatusSpec describes the Probe.\n\nField\tType\tLabel\tDescription\nsuccess\tbool\t\t\nlast_error\tstring\t\t\n\nResolverSpecSpec\n\nResolverSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nResolverStatusSpec\n\nResolverStatusSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\n\nRouteSpecSpec\n\nRouteSpecSpec describes the route.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\nmtu\tuint32\t\t\n\nRouteStatusSpec\n\nRouteStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_index\tuint32\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nmtu\tuint32\t\t\n\nSTPSpec\n\nSTPSpec describes Spanning Tree Protocol (STP) settings of a bridge.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\n\nStatusSpec\n\nStatusSpec describes network state.\n\nField\tType\tLabel\tDescription\naddress_ready\tbool\t\t\nconnectivity_ready\tbool\t\t\nhostname_ready\tbool\t\t\netc_files_ready\tbool\t\t\n\nTCPProbeSpec\n\nTCPProbeSpec describes the TCP Probe.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\ntimeout\tgoogle.protobuf.Duration\t\t\n\nTimeServerSpecSpec\n\nTimeServerSpecSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nTimeServerStatusSpec\n\nTimeServerStatusSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\n\nVIPEquinixMetalSpec\n\nVIPEquinixMetalSpec describes virtual (elastic) IP settings for Equinix Metal.\n\nField\tType\tLabel\tDescription\nproject_id\tstring\t\t\ndevice_id\tstring\t\t\napi_token\tstring\t\t\n\nVIPHCloudSpec\n\nVIPHCloudSpec describes virtual (elastic) IP settings for Hetzner Cloud.\n\nField\tType\tLabel\tDescription\ndevice_id\tint64\t\t\nnetwork_id\tint64\t\t\napi_token\tstring\t\t\n\nVIPOperatorSpec\n\nVIPOperatorSpec describes virtual IP operator options.\n\nField\tType\tLabel\tDescription\nip\tcommon.NetIP\t\t\ngratuitous_arp\tbool\t\t\nequinix_metal\tVIPEquinixMetalSpec\t\t\nh_cloud\tVIPHCloudSpec\t\t\n\nVLANSpec\n\nVLANSpec describes VLAN settings if Kind == “vlan”.\n\nField\tType\tLabel\tDescription\nvid\tfixed32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersVLANProtocol\t\t\n\nWireguardPeer\n\nWireguardPeer describes a single peer.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\npreshared_key\tstring\t\t\nendpoint\tstring\t\t\npersistent_keepalive_interval\tgoogle.protobuf.Duration\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\n\nWireguardSpec\n\nWireguardSpec describes Wireguard settings if Kind == “wireguard”.\n\nField\tType\tLabel\tDescription\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\nlisten_port\tint64\t\t\nfirewall_mark\tint64\t\t\npeers\tWireguardPeer\trepeated\t\n\nTop\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\n\nCPUSpec represents the last CPU stats snapshot.\n\nField\tType\tLabel\tDescription\ncpu\tCPUStat\trepeated\t\ncpu_total\tCPUStat\t\t\nirq_total\tuint64\t\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\n\nCPUStat\n\nCPUStat represents a single cpu stat.\n\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nMemorySpec\n\nMemorySpec represents the last Memory stats snapshot.\n\nField\tType\tLabel\tDescription\nmem_total\tuint64\t\t\nmem_used\tuint64\t\t\nmem_available\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswap_cached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactive_anon\tuint64\t\t\ninactive_anon\tuint64\t\t\nactive_file\tuint64\t\t\ninactive_file\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswap_total\tuint64\t\t\nswap_free\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanon_pages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\ns_reclaimable\tuint64\t\t\ns_unreclaim\tuint64\t\t\nkernel_stack\tuint64\t\t\npage_tables\tuint64\t\t\nnf_sunstable\tuint64\t\t\nbounce\tuint64\t\t\nwriteback_tmp\tuint64\t\t\ncommit_limit\tuint64\t\t\ncommitted_as\tuint64\t\t\nvmalloc_total\tuint64\t\t\nvmalloc_used\tuint64\t\t\nvmalloc_chunk\tuint64\t\t\nhardware_corrupted\tuint64\t\t\nanon_huge_pages\tuint64\t\t\nshmem_huge_pages\tuint64\t\t\nshmem_pmd_mapped\tuint64\t\t\ncma_total\tuint64\t\t\ncma_free\tuint64\t\t\nhuge_pages_total\tuint64\t\t\nhuge_pages_free\tuint64\t\t\nhuge_pages_rsvd\tuint64\t\t\nhuge_pages_surp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirect_map4k\tuint64\t\t\ndirect_map2m\tuint64\t\t\ndirect_map1g\tuint64\t\t\n\nTop\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\n\nLinuxIDMapping specifies UID/GID mappings.\n\nField\tType\tLabel\tDescription\ncontainer_id\tuint32\t\t\nhost_id\tuint32\t\t\nsize\tuint32\t\t\n\nMount\n\nMount specifies a mount for a container.\n\nField\tType\tLabel\tDescription\ndestination\tstring\t\t\ntype\tstring\t\t\nsource\tstring\t\t\noptions\tstring\trepeated\t\nuid_mappings\tLinuxIDMapping\trepeated\t\ngid_mappings\tLinuxIDMapping\trepeated\t\n\nTop\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\n\nDevicesStatusSpec is the spec for devices status.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\n\nEventSinkConfigSpec\n\nEventSinkConfigSpec describes configuration of Talos event log streaming.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nKernelModuleSpecSpec\n\nKernelModuleSpecSpec describes Linux kernel module to load.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nparameters\tstring\trepeated\t\n\nKernelParamSpecSpec\n\nKernelParamSpecSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\nignore_errors\tbool\t\t\n\nKernelParamStatusSpec\n\nKernelParamStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\ncurrent\tstring\t\t\ndefault\tstring\t\t\nunsupported\tbool\t\t\n\nKmsgLogConfigSpec\n\nKmsgLogConfigSpec describes configuration for kmsg log streaming.\n\nField\tType\tLabel\tDescription\ndestinations\tcommon.URL\trepeated\t\n\nMachineStatusSpec\n\nMachineStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nstage\ttalos.resource.definitions.enums.RuntimeMachineStage\t\t\nstatus\tMachineStatusStatus\t\t\n\nMachineStatusStatus\n\nMachineStatusStatus describes machine current status at the stage.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tUnmetCondition\trepeated\t\n\nMaintenanceServiceConfigSpec\n\nMaintenanceServiceConfigSpec describes configuration for maintenance service API.\n\nField\tType\tLabel\tDescription\nlisten_address\tstring\t\t\nreachable_addresses\tcommon.NetIP\trepeated\t\n\nMetaKeySpec\n\nMetaKeySpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\n\nMetaLoadedSpec\n\nMetaLoadedSpec is the spec for meta loaded. The Done field is always true when resource exists.\n\nField\tType\tLabel\tDescription\ndone\tbool\t\t\n\nMountStatusSpec\n\nMountStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nsource\tstring\t\t\ntarget\tstring\t\t\nfilesystem_type\tstring\t\t\noptions\tstring\trepeated\t\nencrypted\tbool\t\t\nencryption_providers\tstring\trepeated\t\n\nPlatformMetadataSpec\n\nPlatformMetadataSpec describes platform metadata properties.\n\nField\tType\tLabel\tDescription\nplatform\tstring\t\t\nhostname\tstring\t\t\nregion\tstring\t\t\nzone\tstring\t\t\ninstance_type\tstring\t\t\ninstance_id\tstring\t\t\nprovider_id\tstring\t\t\nspot\tbool\t\t\n\nSecurityStateSpec\n\nSecurityStateSpec describes the security state resource properties.\n\nField\tType\tLabel\tDescription\nsecure_boot\tbool\t\t\nuki_signing_key_fingerprint\tstring\t\t\npcr_signing_key_fingerprint\tstring\t\t\n\nUniqueMachineTokenSpec\n\nUniqueMachineTokenSpec is the spec for the machine unique token. Token can be empty if machine wasn’t assigned any.\n\nField\tType\tLabel\tDescription\ntoken\tstring\t\t\n\nUnmetCondition\n\nUnmetCondition is a failure which prevents machine from being ready at the stage.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nTop\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\n\nAPICertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nclient\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nCertSANSpec\n\nCertSANSpec describes fields of the cert SANs.\n\nField\tType\tLabel\tDescription\ni_ps\tcommon.NetIP\trepeated\t\ndns_names\tstring\trepeated\t\nfqdn\tstring\t\t\n\nEtcdCertsSpec\n\nEtcdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\netcd\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_peer\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_admin\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_api_server\tcommon.PEMEncodedCertificateAndKey\t\t\n\nEtcdRootSpec\n\nEtcdRootSpec describes etcd CA secrets.\n\nField\tType\tLabel\tDescription\netcd_ca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubeletSpec\n\nKubeletSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.URL\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\n\nKubernetesCertsSpec\n\nKubernetesCertsSpec describes generated Kubernetes certificates.\n\nField\tType\tLabel\tDescription\nscheduler_kubeconfig\tstring\t\t\ncontroller_manager_kubeconfig\tstring\t\t\nlocalhost_admin_kubeconfig\tstring\t\t\nadmin_kubeconfig\tstring\t\t\n\nKubernetesDynamicCertsSpec\n\nKubernetesDynamicCertsSpec describes generated KubernetesCerts certificates.\n\nField\tType\tLabel\tDescription\napi_server\tcommon.PEMEncodedCertificateAndKey\t\t\napi_server_kubelet_client\tcommon.PEMEncodedCertificateAndKey\t\t\nfront_proxy\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubernetesRootSpec\n\nKubernetesRootSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nendpoint\tcommon.URL\t\t\nlocal_endpoint\tcommon.URL\t\t\ncert_sa_ns\tstring\trepeated\t\ndns_domain\tstring\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nservice_account\tcommon.PEMEncodedKey\t\t\naggregator_ca\tcommon.PEMEncodedCertificateAndKey\t\t\naescbc_encryption_secret\tstring\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\nsecretbox_encryption_secret\tstring\t\t\napi_server_ips\tcommon.NetIP\trepeated\t\n\nMaintenanceRootSpec\n\nMaintenanceRootSpec describes maintenance service CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nMaintenanceServiceCertsSpec\n\nMaintenanceServiceCertsSpec describes maintenance service certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nOSRootSpec\n\nOSRootSpec describes operating system CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\ncert_sani_ps\tcommon.NetIP\trepeated\t\ncert_sandns_names\tstring\trepeated\t\ntoken\tstring\t\t\n\nTrustdCertsSpec\n\nTrustdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nTop\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\napi_endpoint\tstring\t\t\n\nTop\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\n\nAdjtimeStatusSpec describes Linux internal adjtime state.\n\nField\tType\tLabel\tDescription\noffset\tgoogle.protobuf.Duration\t\t\nfrequency_adjustment_ratio\tdouble\t\t\nmax_error\tgoogle.protobuf.Duration\t\t\nest_error\tgoogle.protobuf.Duration\t\t\nstatus\tstring\t\t\nconstant\tint64\t\t\nsync_status\tbool\t\t\nstate\tstring\t\t\n\nStatusSpec\n\nStatusSpec describes time sync state.\n\nField\tType\tLabel\tDescription\nsynced\tbool\t\t\nepoch\tint64\t\t\nsync_disabled\tbool\t\t\n\nTop\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\nServiceSpec describe service state.\n\nField\tType\tLabel\tDescription\nrunning\tbool\t\t\nhealthy\tbool\t\t\nunknown\tbool\t\t\n\nTop\n\ninspect/inspect.proto\n\nControllerDependencyEdge\nField\tType\tLabel\tDescription\ncontroller_name\tstring\t\t\nedge_type\tDependencyEdgeType\t\t\nresource_namespace\tstring\t\t\nresource_type\tstring\t\t\nresource_id\tstring\t\t\n\nControllerRuntimeDependenciesResponse\nField\tType\tLabel\tDescription\nmessages\tControllerRuntimeDependency\trepeated\t\n\nControllerRuntimeDependency\n\nThe ControllerRuntimeDependency message contains the graph of controller-resource dependencies.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nedges\tControllerDependencyEdge\trepeated\t\n\nDependencyEdgeType\nName\tNumber\tDescription\nOUTPUT_EXCLUSIVE\t0\t\nOUTPUT_SHARED\t3\t\nINPUT_STRONG\t1\t\nINPUT_WEAK\t2\t\nINPUT_DESTROY_READY\t4\t\n\nInspectService\n\nThe inspect service definition.\n\nInspectService provides auxiliary API to inspect OS internals.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nControllerRuntimeDependencies\t.google.protobuf.Empty\tControllerRuntimeDependenciesResponse\t\n\nTop\n\nmachine/machine.proto\n\nAddressEvent\n\nAddressEvent reports node endpoints aggregated from k8s.Endpoints and network.Hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\naddresses\tstring\trepeated\t\n\nApplyConfiguration\n\nApplyConfigurationResponse describes the response to a configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nwarnings\tstring\trepeated\tConfiguration validation warnings.\nmode\tApplyConfigurationRequest.Mode\t\tStates which mode was actually chosen.\nmode_details\tstring\t\tHuman-readable message explaining the result of the apply configuration call.\n\nApplyConfigurationRequest\n\nrpc applyConfiguration ApplyConfiguration describes a request to assert a new configuration upon a node.\n\nField\tType\tLabel\tDescription\ndata\tbytes\t\t\nmode\tApplyConfigurationRequest.Mode\t\t\ndry_run\tbool\t\t\ntry_mode_timeout\tgoogle.protobuf.Duration\t\t\n\nApplyConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tApplyConfiguration\trepeated\t\n\nBPFInstruction\nField\tType\tLabel\tDescription\nop\tuint32\t\t\njt\tuint32\t\t\njf\tuint32\t\t\nk\tuint32\t\t\n\nBootstrap\n\nThe bootstrap message containing the bootstrap status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nBootstrapRequest\n\nrpc Bootstrap\n\nField\tType\tLabel\tDescription\nrecover_etcd\tbool\t\tEnable etcd recovery from the snapshot. Snapshot should be uploaded before this call via EtcdRecover RPC.\nrecover_skip_hash_check\tbool\t\tSkip hash check on the snapshot (etcd). Enable this when recovering from data directory copy to skip integrity check.\n\nBootstrapResponse\nField\tType\tLabel\tDescription\nmessages\tBootstrap\trepeated\t\n\nCNIConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurls\tstring\trepeated\t\n\nCPUInfo\nField\tType\tLabel\tDescription\nprocessor\tuint32\t\t\nvendor_id\tstring\t\t\ncpu_family\tstring\t\t\nmodel\tstring\t\t\nmodel_name\tstring\t\t\nstepping\tstring\t\t\nmicrocode\tstring\t\t\ncpu_mhz\tdouble\t\t\ncache_size\tstring\t\t\nphysical_id\tstring\t\t\nsiblings\tuint32\t\t\ncore_id\tstring\t\t\ncpu_cores\tuint32\t\t\napic_id\tstring\t\t\ninitial_apic_id\tstring\t\t\nfpu\tstring\t\t\nfpu_exception\tstring\t\t\ncpu_id_level\tuint32\t\t\nwp\tstring\t\t\nflags\tstring\trepeated\t\nbugs\tstring\trepeated\t\nbogo_mips\tdouble\t\t\ncl_flush_size\tuint32\t\t\ncache_alignment\tuint32\t\t\naddress_sizes\tstring\t\t\npower_management\tstring\t\t\n\nCPUInfoResponse\nField\tType\tLabel\tDescription\nmessages\tCPUsInfo\trepeated\t\n\nCPUStat\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nCPUsInfo\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncpu_info\tCPUInfo\trepeated\t\n\nClusterConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\ncontrol_plane\tControlPlaneConfig\t\t\ncluster_network\tClusterNetworkConfig\t\t\nallow_scheduling_on_control_planes\tbool\t\t\n\nClusterNetworkConfig\nField\tType\tLabel\tDescription\ndns_domain\tstring\t\t\ncni_config\tCNIConfig\t\t\n\nConfigLoadErrorEvent\n\nConfigLoadErrorEvent is reported when the config loading has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConfigValidationErrorEvent\n\nConfigValidationErrorEvent is reported when config validation has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConnectRecord\nField\tType\tLabel\tDescription\nl4proto\tstring\t\t\nlocalip\tstring\t\t\nlocalport\tuint32\t\t\nremoteip\tstring\t\t\nremoteport\tuint32\t\t\nstate\tConnectRecord.State\t\t\ntxqueue\tuint64\t\t\nrxqueue\tuint64\t\t\ntr\tConnectRecord.TimerActive\t\t\ntimerwhen\tuint64\t\t\nretrnsmt\tuint64\t\t\nuid\tuint32\t\t\ntimeout\tuint64\t\t\ninode\tuint64\t\t\nref\tuint64\t\t\npointer\tuint64\t\t\nprocess\tConnectRecord.Process\t\t\nnetns\tstring\t\t\n\nConnectRecord.Process\nField\tType\tLabel\tDescription\npid\tuint32\t\t\nname\tstring\t\t\n\nContainer\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncontainers\tContainerInfo\trepeated\t\n\nContainerInfo\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nimage\tstring\t\t\npid\tuint32\t\t\nstatus\tstring\t\t\npod_id\tstring\t\t\nname\tstring\t\t\nnetwork_namespace\tstring\t\t\n\nContainersRequest\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nContainersResponse\nField\tType\tLabel\tDescription\nmessages\tContainer\trepeated\t\n\nControlPlaneConfig\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nCopyRequest\n\nCopyRequest describes a request to copy data out of Talos node\n\nCopy produces .tar.gz archive which is streamed back to the caller\n\nField\tType\tLabel\tDescription\nroot_path\tstring\t\tRoot path to start copying data out, it might be either a file or directory\n\nDHCPOptionsConfig\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\n\nDiskStat\nField\tType\tLabel\tDescription\nname\tstring\t\t\nread_completed\tuint64\t\t\nread_merged\tuint64\t\t\nread_sectors\tuint64\t\t\nread_time_ms\tuint64\t\t\nwrite_completed\tuint64\t\t\nwrite_merged\tuint64\t\t\nwrite_sectors\tuint64\t\t\nwrite_time_ms\tuint64\t\t\nio_in_progress\tuint64\t\t\nio_time_ms\tuint64\t\t\nio_time_weighted_ms\tuint64\t\t\ndiscard_completed\tuint64\t\t\ndiscard_merged\tuint64\t\t\ndiscard_sectors\tuint64\t\t\ndiscard_time_ms\tuint64\t\t\n\nDiskStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tDiskStat\t\t\ndevices\tDiskStat\trepeated\t\n\nDiskStatsResponse\nField\tType\tLabel\tDescription\nmessages\tDiskStats\trepeated\t\n\nDiskUsageInfo\n\nDiskUsageInfo describes a file or directory’s information for du command\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\n\nDiskUsageRequest\n\nDiskUsageRequest describes a request to list disk usage of directories and regular files\n\nField\tType\tLabel\tDescription\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\nall\tbool\t\tAll write sizes for all files, not just directories.\nthreshold\tint64\t\tThreshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative.\npaths\tstring\trepeated\tDiskUsagePaths is the list of directories to calculate disk usage for.\n\nDmesgRequest\n\ndmesg\n\nField\tType\tLabel\tDescription\nfollow\tbool\t\t\ntail\tbool\t\t\n\nEtcdAlarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarmResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarmDisarm\trepeated\t\n\nEtcdAlarmListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarm\trepeated\t\n\nEtcdDefragment\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdDefragmentResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdDefragment\trepeated\t\n\nEtcdForfeitLeadership\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember\tstring\t\t\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdForfeitLeadership\trepeated\t\n\nEtcdLeaveCluster\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdLeaveCluster\trepeated\t\n\nEtcdMember\n\nEtcdMember describes a single etcd member.\n\nField\tType\tLabel\tDescription\nid\tuint64\t\tmember ID.\nhostname\tstring\t\thuman-readable name of the member.\npeer_urls\tstring\trepeated\tthe list of URLs the member exposes to clients for communication.\nclient_urls\tstring\trepeated\tthe list of URLs the member exposes to the cluster for communication.\nis_learner\tbool\t\tlearner flag\n\nEtcdMemberAlarm\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nalarm\tEtcdMemberAlarm.AlarmType\t\t\n\nEtcdMemberListRequest\nField\tType\tLabel\tDescription\nquery_local\tbool\t\t\n\nEtcdMemberListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdMembers\trepeated\t\n\nEtcdMemberStatus\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nprotocol_version\tstring\t\t\ndb_size\tint64\t\t\ndb_size_in_use\tint64\t\t\nleader\tuint64\t\t\nraft_index\tuint64\t\t\nraft_term\tuint64\t\t\nraft_applied_index\tuint64\t\t\nerrors\tstring\trepeated\t\nis_learner\tbool\t\t\n\nEtcdMembers\n\nEtcdMembers contains the list of members registered on the host.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nlegacy_members\tstring\trepeated\tlist of member hostnames.\nmembers\tEtcdMember\trepeated\tthe list of etcd members registered on the node.\n\nEtcdRecover\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRecoverResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRecover\trepeated\t\n\nEtcdRemoveMember\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByID\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByIDRequest\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\n\nEtcdRemoveMemberByIDResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMemberByID\trepeated\t\n\nEtcdRemoveMemberRequest\nField\tType\tLabel\tDescription\nmember\tstring\t\t\n\nEtcdRemoveMemberResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMember\trepeated\t\n\nEtcdSnapshotRequest\n\nEtcdStatus\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_status\tEtcdMemberStatus\t\t\n\nEtcdStatusResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdStatus\trepeated\t\n\nEvent\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tgoogle.protobuf.Any\t\t\nid\tstring\t\t\nactor_id\tstring\t\t\n\nEventsRequest\nField\tType\tLabel\tDescription\ntail_events\tint32\t\t\ntail_id\tstring\t\t\ntail_seconds\tint32\t\t\nwith_actor_id\tstring\t\t\n\nFeaturesInfo\n\nFeaturesInfo describes individual Talos features that can be switched on or off.\n\nField\tType\tLabel\tDescription\nrbac\tbool\t\tRBAC is true if role-based access control is enabled.\n\nFileInfo\n\nFileInfo describes a file or directory’s information\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nmode\tuint32\t\tMode is the bitmap of UNIX mode/permission flags of the file\nmodified\tint64\t\tModified indicates the UNIX timestamp at which the file was last modified\nis_dir\tbool\t\tIsDir indicates that the file is a directory\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nlink\tstring\t\tLink is filled with symlink target\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\nuid\tuint32\t\tOwner uid\ngid\tuint32\t\tOwner gid\n\nGenerateClientConfiguration\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nca\tbytes\t\tPEM-encoded CA certificate.\ncrt\tbytes\t\tPEM-encoded generated client certificate.\nkey\tbytes\t\tPEM-encoded generated client key.\ntalosconfig\tbytes\t\tClient configuration (talosconfig) file content.\n\nGenerateClientConfigurationRequest\nField\tType\tLabel\tDescription\nroles\tstring\trepeated\tRoles in the generated client certificate.\ncrt_ttl\tgoogle.protobuf.Duration\t\tClient certificate TTL.\n\nGenerateClientConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateClientConfiguration\trepeated\t\n\nGenerateConfiguration\n\nGenerateConfiguration describes the response to a generate configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tbytes\trepeated\t\ntalosconfig\tbytes\t\t\n\nGenerateConfigurationRequest\n\nGenerateConfigurationRequest describes a request to generate a new configuration on a node.\n\nField\tType\tLabel\tDescription\nconfig_version\tstring\t\t\ncluster_config\tClusterConfig\t\t\nmachine_config\tMachineConfig\t\t\noverride_time\tgoogle.protobuf.Timestamp\t\t\n\nGenerateConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateConfiguration\trepeated\t\n\nHostname\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nhostname\tstring\t\t\n\nHostnameResponse\nField\tType\tLabel\tDescription\nmessages\tHostname\trepeated\t\n\nImageListRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\n\nImageListResponse\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\t\ndigest\tstring\t\t\nsize\tint64\t\t\ncreated_at\tgoogle.protobuf.Timestamp\t\t\n\nImagePull\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nImagePullRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\nreference\tstring\t\tImage reference to pull.\n\nImagePullResponse\nField\tType\tLabel\tDescription\nmessages\tImagePull\trepeated\t\n\nInstallConfig\nField\tType\tLabel\tDescription\ninstall_disk\tstring\t\t\ninstall_image\tstring\t\t\n\nListRequest\n\nListRequest describes a request to list the contents of a directory.\n\nField\tType\tLabel\tDescription\nroot\tstring\t\tRoot indicates the root directory for the list. If not indicated, ‘/’ is presumed.\nrecurse\tbool\t\tRecurse indicates that subdirectories should be recursed.\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\ntypes\tListRequest.Type\trepeated\tTypes indicates what file type should be returned. If not indicated, all files will be returned.\n\nLoadAvg\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nload1\tdouble\t\t\nload5\tdouble\t\t\nload15\tdouble\t\t\n\nLoadAvgResponse\nField\tType\tLabel\tDescription\nmessages\tLoadAvg\trepeated\t\n\nLogsRequest\n\nrpc logs The request message containing the process name.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\nfollow\tbool\t\t\ntail_lines\tint32\t\t\n\nMachineConfig\nField\tType\tLabel\tDescription\ntype\tMachineConfig.MachineType\t\t\ninstall_config\tInstallConfig\t\t\nnetwork_config\tNetworkConfig\t\t\nkubernetes_version\tstring\t\t\n\nMachineStatusEvent\n\nMachineStatusEvent reports changes to the MachineStatus resource.\n\nField\tType\tLabel\tDescription\nstage\tMachineStatusEvent.MachineStage\t\t\nstatus\tMachineStatusEvent.MachineStatus\t\t\n\nMachineStatusEvent.MachineStatus\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tMachineStatusEvent.MachineStatus.UnmetCondition\trepeated\t\n\nMachineStatusEvent.MachineStatus.UnmetCondition\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nMemInfo\nField\tType\tLabel\tDescription\nmemtotal\tuint64\t\t\nmemfree\tuint64\t\t\nmemavailable\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswapcached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactiveanon\tuint64\t\t\ninactiveanon\tuint64\t\t\nactivefile\tuint64\t\t\ninactivefile\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswaptotal\tuint64\t\t\nswapfree\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanonpages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\nsreclaimable\tuint64\t\t\nsunreclaim\tuint64\t\t\nkernelstack\tuint64\t\t\npagetables\tuint64\t\t\nnfsunstable\tuint64\t\t\nbounce\tuint64\t\t\nwritebacktmp\tuint64\t\t\ncommitlimit\tuint64\t\t\ncommittedas\tuint64\t\t\nvmalloctotal\tuint64\t\t\nvmallocused\tuint64\t\t\nvmallocchunk\tuint64\t\t\nhardwarecorrupted\tuint64\t\t\nanonhugepages\tuint64\t\t\nshmemhugepages\tuint64\t\t\nshmempmdmapped\tuint64\t\t\ncmatotal\tuint64\t\t\ncmafree\tuint64\t\t\nhugepagestotal\tuint64\t\t\nhugepagesfree\tuint64\t\t\nhugepagesrsvd\tuint64\t\t\nhugepagessurp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirectmap4k\tuint64\t\t\ndirectmap2m\tuint64\t\t\ndirectmap1g\tuint64\t\t\n\nMemory\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmeminfo\tMemInfo\t\t\n\nMemoryResponse\nField\tType\tLabel\tDescription\nmessages\tMemory\trepeated\t\n\nMetaDelete\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaDeleteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\n\nMetaDeleteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaDelete\trepeated\t\n\nMetaWrite\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaWriteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\nvalue\tbytes\t\t\n\nMetaWriteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaWrite\trepeated\t\n\nMountStat\n\nThe messages message containing the requested processes.\n\nField\tType\tLabel\tDescription\nfilesystem\tstring\t\t\nsize\tuint64\t\t\navailable\tuint64\t\t\nmounted_on\tstring\t\t\n\nMounts\n\nThe messages message containing the requested df stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tMountStat\trepeated\t\n\nMountsResponse\nField\tType\tLabel\tDescription\nmessages\tMounts\trepeated\t\n\nNetDev\nField\tType\tLabel\tDescription\nname\tstring\t\t\nrx_bytes\tuint64\t\t\nrx_packets\tuint64\t\t\nrx_errors\tuint64\t\t\nrx_dropped\tuint64\t\t\nrx_fifo\tuint64\t\t\nrx_frame\tuint64\t\t\nrx_compressed\tuint64\t\t\nrx_multicast\tuint64\t\t\ntx_bytes\tuint64\t\t\ntx_packets\tuint64\t\t\ntx_errors\tuint64\t\t\ntx_dropped\tuint64\t\t\ntx_fifo\tuint64\t\t\ntx_collisions\tuint64\t\t\ntx_carrier\tuint64\t\t\ntx_compressed\tuint64\t\t\n\nNetstat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nconnectrecord\tConnectRecord\trepeated\t\n\nNetstatRequest\nField\tType\tLabel\tDescription\nfilter\tNetstatRequest.Filter\t\t\nfeature\tNetstatRequest.Feature\t\t\nl4proto\tNetstatRequest.L4proto\t\t\nnetns\tNetstatRequest.NetNS\t\t\n\nNetstatRequest.Feature\nField\tType\tLabel\tDescription\npid\tbool\t\t\n\nNetstatRequest.L4proto\nField\tType\tLabel\tDescription\ntcp\tbool\t\t\ntcp6\tbool\t\t\nudp\tbool\t\t\nudp6\tbool\t\t\nudplite\tbool\t\t\nudplite6\tbool\t\t\nraw\tbool\t\t\nraw6\tbool\t\t\n\nNetstatRequest.NetNS\nField\tType\tLabel\tDescription\nhostnetwork\tbool\t\t\nnetns\tstring\trepeated\t\nallnetns\tbool\t\t\n\nNetstatResponse\nField\tType\tLabel\tDescription\nmessages\tNetstat\trepeated\t\n\nNetworkConfig\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ninterfaces\tNetworkDeviceConfig\trepeated\t\n\nNetworkDeviceConfig\nField\tType\tLabel\tDescription\ninterface\tstring\t\t\ncidr\tstring\t\t\nmtu\tint32\t\t\ndhcp\tbool\t\t\nignore\tbool\t\t\ndhcp_options\tDHCPOptionsConfig\t\t\nroutes\tRouteConfig\trepeated\t\n\nNetworkDeviceStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tNetDev\t\t\ndevices\tNetDev\trepeated\t\n\nNetworkDeviceStatsResponse\nField\tType\tLabel\tDescription\nmessages\tNetworkDeviceStats\trepeated\t\n\nPacketCaptureRequest\nField\tType\tLabel\tDescription\ninterface\tstring\t\tInterface name to perform packet capture on.\npromiscuous\tbool\t\tEnable promiscuous mode.\nsnap_len\tuint32\t\tSnap length in bytes.\nbpf_filter\tBPFInstruction\trepeated\tBPF filter.\n\nPhaseEvent\nField\tType\tLabel\tDescription\nphase\tstring\t\t\naction\tPhaseEvent.Action\t\t\n\nPlatformInfo\nField\tType\tLabel\tDescription\nname\tstring\t\t\nmode\tstring\t\t\n\nProcess\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nprocesses\tProcessInfo\trepeated\t\n\nProcessInfo\nField\tType\tLabel\tDescription\npid\tint32\t\t\nppid\tint32\t\t\nstate\tstring\t\t\nthreads\tint32\t\t\ncpu_time\tdouble\t\t\nvirtual_memory\tuint64\t\t\nresident_memory\tuint64\t\t\ncommand\tstring\t\t\nexecutable\tstring\t\t\nargs\tstring\t\t\n\nProcessesResponse\n\nrpc processes\n\nField\tType\tLabel\tDescription\nmessages\tProcess\trepeated\t\n\nReadRequest\nField\tType\tLabel\tDescription\npath\tstring\t\t\n\nReboot\n\nThe reboot message containing the reboot status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nRebootRequest\n\nrpc reboot\n\nField\tType\tLabel\tDescription\nmode\tRebootRequest.Mode\t\t\n\nRebootResponse\nField\tType\tLabel\tDescription\nmessages\tReboot\trepeated\t\n\nReset\n\nThe reset message containing the restart status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nResetPartitionSpec\n\nrpc reset\n\nField\tType\tLabel\tDescription\nlabel\tstring\t\t\nwipe\tbool\t\t\n\nResetRequest\nField\tType\tLabel\tDescription\ngraceful\tbool\t\tGraceful indicates whether node should leave etcd before the upgrade, it also enforces etcd checks before leaving.\nreboot\tbool\t\tReboot indicates whether node should reboot or halt after resetting.\nsystem_partitions_to_wipe\tResetPartitionSpec\trepeated\tSystem_partitions_to_wipe lists specific system disk partitions to be reset (wiped). If system_partitions_to_wipe is empty, all the partitions are erased.\nuser_disks_to_wipe\tstring\trepeated\tUserDisksToWipe lists specific connected block devices to be reset (wiped).\nmode\tResetRequest.WipeMode\t\tWipeMode defines which devices should be wiped.\n\nResetResponse\nField\tType\tLabel\tDescription\nmessages\tReset\trepeated\t\n\nRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRestartEvent\nField\tType\tLabel\tDescription\ncmd\tint64\t\t\n\nRestartRequest\n\nrpc restart The request message containing the process to restart.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nRestartResponse\n\nThe messages message containing the restart status.\n\nField\tType\tLabel\tDescription\nmessages\tRestart\trepeated\t\n\nRollback\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRollbackRequest\n\nrpc rollback\n\nRollbackResponse\nField\tType\tLabel\tDescription\nmessages\tRollback\trepeated\t\n\nRouteConfig\nField\tType\tLabel\tDescription\nnetwork\tstring\t\t\ngateway\tstring\t\t\nmetric\tuint32\t\t\n\nSequenceEvent\n\nrpc events\n\nField\tType\tLabel\tDescription\nsequence\tstring\t\t\naction\tSequenceEvent.Action\t\t\nerror\tcommon.Error\t\t\n\nServiceEvent\nField\tType\tLabel\tDescription\nmsg\tstring\t\t\nstate\tstring\t\t\nts\tgoogle.protobuf.Timestamp\t\t\n\nServiceEvents\nField\tType\tLabel\tDescription\nevents\tServiceEvent\trepeated\t\n\nServiceHealth\nField\tType\tLabel\tDescription\nunknown\tbool\t\t\nhealthy\tbool\t\t\nlast_message\tstring\t\t\nlast_change\tgoogle.protobuf.Timestamp\t\t\n\nServiceInfo\nField\tType\tLabel\tDescription\nid\tstring\t\t\nstate\tstring\t\t\nevents\tServiceEvents\t\t\nhealth\tServiceHealth\t\t\n\nServiceList\n\nrpc servicelist\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nservices\tServiceInfo\trepeated\t\n\nServiceListResponse\nField\tType\tLabel\tDescription\nmessages\tServiceList\trepeated\t\n\nServiceRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceRestartRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceRestartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceRestart\trepeated\t\n\nServiceStart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStartRequest\n\nrpc servicestart\n\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStart\trepeated\t\n\nServiceStateEvent\nField\tType\tLabel\tDescription\nservice\tstring\t\t\naction\tServiceStateEvent.Action\t\t\nmessage\tstring\t\t\nhealth\tServiceHealth\t\t\n\nServiceStop\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStopRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStopResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStop\trepeated\t\n\nShutdown\n\nrpc shutdown The messages message containing the shutdown status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nShutdownRequest\nField\tType\tLabel\tDescription\nforce\tbool\t\tForce indicates whether node should shutdown without first cordening and draining\n\nShutdownResponse\nField\tType\tLabel\tDescription\nmessages\tShutdown\trepeated\t\n\nSoftIRQStat\nField\tType\tLabel\tDescription\nhi\tuint64\t\t\ntimer\tuint64\t\t\nnet_tx\tuint64\t\t\nnet_rx\tuint64\t\t\nblock\tuint64\t\t\nblock_io_poll\tuint64\t\t\ntasklet\tuint64\t\t\nsched\tuint64\t\t\nhrtimer\tuint64\t\t\nrcu\tuint64\t\t\n\nStat\n\nThe messages message containing the requested stat.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nmemory_usage\tuint64\t\t\ncpu_usage\tuint64\t\t\npod_id\tstring\t\t\nname\tstring\t\t\n\nStats\n\nThe messages message containing the requested stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tStat\trepeated\t\n\nStatsRequest\n\nThe request message containing the containerd namespace.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nStatsResponse\nField\tType\tLabel\tDescription\nmessages\tStats\trepeated\t\n\nSystemStat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nboot_time\tuint64\t\t\ncpu_total\tCPUStat\t\t\ncpu\tCPUStat\trepeated\t\nirq_total\tuint64\t\t\nirq\tuint64\trepeated\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\nsoft_irq\tSoftIRQStat\t\t\n\nSystemStatResponse\nField\tType\tLabel\tDescription\nmessages\tSystemStat\trepeated\t\n\nTaskEvent\nField\tType\tLabel\tDescription\ntask\tstring\t\t\naction\tTaskEvent.Action\t\t\n\nUpgrade\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nack\tstring\t\t\nactor_id\tstring\t\t\n\nUpgradeRequest\n\nrpc upgrade\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\npreserve\tbool\t\t\nstage\tbool\t\t\nforce\tbool\t\t\nreboot_mode\tUpgradeRequest.RebootMode\t\t\n\nUpgradeResponse\nField\tType\tLabel\tDescription\nmessages\tUpgrade\trepeated\t\n\nVersion\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nversion\tVersionInfo\t\t\nplatform\tPlatformInfo\t\t\nfeatures\tFeaturesInfo\t\tFeatures describe individual Talos features that can be switched on or off.\n\nVersionInfo\nField\tType\tLabel\tDescription\ntag\tstring\t\t\nsha\tstring\t\t\nbuilt\tstring\t\t\ngo_version\tstring\t\t\nos\tstring\t\t\narch\tstring\t\t\n\nVersionResponse\nField\tType\tLabel\tDescription\nmessages\tVersion\trepeated\t\n\nApplyConfigurationRequest.Mode\nName\tNumber\tDescription\nREBOOT\t0\t\nAUTO\t1\t\nNO_REBOOT\t2\t\nSTAGED\t3\t\nTRY\t4\t\n\nConnectRecord.State\nName\tNumber\tDescription\nRESERVED\t0\t\nESTABLISHED\t1\t\nSYN_SENT\t2\t\nSYN_RECV\t3\t\nFIN_WAIT1\t4\t\nFIN_WAIT2\t5\t\nTIME_WAIT\t6\t\nCLOSE\t7\t\nCLOSEWAIT\t8\t\nLASTACK\t9\t\nLISTEN\t10\t\nCLOSING\t11\t\n\nConnectRecord.TimerActive\nName\tNumber\tDescription\nOFF\t0\t\nON\t1\t\nKEEPALIVE\t2\t\nTIMEWAIT\t3\t\nPROBE\t4\t\n\nEtcdMemberAlarm.AlarmType\nName\tNumber\tDescription\nNONE\t0\t\nNOSPACE\t1\t\nCORRUPT\t2\t\n\nListRequest.Type\n\nFile type.\n\nName\tNumber\tDescription\nREGULAR\t0\tRegular file (not directory, symlink, etc).\nDIRECTORY\t1\tDirectory.\nSYMLINK\t2\tSymbolic link.\n\nMachineConfig.MachineType\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\t\nTYPE_INIT\t1\t\nTYPE_CONTROL_PLANE\t2\t\nTYPE_WORKER\t3\t\n\nMachineStatusEvent.MachineStage\nName\tNumber\tDescription\nUNKNOWN\t0\t\nBOOTING\t1\t\nINSTALLING\t2\t\nMAINTENANCE\t3\t\nRUNNING\t4\t\nREBOOTING\t5\t\nSHUTTING_DOWN\t6\t\nRESETTING\t7\t\nUPGRADING\t8\t\n\nNetstatRequest.Filter\nName\tNumber\tDescription\nALL\t0\t\nCONNECTED\t1\t\nLISTENING\t2\t\n\nPhaseEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nRebootRequest.Mode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nResetRequest.WipeMode\nName\tNumber\tDescription\nALL\t0\t\nSYSTEM_DISK\t1\t\nUSER_DISKS\t2\t\n\nSequenceEvent.Action\nName\tNumber\tDescription\nNOOP\t0\t\nSTART\t1\t\nSTOP\t2\t\n\nServiceStateEvent.Action\nName\tNumber\tDescription\nINITIALIZED\t0\t\nPREPARING\t1\t\nWAITING\t2\t\nRUNNING\t3\t\nSTOPPING\t4\t\nFINISHED\t5\t\nFAILED\t6\t\nSKIPPED\t7\t\n\nTaskEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nUpgradeRequest.RebootMode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nMachineService\n\nThe machine service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nApplyConfiguration\tApplyConfigurationRequest\tApplyConfigurationResponse\t\nBootstrap\tBootstrapRequest\tBootstrapResponse\tBootstrap method makes control plane node enter etcd bootstrap mode. Node aborts etcd join sequence and creates single-node etcd cluster. If recover_etcd argument is specified, etcd is recovered from a snapshot uploaded with EtcdRecover.\nContainers\tContainersRequest\tContainersResponse\t\nCopy\tCopyRequest\t.common.Data stream\t\nCPUInfo\t.google.protobuf.Empty\tCPUInfoResponse\t\nDiskStats\t.google.protobuf.Empty\tDiskStatsResponse\t\nDmesg\tDmesgRequest\t.common.Data stream\t\nEvents\tEventsRequest\tEvent stream\t\nEtcdMemberList\tEtcdMemberListRequest\tEtcdMemberListResponse\t\nEtcdRemoveMember\tEtcdRemoveMemberRequest\tEtcdRemoveMemberResponse\tEtcdRemoveMember removes a member from the etcd cluster by hostname. Please use EtcdRemoveMemberByID instead.\nEtcdRemoveMemberByID\tEtcdRemoveMemberByIDRequest\tEtcdRemoveMemberByIDResponse\tEtcdRemoveMemberByID removes a member from the etcd cluster identified by member ID. This API should be used to remove members which don’t have an associated Talos node anymore. To remove a member with a running Talos node, use EtcdLeaveCluster API on the node to be removed.\nEtcdLeaveCluster\tEtcdLeaveClusterRequest\tEtcdLeaveClusterResponse\t\nEtcdForfeitLeadership\tEtcdForfeitLeadershipRequest\tEtcdForfeitLeadershipResponse\t\nEtcdRecover\t.common.Data stream\tEtcdRecoverResponse\tEtcdRecover method uploads etcd data snapshot created with EtcdSnapshot to the node. Snapshot can be later used to recover the cluster via Bootstrap method.\nEtcdSnapshot\tEtcdSnapshotRequest\t.common.Data stream\tEtcdSnapshot method creates etcd data snapshot (backup) from the local etcd instance and streams it back to the client. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmList\t.google.protobuf.Empty\tEtcdAlarmListResponse\tEtcdAlarmList lists etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmDisarm\t.google.protobuf.Empty\tEtcdAlarmDisarmResponse\tEtcdAlarmDisarm disarms etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdDefragment\t.google.protobuf.Empty\tEtcdDefragmentResponse\tEtcdDefragment defragments etcd data directory for the current node. Defragmentation is a resource-heavy operation, so it should only run on a specific node. This method is available only on control plane nodes (which run etcd).\nEtcdStatus\t.google.protobuf.Empty\tEtcdStatusResponse\tEtcdStatus returns etcd status for the current member. This method is available only on control plane nodes (which run etcd).\nGenerateConfiguration\tGenerateConfigurationRequest\tGenerateConfigurationResponse\t\nHostname\t.google.protobuf.Empty\tHostnameResponse\t\nKubeconfig\t.google.protobuf.Empty\t.common.Data stream\t\nList\tListRequest\tFileInfo stream\t\nDiskUsage\tDiskUsageRequest\tDiskUsageInfo stream\t\nLoadAvg\t.google.protobuf.Empty\tLoadAvgResponse\t\nLogs\tLogsRequest\t.common.Data stream\t\nMemory\t.google.protobuf.Empty\tMemoryResponse\t\nMounts\t.google.protobuf.Empty\tMountsResponse\t\nNetworkDeviceStats\t.google.protobuf.Empty\tNetworkDeviceStatsResponse\t\nProcesses\t.google.protobuf.Empty\tProcessesResponse\t\nRead\tReadRequest\t.common.Data stream\t\nReboot\tRebootRequest\tRebootResponse\t\nRestart\tRestartRequest\tRestartResponse\t\nRollback\tRollbackRequest\tRollbackResponse\t\nReset\tResetRequest\tResetResponse\t\nServiceList\t.google.protobuf.Empty\tServiceListResponse\t\nServiceRestart\tServiceRestartRequest\tServiceRestartResponse\t\nServiceStart\tServiceStartRequest\tServiceStartResponse\t\nServiceStop\tServiceStopRequest\tServiceStopResponse\t\nShutdown\tShutdownRequest\tShutdownResponse\t\nStats\tStatsRequest\tStatsResponse\t\nSystemStat\t.google.protobuf.Empty\tSystemStatResponse\t\nUpgrade\tUpgradeRequest\tUpgradeResponse\t\nVersion\t.google.protobuf.Empty\tVersionResponse\t\nGenerateClientConfiguration\tGenerateClientConfigurationRequest\tGenerateClientConfigurationResponse\tGenerateClientConfiguration generates talosctl client configuration (talosconfig).\nPacketCapture\tPacketCaptureRequest\t.common.Data stream\tPacketCapture performs packet capture and streams back pcap file.\nNetstat\tNetstatRequest\tNetstatResponse\tNetstat provides information about network connections.\nMetaWrite\tMetaWriteRequest\tMetaWriteResponse\tMetaWrite writes a META key-value pair.\nMetaDelete\tMetaDeleteRequest\tMetaDeleteResponse\tMetaDelete deletes a META key.\nImageList\tImageListRequest\tImageListResponse stream\tImageList lists images in the CRI.\nImagePull\tImagePullRequest\tImagePullResponse\tImagePull pulls an image into the CRI.\n\nTop\n\nsecurity/security.proto\n\nCertificateRequest\n\nThe request message containing the certificate signing request.\n\nField\tType\tLabel\tDescription\ncsr\tbytes\t\tCertificate Signing Request in PEM format.\n\nCertificateResponse\n\nThe response message containing signed certificate.\n\nField\tType\tLabel\tDescription\nca\tbytes\t\tCertificate of the CA that signed the requested certificate in PEM format.\ncrt\tbytes\t\tSigned X.509 requested certificate in PEM format.\n\nSecurityService\n\nThe security service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nCertificate\tCertificateRequest\tCertificateResponse\t\n\nTop\n\nstorage/storage.proto\n\nDisk\n\nDisk represents a disk.\n\nField\tType\tLabel\tDescription\nsize\tuint64\t\tSize indicates the disk size in bytes.\nmodel\tstring\t\tModel idicates the disk model.\ndevice_name\tstring\t\tDeviceName indicates the disk name (e.g. sda).\nname\tstring\t\tName as in /sys/block/<dev>/device/name.\nserial\tstring\t\tSerial as in /sys/block/<dev>/device/serial.\nmodalias\tstring\t\tModalias as in /sys/block/<dev>/device/modalias.\nuuid\tstring\t\tUuid as in /sys/block/<dev>/device/uuid.\nwwid\tstring\t\tWwid as in /sys/block/<dev>/device/wwid.\ntype\tDisk.DiskType\t\tType is a type of the disk: nvme, ssd, hdd, sd card.\nbus_path\tstring\t\tBusPath is the bus path of the disk.\nsystem_disk\tbool\t\tSystemDisk indicates that the disk is used as Talos system disk.\nsubsystem\tstring\t\tSubsystem is the symlink path in the /sys/block/<dev>/subsystem.\nreadonly\tbool\t\tReadonly specifies if the disk is read only.\n\nDisks\n\nDisksResponse represents the response of the Disks RPC.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndisks\tDisk\trepeated\t\n\nDisksResponse\nField\tType\tLabel\tDescription\nmessages\tDisks\trepeated\t\n\nDisk.DiskType\nName\tNumber\tDescription\nUNKNOWN\t0\t\nSSD\t1\t\nHDD\t2\t\nNVME\t3\t\nSD\t4\t\n\nStorageService\n\nStorageService represents the storage service.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nDisks\t.google.protobuf.Empty\tDisksResponse\t\n\nTop\n\ntime/time.proto\n\nTime\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nserver\tstring\t\t\nlocaltime\tgoogle.protobuf.Timestamp\t\t\nremotetime\tgoogle.protobuf.Timestamp\t\t\n\nTimeRequest\n\nThe response message containing the ntp server\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\n\nTimeResponse\n\nThe response message containing the ntp server, time, and offset\n\nField\tType\tLabel\tDescription\nmessages\tTime\trepeated\t\n\nTimeService\n\nThe time service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nTime\t.google.protobuf.Empty\tTimeResponse\t\nTimeCheck\tTimeRequest\tTimeResponse\t\nScalar Value Types\n.proto Type\tNotes\tC++\tJava\tPython\tGo\tC#\tPHP\tRuby\ndouble\t\tdouble\tdouble\tfloat\tfloat64\tdouble\tfloat\tFloat\nfloat\t\tfloat\tfloat\tfloat\tfloat32\tfloat\tfloat\tFloat\nint32\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nint64\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nuint32\tUses variable-length encoding.\tuint32\tint\tint/long\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nuint64\tUses variable-length encoding.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum or Fixnum (as required)\nsint32\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsint64\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nfixed32\tAlways four bytes. More efficient than uint32 if values are often greater than 2^28.\tuint32\tint\tint\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nfixed64\tAlways eight bytes. More efficient than uint64 if values are often greater than 2^56.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum\nsfixed32\tAlways four bytes.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsfixed64\tAlways eight bytes.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nbool\t\tbool\tboolean\tboolean\tbool\tbool\tboolean\tTrueClass/FalseClass\nstring\tA string must always contain UTF-8 encoded or 7-bit ASCII text.\tstring\tString\tstr/unicode\tstring\tstring\tstring\tString (UTF-8)\nbytes\tMay contain any arbitrary sequence of bytes.\tstring\tByteString\tstr\t[]byte\tByteString\tstring\tString (ASCII-8BIT)\n5.2 - CLI\nTalosctl CLI tool reference.\ntalosctl apply-config\n\nApply a new configuration to a node\n\ntalosctl apply-config [flags]\n\nOptions\n      --cert-fingerprint strings                                 list of server certificate fingeprints to accept (defaults to no check)\n  -p, --config-patch strings                                     the list of config patches to apply to the local config file before sending it to the node\n      --dry-run                                                  check how the config change will be applied in dry-run mode\n  -f, --file string                                              the filename of the updated configuration\n  -h, --help                                                     help for apply-config\n  -i, --insecure                                                 apply the config using the insecure (encrypted with no auth) maintenance service\n  -m, --mode auto, interactive, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --timeout duration                                         the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl bootstrap\n\nBootstrap the etcd cluster on the specified node.\n\nSynopsis\n\nWhen Talos cluster is created etcd service on control plane nodes enter the join loop waiting to join etcd peers from other control plane nodes. One node should be picked as the boostrap node. When boostrap command is issued, the node aborts join process and bootstraps etcd cluster as a single node cluster. Other control plane nodes will join etcd cluster once Kubernetes is boostrapped on the bootstrap node.\n\nThis command should not be used when “init” type node are used.\n\nTalos etcd cluster can be recovered from a known snapshot with ‘–recover-from=’ flag.\n\ntalosctl bootstrap [flags]\n\nOptions\n  -h, --help                      help for bootstrap\n      --recover-from string       recover etcd cluster from the snapshot\n      --recover-skip-hash-check   skip integrity check when recovering etcd (use when recovering from data directory copy)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create\n\nCreates a local docker-based or QEMU-based kubernetes cluster\n\ntalosctl cluster create [flags]\n\nOptions\n      --arch string                              cluster architecture (default \"amd64\")\n      --bad-rtc                                  launch VM with bad RTC state (QEMU only)\n      --cidr string                              CIDR of the cluster network (IPv4, ULA network for IPv6 is derived in automated way) (default \"10.5.0.0/24\")\n      --cni-bin-path strings                     search path for CNI binaries (VM only) (default [/home/user/.talos/cni/bin])\n      --cni-bundle-url string                    URL to download CNI bundle from (VM only) (default \"https://github.com/siderolabs/talos/releases/download/v1.6.0-alpha.2/talosctl-cni-bundle-${ARCH}.tar.gz\")\n      --cni-cache-dir string                     CNI cache directory path (VM only) (default \"/home/user/.talos/cni/cache\")\n      --cni-conf-dir string                      CNI config directory path (VM only) (default \"/home/user/.talos/cni/conf.d\")\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --control-plane-port int                   control plane port (load balancer and local API port) (default 6443)\n      --controlplanes int                        the number of controlplanes to create (default 1)\n      --cpus string                              the share of CPUs as fraction (each control plane/VM) (default \"2.0\")\n      --cpus-workers string                      the share of CPUs as fraction (each worker/VM) (default \"2.0\")\n      --crashdump                                print debug crashdump to stderr when cluster startup fails\n      --custom-cni-url string                    install custom CNI from the URL (Talos cluster)\n      --disable-dhcp-hostname                    skip announcing hostname via DHCP (QEMU only)\n      --disk int                                 default limit on disk size in MB (each VM) (default 6144)\n      --disk-encryption-key-types stringArray    encryption key types to use for disk encryption (uuid, kms) (default [uuid])\n      --disk-image-path string                   disk image to use\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n      --docker-disable-ipv6                      skip enabling IPv6 in containers (Docker only)\n      --docker-host-ip string                    Host IP to forward exposed ports to (Docker provisioner only) (default \"0.0.0.0\")\n      --encrypt-ephemeral                        enable ephemeral partition encryption\n      --encrypt-state                            enable state partition encryption\n      --endpoint string                          use endpoint instead of provider defaults\n  -p, --exposed-ports string                     Comma-separated list of ports/protocols to expose on init node. Ex -p <hostPort>:<containerPort>/<protocol (tcp or udp)> (Docker provisioner only)\n      --extra-boot-kernel-args string            add extra kernel args to the initial boot from vmlinuz and initramfs (QEMU only)\n      --extra-disks int                          number of extra disks to create for each worker VM\n      --extra-disks-size int                     default limit on disk size in MB (each VM) (default 5120)\n      --extra-uefi-search-paths strings          additional search paths for UEFI firmware (only applies when UEFI is enabled)\n  -h, --help                                     help for create\n      --image string                             the image to use (default \"ghcr.io/siderolabs/talos:latest\")\n      --init-node-as-endpoint                    use init node as endpoint instead of any load balancer endpoint\n      --initrd-path string                       initramfs image to use (default \"_out/initramfs-${ARCH}.xz\")\n  -i, --input-dir string                         location of pre-generated config files\n      --install-image string                     the installer image to use (default \"ghcr.io/siderolabs/installer:latest\")\n      --ipv4                                     enable IPv4 network in the cluster (default true)\n      --ipv6                                     enable IPv6 network in the cluster (QEMU provisioner only)\n      --ipxe-boot-script string                  iPXE boot script (URL) to use\n      --iso-path string                          the ISO path to use for the initial boot (VM only)\n      --kubeprism-port int                       KubePrism port (set to 0 to disable) (default 7445)\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n      --memory int                               the limit on memory usage in MB (each control plane/VM) (default 2048)\n      --memory-workers int                       the limit on memory usage in MB (each worker/VM) (default 2048)\n      --mtu int                                  MTU of the cluster network (default 1500)\n      --nameservers strings                      list of nameservers to use (default [8.8.8.8,1.1.1.1,2001:4860:4860::8888,2606:4700:4700::1111])\n      --registry-insecure-skip-verify strings    list of registry hostnames to skip TLS verification for\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --skip-boot-phase-finished-check           skip waiting for node to finish boot phase\n      --skip-injecting-config                    skip injecting config from embedded metadata server, write config files to current directory\n      --skip-kubeconfig                          skip merging kubeconfig from the created cluster\n      --talos-version string                     the desired Talos version to generate config for (if not set, defaults to image version)\n      --talosconfig string                       The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n      --use-vip                                  use a virtual IP for the controlplane endpoint instead of the loadbalancer\n      --user-disk strings                        list of disks to create for each VM in format: <mount_point1>:<size1>:<mount_point2>:<size2>\n      --vmlinuz-path string                      the compressed kernel image to use (default \"_out/vmlinuz-${ARCH}\")\n      --wait                                     wait for the cluster to be ready before returning (default true)\n      --wait-timeout duration                    timeout to wait for the cluster to be ready (default 20m0s)\n      --wireguard-cidr string                    CIDR of the wireguard network\n      --with-apply-config                        enable apply config when the VM is starting in maintenance mode\n      --with-bootloader                          enable bootloader to load kernel and initramfs from disk image after install (default true)\n      --with-cluster-discovery                   enable cluster discovery (default true)\n      --with-debug                               enable debug in Talos config to send service logs to the console\n      --with-firewall string                     inject firewall rules into the cluster, value is default policy - accept/block (QEMU only)\n      --with-init-node                           create the cluster with an init node\n      --with-kubespan                            enable KubeSpan system\n      --with-network-bandwidth int               specify bandwidth restriction (in kbps) on the bridge interface when creating a qemu cluster\n      --with-network-chaos                       enable to use network chaos parameters when creating a qemu cluster\n      --with-network-jitter duration             specify jitter on the bridge interface when creating a qemu cluster\n      --with-network-latency duration            specify latency on the bridge interface when creating a qemu cluster\n      --with-network-packet-corrupt float        specify percent of corrupt packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-loss float           specify percent of packet loss on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-reorder float        specify percent of reordered packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-tpm2                                enable TPM2 emulation support using swtpm\n      --with-uefi                                enable UEFI on x86_64 architecture (default true)\n      --workers int                              the number of workers to create (default 1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster destroy\n\nDestroys a local docker-based or firecracker-based kubernetes cluster\n\ntalosctl cluster destroy [flags]\n\nOptions\n  -f, --force   force deletion of cluster directory if there were errors\n  -h, --help    help for destroy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster show\n\nShows info about a local provisioned kubernetes cluster\n\ntalosctl cluster show [flags]\n\nOptions\n  -h, --help   help for show\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster\n\nA collection of commands for managing local docker-based or QEMU-based clusters\n\nOptions\n  -h, --help                 help for cluster\n      --name string          the name of the cluster (default \"talos-default\")\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create - Creates a local docker-based or QEMU-based kubernetes cluster\ntalosctl cluster destroy - Destroys a local docker-based or firecracker-based kubernetes cluster\ntalosctl cluster show - Shows info about a local provisioned kubernetes cluster\ntalosctl completion\n\nOutput shell completion code for the specified shell (bash, fish or zsh)\n\nSynopsis\n\nOutput shell completion code for the specified shell (bash, fish or zsh). The shell code must be evaluated to provide interactive completion of talosctl commands. This can be done by sourcing it from the .bash_profile.\n\nNote for zsh users: [1] zsh completions are only supported in versions of zsh >= 5.2\n\ntalosctl completion SHELL [flags]\n\nExamples\n# Installing bash completion on macOS using homebrew\n## If running Bash 3.2 included with macOS\n\tbrew install bash-completion\n## or, if running Bash 4.1+\n\tbrew install bash-completion@2\n## If talosctl is installed via homebrew, this should start working immediately.\n## If you've installed via other means, you may need add the completion to your completion directory\n\ttalosctl completion bash > $(brew --prefix)/etc/bash_completion.d/talosctl\n\n# Installing bash completion on Linux\n## If bash-completion is not installed on Linux, please install the 'bash-completion' package\n## via your distribution's package manager.\n## Load the talosctl completion code for bash into the current shell\n\tsource <(talosctl completion bash)\n## Write bash completion code to a file and source if from .bash_profile\n\ttalosctl completion bash > ~/.talos/completion.bash.inc\n\tprintf \"\n\t\t# talosctl shell completion\n\t\tsource '$HOME/.talos/completion.bash.inc'\n\t\t\" >> $HOME/.bash_profile\n\tsource $HOME/.bash_profile\n# Load the talosctl completion code for fish[1] into the current shell\n\ttalosctl completion fish | source\n# Set the talosctl completion code for fish[1] to autoload on startup\n    talosctl completion fish > ~/.config/fish/completions/talosctl.fish\n# Load the talosctl completion code for zsh[1] into the current shell\n\tsource <(talosctl completion zsh)\n# Set the talosctl completion code for zsh[1] to autoload on startup\n    talosctl completion zsh > \"${fpath[1]}/_talosctl\"\n\nOptions\n  -h, --help   help for completion\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add\n\nAdd a new context\n\ntalosctl config add <context> [flags]\n\nOptions\n      --ca string    the path to the CA certificate\n      --crt string   the path to the certificate\n  -h, --help         help for add\n      --key string   the path to the key\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config context\n\nSet the current context\n\ntalosctl config context <context> [flags]\n\nOptions\n  -h, --help   help for context\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config contexts\n\nList defined contexts\n\ntalosctl config contexts [flags]\n\nOptions\n  -h, --help   help for contexts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config endpoint\n\nSet the endpoint(s) for the current context\n\ntalosctl config endpoint <endpoint>... [flags]\n\nOptions\n  -h, --help   help for endpoint\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config info\n\nShow information about the current context\n\ntalosctl config info [flags]\n\nOptions\n  -h, --help            help for info\n  -o, --output string   output format (json|yaml|text). Default text. (default \"text\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config merge\n\nMerge additional contexts from another client configuration file\n\nSynopsis\n\nContexts with the same name are renamed while merging configs.\n\ntalosctl config merge <from> [flags]\n\nOptions\n  -h, --help   help for merge\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config new\n\nGenerate a new client configuration file\n\ntalosctl config new [<path>] [flags]\n\nOptions\n      --crt-ttl duration   certificate TTL (default 87600h0m0s)\n  -h, --help               help for new\n      --roles strings      roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config node\n\nSet the node(s) for the current context\n\ntalosctl config node <endpoint>... [flags]\n\nOptions\n  -h, --help   help for node\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config remove\n\nRemove contexts\n\ntalosctl config remove <context> [flags]\n\nOptions\n      --dry-run     dry run\n  -h, --help        help for remove\n  -y, --noconfirm   do not ask for confirmation\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config\n\nManage the client configuration file (talosconfig)\n\nOptions\n  -h, --help   help for config\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add - Add a new context\ntalosctl config context - Set the current context\ntalosctl config contexts - List defined contexts\ntalosctl config endpoint - Set the endpoint(s) for the current context\ntalosctl config info - Show information about the current context\ntalosctl config merge - Merge additional contexts from another client configuration file\ntalosctl config new - Generate a new client configuration file\ntalosctl config node - Set the node(s) for the current context\ntalosctl config remove - Remove contexts\ntalosctl conformance kubernetes\n\nRun Kubernetes conformance tests\n\ntalosctl conformance kubernetes [flags]\n\nOptions\n  -h, --help          help for kubernetes\n      --mode string   conformance test mode: [fast, certified] (default \"fast\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl conformance - Run conformance tests\ntalosctl conformance\n\nRun conformance tests\n\nOptions\n  -h, --help   help for conformance\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl conformance kubernetes - Run Kubernetes conformance tests\ntalosctl containers\n\nList containers\n\ntalosctl containers [flags]\n\nOptions\n  -h, --help         help for containers\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl copy\n\nCopy data out from the node\n\nSynopsis\n\nCreates an .tar.gz archive at the node starting at and streams it back to the client.\n\nIf ‘-’ is given for , archive is written to stdout. Otherwise archive is extracted to which should be an empty directory or talosctl creates a directory if doesn’t exist. Command doesn’t preserve ownership and access mode for the files in extract mode, while streamed .tar archive captures ownership and permission bits.\n\ntalosctl copy <src-path> -|<local-path> [flags]\n\nOptions\n  -h, --help   help for copy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dashboard\n\nCluster dashboard with node overview, logs and real-time metrics\n\nSynopsis\n\nProvide a text-based UI to navigate node overview, logs and real-time metrics.\n\nKeyboard shortcuts:\n\nh, <Left> - switch one node to the left\nl, <Right> - switch one node to the right\nj, <Down> - scroll logs/process list down\nk, <Up> - scroll logs/process list up\n<C-d> - scroll logs/process list half page down\n<C-u> - scroll logs/process list half page up\n<C-f> - scroll logs/process list one page down\n<C-b> - scroll logs/process list one page up\ntalosctl dashboard [flags]\n\nOptions\n  -h, --help                       help for dashboard\n  -d, --update-interval duration   interval between updates (default 3s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl disks\n\nGet the list of disks from /sys/block on the machine\n\ntalosctl disks [flags]\n\nOptions\n  -h, --help       help for disks\n  -i, --insecure   get disks using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dmesg\n\nRetrieve kernel logs\n\ntalosctl dmesg [flags]\n\nOptions\n  -f, --follow   specify if the kernel log should be streamed\n  -h, --help     help for dmesg\n      --tail     specify if only new messages should be sent (makes sense only when combined with --follow)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl edit\n\nEdit a resource from the default editor.\n\nSynopsis\n\nThe edit command allows you to directly edit any API resource you can retrieve via the command line tools.\n\nIt will open the editor defined by your TALOS_EDITOR, or EDITOR environment variables, or fall back to ‘vi’ for Linux or ’notepad’ for Windows.\n\ntalosctl edit <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     do not apply the change after editing and print the change summary instead\n  -h, --help                                        help for edit\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm disarm\n\nDisarm the etcd alarms for the node.\n\ntalosctl etcd alarm disarm [flags]\n\nOptions\n  -h, --help   help for disarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm list\n\nList the etcd alarms for the node.\n\ntalosctl etcd alarm list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm\n\nManage etcd alarms\n\nOptions\n  -h, --help   help for alarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd alarm disarm - Disarm the etcd alarms for the node.\ntalosctl etcd alarm list - List the etcd alarms for the node.\ntalosctl etcd defrag\n\nDefragment etcd database on the node\n\nSynopsis\n\nDefragmentation is a maintenance operation that releases unused space from the etcd database file. Defragmentation is a resource heavy operation and should be performed only when necessary on a single node at a time.\n\ntalosctl etcd defrag [flags]\n\nOptions\n  -h, --help   help for defrag\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd forfeit-leadership\n\nTell node to forfeit etcd cluster leadership\n\ntalosctl etcd forfeit-leadership [flags]\n\nOptions\n  -h, --help   help for forfeit-leadership\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd leave\n\nTell nodes to leave etcd cluster\n\ntalosctl etcd leave [flags]\n\nOptions\n  -h, --help   help for leave\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd members\n\nGet the list of etcd cluster members\n\ntalosctl etcd members [flags]\n\nOptions\n  -h, --help   help for members\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd remove-member\n\nRemove the node from etcd cluster\n\nSynopsis\n\nUse this command only if you want to remove a member which is in broken state. If there is no access to the node, or the node can’t access etcd to call etcd leave. Always prefer etcd leave over this command. It’s always better to use member ID than hostname, as hostname might not be set consistently.\n\ntalosctl etcd remove-member <member ID>|<hostname> [flags]\n\nOptions\n  -h, --help   help for remove-member\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd snapshot\n\nStream snapshot of the etcd node to the path.\n\ntalosctl etcd snapshot <path> [flags]\n\nOptions\n  -h, --help   help for snapshot\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd status\n\nGet the status of etcd cluster member\n\nSynopsis\n\nReturns the status of etcd member on the node, use multiple nodes to get status of all members.\n\ntalosctl etcd status [flags]\n\nOptions\n  -h, --help   help for status\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd\n\nManage etcd\n\nOptions\n  -h, --help   help for etcd\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd defrag - Defragment etcd database on the node\ntalosctl etcd forfeit-leadership - Tell node to forfeit etcd cluster leadership\ntalosctl etcd leave - Tell nodes to leave etcd cluster\ntalosctl etcd members - Get the list of etcd cluster members\ntalosctl etcd remove-member - Remove the node from etcd cluster\ntalosctl etcd snapshot - Stream snapshot of the etcd node to the path.\ntalosctl etcd status - Get the status of etcd cluster member\ntalosctl events\n\nStream runtime events\n\ntalosctl events [flags]\n\nOptions\n      --actor-id string     filter events by the specified actor ID (default is no filter)\n      --duration duration   show events for the past duration interval (one second resolution, default is to show no history)\n  -h, --help                help for events\n      --since string        show events after the specified event ID (default is to show no history)\n      --tail int32          show specified number of past events (use -1 to show full history, default is to show no history)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca\n\nGenerates a self-signed X.509 certificate authority\n\ntalosctl gen ca [flags]\n\nOptions\n  -h, --help                  help for ca\n      --hours int             the hours from now on which the certificate validity period ends (default 87600)\n      --organization string   X.509 distinguished name for the Organization\n      --rsa                   generate in RSA format\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen config\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl gen config <cluster name> <cluster endpoint> [flags]\n\nOptions\n      --additional-sans strings                  additional Subject-Alt-Names for the APIServer certificate\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n  -h, --help                                     help for config\n      --install-disk string                      the disk to install to (default \"/dev/sda\")\n      --install-image string                     the image used to perform an installation (default \"ghcr.io/siderolabs/installer:latest\")\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n  -o, --output string                            destination to output generated files. when multiple output types are specified, it must be a directory. for a single output type, it must either be a file path, or \"-\" for stdout\n  -t, --output-types strings                     types of outputs to be generated. valid types are: [\"controlplane\" \"worker\" \"talosconfig\"] (default [controlplane,worker,talosconfig])\n  -p, --persist                                  the desired persist value for configs (default true)\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --talos-version string                     the desired Talos version to generate config for (backwards compatibility, e.g. v0.8)\n      --version string                           the desired machine config version to generate (default \"v1alpha1\")\n      --with-cluster-discovery                   enable cluster discovery feature (default true)\n      --with-docs                                renders all machine configs adding the documentation for each field (default true)\n      --with-examples                            renders all machine configs with the commented examples (default true)\n      --with-kubespan                            enable KubeSpan feature\n      --with-secrets string                      use a secrets file generated using 'gen secrets'\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen crt\n\nGenerates an X.509 Ed25519 certificate\n\ntalosctl gen crt [flags]\n\nOptions\n      --ca string     path to the PEM encoded CERTIFICATE\n      --csr string    path to the PEM encoded CERTIFICATE REQUEST\n  -h, --help          help for crt\n      --hours int     the hours from now on which the certificate validity period ends (default 24)\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen csr\n\nGenerates a CSR using an Ed25519 private key\n\ntalosctl gen csr [flags]\n\nOptions\n  -h, --help            help for csr\n      --ip string       generate the certificate for this IP address\n      --key string      path to the PEM encoded EC or RSA PRIVATE KEY\n      --roles strings   roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen key\n\nGenerates an Ed25519 private key\n\ntalosctl gen key [flags]\n\nOptions\n  -h, --help          help for key\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen keypair\n\nGenerates an X.509 Ed25519 key pair\n\ntalosctl gen keypair [flags]\n\nOptions\n  -h, --help                  help for keypair\n      --ip string             generate the certificate for this IP address\n      --organization string   X.509 distinguished name for the Organization\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secrets\n\nGenerates a secrets bundle file which can later be used to generate a config\n\ntalosctl gen secrets [flags]\n\nOptions\n      --from-controlplane-config string     use the provided controlplane Talos machine configuration as input\n  -p, --from-kubernetes-pki string          use a Kubernetes PKI directory (e.g. /etc/kubernetes/pki) as input\n  -h, --help                                help for secrets\n  -t, --kubernetes-bootstrap-token string   use the provided bootstrap token as input\n  -o, --output-file string                  path of the output file (default \"secrets.yaml\")\n      --talos-version string                the desired Talos version to generate secrets bundle for (backwards compatibility, e.g. v0.8)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database\n\nGenerates a UEFI database to enroll the signing certificate\n\ntalosctl gen secureboot database [flags]\n\nOptions\n      --enrolled-certificate string   path to the certificate to enroll (default \"_out/uki-signing-cert.pem\")\n  -h, --help                          help for database\n      --signing-certificate string    path to the certificate used to sign the database (default \"_out/uki-signing-cert.pem\")\n      --signing-key string            path to the key used to sign the database (default \"_out/uki-signing-key.pem\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot pcr\n\nGenerates a key which is used to sign TPM PCR values\n\ntalosctl gen secureboot pcr [flags]\n\nOptions\n  -h, --help   help for pcr\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot uki\n\nGenerates a certificate which is used to sign boot assets (UKI)\n\ntalosctl gen secureboot uki [flags]\n\nOptions\n      --common-name string   common name for the certificate (default \"Test UKI Signing Key\")\n  -h, --help                 help for uki\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot\n\nGenerates secrets for the SecureBoot process\n\nOptions\n  -h, --help            help for secureboot\n  -o, --output string   path to the directory storing the generated files (default \"_out\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database - Generates a UEFI database to enroll the signing certificate\ntalosctl gen secureboot pcr - Generates a key which is used to sign TPM PCR values\ntalosctl gen secureboot uki - Generates a certificate which is used to sign boot assets (UKI)\ntalosctl gen\n\nGenerate CAs, certificates, and private keys\n\nOptions\n  -f, --force   will overwrite existing files\n  -h, --help    help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca - Generates a self-signed X.509 certificate authority\ntalosctl gen config - Generates a set of configuration files for Talos cluster\ntalosctl gen crt - Generates an X.509 Ed25519 certificate\ntalosctl gen csr - Generates a CSR using an Ed25519 private key\ntalosctl gen key - Generates an Ed25519 private key\ntalosctl gen keypair - Generates an X.509 Ed25519 key pair\ntalosctl gen secrets - Generates a secrets bundle file which can later be used to generate a config\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl get\n\nGet a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\n\nSynopsis\n\nSimilar to ‘kubectl get’, ’talosctl get’ returns a set of resources from the OS. To get a list of all available resource definitions, issue ’talosctl get rd’\n\ntalosctl get <type> [<id>] [flags]\n\nOptions\n  -h, --help               help for get\n  -i, --insecure           get resources using the insecure (encrypted with no auth) maintenance service\n      --namespace string   resource namespace (default is to use default namespace per resource)\n  -o, --output string      output mode (json, table, yaml, jsonpath) (default \"table\")\n  -w, --watch              watch resource changes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl health\n\nCheck cluster health\n\ntalosctl health [flags]\n\nOptions\n      --control-plane-nodes strings   specify IPs of control plane nodes\n  -h, --help                          help for health\n      --init-node string              specify IPs of init node\n      --k8s-endpoint string           use endpoint instead of kubeconfig default\n      --run-e2e                       run Kubernetes e2e test\n      --server                        run server-side check (default true)\n      --wait-timeout duration         timeout to wait for the cluster to be ready (default 20m0s)\n      --worker-nodes strings          specify IPs of worker nodes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default\n\nList the default images used by Talos\n\ntalosctl image default [flags]\n\nOptions\n  -h, --help   help for default\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image list\n\nList CRI images\n\ntalosctl image list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image pull\n\nPull an image into CRI\n\ntalosctl image pull [flags]\n\nOptions\n  -h, --help   help for pull\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image\n\nManage CRI containter images\n\nOptions\n  -h, --help               help for image\n      --namespace system   namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default - List the default images used by Talos\ntalosctl image list - List CRI images\ntalosctl image pull - Pull an image into CRI\ntalosctl inject serviceaccount\n\nInject Talos API ServiceAccount into Kubernetes manifests\n\ntalosctl inject serviceaccount [--roles='<ROLE_1>,<ROLE_2>'] -f <manifest.yaml> [flags]\n\nExamples\ntalosctl inject serviceaccount --roles=\"os:admin\" -f deployment.yaml > deployment-injected.yaml\n\nAlternatively, stdin can be piped to the command:\ncat deployment.yaml | talosctl inject serviceaccount --roles=\"os:admin\" -f - > deployment-injected.yaml\n\nOptions\n  -f, --file string     file with Kubernetes manifests to be injected with ServiceAccount\n  -h, --help            help for serviceaccount\n  -r, --roles strings   roles to add to the generated ServiceAccount manifests (default [os:reader])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inject\n\nInject Talos API resources into Kubernetes manifests\n\nOptions\n  -h, --help   help for inject\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inject serviceaccount - Inject Talos API ServiceAccount into Kubernetes manifests\ntalosctl inspect dependencies\n\nInspect controller-resource dependencies as graphviz graph.\n\nSynopsis\n\nInspect controller-resource dependencies as graphviz graph.\n\nPipe the output of the command through the “dot” program (part of graphviz package) to render the graph:\n\ntalosctl inspect dependencies | dot -Tpng > graph.png\n\ntalosctl inspect dependencies [flags]\n\nOptions\n  -h, --help             help for dependencies\n      --with-resources   display live resource information with dependencies\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inspect - Inspect internals of Talos\ntalosctl inspect\n\nInspect internals of Talos\n\nOptions\n  -h, --help   help for inspect\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inspect dependencies - Inspect controller-resource dependencies as graphviz graph.\ntalosctl kubeconfig\n\nDownload the admin kubeconfig from the node\n\nSynopsis\n\nDownload the admin kubeconfig from the node. If merge flag is defined, config will be merged with ~/.kube/config or [local-path] if specified. Otherwise kubeconfig will be written to PWD or [local-path] if specified.\n\ntalosctl kubeconfig [local-path] [flags]\n\nOptions\n  -f, --force                       Force overwrite of kubeconfig if already present, force overwrite on kubeconfig merge\n      --force-context-name string   Force context name for kubeconfig merge\n  -h, --help                        help for kubeconfig\n  -m, --merge                       Merge with existing kubeconfig (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl list\n\nRetrieve a directory listing\n\ntalosctl list [path] [flags]\n\nOptions\n  -d, --depth int32    maximum recursion depth (default 1)\n  -h, --help           help for list\n  -H, --humanize       humanize size and time in the output\n  -l, --long           display additional file details\n  -r, --recurse        recurse into subdirectories\n  -t, --type strings   filter by specified types:\n                       f\tregular file\n                       d\tdirectory\n                       l, L\tsymbolic link\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl logs\n\nRetrieve logs for a service\n\ntalosctl logs <service name> [flags]\n\nOptions\n  -f, --follow       specify if the logs should be streamed\n  -h, --help         help for logs\n  -k, --kubernetes   use the k8s.io containerd namespace\n      --tail int32   lines of log file to display (default is to show from the beginning) (default -1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl machineconfig gen <cluster name> <cluster endpoint> [flags]\n\nOptions\n  -h, --help   help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig patch\n\nPatch a machine config\n\ntalosctl machineconfig patch <machineconfig-file> [flags]\n\nOptions\n  -h, --help                help for patch\n  -o, --output string       output destination. if not specified, output will be printed to stdout\n  -p, --patch stringArray   patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig\n\nMachine config related commands\n\nOptions\n  -h, --help   help for machineconfig\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen - Generates a set of configuration files for Talos cluster\ntalosctl machineconfig patch - Patch a machine config\ntalosctl memory\n\nShow memory usage\n\ntalosctl memory [flags]\n\nOptions\n  -h, --help      help for memory\n  -v, --verbose   display extended memory statistics\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete\n\nDelete a key from the META partition.\n\ntalosctl meta delete key [flags]\n\nOptions\n  -h, --help   help for delete\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta write\n\nWrite a key-value pair to the META partition.\n\ntalosctl meta write key value [flags]\n\nOptions\n  -h, --help   help for write\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta\n\nWrite and delete keys in the META partition\n\nOptions\n  -h, --help       help for meta\n  -i, --insecure   write|delete meta using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete - Delete a key from the META partition.\ntalosctl meta write - Write a key-value pair to the META partition.\ntalosctl mounts\n\nList mounts\n\ntalosctl mounts [flags]\n\nOptions\n  -h, --help   help for mounts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl netstat\n\nShow network connections and sockets\n\nSynopsis\n\nShow network connections and sockets.\n\nYou can pass an optional argument to view a specific pod’s connections. To do this, format the argument as “namespace/pod”. Note that only pods with a pod network namespace are allowed. If you don’t pass an argument, the command will show host connections.\n\ntalosctl netstat [flags]\n\nOptions\n  -a, --all         display all sockets states (default: connected)\n  -x, --extend      show detailed socket information\n  -h, --help        help for netstat\n  -4, --ipv4        display only ipv4 sockets\n  -6, --ipv6        display only ipv6 sockets\n  -l, --listening   display listening server sockets\n  -k, --pods        show sockets used by Kubernetes pods\n  -p, --programs    show process using socket\n  -w, --raw         display only RAW sockets\n  -t, --tcp         display only TCP sockets\n  -o, --timers      display timers\n  -u, --udp         display only UDP sockets\n  -U, --udplite     display only UDPLite sockets\n  -v, --verbose     display sockets of all supported transport protocols\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl patch\n\nUpdate field(s) of a resource using a JSON patch.\n\ntalosctl patch <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     print the change summary and patch preview without applying the changes\n  -h, --help                                        help for patch\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n  -p, --patch stringArray                           the patch to be applied to the resource file, use @file to read a patch from file.\n      --patch-file string                           a file containing a patch to be applied to the resource.\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl pcap\n\nCapture the network packets from the node.\n\nSynopsis\n\nThe command launches packet capture on the node and streams back the packets as raw pcap file.\n\nDefault behavior is to decode the packets with internal decoder to stdout:\n\ntalosctl pcap -i eth0\n\n\nRaw pcap file can be saved with --output flag:\n\ntalosctl pcap -i eth0 --output eth0.pcap\n\n\nOutput can be piped to tcpdump:\n\ntalosctl pcap -i eth0 -o - | tcpdump -vvv -r -\n\n\nBPF filter can be applied, but it has to compiled to BPF instructions first using tcpdump. Correct link type should be specified for the tcpdump: EN10MB for Ethernet links and RAW for e.g. Wireguard tunnels:\n\ntalosctl pcap -i eth0 --bpf-filter \"$(tcpdump -dd -y EN10MB 'tcp and dst port 80')\"\n\ntalosctl pcap -i kubespan --bpf-filter \"$(tcpdump -dd -y RAW 'port 50000')\"\n\n\nAs packet capture is transmitted over the network, it is recommended to filter out the Talos API traffic, e.g. by excluding packets with the port 50000.\n\ntalosctl pcap [flags]\n\nOptions\n      --bpf-filter string   bpf filter to apply, tcpdump -dd format\n      --duration duration   duration of the capture\n  -h, --help                help for pcap\n  -i, --interface string    interface name to capture packets on (default \"eth0\")\n  -o, --output string       if not set, decode packets to stdout; if set write raw pcap data to a file, use '-' for stdout\n      --promiscuous         put interface into promiscuous mode\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl processes\n\nList running processes\n\ntalosctl processes [flags]\n\nOptions\n  -h, --help          help for processes\n  -s, --sort string   Column to sort output by. [rss|cpu] (default \"rss\")\n  -w, --watch         Stream running processes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl read\n\nRead a file on the machine\n\ntalosctl read <path> [flags]\n\nOptions\n  -h, --help   help for read\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reboot\n\nReboot a node\n\ntalosctl reboot [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n  -h, --help               help for reboot\n  -m, --mode string        select the reboot mode: \"default\", \"powercycle\" (skips kexec) (default \"default\")\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reset\n\nReset a node\n\ntalosctl reset [flags]\n\nOptions\n      --debug                                    debug operation from kernel logs. --wait is set to true when this flag is set\n      --graceful                                 if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n  -h, --help                                     help for reset\n      --insecure                                 reset using the insecure (encrypted with no auth) maintenance service\n      --reboot                                   if true, reboot the node after resetting instead of shutting down\n      --system-labels-to-wipe strings            if set, just wipe selected system disk partitions by label but keep other partitions intact\n      --timeout duration                         time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --user-disks-to-wipe strings               if set, wipes defined devices in the list\n      --wait                                     wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n      --wipe-mode all, system-disk, user-disks   disk reset mode (default all)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl restart\n\nRestart a process\n\ntalosctl restart <id> [flags]\n\nOptions\n  -h, --help         help for restart\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl rollback\n\nRollback a node to the previous installation\n\ntalosctl rollback [flags]\n\nOptions\n  -h, --help   help for rollback\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl service\n\nRetrieve the state of a service (or all services), control service state\n\nSynopsis\n\nService control command. If run without arguments, lists all the services and their state. If service ID is specified, default action ‘status’ is executed which shows status of a single list service. With actions ‘start’, ‘stop’, ‘restart’, service state is updated respectively.\n\ntalosctl service [<id> [start|stop|restart|status]] [flags]\n\nOptions\n  -h, --help   help for service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl shutdown\n\nShutdown a node\n\ntalosctl shutdown [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n      --force              if true, force a node to shutdown without a cordon/drain\n  -h, --help               help for shutdown\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl stats\n\nGet container stats\n\ntalosctl stats [flags]\n\nOptions\n  -h, --help         help for stats\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl support\n\nDump debug information about the cluster\n\nSynopsis\n\nGenerated bundle contains the following debug information:\n\nFor each node:\n\nKernel logs.\nAll Talos internal services logs.\nAll kube-system pods logs.\nTalos COSI resources without secrets.\nCOSI runtime state graph.\nProcesses snapshot.\nIO pressure snapshot.\nMounts list.\nPCI devices info.\nTalos version.\n\nFor the cluster:\n\nKubernetes nodes and kube-system pods manifests.\ntalosctl support [flags]\n\nOptions\n  -h, --help              help for support\n  -w, --num-workers int   number of workers per node (default 1)\n  -O, --output string     output file to write support archive to\n  -v, --verbose           verbose output\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl time\n\nGets current server time\n\ntalosctl time [--check server] [flags]\n\nOptions\n  -c, --check string   checks server time against specified ntp server\n  -h, --help           help for time\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade\n\nUpgrade Talos on the target node\n\ntalosctl upgrade [flags]\n\nOptions\n      --debug                debug operation from kernel logs. --wait is set to true when this flag is set\n  -f, --force                force the upgrade (skip checks on etcd health and members, might lead to data loss)\n  -h, --help                 help for upgrade\n  -i, --image string         the container image to use for performing the install (default \"ghcr.io/siderolabs/installer:v1.6.0-alpha.2\")\n      --insecure             upgrade using the insecure (encrypted with no auth) maintenance service\n  -p, --preserve             preserve data\n  -m, --reboot-mode string   select the reboot mode during upgrade. Mode \"powercycle\" bypasses kexec. Valid values are: [\"default\" \"powercycle\"]. (default \"default\")\n  -s, --stage                stage the upgrade to perform it after a reboot\n      --timeout duration     time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait                 wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade-k8s\n\nUpgrade Kubernetes control plane in the Talos cluster.\n\nSynopsis\n\nCommand runs upgrade of Kubernetes control plane components between specified versions.\n\ntalosctl upgrade-k8s [flags]\n\nOptions\n      --dry-run           skip the actual upgrade and show the upgrade plan instead\n      --endpoint string   the cluster control plane endpoint\n      --from string       the Kubernetes control plane version to upgrade from\n  -h, --help              help for upgrade-k8s\n      --pre-pull-images   pre-pull images before upgrade (default true)\n      --to string         the Kubernetes control plane version to upgrade to (default \"1.29.0\")\n      --upgrade-kubelet   upgrade kubelet service (default true)\n      --with-docs         patch all machine configs adding the documentation for each field (default true)\n      --with-examples     patch all machine configs with the commented examples (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl usage\n\nRetrieve a disk usage\n\ntalosctl usage [path1] [path2] ... [pathN] [flags]\n\nOptions\n  -a, --all             write counts for all files, not just directories\n  -d, --depth int32     maximum recursion depth\n  -h, --help            help for usage\n  -H, --humanize        humanize size and time in the output\n  -t, --threshold int   threshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl validate\n\nValidate config\n\ntalosctl validate [flags]\n\nOptions\n  -c, --config string   the path of the config file\n  -h, --help            help for validate\n  -m, --mode string     the mode to validate the config for (valid values are metal, cloud, and container)\n      --strict          treat validation warnings as errors\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl version\n\nPrints the version\n\ntalosctl version [flags]\n\nOptions\n      --client     Print client version only\n  -h, --help       help for version\n  -i, --insecure   use Talos maintenance mode API\n      --short      Print the short version\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl\n\nA CLI for out-of-band management of Kubernetes nodes created by Talos\n\nOptions\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -h, --help                 help for talosctl\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl apply-config - Apply a new configuration to a node\ntalosctl bootstrap - Bootstrap the etcd cluster on the specified node.\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl completion - Output shell completion code for the specified shell (bash, fish or zsh)\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl conformance - Run conformance tests\ntalosctl containers - List containers\ntalosctl copy - Copy data out from the node\ntalosctl dashboard - Cluster dashboard with node overview, logs and real-time metrics\ntalosctl disks - Get the list of disks from /sys/block on the machine\ntalosctl dmesg - Retrieve kernel logs\ntalosctl edit - Edit a resource from the default editor.\ntalosctl etcd - Manage etcd\ntalosctl events - Stream runtime events\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl get - Get a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\ntalosctl health - Check cluster health\ntalosctl image - Manage CRI containter images\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inspect - Inspect internals of Talos\ntalosctl kubeconfig - Download the admin kubeconfig from the node\ntalosctl list - Retrieve a directory listing\ntalosctl logs - Retrieve logs for a service\ntalosctl machineconfig - Machine config related commands\ntalosctl memory - Show memory usage\ntalosctl meta - Write and delete keys in the META partition\ntalosctl mounts - List mounts\ntalosctl netstat - Show network connections and sockets\ntalosctl patch - Update field(s) of a resource using a JSON patch.\ntalosctl pcap - Capture the network packets from the node.\ntalosctl processes - List running processes\ntalosctl read - Read a file on the machine\ntalosctl reboot - Reboot a node\ntalosctl reset - Reset a node\ntalosctl restart - Restart a process\ntalosctl rollback - Rollback a node to the previous installation\ntalosctl service - Retrieve the state of a service (or all services), control service state\ntalosctl shutdown - Shutdown a node\ntalosctl stats - Get container stats\ntalosctl support - Dump debug information about the cluster\ntalosctl time - Gets current server time\ntalosctl upgrade - Upgrade Talos on the target node\ntalosctl upgrade-k8s - Upgrade Kubernetes control plane in the Talos cluster.\ntalosctl usage - Retrieve a disk usage\ntalosctl validate - Validate config\ntalosctl version - Prints the version\n5.3 - Configuration\nTalos Linux machine configuration reference.\n\nTalos Linux machine is fully configured via a single YAML file called machine configuration.\n\nThe file might contain one or more configuration documents separated by --- (three dashes) lines. At the moment, majority of the configuration options are within the v1alpha1 document, so this is the only mandatory document in the configuration file.\n\nConfiguration documents might be named (contain a name: field) or unnamed. Unnamed documents can be supplied to the machine configuration file only once, while named documents can be supplied multiple times with unique names.\n\nThe v1alpha1 document has its own (legacy) structure, while every other document has the following set of fields:\n\napiVersion: v1alpha1 # version of the document\n\nkind: NetworkRuleConfig # type of document\n\nname: rule1 # only for named documents\n\n\nThis section contains the configuration reference, to learn more about Talos Linux machine configuration management, please see:\n\nquick guide to configuration generation\nconfiguration management in production\nconfiguration patches\nediting live machine configuration\n5.3.1 - network\nPackage network provides network machine configuration documents.\n5.3.1.1 - NetworkDefaultActionConfig\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: accept # Default action for all not explicitly configured ingress traffic: accept or block.\nField\tType\tDescription\tValue(s)\ningress\tDefaultAction\tDefault action for all not explicitly configured ingress traffic: accept or block.\taccept\nblock\n\n5.3.1.2 - NetworkRuleConfig\nNetworkRuleConfig is a network firewall rule config document.\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: ingress-apid # Name of the config document.\n\n# Port selector defines which ports and protocols on the host are affected by the rule.\n\nportSelector:\n\n    # Ports defines a list of port ranges or single ports.\n\n    ports:\n\n        - 50000\n\n    protocol: tcp # Protocol defines traffic protocol (e.g. TCP or UDP).\n\n# Ingress defines which source subnets are allowed to access the host ports/protocols defined by the `portSelector`.\n\ningress:\n\n    - subnet: 192.168.0.0/16 # Subnet defines a source subnet.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nportSelector\tRulePortSelector\tPort selector defines which ports and protocols on the host are affected by the rule.\t\ningress\t[]IngressRule\tIngress defines which source subnets are allowed to access the host ports/protocols defined by the portSelector.\t\nportSelector\n\nRulePortSelector is a port selector for the network rule.\n\nField\tType\tDescription\tValue(s)\nports\tPortRanges\t\nPorts defines a list of port ranges or single ports.\nShow example(s)\n\t\nprotocol\tProtocol\tProtocol defines traffic protocol (e.g. TCP or UDP).\ttcp\nudp\nicmp\nicmpv6\n\ningress[]\n\nIngressRule is a ingress rule.\n\nField\tType\tDescription\tValue(s)\nsubnet\tPrefix\tSubnet defines a source subnet.\nShow example(s)\n\t\nexcept\tPrefix\tExcept defines a source subnet to exclude from the rule, it gets excluded from the subnet.\t\n5.3.2 - runtime\nPackage runtime provides runtime machine configuration documents.\n5.3.2.1 - EventSinkConfig\nEventSinkConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: EventSinkConfig\n\nendpoint: 192.168.10.3:3247 # The endpoint for the event sink as 'host:port'.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tThe endpoint for the event sink as ‘host:port’.\nShow example(s)\n\t\n5.3.2.2 - KmsgLogConfig\nKmsgLogConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log # Name of the config document.\n\nurl: tcp://192.168.3.7:3478/ # The URL encodes the log destination.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nurl\tURL\t\nThe URL encodes the log destination.\nShow example(s)\n\t\n5.3.3 - siderolink\nPackage siderolink provides SideroLink machine configuration documents.\n5.3.3.1 - SideroLinkConfig\nSideroLinkConfig is a SideroLink connection machine configuration document.\napiVersion: v1alpha1\n\nkind: SideroLinkConfig\n\napiUrl: https://siderolink.api/join?token=secret # SideroLink API URL to connect to.\nField\tType\tDescription\tValue(s)\napiUrl\tURL\tSideroLink API URL to connect to.\nShow example(s)\n\t\n5.3.4 - v1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\n5.3.4.1 - Config\nConfig defines the v1alpha1.Config Talos machine configuration document.\nversion: v1alpha1\n\nmachine: # ...\n\ncluster: # ...\nField\tType\tDescription\tValue(s)\nversion\tstring\tIndicates the schema used to decode the contents.\tv1alpha1\n\ndebug\tbool\t\nEnable verbose logging to the console.\n\ttrue\nyes\nfalse\nno\n\nmachine\tMachineConfig\tProvides machine specific configuration options.\t\ncluster\tClusterConfig\tProvides cluster specific configuration options.\t\nmachine\n\nMachineConfig represents the machine-specific config values.\n\nmachine:\n\n    type: controlplane\n\n    # InstallConfig represents the installation options for preparing a node.\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ntype\tstring\t\nDefines the role of the machine within the cluster.\n\tcontrolplane\nworker\n\ntoken\tstring\t\nThe token is used by a machine to join the PKI of the cluster.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe root certificate authority of the PKI.\nShow example(s)\n\t\ncertSANs\t[]string\t\nExtra certificate subject alternative names for the machine’s certificate.\nShow example(s)\n\t\ncontrolPlane\tMachineControlPlaneConfig\tProvides machine specific control plane configuration options.\nShow example(s)\n\t\nkubelet\tKubeletConfig\tUsed to provide additional options to the kubelet.\nShow example(s)\n\t\npods\t[]Unstructured\t\nUsed to provide static pod definitions to be run by the kubelet directly bypassing the kube-apiserver.\nShow example(s)\n\t\nnetwork\tNetworkConfig\tProvides machine specific network configuration options.\nShow example(s)\n\t\ndisks\t[]MachineDisk\t\nUsed to partition, format and mount additional disks.\nShow example(s)\n\t\ninstall\tInstallConfig\t\nUsed to provide instructions for installations.\nShow example(s)\n\t\nfiles\t[]MachineFile\t\nAllows the addition of user specified files.\nShow example(s)\n\t\nenv\tEnv\t\nThe env field allows for the addition of environment variables.\nShow example(s)\n\tGRPC_GO_LOG_VERBOSITY_LEVEL\nGRPC_GO_LOG_SEVERITY_LEVEL\nhttp_proxy\nhttps_proxy\nno_proxy\n\ntime\tTimeConfig\tUsed to configure the machine’s time settings.\nShow example(s)\n\t\nsysctls\tmap[string]string\tUsed to configure the machine’s sysctls.\nShow example(s)\n\t\nsysfs\tmap[string]string\tUsed to configure the machine’s sysfs.\nShow example(s)\n\t\nregistries\tRegistriesConfig\t\nUsed to configure the machine’s container image registry mirrors.\nShow example(s)\n\t\nsystemDiskEncryption\tSystemDiskEncryptionConfig\t\nMachine system disk encryption configuration.\nShow example(s)\n\t\nfeatures\tFeaturesConfig\tFeatures describe individual Talos features that can be switched on or off.\nShow example(s)\n\t\nudev\tUdevConfig\tConfigures the udev system.\nShow example(s)\n\t\nlogging\tLoggingConfig\tConfigures the logging system.\nShow example(s)\n\t\nkernel\tKernelConfig\tConfigures the kernel.\nShow example(s)\n\t\nseccompProfiles\t[]MachineSeccompProfile\tConfigures the seccomp profiles for the machine.\nShow example(s)\n\t\nnodeLabels\tmap[string]string\tConfigures the node labels for the machine.\nShow example(s)\n\t\nnodeTaints\tmap[string]string\tConfigures the node taints for the machine. Effect is optional.\nShow example(s)\n\t\ncontrolPlane\n\nMachineControlPlaneConfig machine specific configuration options.\n\nmachine:\n\n    controlPlane:\n\n        # Controller manager machine specific configuration options.\n\n        controllerManager:\n\n            disabled: false # Disable kube-controller-manager on the node.\n\n        # Scheduler machine specific configuration options.\n\n        scheduler:\n\n            disabled: true # Disable kube-scheduler on the node.\nField\tType\tDescription\tValue(s)\ncontrollerManager\tMachineControllerManagerConfig\tController manager machine specific configuration options.\t\nscheduler\tMachineSchedulerConfig\tScheduler machine specific configuration options.\t\ncontrollerManager\n\nMachineControllerManagerConfig represents the machine specific ControllerManager config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-controller-manager on the node.\t\nscheduler\n\nMachineSchedulerConfig represents the machine specific Scheduler config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-scheduler on the node.\t\nkubelet\n\nKubeletConfig represents the kubelet config values.\n\nmachine:\n\n    kubelet:\n\n        image: ghcr.io/siderolabs/kubelet:v1.29.0 # The `image` field is an optional reference to an alternative kubelet image.\n\n        # The `extraArgs` field is used to provide additional flags to the kubelet.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n\n\n        # # The `ClusterDNS` field is an optional reference to an alternative kubelet clusterDNS ip list.\n\n        # clusterDNS:\n\n        #     - 10.96.0.10\n\n        #     - 169.254.2.53\n\n\n\n        # # The `extraMounts` field is used to add additional mounts to the kubelet container.\n\n        # extraMounts:\n\n        #     - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n        #       type: bind # Type specifies the mount kind.\n\n        #       source: /var/lib/example # Source specifies the source path of the mount.\n\n        #       # Options are fstab style mount options.\n\n        #       options:\n\n        #         - bind\n\n        #         - rshared\n\n        #         - rw\n\n\n\n        # # The `extraConfig` field is used to provide kubelet configuration overrides.\n\n        # extraConfig:\n\n        #     serverTLSBootstrap: true\n\n\n\n        # # The `KubeletCredentialProviderConfig` field is used to provide kubelet credential configuration.\n\n        # credentialProviderConfig:\n\n        #     apiVersion: kubelet.config.k8s.io/v1\n\n        #     kind: CredentialProviderConfig\n\n        #     providers:\n\n        #         - apiVersion: credentialprovider.kubelet.k8s.io/v1\n\n        #           defaultCacheDuration: 12h\n\n        #           matchImages:\n\n        #             - '*.dkr.ecr.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.*.amazonaws.com.cn'\n\n        #             - '*.dkr.ecr-fips.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.us-iso-east-1.c2s.ic.gov'\n\n        #             - '*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov'\n\n        #           name: ecr-credential-provider\n\n\n\n        # # The `nodeIP` field is used to configure `--node-ip` flag for the kubelet.\n\n        # nodeIP:\n\n        #     # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n        #     validSubnets:\n\n        #         - 10.0.0.0/8\n\n        #         - '!10.0.0.3/32'\n\n        #         - fdc7::/16\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe image field is an optional reference to an alternative kubelet image.\nShow example(s)\n\t\nclusterDNS\t[]string\tThe ClusterDNS field is an optional reference to an alternative kubelet clusterDNS ip list.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tThe extraArgs field is used to provide additional flags to the kubelet.\nShow example(s)\n\t\nextraMounts\t[]ExtraMount\t\nThe extraMounts field is used to add additional mounts to the kubelet container.\nShow example(s)\n\t\nextraConfig\tUnstructured\t\nThe extraConfig field is used to provide kubelet configuration overrides.\nShow example(s)\n\t\ncredentialProviderConfig\tUnstructured\tThe KubeletCredentialProviderConfig field is used to provide kubelet credential configuration.\nShow example(s)\n\t\ndefaultRuntimeSeccompProfileEnabled\tbool\tEnable container runtime default Seccomp profile.\ttrue\nyes\nfalse\nno\n\nregisterWithFQDN\tbool\t\nThe registerWithFQDN field is used to force kubelet to use the node FQDN for registration.\n\ttrue\nyes\nfalse\nno\n\nnodeIP\tKubeletNodeIPConfig\t\nThe nodeIP field is used to configure --node-ip flag for the kubelet.\nShow example(s)\n\t\nskipNodeRegistration\tbool\t\nThe skipNodeRegistration is used to run the kubelet without registering with the apiserver.\n\ttrue\nyes\nfalse\nno\n\ndisableManifestsDirectory\tbool\t\nThe disableManifestsDirectory field configures the kubelet to get static pod manifests from the /etc/kubernetes/manifests directory.\n\ttrue\nyes\nfalse\nno\n\nextraMounts[]\n\nExtraMount wraps OCI Mount specification.\n\nmachine:\n\n    kubelet:\n\n        extraMounts:\n\n            - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n              type: bind # Type specifies the mount kind.\n\n              source: /var/lib/example # Source specifies the source path of the mount.\n\n              # Options are fstab style mount options.\n\n              options:\n\n                - bind\n\n                - rshared\n\n                - rw\nField\tType\tDescription\tValue(s)\ndestination\tstring\tDestination is the absolute path where the mount will be placed in the container.\t\ntype\tstring\tType specifies the mount kind.\t\nsource\tstring\tSource specifies the source path of the mount.\t\noptions\t[]string\tOptions are fstab style mount options.\t\nuidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\ngidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\nuidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\ngidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\nnodeIP\n\nKubeletNodeIPConfig represents the kubelet node IP configuration.\n\nmachine:\n\n    kubelet:\n\n        nodeIP:\n\n            # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n            validSubnets:\n\n                - 10.0.0.0/8\n\n                - '!10.0.0.3/32'\n\n                - fdc7::/16\nField\tType\tDescription\tValue(s)\nvalidSubnets\t[]string\t\nThe validSubnets field configures the networks to pick kubelet node IP from.\n\t\nnetwork\n\nNetworkConfig represents the machine’s networking config values.\n\nmachine:\n\n    network:\n\n        hostname: worker-1 # Used to statically set the hostname for the machine.\n\n        # `interfaces` is used to define the network interface configuration.\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\n\n        # Used to statically set the nameservers for the machine.\n\n        nameservers:\n\n            - 9.8.7.6\n\n            - 8.7.6.5\n\n\n\n        # # Allows for extra entries to be added to the `/etc/hosts` file\n\n        # extraHostEntries:\n\n        #     - ip: 192.168.1.100 # The IP of the host.\n\n        #       # The host alias.\n\n        #       aliases:\n\n        #         - example\n\n        #         - example.domain.tld\n\n\n\n        # # Configures KubeSpan feature.\n\n        # kubespan:\n\n        #     enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nhostname\tstring\tUsed to statically set the hostname for the machine.\t\ninterfaces\t[]Device\t\ninterfaces is used to define the network interface configuration.\nShow example(s)\n\t\nnameservers\t[]string\t\nUsed to statically set the nameservers for the machine.\nShow example(s)\n\t\nextraHostEntries\t[]ExtraHost\tAllows for extra entries to be added to the /etc/hosts file\nShow example(s)\n\t\nkubespan\tNetworkKubeSpan\tConfigures KubeSpan feature.\nShow example(s)\n\t\ndisableSearchDomain\tbool\t\nDisable generating a default search domain in /etc/resolv.conf\n\ttrue\nyes\nfalse\nno\n\ninterfaces[]\n\nDevice represents a network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\ninterface\tstring\t\nThe interface name.\nShow example(s)\n\t\ndeviceSelector\tNetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\naddresses\t[]string\t\nAssigns static IP addresses to the interface.\nShow example(s)\n\t\nroutes\t[]Route\t\nA list of routes associated with the interface.\nShow example(s)\n\t\nbond\tBond\tBond specific options.\nShow example(s)\n\t\nbridge\tBridge\tBridge specific options.\nShow example(s)\n\t\nvlans\t[]Vlan\tVLAN specific options.\t\nmtu\tint\t\nThe interface’s MTU.\n\t\ndhcp\tbool\t\nIndicates if DHCP should be used to configure the interface.\nShow example(s)\n\t\nignore\tbool\tIndicates if the interface should be ignored (skips configuration).\t\ndummy\tbool\t\nIndicates if the interface is a dummy interface.\n\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\nShow example(s)\n\t\nwireguard\tDeviceWireguardConfig\t\nWireguard specific configuration.\nShow example(s)\n\t\nvip\tDeviceVIPConfig\tVirtual (shared) IP address configuration.\nShow example(s)\n\t\ndeviceSelector\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                  driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                - network: 10.2.0.0/16 # The route's network (destination).\n\n                  gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nbond\n\nBond contains the various options for configuring a bonded interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                # The interfaces that make up the bond.\n\n                interfaces:\n\n                    - enp2s0\n\n                    - enp2s1\n\n                mode: 802.3ad # A bond option.\n\n                lacpRate: fast # A bond option.\n\n\n\n                # # Picks a network device using the selector.\n\n\n\n                # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n                # deviceSelectors:\n\n                #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                #       driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bond.\t\ndeviceSelectors\t[]NetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\narpIPTarget\t[]string\t\nA bond option.\n\t\nmode\tstring\t\nA bond option.\n\t\nxmitHashPolicy\tstring\t\nA bond option.\n\t\nlacpRate\tstring\t\nA bond option.\n\t\nadActorSystem\tstring\t\nA bond option.\n\t\narpValidate\tstring\t\nA bond option.\n\t\narpAllTargets\tstring\t\nA bond option.\n\t\nprimary\tstring\t\nA bond option.\n\t\nprimaryReselect\tstring\t\nA bond option.\n\t\nfailOverMac\tstring\t\nA bond option.\n\t\nadSelect\tstring\t\nA bond option.\n\t\nmiimon\tuint32\t\nA bond option.\n\t\nupdelay\tuint32\t\nA bond option.\n\t\ndowndelay\tuint32\t\nA bond option.\n\t\narpInterval\tuint32\t\nA bond option.\n\t\nresendIgmp\tuint32\t\nA bond option.\n\t\nminLinks\tuint32\t\nA bond option.\n\t\nlpInterval\tuint32\t\nA bond option.\n\t\npacketsPerSlave\tuint32\t\nA bond option.\n\t\nnumPeerNotif\tuint8\t\nA bond option.\n\t\ntlbDynamicLb\tuint8\t\nA bond option.\n\t\nallSlavesActive\tuint8\t\nA bond option.\n\t\nuseCarrier\tbool\t\nA bond option.\n\t\nadActorSysPrio\tuint16\t\nA bond option.\n\t\nadUserPortKey\tuint16\t\nA bond option.\n\t\npeerNotifyDelay\tuint32\t\nA bond option.\n\t\ndeviceSelectors[]\n\nNetworkDeviceSelector struct describes network device selector.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                    driver: virtio # Kernel driver, supports matching by wildcard.\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                    - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                      driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nbridge\n\nBridge contains the various options for configuring a bridge interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bridge:\n\n                # The interfaces that make up the bridge.\n\n                interfaces:\n\n                    - enxda4042ca9a51\n\n                    - enxae2a6774c259\n\n                # A bridge option.\n\n                stp:\n\n                    enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bridge.\t\nstp\tSTP\t\nA bridge option.\n\t\nstp\n\nSTP contains the various options for configuring the STP properties of a bridge interface.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tWhether Spanning Tree Protocol (STP) is enabled.\t\nvlans[]\n\nVlan represents vlan settings for a device.\n\nField\tType\tDescription\tValue(s)\naddresses\t[]string\tThe addresses in CIDR notation or as plain IPs to use.\t\nroutes\t[]Route\tA list of routes associated with the VLAN.\t\ndhcp\tbool\tIndicates if DHCP should be used.\t\nvlanId\tuint16\tThe VLAN’s ID.\t\nmtu\tuint32\tThe VLAN’s MTU.\t\nvip\tDeviceVIPConfig\tThe VLAN’s virtual IP address configuration.\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\n\t\nroutes[]\n\nRoute represents a network route.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - routes:\n\n                    - network: 0.0.0.0/0 # The route's network (destination).\n\n                      gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                    - network: 10.2.0.0/16 # The route's network (destination).\n\n                      gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - vip:\n\n                    ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - dhcpOptions:\n\n                    routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - dhcpOptions:\n\n                routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\nwireguard\n\nDeviceWireguardConfig contains settings for configuring Wireguard network interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                listenPort: 51111 # Specifies a device's listening port.\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n                      persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nField\tType\tDescription\tValue(s)\nprivateKey\tstring\t\nSpecifies a private key configuration (base64 encoded).\n\t\nlistenPort\tint\tSpecifies a device’s listening port.\t\nfirewallMark\tint\tSpecifies a device’s firewall mark.\t\npeers\t[]DeviceWireguardPeer\tSpecifies a list of peer configurations to apply to a device.\t\npeers[]\n\nDeviceWireguardPeer a WireGuard device peer configuration.\n\nField\tType\tDescription\tValue(s)\npublicKey\tstring\t\nSpecifies the public key of this peer.\n\t\nendpoint\tstring\tSpecifies the endpoint of this peer entry.\t\npersistentKeepaliveInterval\tDuration\t\nSpecifies the persistent keepalive interval for this peer.\n\t\nallowedIPs\t[]string\tAllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vip:\n\n                ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\nextraHostEntries[]\n\nExtraHost represents a host entry in /etc/hosts.\n\nmachine:\n\n    network:\n\n        extraHostEntries:\n\n            - ip: 192.168.1.100 # The IP of the host.\n\n              # The host alias.\n\n              aliases:\n\n                - example\n\n                - example.domain.tld\nField\tType\tDescription\tValue(s)\nip\tstring\tThe IP of the host.\t\naliases\t[]string\tThe host alias.\t\nkubespan\n\nNetworkKubeSpan struct describes KubeSpan configuration.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the KubeSpan feature.\n\t\nadvertiseKubernetesNetworks\tbool\t\nControl whether Kubernetes pod CIDRs are announced over KubeSpan from the node.\n\t\nallowDownPeerBypass\tbool\t\nSkip sending traffic via KubeSpan if the peer connection state is not up.\n\t\nharvestExtraEndpoints\tbool\t\nKubeSpan can collect and publish extra endpoints for each member of the cluster\n\t\nmtu\tuint32\t\nKubeSpan link MTU size.\n\t\nfilters\tKubeSpanFilters\t\nKubeSpan advanced filtering of network addresses .\n\t\nfilters\n\nKubeSpanFilters struct describes KubeSpan advanced network addresses filtering.\n\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nFilter node addresses which will be advertised as KubeSpan endpoints for peer-to-peer Wireguard connections.\nShow example(s)\n\t\ndisks[]\n\nMachineDisk represents the options available for partitioning, formatting, and mounting extra disks.\n\nmachine:\n\n    disks:\n\n        - device: /dev/sdb # The name of the disk to use.\n\n          # A list of partitions to create on the disk.\n\n          partitions:\n\n            - mountpoint: /var/mnt/extra # Where to mount the partition.\n\n\n\n              # # The size of partition: either bytes or human readable representation. If `size:` is omitted, the partition is sized to occupy the full disk.\n\n\n\n              # # Human readable representation.\n\n              # size: 100 MB\n\n              # # Precise value in bytes.\n\n              # size: 1073741824\nField\tType\tDescription\tValue(s)\ndevice\tstring\tThe name of the disk to use.\t\npartitions\t[]DiskPartition\tA list of partitions to create on the disk.\t\npartitions[]\n\nDiskPartition represents the options for a disk partition.\n\nField\tType\tDescription\tValue(s)\nsize\tDiskSize\tThe size of partition: either bytes or human readable representation. If size: is omitted, the partition is sized to occupy the full disk.\nShow example(s)\n\t\nmountpoint\tstring\tWhere to mount the partition.\t\ninstall\n\nInstallConfig represents the installation options for preparing a node.\n\nmachine:\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ndisk\tstring\tThe disk used for installations.\nShow example(s)\n\t\ndiskSelector\tInstallDiskSelector\t\nLook up disk using disk attributes like model, size, serial and others.\nShow example(s)\n\t\nextraKernelArgs\t[]string\t\nAllows for supplying extra kernel args via the bootloader.\nShow example(s)\n\t\nimage\tstring\t\nAllows for supplying the image used to perform the installation.\nShow example(s)\n\t\nextensions\t[]InstallExtensionConfig\tAllows for supplying additional system extension images to install on top of base Talos image.\nShow example(s)\n\t\nwipe\tbool\t\nIndicates if the installation disk should be wiped at installation time.\n\ttrue\nyes\nfalse\nno\n\nlegacyBIOSSupport\tbool\t\nIndicates if MBR partition should be marked as bootable (active).\n\t\ndiskSelector\n\nInstallDiskSelector represents a disk query parameters for the install disk lookup.\n\nmachine:\n\n    install:\n\n        diskSelector:\n\n            size: '>= 1TB' # Disk size.\n\n            model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n\n\n            # # Disk bus path.\n\n            # busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0\n\n            # busPath: /pci0000:00/*\nField\tType\tDescription\tValue(s)\nsize\tInstallDiskSizeMatcher\tDisk size.\nShow example(s)\n\t\nname\tstring\tDisk name /sys/block/<dev>/device/name.\t\nmodel\tstring\tDisk model /sys/block/<dev>/device/model.\t\nserial\tstring\tDisk serial number /sys/block/<dev>/serial.\t\nmodalias\tstring\tDisk modalias /sys/block/<dev>/device/modalias.\t\nuuid\tstring\tDisk UUID /sys/block/<dev>/uuid.\t\nwwid\tstring\tDisk WWID /sys/block/<dev>/wwid.\t\ntype\tInstallDiskType\tDisk Type.\tssd\nhdd\nnvme\nsd\n\nbusPath\tstring\tDisk bus path.\nShow example(s)\n\t\nextensions[]\n\nInstallExtensionConfig represents a configuration for a system extension.\n\nmachine:\n\n    install:\n\n        extensions:\n\n            - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\nimage\tstring\tSystem extension image.\t\nfiles[]\n\nMachineFile represents a file to write to disk.\n\nmachine:\n\n    files:\n\n        - content: '...' # The contents of the file.\n\n          permissions: 0o666 # The file's permissions in octal.\n\n          path: /tmp/file.txt # The path of the file.\n\n          op: append # The operation to use\nField\tType\tDescription\tValue(s)\ncontent\tstring\tThe contents of the file.\t\npermissions\tFileMode\tThe file’s permissions in octal.\t\npath\tstring\tThe path of the file.\t\nop\tstring\tThe operation to use\tcreate\nappend\noverwrite\n\ntime\n\nTimeConfig represents the options for configuring time on a machine.\n\nmachine:\n\n    time:\n\n        disabled: false # Indicates if the time service is disabled for the machine.\n\n        # Specifies time (NTP) servers to use for setting the system time.\n\n        servers:\n\n            - time.cloudflare.com\n\n        bootTimeout: 2m0s # Specifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\t\nIndicates if the time service is disabled for the machine.\n\t\nservers\t[]string\t\nSpecifies time (NTP) servers to use for setting the system time.\n\t\nbootTimeout\tDuration\t\nSpecifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\n\t\nregistries\n\nRegistriesConfig represents the image pull options.\n\nmachine:\n\n    registries:\n\n        # Specifies mirror configuration for each registry host namespace.\n\n        mirrors:\n\n            docker.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.local\n\n        # Specifies TLS & auth configuration for HTTPS image registries.\n\n        config:\n\n            registry.local:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n                # The auth configuration for this registry.\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nmirrors\tmap[string]RegistryMirrorConfig\t\nSpecifies mirror configuration for each registry host namespace.\nShow example(s)\n\t\nconfig\tmap[string]RegistryConfig\t\nSpecifies TLS & auth configuration for HTTPS image registries.\nShow example(s)\n\t\nmirrors.*\n\nRegistryMirrorConfig represents mirror configuration for a registry.\n\nmachine:\n\n    registries:\n\n        mirrors:\n\n            ghcr.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.insecure\n\n                    - https://ghcr.io/v2/\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nList of endpoints (URLs) for registry mirrors to use.\n\t\noverridePath\tbool\t\nUse the exact path specified for the endpoint (don’t append /v2/).\n\t\nconfig.*\n\nRegistryConfig specifies auth & TLS config per registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            registry.insecure:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n\n\n                # # The auth configuration for this registry.\n\n                # auth:\n\n                #     username: username # Optional registry authentication.\n\n                #     password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\ntls\tRegistryTLSConfig\tThe TLS configuration for the registry.\nShow example(s)\n\t\nauth\tRegistryAuthConfig\t\nThe auth configuration for this registry.\nShow example(s)\n\t\ntls\n\nRegistryTLSConfig specifies TLS config for HTTPS registries.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nField\tType\tDescription\tValue(s)\nclientIdentity\tPEMEncodedCertificateAndKey\t\nEnable mutual TLS authentication with the registry.\nShow example(s)\n\t\nca\tBase64Bytes\t\nCA registry certificate to add the list of trusted certificates.\n\t\ninsecureSkipVerify\tbool\tSkip TLS server certificate verification (not recommended).\t\nauth\n\nRegistryAuthConfig specifies authentication configuration for a registry.\n\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nusername\tstring\t\nOptional registry authentication.\n\t\npassword\tstring\t\nOptional registry authentication.\n\t\nauth\tstring\t\nOptional registry authentication.\n\t\nidentityToken\tstring\t\nOptional registry authentication.\n\t\nsystemDiskEncryption\n\nSystemDiskEncryptionConfig specifies system disk partitions encryption settings.\n\nmachine:\n\n    systemDiskEncryption:\n\n        # Ephemeral partition encryption.\n\n        ephemeral:\n\n            provider: luks2 # Encryption provider to use for the encryption.\n\n            # Defines the encryption keys generation and storage method.\n\n            keys:\n\n                - # Deterministically generated key from the node UUID and PartitionLabel.\n\n                  nodeID: {}\n\n                  slot: 0 # Key slot number for LUKS2 encryption.\n\n\n\n                  # # KMS managed encryption key.\n\n                  # kms:\n\n                  #     endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\n\n\n\n            # # Cipher kind to use for the encryption. Depends on the encryption provider.\n\n            # cipher: aes-xts-plain64\n\n\n\n            # # Defines the encryption sector size.\n\n            # blockSize: 4096\n\n\n\n            # # Additional --perf parameters for the LUKS2 encryption.\n\n            # options:\n\n            #     - no_read_workqueue\n\n            #     - no_write_workqueue\nField\tType\tDescription\tValue(s)\nstate\tEncryptionConfig\tState partition encryption.\t\nephemeral\tEncryptionConfig\tEphemeral partition encryption.\t\nstate\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        state:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nephemeral\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nmachine:\n\n    systemDiskEncryption:\n\n        ephemeral:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nfeatures\n\nFeaturesConfig describes individual Talos features that can be switched on or off.\n\nmachine:\n\n    features:\n\n        rbac: true # Enable role-based access control (RBAC).\n\n\n\n        # # Configure Talos API access from Kubernetes pods.\n\n        # kubernetesTalosAPIAccess:\n\n        #     enabled: true # Enable Talos API access from Kubernetes pods.\n\n        #     # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n        #     allowedRoles:\n\n        #         - os:reader\n\n        #     # The list of Kubernetes namespaces Talos API access is available from.\n\n        #     allowedKubernetesNamespaces:\n\n        #         - kube-system\nField\tType\tDescription\tValue(s)\nrbac\tbool\tEnable role-based access control (RBAC).\t\nstableHostname\tbool\tEnable stable default hostname.\t\nkubernetesTalosAPIAccess\tKubernetesTalosAPIAccessConfig\t\nConfigure Talos API access from Kubernetes pods.\nShow example(s)\n\t\napidCheckExtKeyUsage\tbool\tEnable checks for extended key usage of client certificates in apid.\t\ndiskQuotaSupport\tbool\t\nEnable XFS project quota support for EPHEMERAL partition and user disks.\n\t\nkubePrism\tKubePrism\t\nKubePrism - local proxy/load balancer on defined port that will distribute\n\t\nkubernetesTalosAPIAccess\n\nKubernetesTalosAPIAccessConfig describes the configuration for the Talos API access from Kubernetes pods.\n\nmachine:\n\n    features:\n\n        kubernetesTalosAPIAccess:\n\n            enabled: true # Enable Talos API access from Kubernetes pods.\n\n            # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n            allowedRoles:\n\n                - os:reader\n\n            # The list of Kubernetes namespaces Talos API access is available from.\n\n            allowedKubernetesNamespaces:\n\n                - kube-system\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable Talos API access from Kubernetes pods.\t\nallowedRoles\t[]string\t\nThe list of Talos API roles which can be granted for access from Kubernetes pods.\n\t\nallowedKubernetesNamespaces\t[]string\tThe list of Kubernetes namespaces Talos API access is available from.\t\nkubePrism\n\nKubePrism describes the configuration for the KubePrism load balancer.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable KubePrism support - will start local load balacing proxy.\t\nport\tint\tKubePrism port.\t\nudev\n\nUdevConfig describes how the udev system should be configured.\n\nmachine:\n\n    udev:\n\n        # List of udev rules to apply to the udev system\n\n        rules:\n\n            - SUBSYSTEM==\"drm\", KERNEL==\"renderD*\", GROUP=\"44\", MODE=\"0660\"\nField\tType\tDescription\tValue(s)\nrules\t[]string\tList of udev rules to apply to the udev system\t\nlogging\n\nLoggingConfig struct configures Talos logging.\n\nmachine:\n\n    logging:\n\n        # Logging destination.\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345 # Where to send logs. Supported protocols are \"tcp\" and \"udp\".\n\n              format: json_lines # Logs format.\nField\tType\tDescription\tValue(s)\ndestinations\t[]LoggingDestination\tLogging destination.\t\ndestinations[]\n\nLoggingDestination struct configures Talos logging destination.\n\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\tWhere to send logs. Supported protocols are “tcp” and “udp”.\nShow example(s)\n\t\nformat\tstring\tLogs format.\tjson_lines\n\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://1.2.3.4:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://cluster1.internal:6443\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: udp://127.0.0.1:12345\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nkernel\n\nKernelConfig struct configures Talos Linux kernel.\n\nmachine:\n\n    kernel:\n\n        # Kernel modules to load.\n\n        modules:\n\n            - name: brtfs # Module name.\nField\tType\tDescription\tValue(s)\nmodules\t[]KernelModuleConfig\tKernel modules to load.\t\nmodules[]\n\nKernelModuleConfig struct configures Linux kernel modules to load.\n\nField\tType\tDescription\tValue(s)\nname\tstring\tModule name.\t\nparameters\t[]string\tModule parameters, changes applied after reboot.\t\nseccompProfiles[]\n\nMachineSeccompProfile defines seccomp profiles for the machine.\n\nmachine:\n\n    seccompProfiles:\n\n        - name: audit.json # The `name` field is used to provide the file name of the seccomp profile.\n\n          # The `value` field is used to provide the seccomp profile.\n\n          value:\n\n            defaultAction: SCMP_ACT_LOG\nField\tType\tDescription\tValue(s)\nname\tstring\tThe name field is used to provide the file name of the seccomp profile.\t\nvalue\tUnstructured\tThe value field is used to provide the seccomp profile.\t\ncluster\n\nClusterConfig represents the cluster-wide config values.\n\ncluster:\n\n    # ControlPlaneConfig represents the control plane configuration options.\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\n\n    clusterName: talos.local\n\n    # ClusterNetworkConfig represents kube networking configuration options.\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\nid\tstring\tGlobally unique identifier for this cluster (base64 encoded random 32 bytes).\t\nsecret\tstring\t\nShared secret of cluster (base64 encoded random 32 bytes).\n\t\ncontrolPlane\tControlPlaneConfig\tProvides control plane specific configuration options.\nShow example(s)\n\t\nclusterName\tstring\tConfigures the cluster’s name.\t\nnetwork\tClusterNetworkConfig\tProvides cluster specific network configuration options.\nShow example(s)\n\t\ntoken\tstring\tThe bootstrap token used to join the cluster.\nShow example(s)\n\t\naescbcEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nsecretboxEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\tThe base64 encoded root certificate authority used by Kubernetes.\nShow example(s)\n\t\naggregatorCA\tPEMEncodedCertificateAndKey\t\nThe base64 encoded aggregator certificate authority used by Kubernetes for front-proxy certificate generation.\nShow example(s)\n\t\nserviceAccount\tPEMEncodedKey\tThe base64 encoded private key for service account token generation.\nShow example(s)\n\t\napiServer\tAPIServerConfig\tAPI server specific configuration options.\nShow example(s)\n\t\ncontrollerManager\tControllerManagerConfig\tController manager server specific configuration options.\nShow example(s)\n\t\nproxy\tProxyConfig\tKube-proxy server-specific configuration options\nShow example(s)\n\t\nscheduler\tSchedulerConfig\tScheduler server specific configuration options.\nShow example(s)\n\t\ndiscovery\tClusterDiscoveryConfig\tConfigures cluster member discovery.\nShow example(s)\n\t\netcd\tEtcdConfig\tEtcd specific configuration options.\nShow example(s)\n\t\ncoreDNS\tCoreDNS\tCore DNS specific configuration options.\nShow example(s)\n\t\nexternalCloudProvider\tExternalCloudProviderConfig\tExternal cloud provider configuration.\nShow example(s)\n\t\nextraManifests\t[]string\t\nA list of urls that point to additional manifests.\nShow example(s)\n\t\nextraManifestHeaders\tmap[string]string\tA map of key value pairs that will be added while fetching the extraManifests.\nShow example(s)\n\t\ninlineManifests\t[]ClusterInlineManifest\t\nA list of inline Kubernetes manifests.\nShow example(s)\n\t\nadminKubeconfig\tAdminKubeconfigConfig\t\nSettings for admin kubeconfig generation.\nShow example(s)\n\t\nallowSchedulingOnControlPlanes\tbool\tAllows running workload on control-plane nodes.\nShow example(s)\n\ttrue\nyes\nfalse\nno\n\ncontrolPlane\n\nControlPlaneConfig represents the control plane configuration options.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\t\nEndpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\nShow example(s)\n\t\nlocalAPIServerPort\tint\t\nThe port that the API server listens on internally.\n\t\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: https://cluster1.internal:6443\ncluster:\n\n    controlPlane:\n\n        endpoint: udp://127.0.0.1:12345\ncluster:\n\n    controlPlane:\n\n        endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nnetwork\n\nClusterNetworkConfig represents kube networking configuration options.\n\ncluster:\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\ncni\tCNIConfig\t\nThe CNI used.\nShow example(s)\n\t\ndnsDomain\tstring\t\nThe domain used by Kubernetes DNS.\nShow example(s)\n\t\npodSubnets\t[]string\tThe pod subnet CIDR.\nShow example(s)\n\t\nserviceSubnets\t[]string\tThe service subnet CIDR.\nShow example(s)\n\t\ncni\n\nCNIConfig represents the CNI configuration options.\n\ncluster:\n\n    network:\n\n        cni:\n\n            name: custom # Name of CNI to use.\n\n            # URLs containing manifests to apply for the CNI.\n\n            urls:\n\n                - https://docs.projectcalico.org/archive/v3.20/manifests/canal.yaml\nField\tType\tDescription\tValue(s)\nname\tstring\tName of CNI to use.\tflannel\ncustom\nnone\n\nurls\t[]string\t\nURLs containing manifests to apply for the CNI.\n\t\nflannel\tFlannelCNIConfig\t\ndescription:\n\tFlannel configuration options.\n\nflannel\n\nFlannelCNIConfig represents the Flannel CNI configuration options.\n\nField\tType\tDescription\tValue(s)\nextraArgs\t[]string\tExtra arguments for ‘flanneld’.\nShow example(s)\n\t\napiServer\n\nAPIServerConfig represents the kube apiserver configuration options.\n\ncluster:\n\n    apiServer:\n\n        image: registry.k8s.io/kube-apiserver:v1.29.0 # The container image used in the API server manifest.\n\n        # Extra arguments to supply to the API server.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n            http2-max-streams-per-connection: \"32\"\n\n        # Extra certificate subject alternative names for the API server's certificate.\n\n        certSANs:\n\n            - 1.2.3.4\n\n            - 4.5.6.7\n\n\n\n        # # Configure the API server admission plugins.\n\n        # admissionControl:\n\n        #     - name: PodSecurity # Name is the name of the admission controller.\n\n        #       # Configuration is an embedded configuration object to be used as the plugin's\n\n        #       configuration:\n\n        #         apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n        #         defaults:\n\n        #             audit: restricted\n\n        #             audit-version: latest\n\n        #             enforce: baseline\n\n        #             enforce-version: latest\n\n        #             warn: restricted\n\n        #             warn-version: latest\n\n        #         exemptions:\n\n        #             namespaces:\n\n        #                 - kube-system\n\n        #             runtimeClasses: []\n\n        #             usernames: []\n\n        #         kind: PodSecurityConfiguration\n\n\n\n        # # Configure the API server audit policy.\n\n        # auditPolicy:\n\n        #     apiVersion: audit.k8s.io/v1\n\n        #     kind: Policy\n\n        #     rules:\n\n        #         - level: Metadata\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the API server manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the API server.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the API server static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\ncertSANs\t[]string\tExtra certificate subject alternative names for the API server’s certificate.\t\ndisablePodSecurityPolicy\tbool\tDisable PodSecurityPolicy in the API server and default manifests.\t\nadmissionControl\t[]AdmissionPluginConfig\tConfigure the API server admission plugins.\nShow example(s)\n\t\nauditPolicy\tUnstructured\tConfigure the API server audit policy.\nShow example(s)\n\t\nresources\tResourcesConfig\tConfigure the API server resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nadmissionControl[]\n\nAdmissionPluginConfig represents the API server admission plugin configuration.\n\ncluster:\n\n    apiServer:\n\n        admissionControl:\n\n            - name: PodSecurity # Name is the name of the admission controller.\n\n              # Configuration is an embedded configuration object to be used as the plugin's\n\n              configuration:\n\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n                defaults:\n\n                    audit: restricted\n\n                    audit-version: latest\n\n                    enforce: baseline\n\n                    enforce-version: latest\n\n                    warn: restricted\n\n                    warn-version: latest\n\n                exemptions:\n\n                    namespaces:\n\n                        - kube-system\n\n                    runtimeClasses: []\n\n                    usernames: []\n\n                kind: PodSecurityConfiguration\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName is the name of the admission controller.\n\t\nconfiguration\tUnstructured\t\nConfiguration is an embedded configuration object to be used as the plugin’s\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ncontrollerManager\n\nControllerManagerConfig represents the kube controller manager configuration options.\n\ncluster:\n\n    controllerManager:\n\n        image: registry.k8s.io/kube-controller-manager:v1.29.0 # The container image used in the controller manager manifest.\n\n        # Extra arguments to supply to the controller manager.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the controller manager manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the controller manager.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the controller manager static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the controller manager resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\nproxy\n\nProxyConfig represents the kube proxy configuration options.\n\ncluster:\n\n    proxy:\n\n        image: registry.k8s.io/kube-proxy:v1.29.0 # The container image used in the kube-proxy manifest.\n\n        mode: ipvs # proxy mode of kube-proxy.\n\n        # Extra arguments to supply to kube-proxy.\n\n        extraArgs:\n\n            proxy-mode: iptables\n\n\n\n        # # Disable kube-proxy deployment on cluster bootstrap.\n\n        # disabled: false\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-proxy deployment on cluster bootstrap.\nShow example(s)\n\t\nimage\tstring\tThe container image used in the kube-proxy manifest.\nShow example(s)\n\t\nmode\tstring\t\nproxy mode of kube-proxy.\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to kube-proxy.\t\nscheduler\n\nSchedulerConfig represents the kube scheduler configuration options.\n\ncluster:\n\n    scheduler:\n\n        image: registry.k8s.io/kube-scheduler:v1.29.0 # The container image used in the scheduler manifest.\n\n        # Extra arguments to supply to the scheduler.\n\n        extraArgs:\n\n            feature-gates: AllBeta=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the scheduler manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the scheduler.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the scheduler static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the scheduler resources.\t\nconfig\tUnstructured\tSpecify custom kube-scheduler configuration.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ndiscovery\n\nClusterDiscoveryConfig struct configures cluster membership discovery.\n\ncluster:\n\n    discovery:\n\n        enabled: true # Enable the cluster membership discovery feature.\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            # Kubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\n            kubernetes: {}\n\n            # Service registry is using an external service to push and pull information about cluster members.\n\n            service:\n\n                endpoint: https://discovery.talos.dev/ # External service endpoint.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the cluster membership discovery feature.\n\t\nregistries\tDiscoveryRegistriesConfig\tConfigure registries used for cluster member discovery.\t\nregistries\n\nDiscoveryRegistriesConfig struct configures cluster membership discovery.\n\nField\tType\tDescription\tValue(s)\nkubernetes\tRegistryKubernetesConfig\t\nKubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\t\nservice\tRegistryServiceConfig\tService registry is using an external service to push and pull information about cluster members.\t\nkubernetes\n\nRegistryKubernetesConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable Kubernetes discovery registry.\t\nservice\n\nRegistryServiceConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable external service discovery registry.\t\nendpoint\tstring\tExternal service endpoint.\nShow example(s)\n\t\netcd\n\nEtcdConfig represents the etcd configuration options.\n\ncluster:\n\n    etcd:\n\n        image: gcr.io/etcd-development/etcd:v3.5.11 # The container image used to create the etcd service.\n\n        # The `ca` is the root certificate authority of the PKI.\n\n        ca:\n\n            crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n            key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n        # Extra arguments to supply to etcd.\n\n        extraArgs:\n\n            election-timeout: \"5000\"\n\n\n\n        # # The `advertisedSubnets` field configures the networks to pick etcd advertised IP from.\n\n        # advertisedSubnets:\n\n        #     - 10.0.0.0/8\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used to create the etcd service.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe ca is the root certificate authority of the PKI.\nShow example(s)\n\t\nextraArgs\tmap[string]string\t\nExtra arguments to supply to etcd.\n\t\nadvertisedSubnets\t[]string\t\nThe advertisedSubnets field configures the networks to pick etcd advertised IP from.\nShow example(s)\n\t\nlistenSubnets\t[]string\t\nThe listenSubnets field configures the networks for the etcd to listen for peer and client connections.\n\t\ncoreDNS\n\nCoreDNS represents the CoreDNS config values.\n\ncluster:\n\n    coreDNS:\n\n        image: registry.k8s.io/coredns/coredns:v1.11.1 # The `image` field is an override to the default coredns image.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable coredns deployment on cluster bootstrap.\t\nimage\tstring\tThe image field is an override to the default coredns image.\t\nexternalCloudProvider\n\nExternalCloudProviderConfig contains external cloud provider configuration.\n\ncluster:\n\n    externalCloudProvider:\n\n        enabled: true # Enable external cloud provider.\n\n        # A list of urls that point to additional manifests for an external cloud provider.\n\n        manifests:\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/rbac.yaml\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/aws-cloud-controller-manager-daemonset.yaml\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable external cloud provider.\ttrue\nyes\nfalse\nno\n\nmanifests\t[]string\t\nA list of urls that point to additional manifests for an external cloud provider.\nShow example(s)\n\t\ninlineManifests[]\n\nClusterInlineManifest struct describes inline bootstrap manifests for the user.\n\ncluster:\n\n    inlineManifests:\n\n        - name: namespace-ci # Name of the manifest.\n\n          contents: |- # Manifest contents as a string.\n\n            apiVersion: v1\n\n            kind: Namespace\n\n            metadata:\n\n            \tname: ci\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName of the manifest.\nShow example(s)\n\t\ncontents\tstring\tManifest contents as a string.\nShow example(s)\n\t\nadminKubeconfig\n\nAdminKubeconfigConfig contains admin kubeconfig settings.\n\ncluster:\n\n    adminKubeconfig:\n\n        certLifetime: 1h0m0s # Admin kubeconfig certificate lifetime (default is 1 year).\nField\tType\tDescription\tValue(s)\ncertLifetime\tDuration\t\nAdmin kubeconfig certificate lifetime (default is 1 year).\n\t\n5.4 - Kernel\nLinux kernel reference.\nCommandline Parameters\n\nTalos supports a number of kernel commandline parameters. Some are required for it to operate. Others are optional and useful in certain circumstances.\n\nSeveral of these are enforced by the Kernel Self Protection Project KSPP.\n\nRequired parameters:\n\ntalos.platform: can be one of aws, azure, container, digitalocean, equinixMetal, gcp, hcloud, metal, nocloud, openstack, oracle, scaleway, upcloud, vmware or vultr\nslab_nomerge: required by KSPP\npti=on: required by KSPP\n\nRecommended parameters:\n\ninit_on_alloc=1: advised by KSPP, enabled by default in kernel config\ninit_on_free=1: advised by KSPP, enabled by default in kernel config\nAvailable Talos-specific parameters\nip\n\nInitial configuration of the interface, routes, DNS, NTP servers (multiple ip= kernel parameters are accepted).\n\nFull documentation is available in the Linux kernel docs.\n\nip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>\n\nTalos will use the configuration supplied via the kernel parameter as the initial network configuration. This parameter is useful in the environments where DHCP doesn’t provide IP addresses or when default DNS and NTP servers should be overridden before loading machine configuration. Partial configuration can be applied as well, e.g. ip=:::::::<dns0-ip>:<dns1-ip>:<ntp0-ip> sets only the DNS and NTP servers.\n\nIPv6 addresses can be specified by enclosing them in the square brackets, e.g. ip=[2001:db8::a]:[2001:db8::b]:[fe80::1]::controlplane1:eth1::[2001:4860:4860::6464]:[2001:4860:4860::64]:[2001:4860:4806::].\n\n<netmask> can use either an IP address notation (IPv4: 255.255.255.0, IPv6: [ffff:ffff:ffff:ffff::0]), or simply a number of one bits in the netmask (24).\n\n<device> can be traditional interface naming scheme eth0, eth1 or enx<MAC>, example: enx78e7d1ea46da\n\nDHCP can be enabled by setting <autoconf> to dhcp, example: ip=:::::eth0.3:dhcp. Alternative syntax is ip=eth0.3:dhcp.\n\nbond\n\nBond interface configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nbond=<bondname>:<bondslaves>:<options>:<mtu>\n\nTalos will use the bond= kernel parameter if supplied to set the initial bond configuration. This parameter is useful in environments where the switch ports are suspended if the machine doesn’t setup a LACP bond.\n\nIf only the bond name is supplied, the bond will be created with eth0 and eth1 as slaves and bond mode set as balance-rr\n\nAll these below configurations are equivalent:\n\nbond=bond0\nbond=bond0:\nbond=bond0::\nbond=bond0:::\nbond=bond0:eth0,eth1\nbond=bond0:eth0,eth1:balance-rr\n\nAn example of a bond configuration with all options specified:\n\nbond=bond1:eth3,eth4:mode=802.3ad,xmit_hash_policy=layer2+3:1450\n\nThis will create a bond interface named bond1 with eth3 and eth4 as slaves and set the bond mode to 802.3ad, the transmit hash policy to layer2+3 and bond interface MTU to 1450.\n\nvlan\n\nThe interface vlan configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nTalos will use the vlan= kernel parameter if supplied to set the initial vlan configuration. This parameter is useful in environments where the switch ports are VLAN tagged with no native VLAN.\n\nOnly one vlan can be configured at this stage.\n\nAn example of a vlan configuration including static ip configuration:\n\nvlan=eth0.100:eth0 ip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\nThis will create a vlan interface named eth0.100 with eth0 as the underlying interface and set the vlan id to 100 with static IP 172.20.0.2/24 and 172.20.0.1 as default gateway.\n\nnet.ifnames=0\n\nDisable the predictable network interface names by specifying net.ifnames=0 on the kernel command line.\n\npanic\n\nThe amount of time to wait after a panic before a reboot is issued.\n\nTalos will always reboot if it encounters an unrecoverable error. However, when collecting debug information, it may reboot too quickly for humans to read the logs. This option allows the user to delay the reboot to give time to collect debug information from the console screen.\n\nA value of 0 disables automatic rebooting entirely.\n\ntalos.config\n\nThe URL at which the machine configuration data may be found (only for metal platform, with the kernel parameter talos.platform=metal).\n\nThis parameter supports variable substitution inside URL query values for the following case-insensitive placeholders:\n\n${uuid} the SMBIOS UUID\n${serial} the SMBIOS Serial Number\n${mac} the MAC address of the first network interface attaining link state up\n${hostname} the hostname of the machine\n\nThe following example\n\nhttp://example.com/metadata?h=${hostname}&m=${mac}&s=${serial}&u=${uuid}\n\nmay translate to\n\nhttp://example.com/metadata?h=myTestHostname&m=52%3A2f%3Afd%3Adf%3Afc%3Ac0&s=0OCZJ19N65&u=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nFor backwards compatibility we insert the system UUID into the query parameter uuid if its value is empty. As in http://example.com/metadata?uuid= => http://example.com/metadata?uuid=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nmetal-iso\n\nWhen the kernel parameter talos.config=metal-iso is set, Talos will attempt to load the machine configuration from any block device with a filesystem label of metal-iso. Talos will look for a file named config.yaml in the root of the filesystem.\n\nFor example, such ISO filesystem can be created with:\n\nmkdir iso/\n\ncp config.yaml iso/\n\nmkisofs -joliet -rock -volid 'metal-iso' -output config.iso iso/\n\ntalos.config.auth.*\n\nKernel parameters prefixed with talos.config.auth. are used to configure OAuth2 authentication for the machine configuration.\n\ntalos.platform\n\nThe platform name on which Talos will run.\n\nValid options are:\n\naws\nazure\ncontainer\ndigitalocean\nequinixMetal\ngcp\nhcloud\nmetal\nnocloud\nopenstack\noracle\nscaleway\nupcloud\nvmware\nvultr\ntalos.board\n\nThe board name, if Talos is being used on an ARM64 SBC.\n\nSupported boards are:\n\nbananapi_m64: Banana Pi M64\nlibretech_all_h3_cc_h5: Libre Computer ALL-H3-CC\nrock64: Pine64 Rock64\n…\ntalos.hostname\n\nThe hostname to be used. The hostname is generally specified in the machine config. However, in some cases, the DHCP server needs to know the hostname before the machine configuration has been acquired.\n\nUnless specifically required, the machine configuration should be used instead.\n\ntalos.shutdown\n\nThe type of shutdown to use when Talos is told to shutdown.\n\nValid options are:\n\nhalt\npoweroff\ntalos.network.interface.ignore\n\nA network interface which should be ignored and not configured by Talos.\n\nBefore a configuration is applied (early on each boot), Talos attempts to configure each network interface by DHCP. If there are many network interfaces on the machine which have link but no DHCP server, this can add significant boot delays.\n\nThis option may be specified multiple times for multiple network interfaces.\n\ntalos.experimental.wipe\n\nResets the disk before starting up the system.\n\nValid options are:\n\nsystem resets system disk.\nsystem:EPHEMERAL,STATE resets ephemeral and state partitions. Doing this reverts Talos into maintenance mode.\ntalos.unified_cgroup_hierarchy\n\nTalos defaults to always using the unified cgroup hierarchy (cgroupsv2), but cgroupsv1 can be forced with talos.unified_cgroup_hierarchy=0.\n\nNote: cgroupsv1 is deprecated and it should be used only for compatibility with workloads which don’t support cgroupsv2 yet.\n\ntalos.dashboard.disabled\n\nBy default, Talos redirects kernel logs to virtual console /dev/tty1 and starts the dashboard on /dev/tty2, then switches to the dashboard tty.\n\nIf you set talos.dashboard.disabled=1, this behavior will be disabled. Kernel logs will be sent to the currently active console and the dashboard will not be started.\n\nIt is set to be 1 by default on SBCs.\n\ntalos.environment\n\nEach value of the argument sets a default environment variable. The expected format is key=value.\n\nExample:\n\ntalos.environment=http_proxy=http://proxy.example.com:8080 talos.environment=https_proxy=http://proxy.example.com:8080\n\n6 - Learn More\n6.1 - Philosophy\nLearn about the philosophy behind the need for Talos Linux.\nDistributed\n\nTalos is intended to be operated in a distributed manner: it is built for a high-availability dataplane first. Its etcd cluster is built in an ad-hoc manner, with each appointed node joining on its own directive (with proper security validations enforced, of course). Like Kubernetes, workloads are intended to be distributed across any number of compute nodes.\n\nThere should be no single points of failure, and the level of required coordination is as low as each platform allows.\n\nImmutable\n\nTalos takes immutability very seriously. Talos itself, even when installed on a disk, always runs from a SquashFS image, meaning that even if a directory is mounted to be writable, the image itself is never modified. All images are signed and delivered as single, versioned files. We can always run integrity checks on our image to verify that it has not been modified.\n\nWhile Talos does allow a few, highly-controlled write points to the filesystem, we strive to make them as non-unique and non-critical as possible. We call the writable partition the “ephemeral” partition precisely because we want to make sure none of us ever uses it for unique, non-replicated, non-recreatable data. Thus, if all else fails, we can always wipe the disk and get back up and running.\n\nMinimal\n\nWe are always trying to reduce Talos’ footprint. Because nearly the entire OS is built from scratch in Go, we are in a good position. We have no shell. We have no SSH. We have none of the GNU utilities, not even a rollup tool such as busybox. Everything in Talos is there because it is necessary, and nothing is included which isn’t.\n\nAs a result, the OS right now produces a SquashFS image size of less than 80 MB.\n\nEphemeral\n\nEverything Talos writes to its disk is either replicated or reconstructable. Since the controlplane is highly available, the loss of any node will cause neither service disruption nor loss of data. No writes are even allowed to the vast majority of the filesystem. We even call the writable partition “ephemeral” to keep this idea always in focus.\n\nSecure\n\nTalos has always been designed with security in mind. With its immutability, its minimalism, its signing, and its componenture, we are able to simply bypass huge classes of vulnerabilities. Moreover, because of the way we have designed Talos, we are able to take advantage of a number of additional settings, such as the recommendations of the Kernel Self Protection Project (kspp) and completely disabling dynamic modules.\n\nThere are no passwords in Talos. All networked communication is encrypted and key-authenticated. The Talos certificates are short-lived and automatically-rotating. Kubernetes is always constructed with its own separate PKI structure which is enforced.\n\nDeclarative\n\nEverything which can be configured in Talos is done through a single YAML manifest. There is no scripting and no procedural steps. Everything is defined by the one declarative YAML file. This configuration includes that of both Talos itself and the Kubernetes which it forms.\n\nThis is achievable because Talos is tightly focused to do one thing: run Kubernetes, in the easiest, most secure, most reliable way it can.\n\nNot based on X distro\n\nTalos Linux isn’t based on any other distribution. We think of ourselves as being the second-generation of container-optimised operating systems, where things like CoreOS, Flatcar, and Rancher represent the first generation (but the technology is not derived from any of those.)\n\nTalos Linux is actually a ground-up rewrite of the userspace, from PID 1. We run the Linux kernel, but everything downstream of that is our own custom code, written in Go, rigorously-tested, and published as an immutable, integrated image. The Linux kernel launches what we call machined, for instance, not systemd. There is no systemd on our system. There are no GNU utilities, no shell, no SSH, no packages, nothing you could associate with any other distribution.\n\nAn Operating System designed for Kubernetes\n\nTechnically, Talos Linux installs to a computer like any other operating system. Unlike other operating systems, Talos is not meant to run alone, on a single machine. A design goal of Talos Linux is eliminating the management of individual nodes as much as possible. In order to do that, Talos Linux operates as a cluster of machines, with lots of checking and coordination between them, at all levels.\n\nThere is only a cluster. Talos is meant to do one thing: maintain a Kubernetes cluster, and it does this very, very well.\n\nThe entirety of the configuration of any machine is specified by a single configuration file, which can often be the same configuration file used across many machines. Much like a biological system, if some component misbehaves, just cut it out and let a replacement grow. Rebuilds of Talos are remarkably fast, whether they be new machines, upgrades, or reinstalls. Never get hung up on an individual machine.\n\n6.2 - Architecture\nLearn the system architecture of Talos Linux itself.\n\nTalos is designed to be atomic in deployment and modular in composition.\n\nIt is atomic in that the entirety of Talos is distributed as a single, self-contained image, which is versioned, signed, and immutable.\n\nIt is modular in that it is composed of many separate components which have clearly defined gRPC interfaces which facilitate internal flexibility and external operational guarantees.\n\nAll of the main Talos components communicate with each other by gRPC, through a socket on the local machine. This imposes a clear separation of concerns and ensures that changes over time which affect the interoperation of components are a part of the public git record. The benefit is that each component may be iterated and changed as its needs dictate, so long as the external API is controlled. This is a key component in reducing coupling and maintaining modularity.\n\nFile system partitions\n\nTalos uses these partitions with the following labels:\n\nEFI - stores EFI boot data.\nBIOS - used for GRUB’s second stage boot.\nBOOT - used for the boot loader, stores initramfs and kernel data.\nMETA - stores metadata about the talos node, such as node id’s.\nSTATE - stores machine configuration, node identity data for cluster discovery and KubeSpan info\nEPHEMERAL - stores ephemeral state information, mounted at /var\nThe File System\n\nOne of the unique design decisions in Talos is the layout of the root file system. There are three “layers” to the Talos root file system. At its core the rootfs is a read-only squashfs. The squashfs is then mounted as a loop device into memory. This provides Talos with an immutable base.\n\nThe next layer is a set of tmpfs file systems for runtime specific needs. Aside from the standard pseudo file systems such as /dev, /proc, /run, /sys and /tmp, a special /system is created for internal needs. One reason for this is that we need special files such as /etc/hosts, and /etc/resolv.conf to be writable (remember that the rootfs is read-only). For example, at boot Talos will write /system/etc/hosts and then bind mount it over /etc/hosts. This means that instead of making all of /etc writable, Talos only makes very specific files writable under /etc.\n\nAll files under /system are completely recreated on each boot. For files and directories that need to persist across boots, Talos creates overlayfs file systems. The /etc/kubernetes is a good example of this. Directories like this are overlayfs backed by an XFS file system mounted at /var.\n\nThe /var directory is owned by Kubernetes with the exception of the above overlayfs file systems. This directory is writable and used by etcd (in the case of control plane nodes), the kubelet, and the CRI (containerd). Its content survives machine reboots, but it is wiped and lost on machine upgrades and resets, unless the --preserve option of talosctl upgrade or the --system-labels-to-wipe option of talosctl reset is used.\n\n6.3 - Components\nUnderstand the system components that make up Talos Linux.\n\nIn this section, we discuss the various components that underpin Talos.\n\nComponents\n\nTalos Linux and Kubernetes are tightly integrated.\n\nIn the following, the focus is on the Talos Linux specific components.\n\nComponent\tDescription\napid\tWhen interacting with Talos, the gRPC API endpoint you interact with directly is provided by apid. apid acts as the gateway for all component interactions and forwards the requests to machined.\ncontainerd\tAn industry-standard container runtime with an emphasis on simplicity, robustness, and portability. To learn more, see the containerd website.\nmachined\tTalos replacement for the traditional Linux init-process. Specially designed to run Kubernetes and does not allow starting arbitrary user services.\nkernel\tThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project.\ntrustd\tTo run and operate a Kubernetes cluster, a certain level of trust is required. Based on the concept of a ‘Root of Trust’, trustd is a simple daemon responsible for establishing trust within the system.\nudevd\tImplementation of eudev into machined. eudev is Gentoo’s fork of udev, systemd’s device file manager for the Linux kernel. It manages device nodes in /dev and handles all user space actions when adding or removing devices. To learn more, see the Gentoo Wiki.\napid\n\nWhen interacting with Talos, the gRPC api endpoint you will interact with directly is apid. Apid acts as the gateway for all component interactions. Apid provides a mechanism to route requests to the appropriate destination when running on a control plane node.\n\nWe’ll use some examples below to illustrate what apid is doing.\n\nWhen a user wants to interact with a Talos component via talosctl, there are two flags that control the interaction with apid. The -e | --endpoints flag specifies which Talos node ( via apid ) should handle the connection. Typically this is a public-facing server. The -n | --nodes flag specifies which Talos node(s) should respond to the request. If --nodes is omitted, the first endpoint will be used.\n\nNote: Typically, there will be an endpoint already defined in the Talos config file. Optionally, nodes can be included here as well.\n\nFor example, if a user wants to interact with machined, a command like talosctl -e cluster.talos.dev memory may be used.\n\n$ talosctl -e cluster.talos.dev memory\n\nNODE                TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\ncluster.talos.dev   7938    1768   2390   145      53        3724    6571\n\n\nIn this case, talosctl is interacting with apid running on cluster.talos.dev and forwarding the request to the machined api.\n\nIf we wanted to extend our example to retrieve memory from another node in our cluster, we could use the command talosctl -e cluster.talos.dev -n node02 memory.\n\n$ talosctl -e cluster.talos.dev -n node02 memory\n\nNODE    TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode02  7938    1768   2390   145      53        3724    6571\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to apid running on node02, which forwards the request to the machined api.\n\nWe can further extend our example to retrieve memory for all nodes in our cluster by appending additional -n node flags or using a comma separated list of nodes ( -n node01,node02,node03 ):\n\n$ talosctl -e cluster.talos.dev -n node01 -n node02 -n node03 memory\n\nNODE     TOTAL    USED    FREE     SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode01   7938     871     4071     137      49        2945    7042\n\nnode02   257844   14408   190796   18138    49        52589   227492\n\nnode03   257844   1830    255186   125      49        777     254556\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to node01, node02, and node03, which then forwards the request to their local machined api.\n\ncontainerd\n\nContainerd provides the container runtime to launch workloads on Talos and Kubernetes.\n\nTalos services are namespaced under the system namespace in containerd, whereas the Kubernetes services are namespaced under the k8s.io namespace.\n\nmachined\n\nA common theme throughout the design of Talos is minimalism. We believe strongly in the UNIX philosophy that each program should do one job well. The init included in Talos is one example of this, and we are calling it “machined”.\n\nWe wanted to create a focused init that had one job - run Kubernetes. To that extent, machined is relatively static in that it does not allow for arbitrary user-defined services. Only the services necessary to run Kubernetes and manage the node are available. This includes:\n\ncontainerd\netcd\nkubelet\nnetworkd\ntrustd\nudevd\n\nThe machined process handles all machine configuration, API handling, resource and controller management.\n\nkernel\n\nThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project (KSSP).\n\ntrustd\n\nSecurity is one of the highest priorities within Talos. To run a Kubernetes cluster, a certain level of trust is required to operate a cluster. For example, orchestrating the bootstrap of a highly available control plane requires sensitive PKI data distribution.\n\nTo that end, we created trustd. Based on a Root of Trust concept, trustd is a simple daemon responsible for establishing trust within the system. Once trust is established, various methods become available to the trustee. For example, it can accept a write request from another node to place a file on disk.\n\nAdditional methods and capabilities will be added to the trustd component to support new functionality in the rest of the Talos environment.\n\nudevd\n\nUdevd handles the kernel device notifications and sets up the necessary links in /dev.\n\n6.4 - Control Plane\nUnderstand the Kubernetes Control Plane.\n\nThis guide provides information about the Kubernetes control plane, and details on how Talos runs and bootstraps the Kubernetes control plane.\n\nWhat is a control plane node?\n\nA control plane node is a node which:\n\nruns etcd, the Kubernetes database\nruns the Kubernetes control plane\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nserves as an administrative proxy to the worker nodes\n\nThese nodes are critical to the operation of your cluster. Without control plane nodes, Kubernetes will not respond to changes in the system, and certain central services may not be available.\n\nTalos nodes which have .machine.type of controlplane are control plane nodes. (check via talosctl get member)\n\nControl plane nodes are tainted by default to prevent workloads from being scheduled onto them. This is both to protect the control plane from workloads consuming resources and starving the control plane processes, and also to reduce the risk of a vulnerability exposes the control plane’s credentials to a workload.\n\nThe Control Plane and Etcd\n\nA critical design concept of Kubernetes (and Talos) is the etcd database.\n\nProperly managed (which Talos Linux does), etcd should never have split brain or noticeable down time. In order to do this, etcd maintains the concept of “membership” and of “quorum”. To perform any operation, read or write, the database requires quorum. That is, a majority of members must agree on the current leader, and absenteeism (members that are down, or not reachable) counts as a negative. For example, if there are three members, at least two out of the three must agree on the current leader. If two disagree or fail to answer, the etcd database will lock itself until quorum is achieved in order to protect the integrity of the data.\n\nThis design means that having two controlplane nodes is worse than having only one, because if either goes down, your database will lock (and the chance of one of two nodes going down is greater than the chance of just a single node going down). Similarly, a 4 node etcd cluster is worse than a 3 node etcd cluster - a 4 node cluster requires 3 nodes to be up to achieve quorum (in order to have a majority), while the 3 node cluster requires 2 nodes: i.e. both can support a single node failure and keep running - but the chance of a node failing in a 4 node cluster is higher than that in a 3 node cluster.\n\nAnother note about etcd: due to the need to replicate data amongst members, performance of etcd decreases as the cluster scales. A 5 node cluster can commit about 5% less writes per second than a 3 node cluster running on the same hardware.\n\nRecommendations for your control plane\nRun your clusters with three or five control plane nodes. Three is enough for most use cases. Five will give you better availability (in that it can tolerate two node failures simultaneously), but cost you more both in the number of nodes required, and also as each node may require more hardware resources to offset the performance degradation seen in larger clusters.\nImplement good monitoring and put processes in place to deal with a failed node in a timely manner (and test them!)\nEven with robust monitoring and procedures for replacing failed nodes in place, backup etcd and your control plane node configuration to guard against unforeseen disasters.\nMonitor the performance of your etcd clusters. If etcd performance is slow, vertically scale the nodes, not the number of nodes.\nIf a control plane node fails, remove it first, then add the replacement node. (This ensures that the failed node does not “vote” when adding in the new node, minimizing the chances of a quorum violation.)\nIf replacing a node that has not failed, add the new one, then remove the old.\nBootstrapping the Control Plane\n\nEvery new cluster must be bootstrapped only once, which is achieved by telling a single control plane node to initiate the bootstrap.\n\nBootstrapping itself does not do anything with Kubernetes. Bootstrapping only tells etcd to form a cluster, so don’t judge the success of a bootstrap by the failure of Kubernetes to start. Kubernetes relies on etcd, so bootstrapping is required, but it is not sufficient for Kubernetes to start. If your Kubernetes cluster fails to form for other reasons (say, a bad configuration option or unavailable container repository), if the bootstrap API call returns successfully, you do NOT need to bootstrap again: just fix the config or let Kubernetes retry.\n\nHigh-level Overview\n\nTalos cluster bootstrap flow:\n\nThe etcd service is started on control plane nodes. Instances of etcd on control plane nodes build the etcd cluster.\nThe kubelet service is started.\nControl plane components are started as static pods via the kubelet, and the kube-apiserver component connects to the local (running on the same node) etcd instance.\nThe kubelet issues client certificate using the bootstrap token using the control plane endpoint (via kube-apiserver and kube-controller-manager).\nThe kubelet registers the node in the API server.\nKubernetes control plane schedules pods on the nodes.\nCluster Bootstrapping\n\nAll nodes start the kubelet service. The kubelet tries to contact the control plane endpoint, but as it is not up yet, it keeps retrying.\n\nOne of the control plane nodes is chosen as the bootstrap node, and promoted using the bootstrap API (talosctl bootstrap). The bootstrap node initiates the etcd bootstrap process by initializing etcd as the first member of the cluster.\n\nOnce etcd is bootstrapped, the bootstrap node has no special role and acts the same way as other control plane nodes.\n\nServices etcd on non-bootstrap nodes try to get Endpoints resource via control plane endpoint, but that request fails as control plane endpoint is not up yet.\n\nAs soon as etcd is up on the bootstrap node, static pod definitions for the Kubernetes control plane components (kube-apiserver, kube-controller-manager, kube-scheduler) are rendered to disk. The kubelet service on the bootstrap node picks up the static pod definitions and starts the Kubernetes control plane components. As soon as kube-apiserver is launched, the control plane endpoint comes up.\n\nThe bootstrap node acquires an etcd mutex and injects the bootstrap manifests into the API server. The set of the bootstrap manifests specify the Kubernetes join token and kubelet CSR auto-approval. The kubelet service on all the nodes is now able to issue client certificates for themselves and register nodes in the API server.\n\nOther bootstrap manifests specify additional resources critical for Kubernetes operations (i.e. CNI, PSP, etc.)\n\nThe etcd service on non-bootstrap nodes is now able to discover other members of the etcd cluster via the Kubernetes Endpoints resource. The etcd cluster is now formed and consists of all control plane nodes.\n\nAll control plane nodes render static pod manifests for the control plane components. Each node now runs a full set of components to make the control plane HA.\n\nThe kubelet service on worker nodes is now able to issue the client certificate and register itself with the API server.\n\nScaling Up the Control Plane\n\nWhen new nodes are added to the control plane, the process is the same as the bootstrap process above: the etcd service discovers existing members of the control plane via the control plane endpoint, joins the etcd cluster, and the control plane components are scheduled on the node.\n\nScaling Down the Control Plane\n\nScaling down the control plane involves removing a node from the cluster. The most critical part is making sure that the node which is being removed leaves the etcd cluster. The recommended way to do this is to use:\n\ntalosctl -n IP.of.node.to.remove reset\nkubectl delete node\n\nWhen using talosctl reset command, the targeted control plane node leaves the etcd cluster as part of the reset sequence, and its disks are erased.\n\nUpgrading Talos on Control Plane Nodes\n\nWhen a control plane node is upgraded, Talos leaves etcd, wipes the system disk, installs a new version of itself, and reboots. The upgraded node then joins the etcd cluster on reboot. So upgrading a control plane node is equivalent to scaling down the control plane node followed by scaling up with a new version of Talos.\n\n6.5 - Image Factory\nImage Factory generates customized Talos Linux images based on configured schematics.\n\nThe Image Factory provides a way to download Talos Linux artifacts. Artifacts can be generated with customizations defined by a “schematic”. A schematic can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”.\n\nThe following assets are provided:\n\nISO\nkernel, initramfs, and kernel command line\nUKI\ndisk images in various formats (e.g. AWS, GCP, VMware, etc.)\ninstaller container images\n\nThe supported frontends are:\n\nHTTP\nPXE\nContainer Registry\n\nThe official instance of Image Factory is available at https://factory.talos.dev.\n\nSee Boot Assets for an example of how to use the Image Factory to boot and upgrade Talos on different platforms. Full API documentation for the Image Factory is available at GitHub.\n\nSchematics\n\nSchematics are YAML files that define customizations to be applied to a Talos Linux image. Schematics can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”, which is a Talos Linux image with the customizations applied.\n\nSchematics are content-addressable, that is, the content of the schematic is used to generate a unique ID. The schematic should be uploaded to the Image Factory first, and then the ID can be used to reference the schematic in a model.\n\nSchematics can be generated using the Image Factory UI, or using the Image Factory API:\n\ncustomization:\n\n  extraKernelArgs: # optional\n\n    - vga=791\n\n  meta: # optional, allows to set initial Talos META\n\n    - key: 0xa\n\n      value: \"{}\"\n\n  systemExtensions: # optional\n\n    officialExtensions: # optional\n\n      - siderolabs/gvisor\n\n      - siderolabs/amd-ucode\n\n\nThe “vanilla” schematic is:\n\ncustomization:\n\n\nand has an ID of 376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba.\n\nThe schematic can be applied by uploading it to the Image Factory:\n\ncurl -X POST --data-binary @schematic.yaml https://factory.talos.dev/schematics\n\n\nAs the schematic is content-addressable, the same schematic can be uploaded multiple times, and the Image Factory will return the same ID.\n\nModels\n\nModels are Talos Linux images with customizations applied. The inputs to generate a model are:\n\nschematic ID\nTalos Linux version\nmodel type (e.g. ISO, UKI, etc.)\narchitecture (e.g. amd64, arm64)\nvarious model type specific options (e.g. disk image format, disk image size, etc.)\nFrontends\n\nImage Factory provides several frontends to retrieve models:\n\nHTTP frontend to download models (e.g. download an ISO or a disk image)\nPXE frontend to boot bare-metal machines (PXE script references kernel/initramfs from HTTP frontend)\nRegistry frontend to fetch customized installer images (for initial Talos Linux installation and upgrades)\n\nThe links to different models are available in the Image Factory UI, and a full list of possible models is documented at GitHub.\n\nIn this guide we will provide a list of examples:\n\namd64 ISO (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64.iso\narm64 AWS image (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/aws-arm64.raw.xz\namd64 PXE boot script (for Talos v1.6.2, “vanilla” schematic) https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64\nTalos installer image (for Talos v1.6.2, “vanilla” schematic, architecture is detected automatically): factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2\n\nThe installer image can be used to install Talos Linux on a bare-metal machine, or to upgrade an existing Talos Linux installation. As the Talos version and schematic ID can be changed, via an upgrade process, the installer image can be used to upgrade to any version of Talos Linux, or replace a set of installed system extensions.\n\nUI\n\nThe Image Factory UI is available at https://factory.talos.dev. The UI provides a way to list supported Talos Linux versions, list of system extensions available for each release, and a way to generate schematic based on the selected system extensions.\n\nThe UI operations are equivalent to API operations.\n\nFind Schematic ID from Talos Installation\n\nImage Factory always appends “virtual” system extension with the version matching schematic ID used to generate the model. So, for any running Talos Linux instance the schematic ID can be found by looking at the list of system extensions:\n\n$ talosctl get extensions\n\nNAMESPACE   TYPE              ID   VERSION   NAME       VERSION\n\nruntime     ExtensionStatus   0    1         schematic  376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba\n\nRestrictions\n\nSome models don’t include every customization of the schematic:\n\ninstaller and initramfs images only support system extensions (kernel args and META are ignored)\nkernel assets don’t depend on the schematic\n\nOther models have full support for all customizations:\n\nany disk image format\nISO, PXE boot script\n\nWhen installing Talos Linux using ISO/PXE boot, Talos will be installed on the disk using the installer image, so the installer image in the machine configuration should be using the same schematic as the ISO/PXE boot image.\n\nSome system extensions are not available for all Talos Linux versions, so an attempt to generate a model with an unsupported system extension will fail. List of supported Talos versions and supported system extensions for each version is available in the Image Factory UI and API.\n\nUnder the Hood\n\nImage Factory is based on the Talos imager container which provides both the Talos base boot assets, and the ability to generate custom assets based on a configuration. Image Factory manages a set of imager container images to acquire base Talos Linux boot assets (kernel, initramfs), a set of Talos Linux system extension images, and a set of schematics. When a model is requested, Image Factory uses the imager container to generate the requested assets based on the schematic and the Talos Linux version.\n\nSecurity\n\nImage Factory verifies signatures of all source container images fetched:\n\nimager container images (base boot assets)\nextensions system extensions catalogs\ninstaller contianer images (base installer layer)\nTalos Linux system extension images\n\nInternally, Image Factory caches generated boot assets and signs all cached images using a private key. Image Factory verifies the signature of the cached images before serving them to clients.\n\nImage Factory signs generated installer images, and verifies the signature of the installer images before serving them to clients.\n\nImage Factory does not provide a way to list all schematics, as schematics may contain sensitive information (e.g. private kernel boot arguments). As the schematic ID is content-addressable, it is not possible to guess the ID of a schematic without knowing the content of the schematic.\n\nRunning your own Image Factory\n\nImage Factory can be deployed on-premises to provide in-house asset generation.\n\nImage Factory requires following components:\n\nan OCI registry to store schematics (private)\nan OCI registry to store cached assets (private)\nan OCI registry to store installer images (should allow public read-only access)\na container image signing key: ECDSA P-256 private key in PEM format\n\nImage Factory is configured using command line flags, use --help to see a list of available flags. Image Factory should be configured to use proper authentication to push to the OCI registries:\n\nby mounting proper credentials via ~/.docker/config.json\nby supplying GITHUB_TOKEN (for ghcr.io)\n\nImage Factory performs HTTP redirects to the public registry endpoint for installer images, so the public endpoint should be available to Talos Linux machines to pull the installer images.\n\n6.6 - Controllers and Resources\nDiscover how Talos Linux uses the concepts on Controllers and Resources.\n\nTalos implements concepts of resources and controllers to facilitate internal operations of the operating system. Talos resources and controllers are very similar to Kubernetes resources and controllers, but there are some differences. The content of this document is not required to operate Talos, but it is useful for troubleshooting.\n\nStarting with Talos 0.9, most of the Kubernetes control plane bootstrapping and operations is implemented via controllers and resources which allows Talos to be reactive to configuration changes, environment changes (e.g. time sync).\n\nResources\n\nA resource captures a piece of system state. Each resource belongs to a “Type” which defines resource contents. Resource state can be split in two parts:\n\nmetadata: fixed set of fields describing resource - namespace, type, ID, etc.\nspec: contents of the resource (depends on resource type).\n\nResource is uniquely identified by (namespace, type, id). Namespaces provide a way to avoid conflicts on duplicate resource IDs.\n\nAt the moment of this writing, all resources are local to the node and stored in memory. So on every reboot resource state is rebuilt from scratch (the only exception is MachineConfig resource which reflects current machine config).\n\nControllers\n\nControllers run as independent lightweight threads in Talos. The goal of the controller is to reconcile the state based on inputs and eventually update outputs.\n\nA controller can have any number of resource types (and namespaces) as inputs. In other words, it watches specified resources for changes and reconciles when these changes occur. A controller might also have additional inputs: running reconcile on schedule, watching etcd keys, etc.\n\nA controller has a single output: a set of resources of fixed type in a fixed namespace. Only one controller can manage resource type in the namespace, so conflicts are avoided.\n\nQuerying Resources\n\nTalos CLI tool talosctl provides read-only access to the resource API which includes getting specific resource, listing resources and watching for changes.\n\nTalos stores resources describing resource types and namespaces in meta namespace:\n\n$ talosctl get resourcedefinitions\n\nNODE         NAMESPACE   TYPE                 ID                                               VERSION\n\n172.20.0.2   meta        ResourceDefinition   bootstrapstatuses.v1alpha1.talos.dev             1\n\n172.20.0.2   meta        ResourceDefinition   etcdsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   kubernetescontrolplaneconfigs.config.talos.dev   1\n\n172.20.0.2   meta        ResourceDefinition   kubernetessecrets.secrets.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   machineconfigs.config.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   machinetypes.config.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   manifests.kubernetes.talos.dev                   1\n\n172.20.0.2   meta        ResourceDefinition   manifeststatuses.kubernetes.talos.dev            1\n\n172.20.0.2   meta        ResourceDefinition   namespaces.meta.cosi.dev                         1\n\n172.20.0.2   meta        ResourceDefinition   resourcedefinitions.meta.cosi.dev                1\n\n172.20.0.2   meta        ResourceDefinition   rootsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   secretstatuses.kubernetes.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   services.v1alpha1.talos.dev                      1\n\n172.20.0.2   meta        ResourceDefinition   staticpods.kubernetes.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   staticpodstatuses.kubernetes.talos.dev           1\n\n172.20.0.2   meta        ResourceDefinition   timestatuses.v1alpha1.talos.dev                  1\n\n$ talosctl get namespaces\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\n172.20.0.2   meta        Namespace   controlplane   1\n\n172.20.0.2   meta        Namespace   meta           1\n\n172.20.0.2   meta        Namespace   runtime        1\n\n172.20.0.2   meta        Namespace   secrets        1\n\n\nMost of the time namespace flag (--namespace) can be omitted, as ResourceDefinition contains default namespace which is used if no namespace is given:\n\n$ talosctl get resourcedefinitions resourcedefinitions.meta.cosi.dev -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: meta\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    id: resourcedefinitions.meta.cosi.dev\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    displayType: ResourceDefinition\n\n    aliases:\n\n        - resourcedefinitions\n\n        - resourcedefinition\n\n        - resourcedefinitions.meta\n\n        - resourcedefinitions.meta.cosi\n\n        - rd\n\n        - rds\n\n    printColumns: []\n\n    defaultNamespace: meta\n\n\nResource definition also contains type aliases which can be used interchangeably with canonical resource name:\n\n$ talosctl get ns config\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\nOutput\n\nCommand talosctl get supports following output modes:\n\ntable (default) prints resource list as a table\nyaml prints pretty formatted resources with details, including full metadata spec. This format carries most details from the backend resource (e.g. comments in MachineConfig resource)\njson prints same information as yaml, some additional details (e.g. comments) might be lost. This format is useful for automated processing with tools like jq.\nWatching Changes\n\nIf flag --watch is appended to the talosctl get command, the command switches to watch mode. If list of resources was requested, talosctl prints initial contents of the list and then appends resource information for every change:\n\n$ talosctl get svc -w\n\nNODE         *   NAMESPACE   TYPE      ID     VERSION   RUNNING   HEALTHY\n\n172.20.0.2   +   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   trustd   2   true   true\n\n172.20.0.2   +   runtime   Service   udevd   2   true   true\n\n172.20.0.2   -   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   timed   1   true   false\n\n172.20.0.2       runtime   Service   timed   2   true   true\n\n\nColumn * specifies event type:\n\n+ is created\n- is deleted\nis updated\n\nIn YAML/JSON output, field event is added to the resource representation to describe the event type.\n\nExamples\n\nGetting machine config:\n\n$ talosctl get machineconfig -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: MachineConfigs.config.talos.dev\n\n    id: v1alpha1\n\n    version: 2\n\n    phase: running\n\nspec:\n\n    version: v1alpha1 # Indicates the schema used to decode the contents.\n\n    debug: false # Enable verbose logging to the console.\n\n    persist: true # Indicates whether to pull the machine config upon every boot.\n\n    # Provides machine specific configuration options.\n\n...\n\n\nGetting control plane static pod statuses:\n\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE      TYPE              ID                                                           VERSION   READY\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-1            3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-1   3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-1            4         True\n\n\nGetting static pod definition for kube-apiserver:\n\n$ talosctl get sp kube-apiserver -n 172.20.0.2 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: StaticPods.kubernetes.talos.dev\n\n    id: kube-apiserver\n\n    version: 3\n\n    phase: running\n\n    finalizers:\n\n        - k8s.StaticPodStatus(\"kube-apiserver\")\n\nspec:\n\n    apiVersion: v1\n\n    kind: Pod\n\n    metadata:\n\n        annotations:\n\n            talos.dev/config-version: \"1\"\n\n            talos.dev/secrets-version: \"2\"\n\n...\n\nInspecting Controller Dependencies\n\nTalos can report current dependencies between controllers and resources for debugging purposes:\n\n$ talosctl inspect dependencies\n\ndigraph  {\n\n\n\n  n1[label=\"config.K8sControlPlaneController\",shape=\"box\"];\n\n  n3[label=\"config.MachineTypeController\",shape=\"box\"];\n\n  n2[fillcolor=\"azure2\",label=\"config:KubernetesControlPlaneConfigs.config.talos.dev\",shape=\"note\",style=\"filled\"];\n\n...\n\n\nThis outputs graph in graphviz format which can be rendered to PNG with command:\n\ntalosctl inspect dependencies | dot -T png > deps.png\n\n\nGraph can be enhanced by replacing resource types with actual resource instances:\n\ntalosctl inspect dependencies --with-resources | dot -T png > deps.png\n\n\n6.7 - Networking Resources\nDelve deeper into networking of Talos Linux.\n\nTalos network configuration subsystem is powered by COSI. Talos translates network configuration from multiple sources: machine configuration, cloud metadata, network automatic configuration (e.g. DHCP) into COSI resources.\n\nNetwork configuration and network state can be inspected using talosctl get command.\n\nNetwork machine configuration can be modified using talosctl edit mc command (also variants talosctl patch mc, talosctl apply-config) without a reboot. As API access requires network connection, --mode=try can be used to test the configuration with automatic rollback to avoid losing network access to the node.\n\nResources\n\nThere are six basic network configuration items in Talos:\n\nAddress (IP address assigned to the interface/link);\nRoute (route to a destination);\nLink (network interface/link configuration);\nResolver (list of DNS servers);\nHostname (node hostname and domainname);\nTimeServer (list of NTP servers).\n\nEach network configuration item has two counterparts:\n\n*Status (e.g. LinkStatus) describes the current state of the system (Linux kernel state);\n*Spec (e.g. LinkSpec) defines the desired configuration.\nResource\tStatus\tSpec\nAddress\tAddressStatus\tAddressSpec\nRoute\tRouteStatus\tRouteSpec\nLink\tLinkStatus\tLinkSpec\nResolver\tResolverStatus\tResolverSpec\nHostname\tHostnameStatus\tHostnameSpec\nTimeServer\tTimeServerStatus\tTimeServerSpec\n\nStatus resources have aliases with the Status suffix removed, so for example AddressStatus is also available as Address.\n\nTalos networking controllers reconcile the state so that *Status equals the desired *Spec.\n\nObserving State\n\nThe current network configuration state can be observed by querying *Status resources via talosctl:\n\n$ talosctl get addresses\n\nNODE         NAMESPACE   TYPE            ID                                       VERSION   ADDRESS                        LINK\n\n172.20.0.2   network     AddressStatus   eth0/172.20.0.2/24                       1         172.20.0.2/24                  eth0\n\n172.20.0.2   network     AddressStatus   eth0/fe80::9804:17ff:fe9d:3058/64        2         fe80::9804:17ff:fe9d:3058/64   eth0\n\n172.20.0.2   network     AddressStatus   flannel.1/10.244.4.0/32                  1         10.244.4.0/32                  flannel.1\n\n172.20.0.2   network     AddressStatus   flannel.1/fe80::10b5:44ff:fe62:6fb8/64   2         fe80::10b5:44ff:fe62:6fb8/64   flannel.1\n\n172.20.0.2   network     AddressStatus   lo/127.0.0.1/8                           1         127.0.0.1/8                    lo\n\n172.20.0.2   network     AddressStatus   lo/::1/128                               1         ::1/128                        lo\n\n\nIn the output there are addresses set up by Talos (e.g. eth0/172.20.0.2/24) and addresses set up by other facilities (e.g. flannel.1/10.244.4.0/32 set up by CNI).\n\nTalos networking controllers watch the kernel state and update resources accordingly.\n\nAdditional details about the address can be accessed via the YAML output:\n\n# talosctl get address eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressStatuses.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 1\n\n    owner: network.AddressStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    address: 172.20.0.2/24\n\n    local: 172.20.0.2\n\n    broadcast: 172.20.0.255\n\n    linkIndex: 4\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n\nResources can be watched for changes with the --watch flag to see how configuration changes over time.\n\nOther networking status resources can be inspected with talosctl get routes, talosctl get links, etc. For example:\n\n$ talosctl get resolvers\n\nNODE         NAMESPACE   TYPE             ID          VERSION   RESOLVERS\n\n172.20.0.2   network     ResolverStatus   resolvers   2         [\"8.8.8.8\",\"1.1.1.1\"]\n\n# talosctl get links -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: LinkStatuses.net.talos.dev\n\n    id: eth0\n\n    version: 2\n\n    owner: network.LinkStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    index: 4\n\n    type: ether\n\n    linkIndex: 0\n\n    flags: UP,BROADCAST,RUNNING,MULTICAST,LOWER_UP\n\n    hardwareAddr: 4e:95:8e:8f:e4:47\n\n    broadcastAddr: ff:ff:ff:ff:ff:ff\n\n    mtu: 1500\n\n    queueDisc: pfifo_fast\n\n    operationalState: up\n\n    kind: \"\"\n\n    slaveKind: \"\"\n\n    driver: virtio_net\n\n    linkState: true\n\n    speedMbit: 4294967295\n\n    port: Other\n\n    duplex: Unknown\n\nInspecting Configuration\n\nThe desired networking configuration is combined from multiple sources and presented as *Spec resources:\n\n$ talosctl get addressspecs\n\nNODE         NAMESPACE   TYPE          ID                   VERSION\n\n172.20.0.2   network     AddressSpec   eth0/172.20.0.2/24   2\n\n172.20.0.2   network     AddressSpec   lo/127.0.0.1/8       2\n\n172.20.0.2   network     AddressSpec   lo/::1/128           2\n\n\nThese AddressSpecs are applied to the Linux kernel to reach the desired state. If, for example, an AddressSpec is removed, the address is removed from the Linux network interface as well.\n\n*Spec resources can’t be manipulated directly, they are generated automatically by Talos from multiple configuration sources (see a section below for details).\n\nIf a *Spec resource is queried in YAML format, some additional information is available:\n\n# talosctl get addressspecs eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressSpecs.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 2\n\n    owner: network.AddressMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.AddressSpecController\n\nspec:\n\n    address: 172.20.0.2/24\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n    layer: operator\n\n\nAn important field is the layer field, which describes a configuration layer this spec is coming from: in this case, it’s generated by a network operator (see below) and is set by the DHCPv4 operator.\n\nConfiguration Merging\n\nSpec resources described in the previous section show the final merged configuration state, while initial specs are put to a different unmerged namespace network-config. Spec resources in the network-config namespace are merged with conflict resolution to produce the final merged representation in the network namespace.\n\nLet’s take HostnameSpec as an example. The final merged representation is:\n\n# talosctl get hostnamespec -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: hostname\n\n    version: 2\n\n    owner: network.HostnameMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.HostnameSpecController\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that the final configuration for the hostname is talos-default-controlplane-1. And this is the hostname that was actually applied. This can be verified by querying a HostnameStatus resource:\n\n$ talosctl get hostnamestatus\n\nNODE         NAMESPACE   TYPE             ID         VERSION   HOSTNAME                 DOMAINNAME\n\n172.20.0.2   network     HostnameStatus   hostname   1         talos-default-controlplane-1\n\n\nInitial configuration for the hostname in the network-config namespace is:\n\n# talosctl get hostnamespec -o yaml --namespace network-config\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: default/hostname\n\n    version: 2\n\n    owner: network.HostnameConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-172-20-0-2\n\n    domainname: \"\"\n\n    layer: default\n\n---\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: dhcp4/eth0/hostname\n\n    version: 1\n\n    owner: network.OperatorSpecController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that there are two specs for the hostname:\n\none from the default configuration layer which defines the hostname as talos-172-20-0-2 (default driven by the default node address);\nanother one from the layer operator that defines the hostname as talos-default-controlplane-1 (DHCP).\n\nTalos merges these two specs into a final HostnameSpec based on the configuration layer and merge rules. Here is the order of precedence from low to high:\n\ndefault (defaults provided by Talos);\ncmdline (from the kernel command line);\nplatform (driven by the cloud provider);\noperator (various dynamic configuration options: DHCP, Virtual IP, etc);\nconfiguration (derived from the machine configuration).\n\nSo in our example the operator layer HostnameSpec overrides the default layer producing the final hostname talos-default-controlplane-1.\n\nThe merge process applies to all six core networking specs. For each spec, the layer controls the merge behavior If multiple configuration specs appear at the same layer, they can be merged together if possible, otherwise merge result is stable but not defined (e.g. if DHCP on multiple interfaces provides two different hostnames for the node).\n\nLinkSpecs are merged across layers, so for example, machine configuration for the interface MTU overrides an MTU set by the DHCP server.\n\nNetwork Operators\n\nNetwork operators provide dynamic network configuration which can change over time as the node is running:\n\nDHCPv4\nDHCPv6\nVirtual IP\n\nNetwork operators produce specs for addresses, routes, links, etc., which are then merged and applied according to the rules described above.\n\nOperators are configured with OperatorSpec resources which describe when operators should run and additional configuration for the operator:\n\n# talosctl get operatorspecs -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: OperatorSpecs.net.talos.dev\n\n    id: dhcp4/eth0\n\n    version: 1\n\n    owner: network.OperatorConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    operator: dhcp4\n\n    linkName: eth0\n\n    requireUp: true\n\n    dhcp4:\n\n        routeMetric: 1024\n\n\nOperatorSpec resources are generated by Talos based on machine configuration mostly. DHCP4 operator is created automatically for all physical network links which are not configured explicitly via the kernel command line or the machine configuration. This also means that on the first boot, without a machine configuration, a DHCP request is made on all physical network interfaces by default.\n\nSpecs generated by operators are prefixed with the operator ID (dhcp4/eth0 in the example above) in the unmerged network-config namespace:\n\n$ talosctl -n 172.20.0.2 get addressspecs --namespace network-config\n\nNODE         NAMESPACE        TYPE          ID                              VERSION\n\n172.20.0.2   network-config   AddressSpec   dhcp4/eth0/eth0/172.20.0.2/24   1\n\nOther Network Resources\n\nThere are some additional resources describing the network subsystem state.\n\nThe NodeAddress resource presents node addresses excluding link-local and loopback addresses:\n\n$ talosctl get nodeaddresses\n\nNODE          NAMESPACE   TYPE          ID             VERSION   ADDRESSES\n\n10.100.2.23   network     NodeAddress   accumulative   6         [\"10.100.2.23\",\"147.75.98.173\",\"147.75.195.143\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   current        5         [\"10.100.2.23\",\"147.75.98.173\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   default        1         [\"10.100.2.23\"]\n\ndefault is the node default address;\ncurrent is the set of addresses a node currently has;\naccumulative is the set of addresses a node had over time (it might include virtual IPs which are not owned by the node at the moment).\n\nNodeAddress resources are used to pick up the default address for etcd peer URL, to populate SANs field in the generated certificates, etc.\n\nAnother important resource is Nodename which provides Node name in Kubernetes:\n\n$ talosctl get nodename\n\nNODE          NAMESPACE      TYPE       ID         VERSION   NODENAME\n\n10.100.2.23   controlplane   Nodename   nodename   1         infra-green-cp-mmf7v\n\n\nDepending on the machine configuration nodename might be just a hostname or the FQDN of the node.\n\nNetworkStatus aggregates the current state of the network configuration:\n\n# talosctl get networkstatus -o yaml\n\nnode: 10.100.2.23\n\nmetadata:\n\n    namespace: network\n\n    type: NetworkStatuses.net.talos.dev\n\n    id: status\n\n    version: 5\n\n    owner: network.StatusController\n\n    phase: running\n\n    created: 2021-06-24T18:56:00Z\n\n    updated: 2021-06-24T18:56:02Z\n\nspec:\n\n    addressReady: true\n\n    connectivityReady: true\n\n    hostnameReady: true\n\n    etcFilesReady: true\n\nNetwork Controllers\n\nFor each of the six basic resource types, there are several controllers:\n\n*StatusController populates *Status resources observing the Linux kernel state.\n*ConfigController produces the initial unmerged *Spec resources in the network-config namespace based on defaults, kernel command line, and machine configuration.\n*MergeController merges *Spec resources into the final representation in the network namespace.\n*SpecController applies merged *Spec resources to the kernel state.\n\nFor the network operators:\n\nOperatorConfigController produces OperatorSpec resources based on machine configuration and deafauls.\nOperatorSpecController runs network operators watching OperatorSpec resources and producing various *Spec resources in the network-config namespace.\nConfiguration Sources\n\nThere are several configuration sources for the network configuration, which are described in this section.\n\nDefaults\nlo interface is assigned addresses 127.0.0.1/8 and ::1/128;\nhostname is set to the talos-<IP> where IP is the default node address;\nresolvers are set to 8.8.8.8, 1.1.1.1;\ntime servers are set to pool.ntp.org;\nDHCP4 operator is run on any physical interface which is not configured explicitly.\nCmdline\n\nThe kernel command line is parsed for the following options:\n\nip= option is parsed for node IP, default gateway, hostname, DNS servers, NTP servers;\nbond= option is parsed for bonding interfaces and their options;\ntalos.hostname= option is used to set node hostname;\ntalos.network.interface.ignore= can be used to make Talos skip network interface configuration completely.\nPlatform\n\nPlatform configuration delivers cloud environment-specific options (e.g. the hostname).\n\nPlatform configuration is specific to the environment metadata: for example, on Equinix Metal, Talos automatically configures public and private IPs, routing, link bonding, hostname.\n\nPlatform configuration is cached across reboots in /system/state/platform-network.yaml.\n\nOperator\n\nNetwork operators provide configuration for all basic resource types.\n\nMachine Configuration\n\nThe machine configuration is parsed for link configuration, addresses, routes, hostname, resolvers and time servers. Any changes to .machine.network configuration can be applied in immediate mode.\n\nNetwork Configuration Debugging\n\nMost of the network controller operations and failures are logged to the kernel console, additional logs with debug level are available with talosctl logs controller-runtime command. If the network configuration can’t be established and the API is not available, debug level logs can be sent to the console with debug: true option in the machine configuration.\n\n6.8 - Network Connectivity\nDescription of the Networking Connectivity needed by Talos Linux\nConfiguring Network Connectivity\n\nThe simplest way to deploy Talos is by ensuring that all the remote components of the system (talosctl, the control plane nodes, and worker nodes) all have layer 2 connectivity. This is not always possible, however, so this page lays out the minimal network access that is required to configure and operate a talos cluster.\n\nNote: These are the ports required for Talos specifically, and should be configured in addition to the ports required by kuberenetes. See the kubernetes docs for information on the ports used by kubernetes itself.\n\nControl plane node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\ttalosctl, control plane nodes\nTCP\tInbound\t50001*\ttrustd\tWorker nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\nWorker node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\tControl plane nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\n6.9 - KubeSpan\nUnderstand more about KubeSpan for Talos Linux.\nWireGuard Peer Discovery\n\nThe key pieces of information needed for WireGuard generally are:\n\nthe public key of the host you wish to connect to\nan IP address and port of the host you wish to connect to\n\nThe latter is really only required of one side of the pair. Once traffic is received, that information is learned and updated by WireGuard automatically.\n\nKubernetes, though, also needs to know which traffic goes to which WireGuard peer. Because this information may be dynamic, we need a way to keep this information up to date.\n\nIf we already have a connection to Kubernetes, it’s fairly easy: we can just keep that information in Kubernetes. Otherwise, we have to have some way to discover it.\n\nTalos Linux implements a multi-tiered approach to gathering this information. Each tier can operate independently, but the amalgamation of the mechanisms produces a more robust set of connection criteria.\n\nThese mechanisms are:\n\nan external service\na Kubernetes-based system\n\nSee discovery service to learn more about the external service.\n\nThe Kubernetes-based system utilizes annotations on Kubernetes Nodes which describe each node’s public key and local addresses.\n\nOn top of this, KubeSpan can optionally route Pod subnets. This is usually taken care of by the CNI, but there are many situations where the CNI fails to be able to do this itself, across networks.\n\nNAT, Multiple Routes, Multiple IPs\n\nOne of the difficulties in communicating across networks is that there is often not a single address and port which can identify a connection for each node on the system. For instance, a node sitting on the same network might see its peer as 192.168.2.10, but a node across the internet may see it as 2001:db8:1ef1::10.\n\nWe need to be able to handle any number of addresses and ports, and we also need to have a mechanism to try them. WireGuard only allows us to select one at a time.\n\nKubeSpan implements a controller which continuously discovers and rotates these IP:port pairs until a connection is established. It then starts trying again if that connection ever fails.\n\nPacket Routing\n\nAfter we have established a WireGuard connection, we have to make sure that the right packets get sent to the WireGuard interface.\n\nWireGuard supplies a convenient facility for tagging packets which come from it, which is great. But in our case, we need to be able to allow traffic which both does not come from WireGuard and also is not destined for another Kubernetes node to flow through the normal mechanisms.\n\nUnlike many corporate or privacy-oriented VPNs, we need to allow general internet traffic to flow normally.\n\nAlso, as our cluster grows, this set of IP addresses can become quite large and quite dynamic. This would be very cumbersome and slow in iptables. Luckily, the kernel supplies a convenient mechanism by which to define this arbitrarily large set of IP addresses: IP sets.\n\nTalos collects all of the IPs and subnets which are considered “in-cluster” and maintains these in the kernel as an IP set.\n\nNow that we have the IP set defined, we need to tell the kernel how to use it.\n\nThe traditional way of doing this would be to use iptables. However, there is a big problem with IPTables. It is a common namespace in which any number of other pieces of software may dump things. We have no surety that what we add will not be wiped out by something else (from Kubernetes itself, to the CNI, to some workload application), be rendered unusable by higher-priority rules, or just generally cause trouble and conflicts.\n\nInstead, we use a three-pronged system which is both more foundational and less centralised.\n\nNFTables offers a separately namespaced, decentralised way of marking packets for later processing based on IP sets. Instead of a common set of well-known tables, NFTables uses hooks into the kernel’s netfilter system, which are less vulnerable to being usurped, bypassed, or a source of interference than IPTables, but which are rendered down by the kernel to the same underlying XTables system.\n\nOur NFTables system is where we store the IP sets. Any packet which enters the system, either by forward from inside Kubernetes or by generation from the host itself, is compared against a hash table of this IP set. If it is matched, it is marked for later processing by our next stage. This is a high-performance system which exists fully in the kernel and which ultimately becomes an eBPF program, so it scales well to hundreds of nodes.\n\nThe next stage is the kernel router’s route rules. These are defined as a common ordered list of operations for the whole operating system, but they are intended to be tightly constrained and are rarely used by applications in any case. The rules we add are very simple: if a packet is marked by our NFTables system, send it to an alternate routing table.\n\nThis leads us to our third and final stage of packet routing. We have a custom routing table with two rules:\n\nsend all IPv4 traffic to the WireGuard interface\nsend all IPv6 traffic to the WireGuard interface\n\nSo in summary, we:\n\nmark packets destined for Kubernetes applications or Kubernetes nodes\nsend marked packets to a special routing table\nsend anything which is sent to that routing table through the WireGuard interface\n\nThis gives us an isolated, resilient, tolerant, and non-invasive way to route Kubernetes traffic safely, automatically, and transparently through WireGuard across almost any set of network topologies.\n\nDesign Decisions\nRouting\n\nRouting for Wireguard is a touch complicated when the set of possible peer endpoints includes at least one member of the set of destinations. That is, packets from Wireguard to a peer endpoint should not be sent to Wireguard, lest a loop be created.\n\nIn order to handle this situation, Wireguard provides the ability to mark packets which it generates, so their routing can be handled separately.\n\nIn our case, though, we actually want the inverse of this: we want to route Wireguard packets however the normal networking routes and rules say they should be routed, while packets destined for the other side of Wireguard Peers should be forced into Wireguard interfaces.\n\nWhile IP Rules allow you to invert matches, they do not support matching based on IP sets. That means, to use simple rules, we would have to add a rule for each destination, which could reach into hundreds or thousands of rules to manage. This is not really much of a performance issue, but it is a management issue, since it is expected that we would not be the only manager of rules in the system, and rules offer no facility to tag for ownership.\n\nIP Sets are supported by IPTables, and we could integrate there. However, IPTables exists in a global namespace, which makes it fragile having multiple parties manipulating it. The newer NFTables replacement for IPTables, though, allows users to independently hook into various points of XTables, keeping all such rules and sets independent. This means that regardless of what CNIs or other user-side routing rules may do, our KubeSpan setup will not be messed up.\n\nTherefore, we utilise NFTables (which natively supports IP sets and owner grouping) instead, to mark matching traffic which should be sent to the Wireguard interface. This way, we can keep all our KubeSpan set logic in one place, allowing us to simply use a single ip rule match: for our fwmark, and sending those matched packets to a separate routing table with one rule: default to the wireguard interface.\n\nSo we have three components:\n\nA routing table for Wireguard-destined packets\nAn NFTables table which defines the set of destinations packets to which will be marked with our firewall mark.\nHook into PreRouting (type Filter)\nHook into Outgoing (type Route)\nOne IP Rule which sends packets marked with our firewall mark to our Wireguard routing table.\nRouting Table\n\nThe routing table (number 180 by default) is simple, containing a single route for each family: send everything through the Wireguard interface.\n\nNFTables\n\nThe logic inside NFTables is fairly simple. First, everything is compiled into a single table: talos_kubespan.\n\nNext, two chains are set up: one for the prerouting hook (kubespan_prerouting) and the other for the outgoing hook (kubespan_outgoing).\n\nWe define two sets of target IP prefixes: one for IPv6 (kubespan_targets_ipv6) and the other for IPv4 (kubespan_targets_ipv4).\n\nLast, we add rules to each chain which basically specify:\n\nIf the packet is marked as from Wireguard, just accept it and terminate the chain.\nIf the packet matches an IP in either of the target IP sets, mark that packet with the to Wireguard mark.\nRules\n\nThere are two route rules defined: one to match IPv6 packets and the other to match IPv4 packets.\n\nThese rules say the same thing for each: if the packet is marked that it should go to Wireguard, send it to the Wireguard routing table.\n\nFirewall Mark\n\nKubeSpan is using only two bits of the firewall mark with the mask 0x00000060.\n\nNote: if other software on the node is using the bits 0x60 of the firewall mark, this might cause conflicts and break KubeSpan.\n\nAt the moment of the writing, it was confirmed that Calico CNI is using bits 0xffff0000 and Cilium CNI is using bits 0xf00, so KubeSpan is compatible with both. Flannel CNI uses 0x4000 mask, so it is also compatible.\n\nIn the routing rules table, we match on the mark 0x40 with the mask 0x60:\n\n32500: from all fwmark 0x40/0x60 lookup 180\n\n\nIn the NFTables table, we match with the same mask 0x60 and we set the mask by only modifying bits from the 0x60 mask:\n\nmeta mark & 0x00000060 == 0x00000020 accept\n\nip daddr @kubespan_targets_ipv4 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\nip6 daddr @kubespan_targets_ipv6 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\n6.10 - Process Capabilities\nUnderstand the Linux process capabilities restrictions with Talos Linux.\n\nLinux defines a set of process capabilities that can be used to fine-tune the process permissions.\n\nTalos Linux for security reasons restricts any process from gaining the following capabilities:\n\nCAP_SYS_MODULE (loading kernel modules)\nCAP_SYS_BOOT (rebooting the system)\n\nThis means that any process including privileged Kubernetes pods will not be able to get these capabilities.\n\nIf you see the following error on starting a pod, make sure it doesn’t have any of the capabilities listed above in the spec:\n\nError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply caps: operation not permitted: unknown\n\n\nNote: even with CAP_SYS_MODULE capability, Linux kernel module loading is restricted by requiring a valid signature. Talos Linux creates a throw away signing key during kernel build, so it’s not possible to build/sign a kernel module for Talos Linux outside of the build process.\n\n6.11 - talosctl\nThe design and use of the Talos Linux control application.\n\nThe talosctl tool acts as a reference implementation for the Talos API, but it also handles a lot of conveniences for the use of Talos and its clusters.\n\nVideo Walkthrough\n\nTo see some live examples of talosctl usage, view the following video:\n\nClient Configuration\n\nTalosctl configuration is located in $XDG_CONFIG_HOME/talos/config.yaml if $XDG_CONFIG_HOME is defined. Otherwise it is in $HOME/.talos/config. The location can always be overridden by the TALOSCONFIG environment variable or the --talosconfig parameter.\n\nLike kubectl, talosctl uses the concept of configuration contexts, so any number of Talos clusters can be managed with a single configuration file. It also comes with some intelligent tooling to manage the merging of new contexts into the config. The default operation is a non-destructive merge, where if a context of the same name already exists in the file, the context to be added is renamed by appending an index number. You can easily overwrite instead, as well. See the talosctl config help for more information.\n\nEndpoints and Nodes\n\nendpoints are the communication endpoints to which the client directly talks. These can be load balancers, DNS hostnames, a list of IPs, etc. If multiple endpoints are specified, the client will automatically load balance and fail over between them. It is recommended that these point to the set of control plane nodes, either directly or through a load balancer.\n\nEach endpoint will automatically proxy requests destined to another node through it, so it is not necessary to change the endpoint configuration just because you wish to talk to a different node within the cluster.\n\nEndpoints do, however, need to be members of the same Talos cluster as the target node, because these proxied connections reply on certificate-based authentication.\n\nThe node is the target node on which you wish to perform the API call. While you can configure the target node (or even set of target nodes) inside the ’talosctl’ configuration file, it is recommended not to do so, but to explicitly declare the target node(s) using the -n or --nodes command-line parameter.\n\nWhen specifying nodes, their IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied first through the endpoints.\n\nKubeconfig\n\nThe configuration for accessing a Talos Kubernetes cluster is obtained with talosctl. By default, talosctl will safely merge the cluster into the default kubeconfig. Like talosctl itself, in the event of a naming conflict, the new context name will be index-appended before insertion. The --force option can be used to overwrite instead.\n\nYou can also specify an alternate path by supplying it as a positional parameter.\n\nThus, like Talos clusters themselves, talosctl makes it easy to manage any number of kubernetes clusters from the same workstation.\n\nCommands\n\nPlease see the CLI reference for the entire list of commands which are available from talosctl.\n\n6.12 - FAQs\nFrequently Asked Questions about Talos Linux.\nHow is Talos different from other container optimized Linux distros?\n\nTalos integrates tightly with Kubernetes, and is not meant to be a general-purpose operating system. The most important difference is that Talos is fully controlled by an API via a gRPC interface, instead of an ordinary shell. We don’t ship SSH, and there is no console access. Removing components such as these has allowed us to dramatically reduce the footprint of Talos, and in turn, improve a number of other areas like security, predictability, reliability, and consistency across platforms. It’s a big change from how operating systems have been managed in the past, but we believe that API-driven OSes are the future.\n\nWhy no shell or SSH?\n\nSince Talos is fully API-driven, all maintenance and debugging operations are possible via the OS API. We would like for Talos users to start thinking about what a “machine” is in the context of a Kubernetes cluster. That is, that a Kubernetes cluster can be thought of as one massive machine, and the nodes are merely additional, undifferentiated resources. We don’t want humans to focus on the nodes, but rather on the machine that is the Kubernetes cluster. Should an issue arise at the node level, talosctl should provide the necessary tooling to assist in the identification, debugging, and remediation of the issue. However, the API is based on the Principle of Least Privilege, and exposes only a limited set of methods. We envision Talos being a great place for the application of control theory in order to provide a self-healing platform.\n\nWhy the name “Talos”?\n\nTalos was an automaton created by the Greek God of the forge to protect the island of Crete. He would patrol the coast and enforce laws throughout the land. We felt it was a fitting name for a security focused operating system designed to run Kubernetes.\n\nWhy does Talos rely on a separate configuration from Kubernetes?\n\nThe talosconfig file contains client credentials to access the Talos Linux API. Sometimes Kubernetes might be down for a number of reasons (etcd issues, misconfiguration, etc.), while Talos API access will always be available. The Talos API is a way to access the operating system and fix issues, e.g. fixing access to Kubernetes. When Talos Linux is running fine, using the Kubernetes APIs (via kubeconfig) is all you should need to deploy and manage Kubernetes workloads.\n\nHow does Talos handle certificates?\n\nDuring the machine config generation process, Talos generates a set of certificate authorities (CAs) that remains valid for 10 years. Talos is responsible for managing certificates for etcd, Talos API (apid), node certificates (kubelet), and other components. It also handles the automatic rotation of server-side certificates.\n\nHowever, client certificates such as talosconfig and kubeconfig are the user’s responsibility, and by default, they have a validity period of 1 year.\n\nTo renew the talosconfig certificate, the follow this process. To renew kubeconfig, use talosctl kubeconfig command, and the time-to-live (TTL) is defined in the configuration.\n\nHow can I set the timezone of my Talos Linux clusters?\n\nTalos doesn’t support timezones, and will always run in UTC. This ensures consistency of log timestamps for all Talos Linux clusters, simplifying debugging. Your containers can run with any timezone configuration you desire, but the timezone of Talos Linux is not configurable.\n\nHow do I see Talos kernel configuration?\nUsing Talos API\n\nCurrent kernel config can be read with talosctl -n <NODE> read /proc/config.gz.\n\nFor example:\n\ntalosctl -n NODE read /proc/config.gz | zgrep E1000\n\nUsing GitHub\n\nFor amd64, see https://github.com/siderolabs/pkgs/blob/main/kernel/build/config-amd64. Use appropriate branch to see the kernel config matching your Talos release.\n\n6.13 - Knowledge Base\nRecipes for common configuration tasks with Talos Linux.\nDisabling GracefulNodeShutdown on a node\n\nTalos Linux enables Graceful Node Shutdown Kubernetes feature by default.\n\nIf this feature should be disabled, modify the kubelet part of the machine configuration with:\n\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      feature-gates: GracefulNodeShutdown=false\n\n    extraConfig:\n\n      shutdownGracePeriod: 0s\n\n      shutdownGracePeriodCriticalPods: 0s\n\nGenerating Talos Linux ISO image with custom kernel arguments\n\nPass additional kernel arguments using --extra-kernel-arg flag:\n\n$ docker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --extra-kernel-arg console=ttyS1 --extra-kernel-arg console=tty0 | tar xz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/vmlinuz to /mnt/boot/vmlinuz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/initramfs.xz to /mnt/boot/initramfs.xz\n\n2022/05/25 13:18:47 creating grub.cfg\n\n2022/05/25 13:18:47 creating ISO\n\n\nISO will be output to the file talos-<arch>.iso in the current directory.\n\nLogging Kubernetes audit logs with loki\n\nIf using loki-stack helm chart to gather logs from the Kubernetes cluster, you can use the helm values to configure loki-stack to log Kubernetes API server audit logs:\n\npromtail:\n\n  extraArgs:\n\n    - -config.expand-env\n\n  # this is required so that the promtail process can read the kube-apiserver audit logs written as `nobody` user\n\n  containerSecurityContext:\n\n    capabilities:\n\n      add:\n\n        - DAC_READ_SEARCH\n\n  extraVolumes:\n\n    - name: audit-logs\n\n      hostPath:\n\n        path: /var/log/audit/kube\n\n  extraVolumeMounts:\n\n    - name: audit-logs\n\n      mountPath: /var/log/audit/kube\n\n      readOnly: true\n\n  config:\n\n    snippets:\n\n      extraScrapeConfigs: |\n\n        - job_name: auditlogs\n\n          static_configs:\n\n            - targets:\n\n                - localhost\n\n              labels:\n\n                job: auditlogs\n\n                host: ${HOSTNAME}\n\n                __path__: /var/log/audit/kube/*.log        \n\nSetting CPU scaling governer\n\nWhile its possible to set CPU scaling governer via .machine.sysfs it’s sometimes cumbersome to set it for all CPU’s individually. A more elegant approach would be set it via a kernel commandline parameter. This also means that the options are applied way early in the boot process.\n\nThis can be set in the machineconfig via the snippet below:\n\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - cpufreq.default_governor=performance\n\n\nNote: Talos needs to be upgraded for the extraKernelArgs to take effect.\n\nDisable admissionControl on control plane nodes\n\nTalos Linux enables admission control in the API Server by default.\n\nAlthough it is not recommended from a security point of view, admission control can be removed by patching your control plane machine configuration:\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch-control-plane '[{\"op\": \"remove\", \"path\": \"/cluster/apiServer/admissionControl\"}]'\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "runtime | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/runtime/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nruntime\nPackage runtime provides runtime machine configuration documents.\n1: EventSinkConfig\n2: KmsgLogConfig\n1 - EventSinkConfig\nEventSinkConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: EventSinkConfig\n\nendpoint: 192.168.10.3:3247 # The endpoint for the event sink as 'host:port'.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tThe endpoint for the event sink as ‘host:port’.\nShow example(s)\n\t\n2 - KmsgLogConfig\nKmsgLogConfig is a event sink config document.\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log # Name of the config document.\n\nurl: tcp://192.168.3.7:3478/ # The URL encodes the log destination.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nurl\tURL\t\nThe URL encodes the log destination.\nShow example(s)\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nConfiguration\nTalos Linux machine configuration reference.\n1: network\n1.1: NetworkDefaultActionConfig\n1.2: NetworkRuleConfig\n2: runtime\n2.1: EventSinkConfig\n2.2: KmsgLogConfig\n3: siderolink\n3.1: SideroLinkConfig\n4: v1alpha1\n4.1: Config\n\nTalos Linux machine is fully configured via a single YAML file called machine configuration.\n\nThe file might contain one or more configuration documents separated by --- (three dashes) lines. At the moment, majority of the configuration options are within the v1alpha1 document, so this is the only mandatory document in the configuration file.\n\nConfiguration documents might be named (contain a name: field) or unnamed. Unnamed documents can be supplied to the machine configuration file only once, while named documents can be supplied multiple times with unique names.\n\nThe v1alpha1 document has its own (legacy) structure, while every other document has the following set of fields:\n\nCopy\napiVersion: v1alpha1 # version of the document\n\nkind: NetworkRuleConfig # type of document\n\nname: rule1 # only for named documents\n\n\nThis section contains the configuration reference, to learn more about Talos Linux machine configuration management, please see:\n\nquick guide to configuration generation\nconfiguration management in production\nconfiguration patches\nediting live machine configuration\n1 - network\nPackage network provides network machine configuration documents.\n1.1 - NetworkDefaultActionConfig\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\nCopy\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: accept # Default action for all not explicitly configured ingress traffic: accept or block.\nField\tType\tDescription\tValue(s)\ningress\tDefaultAction\tDefault action for all not explicitly configured ingress traffic: accept or block.\taccept\nblock\n\n1.2 - NetworkRuleConfig\nNetworkRuleConfig is a network firewall rule config document.\nCopy\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: ingress-apid # Name of the config document.\n\n# Port selector defines which ports and protocols on the host are affected by the rule.\n\nportSelector:\n\n    # Ports defines a list of port ranges or single ports.\n\n    ports:\n\n        - 50000\n\n    protocol: tcp # Protocol defines traffic protocol (e.g. TCP or UDP).\n\n# Ingress defines which source subnets are allowed to access the host ports/protocols defined by the `portSelector`.\n\ningress:\n\n    - subnet: 192.168.0.0/16 # Subnet defines a source subnet.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nportSelector\tRulePortSelector\tPort selector defines which ports and protocols on the host are affected by the rule.\t\ningress\t[]IngressRule\tIngress defines which source subnets are allowed to access the host ports/protocols defined by the portSelector.\t\nportSelector\n\nRulePortSelector is a port selector for the network rule.\n\nField\tType\tDescription\tValue(s)\nports\tPortRanges\t\nPorts defines a list of port ranges or single ports.\nShow example(s)\n\t\nprotocol\tProtocol\tProtocol defines traffic protocol (e.g. TCP or UDP).\ttcp\nudp\nicmp\nicmpv6\n\ningress[]\n\nIngressRule is a ingress rule.\n\nField\tType\tDescription\tValue(s)\nsubnet\tPrefix\tSubnet defines a source subnet.\nShow example(s)\n\t\nexcept\tPrefix\tExcept defines a source subnet to exclude from the rule, it gets excluded from the subnet.\t\n2 - runtime\nPackage runtime provides runtime machine configuration documents.\n2.1 - EventSinkConfig\nEventSinkConfig is a event sink config document.\nCopy\napiVersion: v1alpha1\n\nkind: EventSinkConfig\n\nendpoint: 192.168.10.3:3247 # The endpoint for the event sink as 'host:port'.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tThe endpoint for the event sink as ‘host:port’.\nShow example(s)\n\t\n2.2 - KmsgLogConfig\nKmsgLogConfig is a event sink config document.\nCopy\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log # Name of the config document.\n\nurl: tcp://192.168.3.7:3478/ # The URL encodes the log destination.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nurl\tURL\t\nThe URL encodes the log destination.\nShow example(s)\n\t\n3 - siderolink\nPackage siderolink provides SideroLink machine configuration documents.\n3.1 - SideroLinkConfig\nSideroLinkConfig is a SideroLink connection machine configuration document.\nCopy\napiVersion: v1alpha1\n\nkind: SideroLinkConfig\n\napiUrl: https://siderolink.api/join?token=secret # SideroLink API URL to connect to.\nField\tType\tDescription\tValue(s)\napiUrl\tURL\tSideroLink API URL to connect to.\nShow example(s)\n\t\n4 - v1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\n4.1 - Config\nConfig defines the v1alpha1.Config Talos machine configuration document.\nCopy\nversion: v1alpha1\n\nmachine: # ...\n\ncluster: # ...\nField\tType\tDescription\tValue(s)\nversion\tstring\tIndicates the schema used to decode the contents.\tv1alpha1\n\ndebug\tbool\t\nEnable verbose logging to the console.\n\ttrue\nyes\nfalse\nno\n\nmachine\tMachineConfig\tProvides machine specific configuration options.\t\ncluster\tClusterConfig\tProvides cluster specific configuration options.\t\nmachine\n\nMachineConfig represents the machine-specific config values.\n\nCopy\nmachine:\n\n    type: controlplane\n\n    # InstallConfig represents the installation options for preparing a node.\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ntype\tstring\t\nDefines the role of the machine within the cluster.\n\tcontrolplane\nworker\n\ntoken\tstring\t\nThe token is used by a machine to join the PKI of the cluster.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe root certificate authority of the PKI.\nShow example(s)\n\t\ncertSANs\t[]string\t\nExtra certificate subject alternative names for the machine’s certificate.\nShow example(s)\n\t\ncontrolPlane\tMachineControlPlaneConfig\tProvides machine specific control plane configuration options.\nShow example(s)\n\t\nkubelet\tKubeletConfig\tUsed to provide additional options to the kubelet.\nShow example(s)\n\t\npods\t[]Unstructured\t\nUsed to provide static pod definitions to be run by the kubelet directly bypassing the kube-apiserver.\nShow example(s)\n\t\nnetwork\tNetworkConfig\tProvides machine specific network configuration options.\nShow example(s)\n\t\ndisks\t[]MachineDisk\t\nUsed to partition, format and mount additional disks.\nShow example(s)\n\t\ninstall\tInstallConfig\t\nUsed to provide instructions for installations.\nShow example(s)\n\t\nfiles\t[]MachineFile\t\nAllows the addition of user specified files.\nShow example(s)\n\t\nenv\tEnv\t\nThe env field allows for the addition of environment variables.\nShow example(s)\n\tGRPC_GO_LOG_VERBOSITY_LEVEL\nGRPC_GO_LOG_SEVERITY_LEVEL\nhttp_proxy\nhttps_proxy\nno_proxy\n\ntime\tTimeConfig\tUsed to configure the machine’s time settings.\nShow example(s)\n\t\nsysctls\tmap[string]string\tUsed to configure the machine’s sysctls.\nShow example(s)\n\t\nsysfs\tmap[string]string\tUsed to configure the machine’s sysfs.\nShow example(s)\n\t\nregistries\tRegistriesConfig\t\nUsed to configure the machine’s container image registry mirrors.\nShow example(s)\n\t\nsystemDiskEncryption\tSystemDiskEncryptionConfig\t\nMachine system disk encryption configuration.\nShow example(s)\n\t\nfeatures\tFeaturesConfig\tFeatures describe individual Talos features that can be switched on or off.\nShow example(s)\n\t\nudev\tUdevConfig\tConfigures the udev system.\nShow example(s)\n\t\nlogging\tLoggingConfig\tConfigures the logging system.\nShow example(s)\n\t\nkernel\tKernelConfig\tConfigures the kernel.\nShow example(s)\n\t\nseccompProfiles\t[]MachineSeccompProfile\tConfigures the seccomp profiles for the machine.\nShow example(s)\n\t\nnodeLabels\tmap[string]string\tConfigures the node labels for the machine.\nShow example(s)\n\t\nnodeTaints\tmap[string]string\tConfigures the node taints for the machine. Effect is optional.\nShow example(s)\n\t\ncontrolPlane\n\nMachineControlPlaneConfig machine specific configuration options.\n\nCopy\nmachine:\n\n    controlPlane:\n\n        # Controller manager machine specific configuration options.\n\n        controllerManager:\n\n            disabled: false # Disable kube-controller-manager on the node.\n\n        # Scheduler machine specific configuration options.\n\n        scheduler:\n\n            disabled: true # Disable kube-scheduler on the node.\nField\tType\tDescription\tValue(s)\ncontrollerManager\tMachineControllerManagerConfig\tController manager machine specific configuration options.\t\nscheduler\tMachineSchedulerConfig\tScheduler machine specific configuration options.\t\ncontrollerManager\n\nMachineControllerManagerConfig represents the machine specific ControllerManager config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-controller-manager on the node.\t\nscheduler\n\nMachineSchedulerConfig represents the machine specific Scheduler config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-scheduler on the node.\t\nkubelet\n\nKubeletConfig represents the kubelet config values.\n\nCopy\nmachine:\n\n    kubelet:\n\n        image: ghcr.io/siderolabs/kubelet:v1.29.0 # The `image` field is an optional reference to an alternative kubelet image.\n\n        # The `extraArgs` field is used to provide additional flags to the kubelet.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n\n\n        # # The `ClusterDNS` field is an optional reference to an alternative kubelet clusterDNS ip list.\n\n        # clusterDNS:\n\n        #     - 10.96.0.10\n\n        #     - 169.254.2.53\n\n\n\n        # # The `extraMounts` field is used to add additional mounts to the kubelet container.\n\n        # extraMounts:\n\n        #     - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n        #       type: bind # Type specifies the mount kind.\n\n        #       source: /var/lib/example # Source specifies the source path of the mount.\n\n        #       # Options are fstab style mount options.\n\n        #       options:\n\n        #         - bind\n\n        #         - rshared\n\n        #         - rw\n\n\n\n        # # The `extraConfig` field is used to provide kubelet configuration overrides.\n\n        # extraConfig:\n\n        #     serverTLSBootstrap: true\n\n\n\n        # # The `KubeletCredentialProviderConfig` field is used to provide kubelet credential configuration.\n\n        # credentialProviderConfig:\n\n        #     apiVersion: kubelet.config.k8s.io/v1\n\n        #     kind: CredentialProviderConfig\n\n        #     providers:\n\n        #         - apiVersion: credentialprovider.kubelet.k8s.io/v1\n\n        #           defaultCacheDuration: 12h\n\n        #           matchImages:\n\n        #             - '*.dkr.ecr.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.*.amazonaws.com.cn'\n\n        #             - '*.dkr.ecr-fips.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.us-iso-east-1.c2s.ic.gov'\n\n        #             - '*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov'\n\n        #           name: ecr-credential-provider\n\n\n\n        # # The `nodeIP` field is used to configure `--node-ip` flag for the kubelet.\n\n        # nodeIP:\n\n        #     # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n        #     validSubnets:\n\n        #         - 10.0.0.0/8\n\n        #         - '!10.0.0.3/32'\n\n        #         - fdc7::/16\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe image field is an optional reference to an alternative kubelet image.\nShow example(s)\n\t\nclusterDNS\t[]string\tThe ClusterDNS field is an optional reference to an alternative kubelet clusterDNS ip list.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tThe extraArgs field is used to provide additional flags to the kubelet.\nShow example(s)\n\t\nextraMounts\t[]ExtraMount\t\nThe extraMounts field is used to add additional mounts to the kubelet container.\nShow example(s)\n\t\nextraConfig\tUnstructured\t\nThe extraConfig field is used to provide kubelet configuration overrides.\nShow example(s)\n\t\ncredentialProviderConfig\tUnstructured\tThe KubeletCredentialProviderConfig field is used to provide kubelet credential configuration.\nShow example(s)\n\t\ndefaultRuntimeSeccompProfileEnabled\tbool\tEnable container runtime default Seccomp profile.\ttrue\nyes\nfalse\nno\n\nregisterWithFQDN\tbool\t\nThe registerWithFQDN field is used to force kubelet to use the node FQDN for registration.\n\ttrue\nyes\nfalse\nno\n\nnodeIP\tKubeletNodeIPConfig\t\nThe nodeIP field is used to configure --node-ip flag for the kubelet.\nShow example(s)\n\t\nskipNodeRegistration\tbool\t\nThe skipNodeRegistration is used to run the kubelet without registering with the apiserver.\n\ttrue\nyes\nfalse\nno\n\ndisableManifestsDirectory\tbool\t\nThe disableManifestsDirectory field configures the kubelet to get static pod manifests from the /etc/kubernetes/manifests directory.\n\ttrue\nyes\nfalse\nno\n\nextraMounts[]\n\nExtraMount wraps OCI Mount specification.\n\nCopy\nmachine:\n\n    kubelet:\n\n        extraMounts:\n\n            - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n              type: bind # Type specifies the mount kind.\n\n              source: /var/lib/example # Source specifies the source path of the mount.\n\n              # Options are fstab style mount options.\n\n              options:\n\n                - bind\n\n                - rshared\n\n                - rw\nField\tType\tDescription\tValue(s)\ndestination\tstring\tDestination is the absolute path where the mount will be placed in the container.\t\ntype\tstring\tType specifies the mount kind.\t\nsource\tstring\tSource specifies the source path of the mount.\t\noptions\t[]string\tOptions are fstab style mount options.\t\nuidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\ngidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\nuidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\ngidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\nnodeIP\n\nKubeletNodeIPConfig represents the kubelet node IP configuration.\n\nCopy\nmachine:\n\n    kubelet:\n\n        nodeIP:\n\n            # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n            validSubnets:\n\n                - 10.0.0.0/8\n\n                - '!10.0.0.3/32'\n\n                - fdc7::/16\nField\tType\tDescription\tValue(s)\nvalidSubnets\t[]string\t\nThe validSubnets field configures the networks to pick kubelet node IP from.\n\t\nnetwork\n\nNetworkConfig represents the machine’s networking config values.\n\nCopy\nmachine:\n\n    network:\n\n        hostname: worker-1 # Used to statically set the hostname for the machine.\n\n        # `interfaces` is used to define the network interface configuration.\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\n\n        # Used to statically set the nameservers for the machine.\n\n        nameservers:\n\n            - 9.8.7.6\n\n            - 8.7.6.5\n\n\n\n        # # Allows for extra entries to be added to the `/etc/hosts` file\n\n        # extraHostEntries:\n\n        #     - ip: 192.168.1.100 # The IP of the host.\n\n        #       # The host alias.\n\n        #       aliases:\n\n        #         - example\n\n        #         - example.domain.tld\n\n\n\n        # # Configures KubeSpan feature.\n\n        # kubespan:\n\n        #     enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nhostname\tstring\tUsed to statically set the hostname for the machine.\t\ninterfaces\t[]Device\t\ninterfaces is used to define the network interface configuration.\nShow example(s)\n\t\nnameservers\t[]string\t\nUsed to statically set the nameservers for the machine.\nShow example(s)\n\t\nextraHostEntries\t[]ExtraHost\tAllows for extra entries to be added to the /etc/hosts file\nShow example(s)\n\t\nkubespan\tNetworkKubeSpan\tConfigures KubeSpan feature.\nShow example(s)\n\t\ndisableSearchDomain\tbool\t\nDisable generating a default search domain in /etc/resolv.conf\n\ttrue\nyes\nfalse\nno\n\ninterfaces[]\n\nDevice represents a network interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\ninterface\tstring\t\nThe interface name.\nShow example(s)\n\t\ndeviceSelector\tNetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\naddresses\t[]string\t\nAssigns static IP addresses to the interface.\nShow example(s)\n\t\nroutes\t[]Route\t\nA list of routes associated with the interface.\nShow example(s)\n\t\nbond\tBond\tBond specific options.\nShow example(s)\n\t\nbridge\tBridge\tBridge specific options.\nShow example(s)\n\t\nvlans\t[]Vlan\tVLAN specific options.\t\nmtu\tint\t\nThe interface’s MTU.\n\t\ndhcp\tbool\t\nIndicates if DHCP should be used to configure the interface.\nShow example(s)\n\t\nignore\tbool\tIndicates if the interface should be ignored (skips configuration).\t\ndummy\tbool\t\nIndicates if the interface is a dummy interface.\n\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\nShow example(s)\n\t\nwireguard\tDeviceWireguardConfig\t\nWireguard specific configuration.\nShow example(s)\n\t\nvip\tDeviceVIPConfig\tVirtual (shared) IP address configuration.\nShow example(s)\n\t\ndeviceSelector\n\nNetworkDeviceSelector struct describes network device selector.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                driver: virtio # Kernel driver, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                  driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nroutes[]\n\nRoute represents a network route.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                - network: 10.2.0.0/16 # The route's network (destination).\n\n                  gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nbond\n\nBond contains the various options for configuring a bonded interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                # The interfaces that make up the bond.\n\n                interfaces:\n\n                    - enp2s0\n\n                    - enp2s1\n\n                mode: 802.3ad # A bond option.\n\n                lacpRate: fast # A bond option.\n\n\n\n                # # Picks a network device using the selector.\n\n\n\n                # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n                # deviceSelectors:\n\n                #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                #       driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bond.\t\ndeviceSelectors\t[]NetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\narpIPTarget\t[]string\t\nA bond option.\n\t\nmode\tstring\t\nA bond option.\n\t\nxmitHashPolicy\tstring\t\nA bond option.\n\t\nlacpRate\tstring\t\nA bond option.\n\t\nadActorSystem\tstring\t\nA bond option.\n\t\narpValidate\tstring\t\nA bond option.\n\t\narpAllTargets\tstring\t\nA bond option.\n\t\nprimary\tstring\t\nA bond option.\n\t\nprimaryReselect\tstring\t\nA bond option.\n\t\nfailOverMac\tstring\t\nA bond option.\n\t\nadSelect\tstring\t\nA bond option.\n\t\nmiimon\tuint32\t\nA bond option.\n\t\nupdelay\tuint32\t\nA bond option.\n\t\ndowndelay\tuint32\t\nA bond option.\n\t\narpInterval\tuint32\t\nA bond option.\n\t\nresendIgmp\tuint32\t\nA bond option.\n\t\nminLinks\tuint32\t\nA bond option.\n\t\nlpInterval\tuint32\t\nA bond option.\n\t\npacketsPerSlave\tuint32\t\nA bond option.\n\t\nnumPeerNotif\tuint8\t\nA bond option.\n\t\ntlbDynamicLb\tuint8\t\nA bond option.\n\t\nallSlavesActive\tuint8\t\nA bond option.\n\t\nuseCarrier\tbool\t\nA bond option.\n\t\nadActorSysPrio\tuint16\t\nA bond option.\n\t\nadUserPortKey\tuint16\t\nA bond option.\n\t\npeerNotifyDelay\tuint32\t\nA bond option.\n\t\ndeviceSelectors[]\n\nNetworkDeviceSelector struct describes network device selector.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                    driver: virtio # Kernel driver, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                    - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                      driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nbridge\n\nBridge contains the various options for configuring a bridge interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bridge:\n\n                # The interfaces that make up the bridge.\n\n                interfaces:\n\n                    - enxda4042ca9a51\n\n                    - enxae2a6774c259\n\n                # A bridge option.\n\n                stp:\n\n                    enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bridge.\t\nstp\tSTP\t\nA bridge option.\n\t\nstp\n\nSTP contains the various options for configuring the STP properties of a bridge interface.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tWhether Spanning Tree Protocol (STP) is enabled.\t\nvlans[]\n\nVlan represents vlan settings for a device.\n\nField\tType\tDescription\tValue(s)\naddresses\t[]string\tThe addresses in CIDR notation or as plain IPs to use.\t\nroutes\t[]Route\tA list of routes associated with the VLAN.\t\ndhcp\tbool\tIndicates if DHCP should be used.\t\nvlanId\tuint16\tThe VLAN’s ID.\t\nmtu\tuint32\tThe VLAN’s MTU.\t\nvip\tDeviceVIPConfig\tThe VLAN’s virtual IP address configuration.\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\n\t\nroutes[]\n\nRoute represents a network route.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - routes:\n\n                    - network: 0.0.0.0/0 # The route's network (destination).\n\n                      gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                    - network: 10.2.0.0/16 # The route's network (destination).\n\n                      gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - vip:\n\n                    ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - dhcpOptions:\n\n                    routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - dhcpOptions:\n\n                routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\nwireguard\n\nDeviceWireguardConfig contains settings for configuring Wireguard network interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                listenPort: 51111 # Specifies a device's listening port.\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n                      persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nField\tType\tDescription\tValue(s)\nprivateKey\tstring\t\nSpecifies a private key configuration (base64 encoded).\n\t\nlistenPort\tint\tSpecifies a device’s listening port.\t\nfirewallMark\tint\tSpecifies a device’s firewall mark.\t\npeers\t[]DeviceWireguardPeer\tSpecifies a list of peer configurations to apply to a device.\t\npeers[]\n\nDeviceWireguardPeer a WireGuard device peer configuration.\n\nField\tType\tDescription\tValue(s)\npublicKey\tstring\t\nSpecifies the public key of this peer.\n\t\nendpoint\tstring\tSpecifies the endpoint of this peer entry.\t\npersistentKeepaliveInterval\tDuration\t\nSpecifies the persistent keepalive interval for this peer.\n\t\nallowedIPs\t[]string\tAllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vip:\n\n                ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\nextraHostEntries[]\n\nExtraHost represents a host entry in /etc/hosts.\n\nCopy\nmachine:\n\n    network:\n\n        extraHostEntries:\n\n            - ip: 192.168.1.100 # The IP of the host.\n\n              # The host alias.\n\n              aliases:\n\n                - example\n\n                - example.domain.tld\nField\tType\tDescription\tValue(s)\nip\tstring\tThe IP of the host.\t\naliases\t[]string\tThe host alias.\t\nkubespan\n\nNetworkKubeSpan struct describes KubeSpan configuration.\n\nCopy\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the KubeSpan feature.\n\t\nadvertiseKubernetesNetworks\tbool\t\nControl whether Kubernetes pod CIDRs are announced over KubeSpan from the node.\n\t\nallowDownPeerBypass\tbool\t\nSkip sending traffic via KubeSpan if the peer connection state is not up.\n\t\nharvestExtraEndpoints\tbool\t\nKubeSpan can collect and publish extra endpoints for each member of the cluster\n\t\nmtu\tuint32\t\nKubeSpan link MTU size.\n\t\nfilters\tKubeSpanFilters\t\nKubeSpan advanced filtering of network addresses .\n\t\nfilters\n\nKubeSpanFilters struct describes KubeSpan advanced network addresses filtering.\n\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nFilter node addresses which will be advertised as KubeSpan endpoints for peer-to-peer Wireguard connections.\nShow example(s)\n\t\ndisks[]\n\nMachineDisk represents the options available for partitioning, formatting, and mounting extra disks.\n\nCopy\nmachine:\n\n    disks:\n\n        - device: /dev/sdb # The name of the disk to use.\n\n          # A list of partitions to create on the disk.\n\n          partitions:\n\n            - mountpoint: /var/mnt/extra # Where to mount the partition.\n\n\n\n              # # The size of partition: either bytes or human readable representation. If `size:` is omitted, the partition is sized to occupy the full disk.\n\n\n\n              # # Human readable representation.\n\n              # size: 100 MB\n\n              # # Precise value in bytes.\n\n              # size: 1073741824\nField\tType\tDescription\tValue(s)\ndevice\tstring\tThe name of the disk to use.\t\npartitions\t[]DiskPartition\tA list of partitions to create on the disk.\t\npartitions[]\n\nDiskPartition represents the options for a disk partition.\n\nField\tType\tDescription\tValue(s)\nsize\tDiskSize\tThe size of partition: either bytes or human readable representation. If size: is omitted, the partition is sized to occupy the full disk.\nShow example(s)\n\t\nmountpoint\tstring\tWhere to mount the partition.\t\ninstall\n\nInstallConfig represents the installation options for preparing a node.\n\nCopy\nmachine:\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ndisk\tstring\tThe disk used for installations.\nShow example(s)\n\t\ndiskSelector\tInstallDiskSelector\t\nLook up disk using disk attributes like model, size, serial and others.\nShow example(s)\n\t\nextraKernelArgs\t[]string\t\nAllows for supplying extra kernel args via the bootloader.\nShow example(s)\n\t\nimage\tstring\t\nAllows for supplying the image used to perform the installation.\nShow example(s)\n\t\nextensions\t[]InstallExtensionConfig\tAllows for supplying additional system extension images to install on top of base Talos image.\nShow example(s)\n\t\nwipe\tbool\t\nIndicates if the installation disk should be wiped at installation time.\n\ttrue\nyes\nfalse\nno\n\nlegacyBIOSSupport\tbool\t\nIndicates if MBR partition should be marked as bootable (active).\n\t\ndiskSelector\n\nInstallDiskSelector represents a disk query parameters for the install disk lookup.\n\nCopy\nmachine:\n\n    install:\n\n        diskSelector:\n\n            size: '>= 1TB' # Disk size.\n\n            model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n\n\n            # # Disk bus path.\n\n            # busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0\n\n            # busPath: /pci0000:00/*\nField\tType\tDescription\tValue(s)\nsize\tInstallDiskSizeMatcher\tDisk size.\nShow example(s)\n\t\nname\tstring\tDisk name /sys/block/<dev>/device/name.\t\nmodel\tstring\tDisk model /sys/block/<dev>/device/model.\t\nserial\tstring\tDisk serial number /sys/block/<dev>/serial.\t\nmodalias\tstring\tDisk modalias /sys/block/<dev>/device/modalias.\t\nuuid\tstring\tDisk UUID /sys/block/<dev>/uuid.\t\nwwid\tstring\tDisk WWID /sys/block/<dev>/wwid.\t\ntype\tInstallDiskType\tDisk Type.\tssd\nhdd\nnvme\nsd\n\nbusPath\tstring\tDisk bus path.\nShow example(s)\n\t\nextensions[]\n\nInstallExtensionConfig represents a configuration for a system extension.\n\nCopy\nmachine:\n\n    install:\n\n        extensions:\n\n            - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\nimage\tstring\tSystem extension image.\t\nfiles[]\n\nMachineFile represents a file to write to disk.\n\nCopy\nmachine:\n\n    files:\n\n        - content: '...' # The contents of the file.\n\n          permissions: 0o666 # The file's permissions in octal.\n\n          path: /tmp/file.txt # The path of the file.\n\n          op: append # The operation to use\nField\tType\tDescription\tValue(s)\ncontent\tstring\tThe contents of the file.\t\npermissions\tFileMode\tThe file’s permissions in octal.\t\npath\tstring\tThe path of the file.\t\nop\tstring\tThe operation to use\tcreate\nappend\noverwrite\n\ntime\n\nTimeConfig represents the options for configuring time on a machine.\n\nCopy\nmachine:\n\n    time:\n\n        disabled: false # Indicates if the time service is disabled for the machine.\n\n        # Specifies time (NTP) servers to use for setting the system time.\n\n        servers:\n\n            - time.cloudflare.com\n\n        bootTimeout: 2m0s # Specifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\t\nIndicates if the time service is disabled for the machine.\n\t\nservers\t[]string\t\nSpecifies time (NTP) servers to use for setting the system time.\n\t\nbootTimeout\tDuration\t\nSpecifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\n\t\nregistries\n\nRegistriesConfig represents the image pull options.\n\nCopy\nmachine:\n\n    registries:\n\n        # Specifies mirror configuration for each registry host namespace.\n\n        mirrors:\n\n            docker.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.local\n\n        # Specifies TLS & auth configuration for HTTPS image registries.\n\n        config:\n\n            registry.local:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n                # The auth configuration for this registry.\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nmirrors\tmap[string]RegistryMirrorConfig\t\nSpecifies mirror configuration for each registry host namespace.\nShow example(s)\n\t\nconfig\tmap[string]RegistryConfig\t\nSpecifies TLS & auth configuration for HTTPS image registries.\nShow example(s)\n\t\nmirrors.*\n\nRegistryMirrorConfig represents mirror configuration for a registry.\n\nCopy\nmachine:\n\n    registries:\n\n        mirrors:\n\n            ghcr.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.insecure\n\n                    - https://ghcr.io/v2/\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nList of endpoints (URLs) for registry mirrors to use.\n\t\noverridePath\tbool\t\nUse the exact path specified for the endpoint (don’t append /v2/).\n\t\nconfig.*\n\nRegistryConfig specifies auth & TLS config per registry.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            registry.insecure:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n\n\n                # # The auth configuration for this registry.\n\n                # auth:\n\n                #     username: username # Optional registry authentication.\n\n                #     password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\ntls\tRegistryTLSConfig\tThe TLS configuration for the registry.\nShow example(s)\n\t\nauth\tRegistryAuthConfig\t\nThe auth configuration for this registry.\nShow example(s)\n\t\ntls\n\nRegistryTLSConfig specifies TLS config for HTTPS registries.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nField\tType\tDescription\tValue(s)\nclientIdentity\tPEMEncodedCertificateAndKey\t\nEnable mutual TLS authentication with the registry.\nShow example(s)\n\t\nca\tBase64Bytes\t\nCA registry certificate to add the list of trusted certificates.\n\t\ninsecureSkipVerify\tbool\tSkip TLS server certificate verification (not recommended).\t\nauth\n\nRegistryAuthConfig specifies authentication configuration for a registry.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nusername\tstring\t\nOptional registry authentication.\n\t\npassword\tstring\t\nOptional registry authentication.\n\t\nauth\tstring\t\nOptional registry authentication.\n\t\nidentityToken\tstring\t\nOptional registry authentication.\n\t\nsystemDiskEncryption\n\nSystemDiskEncryptionConfig specifies system disk partitions encryption settings.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        # Ephemeral partition encryption.\n\n        ephemeral:\n\n            provider: luks2 # Encryption provider to use for the encryption.\n\n            # Defines the encryption keys generation and storage method.\n\n            keys:\n\n                - # Deterministically generated key from the node UUID and PartitionLabel.\n\n                  nodeID: {}\n\n                  slot: 0 # Key slot number for LUKS2 encryption.\n\n\n\n                  # # KMS managed encryption key.\n\n                  # kms:\n\n                  #     endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\n\n\n\n            # # Cipher kind to use for the encryption. Depends on the encryption provider.\n\n            # cipher: aes-xts-plain64\n\n\n\n            # # Defines the encryption sector size.\n\n            # blockSize: 4096\n\n\n\n            # # Additional --perf parameters for the LUKS2 encryption.\n\n            # options:\n\n            #     - no_read_workqueue\n\n            #     - no_write_workqueue\nField\tType\tDescription\tValue(s)\nstate\tEncryptionConfig\tState partition encryption.\t\nephemeral\tEncryptionConfig\tEphemeral partition encryption.\t\nstate\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        state:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nephemeral\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        ephemeral:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nfeatures\n\nFeaturesConfig describes individual Talos features that can be switched on or off.\n\nCopy\nmachine:\n\n    features:\n\n        rbac: true # Enable role-based access control (RBAC).\n\n\n\n        # # Configure Talos API access from Kubernetes pods.\n\n        # kubernetesTalosAPIAccess:\n\n        #     enabled: true # Enable Talos API access from Kubernetes pods.\n\n        #     # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n        #     allowedRoles:\n\n        #         - os:reader\n\n        #     # The list of Kubernetes namespaces Talos API access is available from.\n\n        #     allowedKubernetesNamespaces:\n\n        #         - kube-system\nField\tType\tDescription\tValue(s)\nrbac\tbool\tEnable role-based access control (RBAC).\t\nstableHostname\tbool\tEnable stable default hostname.\t\nkubernetesTalosAPIAccess\tKubernetesTalosAPIAccessConfig\t\nConfigure Talos API access from Kubernetes pods.\nShow example(s)\n\t\napidCheckExtKeyUsage\tbool\tEnable checks for extended key usage of client certificates in apid.\t\ndiskQuotaSupport\tbool\t\nEnable XFS project quota support for EPHEMERAL partition and user disks.\n\t\nkubePrism\tKubePrism\t\nKubePrism - local proxy/load balancer on defined port that will distribute\n\t\nkubernetesTalosAPIAccess\n\nKubernetesTalosAPIAccessConfig describes the configuration for the Talos API access from Kubernetes pods.\n\nCopy\nmachine:\n\n    features:\n\n        kubernetesTalosAPIAccess:\n\n            enabled: true # Enable Talos API access from Kubernetes pods.\n\n            # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n            allowedRoles:\n\n                - os:reader\n\n            # The list of Kubernetes namespaces Talos API access is available from.\n\n            allowedKubernetesNamespaces:\n\n                - kube-system\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable Talos API access from Kubernetes pods.\t\nallowedRoles\t[]string\t\nThe list of Talos API roles which can be granted for access from Kubernetes pods.\n\t\nallowedKubernetesNamespaces\t[]string\tThe list of Kubernetes namespaces Talos API access is available from.\t\nkubePrism\n\nKubePrism describes the configuration for the KubePrism load balancer.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable KubePrism support - will start local load balacing proxy.\t\nport\tint\tKubePrism port.\t\nudev\n\nUdevConfig describes how the udev system should be configured.\n\nCopy\nmachine:\n\n    udev:\n\n        # List of udev rules to apply to the udev system\n\n        rules:\n\n            - SUBSYSTEM==\"drm\", KERNEL==\"renderD*\", GROUP=\"44\", MODE=\"0660\"\nField\tType\tDescription\tValue(s)\nrules\t[]string\tList of udev rules to apply to the udev system\t\nlogging\n\nLoggingConfig struct configures Talos logging.\n\nCopy\nmachine:\n\n    logging:\n\n        # Logging destination.\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345 # Where to send logs. Supported protocols are \"tcp\" and \"udp\".\n\n              format: json_lines # Logs format.\nField\tType\tDescription\tValue(s)\ndestinations\t[]LoggingDestination\tLogging destination.\t\ndestinations[]\n\nLoggingDestination struct configures Talos logging destination.\n\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\tWhere to send logs. Supported protocols are “tcp” and “udp”.\nShow example(s)\n\t\nformat\tstring\tLogs format.\tjson_lines\n\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://1.2.3.4:6443\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://cluster1.internal:6443\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: udp://127.0.0.1:12345\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nkernel\n\nKernelConfig struct configures Talos Linux kernel.\n\nCopy\nmachine:\n\n    kernel:\n\n        # Kernel modules to load.\n\n        modules:\n\n            - name: brtfs # Module name.\nField\tType\tDescription\tValue(s)\nmodules\t[]KernelModuleConfig\tKernel modules to load.\t\nmodules[]\n\nKernelModuleConfig struct configures Linux kernel modules to load.\n\nField\tType\tDescription\tValue(s)\nname\tstring\tModule name.\t\nparameters\t[]string\tModule parameters, changes applied after reboot.\t\nseccompProfiles[]\n\nMachineSeccompProfile defines seccomp profiles for the machine.\n\nCopy\nmachine:\n\n    seccompProfiles:\n\n        - name: audit.json # The `name` field is used to provide the file name of the seccomp profile.\n\n          # The `value` field is used to provide the seccomp profile.\n\n          value:\n\n            defaultAction: SCMP_ACT_LOG\nField\tType\tDescription\tValue(s)\nname\tstring\tThe name field is used to provide the file name of the seccomp profile.\t\nvalue\tUnstructured\tThe value field is used to provide the seccomp profile.\t\ncluster\n\nClusterConfig represents the cluster-wide config values.\n\nCopy\ncluster:\n\n    # ControlPlaneConfig represents the control plane configuration options.\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\n\n    clusterName: talos.local\n\n    # ClusterNetworkConfig represents kube networking configuration options.\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\nid\tstring\tGlobally unique identifier for this cluster (base64 encoded random 32 bytes).\t\nsecret\tstring\t\nShared secret of cluster (base64 encoded random 32 bytes).\n\t\ncontrolPlane\tControlPlaneConfig\tProvides control plane specific configuration options.\nShow example(s)\n\t\nclusterName\tstring\tConfigures the cluster’s name.\t\nnetwork\tClusterNetworkConfig\tProvides cluster specific network configuration options.\nShow example(s)\n\t\ntoken\tstring\tThe bootstrap token used to join the cluster.\nShow example(s)\n\t\naescbcEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nsecretboxEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\tThe base64 encoded root certificate authority used by Kubernetes.\nShow example(s)\n\t\naggregatorCA\tPEMEncodedCertificateAndKey\t\nThe base64 encoded aggregator certificate authority used by Kubernetes for front-proxy certificate generation.\nShow example(s)\n\t\nserviceAccount\tPEMEncodedKey\tThe base64 encoded private key for service account token generation.\nShow example(s)\n\t\napiServer\tAPIServerConfig\tAPI server specific configuration options.\nShow example(s)\n\t\ncontrollerManager\tControllerManagerConfig\tController manager server specific configuration options.\nShow example(s)\n\t\nproxy\tProxyConfig\tKube-proxy server-specific configuration options\nShow example(s)\n\t\nscheduler\tSchedulerConfig\tScheduler server specific configuration options.\nShow example(s)\n\t\ndiscovery\tClusterDiscoveryConfig\tConfigures cluster member discovery.\nShow example(s)\n\t\netcd\tEtcdConfig\tEtcd specific configuration options.\nShow example(s)\n\t\ncoreDNS\tCoreDNS\tCore DNS specific configuration options.\nShow example(s)\n\t\nexternalCloudProvider\tExternalCloudProviderConfig\tExternal cloud provider configuration.\nShow example(s)\n\t\nextraManifests\t[]string\t\nA list of urls that point to additional manifests.\nShow example(s)\n\t\nextraManifestHeaders\tmap[string]string\tA map of key value pairs that will be added while fetching the extraManifests.\nShow example(s)\n\t\ninlineManifests\t[]ClusterInlineManifest\t\nA list of inline Kubernetes manifests.\nShow example(s)\n\t\nadminKubeconfig\tAdminKubeconfigConfig\t\nSettings for admin kubeconfig generation.\nShow example(s)\n\t\nallowSchedulingOnControlPlanes\tbool\tAllows running workload on control-plane nodes.\nShow example(s)\n\ttrue\nyes\nfalse\nno\n\ncontrolPlane\n\nControlPlaneConfig represents the control plane configuration options.\n\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\t\nEndpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\nShow example(s)\n\t\nlocalAPIServerPort\tint\t\nThe port that the API server listens on internally.\n\t\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4:6443\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://cluster1.internal:6443\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: udp://127.0.0.1:12345\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nnetwork\n\nClusterNetworkConfig represents kube networking configuration options.\n\nCopy\ncluster:\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\ncni\tCNIConfig\t\nThe CNI used.\nShow example(s)\n\t\ndnsDomain\tstring\t\nThe domain used by Kubernetes DNS.\nShow example(s)\n\t\npodSubnets\t[]string\tThe pod subnet CIDR.\nShow example(s)\n\t\nserviceSubnets\t[]string\tThe service subnet CIDR.\nShow example(s)\n\t\ncni\n\nCNIConfig represents the CNI configuration options.\n\nCopy\ncluster:\n\n    network:\n\n        cni:\n\n            name: custom # Name of CNI to use.\n\n            # URLs containing manifests to apply for the CNI.\n\n            urls:\n\n                - https://docs.projectcalico.org/archive/v3.20/manifests/canal.yaml\nField\tType\tDescription\tValue(s)\nname\tstring\tName of CNI to use.\tflannel\ncustom\nnone\n\nurls\t[]string\t\nURLs containing manifests to apply for the CNI.\n\t\nflannel\tFlannelCNIConfig\t\ndescription:\n\tFlannel configuration options.\n\nflannel\n\nFlannelCNIConfig represents the Flannel CNI configuration options.\n\nField\tType\tDescription\tValue(s)\nextraArgs\t[]string\tExtra arguments for ‘flanneld’.\nShow example(s)\n\t\napiServer\n\nAPIServerConfig represents the kube apiserver configuration options.\n\nCopy\ncluster:\n\n    apiServer:\n\n        image: registry.k8s.io/kube-apiserver:v1.29.0 # The container image used in the API server manifest.\n\n        # Extra arguments to supply to the API server.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n            http2-max-streams-per-connection: \"32\"\n\n        # Extra certificate subject alternative names for the API server's certificate.\n\n        certSANs:\n\n            - 1.2.3.4\n\n            - 4.5.6.7\n\n\n\n        # # Configure the API server admission plugins.\n\n        # admissionControl:\n\n        #     - name: PodSecurity # Name is the name of the admission controller.\n\n        #       # Configuration is an embedded configuration object to be used as the plugin's\n\n        #       configuration:\n\n        #         apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n        #         defaults:\n\n        #             audit: restricted\n\n        #             audit-version: latest\n\n        #             enforce: baseline\n\n        #             enforce-version: latest\n\n        #             warn: restricted\n\n        #             warn-version: latest\n\n        #         exemptions:\n\n        #             namespaces:\n\n        #                 - kube-system\n\n        #             runtimeClasses: []\n\n        #             usernames: []\n\n        #         kind: PodSecurityConfiguration\n\n\n\n        # # Configure the API server audit policy.\n\n        # auditPolicy:\n\n        #     apiVersion: audit.k8s.io/v1\n\n        #     kind: Policy\n\n        #     rules:\n\n        #         - level: Metadata\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the API server manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the API server.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the API server static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\ncertSANs\t[]string\tExtra certificate subject alternative names for the API server’s certificate.\t\ndisablePodSecurityPolicy\tbool\tDisable PodSecurityPolicy in the API server and default manifests.\t\nadmissionControl\t[]AdmissionPluginConfig\tConfigure the API server admission plugins.\nShow example(s)\n\t\nauditPolicy\tUnstructured\tConfigure the API server audit policy.\nShow example(s)\n\t\nresources\tResourcesConfig\tConfigure the API server resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nadmissionControl[]\n\nAdmissionPluginConfig represents the API server admission plugin configuration.\n\nCopy\ncluster:\n\n    apiServer:\n\n        admissionControl:\n\n            - name: PodSecurity # Name is the name of the admission controller.\n\n              # Configuration is an embedded configuration object to be used as the plugin's\n\n              configuration:\n\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n                defaults:\n\n                    audit: restricted\n\n                    audit-version: latest\n\n                    enforce: baseline\n\n                    enforce-version: latest\n\n                    warn: restricted\n\n                    warn-version: latest\n\n                exemptions:\n\n                    namespaces:\n\n                        - kube-system\n\n                    runtimeClasses: []\n\n                    usernames: []\n\n                kind: PodSecurityConfiguration\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName is the name of the admission controller.\n\t\nconfiguration\tUnstructured\t\nConfiguration is an embedded configuration object to be used as the plugin’s\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ncontrollerManager\n\nControllerManagerConfig represents the kube controller manager configuration options.\n\nCopy\ncluster:\n\n    controllerManager:\n\n        image: registry.k8s.io/kube-controller-manager:v1.29.0 # The container image used in the controller manager manifest.\n\n        # Extra arguments to supply to the controller manager.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the controller manager manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the controller manager.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the controller manager static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the controller manager resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\nproxy\n\nProxyConfig represents the kube proxy configuration options.\n\nCopy\ncluster:\n\n    proxy:\n\n        image: registry.k8s.io/kube-proxy:v1.29.0 # The container image used in the kube-proxy manifest.\n\n        mode: ipvs # proxy mode of kube-proxy.\n\n        # Extra arguments to supply to kube-proxy.\n\n        extraArgs:\n\n            proxy-mode: iptables\n\n\n\n        # # Disable kube-proxy deployment on cluster bootstrap.\n\n        # disabled: false\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-proxy deployment on cluster bootstrap.\nShow example(s)\n\t\nimage\tstring\tThe container image used in the kube-proxy manifest.\nShow example(s)\n\t\nmode\tstring\t\nproxy mode of kube-proxy.\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to kube-proxy.\t\nscheduler\n\nSchedulerConfig represents the kube scheduler configuration options.\n\nCopy\ncluster:\n\n    scheduler:\n\n        image: registry.k8s.io/kube-scheduler:v1.29.0 # The container image used in the scheduler manifest.\n\n        # Extra arguments to supply to the scheduler.\n\n        extraArgs:\n\n            feature-gates: AllBeta=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the scheduler manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the scheduler.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the scheduler static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the scheduler resources.\t\nconfig\tUnstructured\tSpecify custom kube-scheduler configuration.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ndiscovery\n\nClusterDiscoveryConfig struct configures cluster membership discovery.\n\nCopy\ncluster:\n\n    discovery:\n\n        enabled: true # Enable the cluster membership discovery feature.\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            # Kubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\n            kubernetes: {}\n\n            # Service registry is using an external service to push and pull information about cluster members.\n\n            service:\n\n                endpoint: https://discovery.talos.dev/ # External service endpoint.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the cluster membership discovery feature.\n\t\nregistries\tDiscoveryRegistriesConfig\tConfigure registries used for cluster member discovery.\t\nregistries\n\nDiscoveryRegistriesConfig struct configures cluster membership discovery.\n\nField\tType\tDescription\tValue(s)\nkubernetes\tRegistryKubernetesConfig\t\nKubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\t\nservice\tRegistryServiceConfig\tService registry is using an external service to push and pull information about cluster members.\t\nkubernetes\n\nRegistryKubernetesConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable Kubernetes discovery registry.\t\nservice\n\nRegistryServiceConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable external service discovery registry.\t\nendpoint\tstring\tExternal service endpoint.\nShow example(s)\n\t\netcd\n\nEtcdConfig represents the etcd configuration options.\n\nCopy\ncluster:\n\n    etcd:\n\n        image: gcr.io/etcd-development/etcd:v3.5.11 # The container image used to create the etcd service.\n\n        # The `ca` is the root certificate authority of the PKI.\n\n        ca:\n\n            crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n            key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n        # Extra arguments to supply to etcd.\n\n        extraArgs:\n\n            election-timeout: \"5000\"\n\n\n\n        # # The `advertisedSubnets` field configures the networks to pick etcd advertised IP from.\n\n        # advertisedSubnets:\n\n        #     - 10.0.0.0/8\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used to create the etcd service.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe ca is the root certificate authority of the PKI.\nShow example(s)\n\t\nextraArgs\tmap[string]string\t\nExtra arguments to supply to etcd.\n\t\nadvertisedSubnets\t[]string\t\nThe advertisedSubnets field configures the networks to pick etcd advertised IP from.\nShow example(s)\n\t\nlistenSubnets\t[]string\t\nThe listenSubnets field configures the networks for the etcd to listen for peer and client connections.\n\t\ncoreDNS\n\nCoreDNS represents the CoreDNS config values.\n\nCopy\ncluster:\n\n    coreDNS:\n\n        image: registry.k8s.io/coredns/coredns:v1.11.1 # The `image` field is an override to the default coredns image.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable coredns deployment on cluster bootstrap.\t\nimage\tstring\tThe image field is an override to the default coredns image.\t\nexternalCloudProvider\n\nExternalCloudProviderConfig contains external cloud provider configuration.\n\nCopy\ncluster:\n\n    externalCloudProvider:\n\n        enabled: true # Enable external cloud provider.\n\n        # A list of urls that point to additional manifests for an external cloud provider.\n\n        manifests:\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/rbac.yaml\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/aws-cloud-controller-manager-daemonset.yaml\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable external cloud provider.\ttrue\nyes\nfalse\nno\n\nmanifests\t[]string\t\nA list of urls that point to additional manifests for an external cloud provider.\nShow example(s)\n\t\ninlineManifests[]\n\nClusterInlineManifest struct describes inline bootstrap manifests for the user.\n\nCopy\ncluster:\n\n    inlineManifests:\n\n        - name: namespace-ci # Name of the manifest.\n\n          contents: |- # Manifest contents as a string.\n\n            apiVersion: v1\n\n            kind: Namespace\n\n            metadata:\n\n            \tname: ci\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName of the manifest.\nShow example(s)\n\t\ncontents\tstring\tManifest contents as a string.\nShow example(s)\n\t\nadminKubeconfig\n\nAdminKubeconfigConfig contains admin kubeconfig settings.\n\nCopy\ncluster:\n\n    adminKubeconfig:\n\n        certLifetime: 1h0m0s # Admin kubeconfig certificate lifetime (default is 1 year).\nField\tType\tDescription\tValue(s)\ncertLifetime\tDuration\t\nAdmin kubeconfig certificate lifetime (default is 1 year).\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/network/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nnetwork\nPackage network provides network machine configuration documents.\n1: NetworkDefaultActionConfig\n2: NetworkRuleConfig\n1 - NetworkDefaultActionConfig\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: accept # Default action for all not explicitly configured ingress traffic: accept or block.\nField\tType\tDescription\tValue(s)\ningress\tDefaultAction\tDefault action for all not explicitly configured ingress traffic: accept or block.\taccept\nblock\n\n2 - NetworkRuleConfig\nNetworkRuleConfig is a network firewall rule config document.\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: ingress-apid # Name of the config document.\n\n# Port selector defines which ports and protocols on the host are affected by the rule.\n\nportSelector:\n\n    # Ports defines a list of port ranges or single ports.\n\n    ports:\n\n        - 50000\n\n    protocol: tcp # Protocol defines traffic protocol (e.g. TCP or UDP).\n\n# Ingress defines which source subnets are allowed to access the host ports/protocols defined by the `portSelector`.\n\ningress:\n\n    - subnet: 192.168.0.0/16 # Subnet defines a source subnet.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nportSelector\tRulePortSelector\tPort selector defines which ports and protocols on the host are affected by the rule.\t\ningress\t[]IngressRule\tIngress defines which source subnets are allowed to access the host ports/protocols defined by the portSelector.\t\nportSelector\n\nRulePortSelector is a port selector for the network rule.\n\nField\tType\tDescription\tValue(s)\nports\tPortRanges\t\nPorts defines a list of port ranges or single ports.\nShow example(s)\n\t\nprotocol\tProtocol\tProtocol defines traffic protocol (e.g. TCP or UDP).\ttcp\nudp\nicmp\nicmpv6\n\ningress[]\n\nIngressRule is a ingress rule.\n\nField\tType\tDescription\tValue(s)\nsubnet\tPrefix\tSubnet defines a source subnet.\nShow example(s)\n\t\nexcept\tPrefix\tExcept defines a source subnet to exclude from the rule, it gets excluded from the subnet.\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "404 Page not found | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/metal-network-configuration/%7B%7B%20relref%20%22../talos-guides/install/boot-assets%22%20%7D%7D",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\nNot found\n\nOops! This page doesn't exist. Try going back to our home page.\n\nYou can learn how to make a 404 page like this in Custom 404 Pages.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "404 Page not found | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/metal-network-configuration/%7B%7B%20relref%20%22../learn-more/networking-resources%22%20%7D%7D",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\nNot found\n\nOops! This page doesn't exist. Try going back to our home page.\n\nYou can learn how to make a 404 page like this in Custom 404 Pages.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "404 Page not found | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/metal-network-configuration/%7B%7B%20relref%20%22../talos-guides/interactive-dashboard%22%20%7D%7D",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\nNot found\n\nOops! This page doesn't exist. Try going back to our home page.\n\nYou can learn how to make a 404 page like this in Custom 404 Pages.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Kubernetes Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nKubernetes Guides\nManagement of a Kubernetes Cluster hosted by Talos Linux\n1: Configuration\n1.1: Ceph Storage cluster with Rook\n1.2: Deploying Metrics Server\n1.3: iSCSI Storage with Synology CSI\n1.4: KubePrism\n1.5: Local Storage\n1.6: Pod Security\n1.7: Replicated Local Storage\n1.8: Seccomp Profiles\n1.9: Storage\n2: Network\n2.1: Deploying Cilium CNI\n3: Upgrading Kubernetes\n1 - Configuration\nHow to configure components of the Kubernetes cluster itself.\n1.1 - Ceph Storage cluster with Rook\nGuide on how to create a simple Ceph storage cluster with Rook for Kubernetes\nPreparation\n\nTalos Linux reserves an entire disk for the OS installation, so machines with multiple available disks are needed for a reliable Ceph cluster with Rook and Talos Linux. Rook requires that the block devices or partitions used by Ceph have no partitions or formatted filesystems before use. Rook also requires a minimum Kubernetes version of v1.16 and Helm v3.0 for installation of charts. It is highly recommended that the Rook Ceph overview is read and understood before deploying a Ceph cluster with Rook.\n\nInstallation\n\nCreating a Ceph cluster with Rook requires two steps; first the Rook Operator needs to be installed which can be done with a Helm Chart. The example below installs the Rook Operator into the rook-ceph namespace, which is the default for a Ceph cluster with Rook.\n\n$ helm repo add rook-release https://charts.rook.io/release\n\n\"rook-release\" has been added to your repositories\n\n\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph\n\nW0327 17:52:44.277830   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nW0327 17:52:44.612243   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nNAME: rook-ceph\n\nLAST DEPLOYED: Sun Mar 27 17:52:42 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Rook Operator has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get pods -l \"app=rook-ceph-operator\"\n\n\n\nVisit https://rook.io/docs/rook/latest for instructions on how to create and configure Rook clusters\n\n\n\nImportant Notes:\n\n- You must customize the 'CephCluster' resource in the sample manifests for your cluster.\n\n- Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the namespace.\n\n- The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.\n\n- The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.\n\n- Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).\n\n\nOnce that is complete, the Ceph cluster can be installed with the official Helm Chart. The Chart can be installed with default values, which will attempt to use all nodes in the Kubernetes cluster, and all unused disks on each node for Ceph storage, and make available block storage, object storage, as well as a shared filesystem. Generally more specific node/device/cluster configuration is used, and the Rook documentation explains all the available options in detail. For this example the defaults will be adequate.\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster\n\nNAME: rook-ceph-cluster\n\nLAST DEPLOYED: Sun Mar 27 18:12:46 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Ceph Cluster has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get cephcluster\n\n\n\nVisit https://rook.github.io/docs/rook/latest/ceph-cluster-crd.html for more information about the Ceph CRD.\n\n\n\nImportant Notes:\n\n- You can only deploy a single cluster per namespace\n\n- If you wish to delete this cluster and start fresh, you will also have to wipe the OSD disks using `sfdisk`\n\n\nNow the Ceph cluster configuration has been created, the Rook operator needs time to install the Ceph cluster and bring all the components online. The progression of the Ceph cluster state can be followed with the following command.\n\n$ watch kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nEvery 2.0s: kubectl --namespace rook-ceph get cephcluster rook-ceph\n\n\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                 HEALTH   EXTERNAL\n\nrook-ceph   /var/lib/rook     3          57s   Progressing   Configuring Ceph Mons\n\n\nDepending on the size of the Ceph cluster and the availability of resources the Ceph cluster should become available, and with it the storage classes that can be used with Kubernetes Physical Volumes.\n\n$ kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          40m   Ready   Cluster created successfully   HEALTH_OK\n\n\n\n$ kubectl  get storageclass\n\nNAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n\nceph-block (default)   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   77m\n\nceph-bucket            rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  77m\n\nceph-filesystem        rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   77m\n\nTalos Linux Considerations\n\nIt is important to note that a Rook Ceph cluster saves cluster information directly onto the node (by default dataDirHostPath is set to /var/lib/rook). If running only a single mon instance, cluster management is little bit more involved, as any time a Talos Linux node is reconfigured or upgraded, the partition that stores the /var file system is wiped, but the --preserve option of talosctl upgrade will ensure that doesn’t happen.\n\nBy default, Rook configues Ceph to have 3 mon instances, in which case the data stored in dataDirHostPath can be regenerated from the other mon instances. So when performing maintenance on a Talos Linux node with a Rook Ceph cluster (e.g. upgrading the Talos Linux version), it is imperative that care be taken to maintain the health of the Ceph cluster. Before upgrading, you should always check the health status of the Ceph cluster to ensure that it is healthy.\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          98m   Ready   Cluster created successfully   HEALTH_OK\n\n\nIf it is, you can begin the upgrade process for the Talos Linux node, during which time the Ceph cluster will become unhealthy as the node is reconfigured. Before performing any other action on the Talos Linux nodes, the Ceph cluster must return to a healthy status.\n\n$ talosctl upgrade --nodes 172.20.15.5 --image ghcr.io/talos-systems/installer:v0.14.3\n\nNODE          ACK                        STARTED\n\n172.20.15.5   Upgrade request received   2022-03-27 20:29:55.292432887 +0200 CEST m=+10.050399758\n\n\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                   HEALTH        EXTERNAL\n\nrook-ceph   /var/lib/rook     3          99m   Progressing   Configuring Ceph Mgr(s)   HEALTH_WARN\n\n\n\n$ kubectl --namespace rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n\ncephcluster.ceph.rook.io/rook-ceph condition met\n\n\nThe above steps need to be performed for each Talos Linux node undergoing maintenance, one at a time.\n\nCleaning Up\nRook Ceph Cluster Removal\n\nRemoving a Rook Ceph cluster requires a few steps, starting with signalling to Rook that the Ceph cluster is really being destroyed. Then all Persistent Volumes (and Claims) backed by the Ceph cluster must be deleted, followed by the Storage Classes and the Ceph storage types.\n\n$ kubectl --namespace rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\ncephcluster.ceph.rook.io/rook-ceph patched\n\n\n\n$ kubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nstorageclass.storage.k8s.io \"ceph-block\" deleted\n\nstorageclass.storage.k8s.io \"ceph-bucket\" deleted\n\nstorageclass.storage.k8s.io \"ceph-filesystem\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephblockpools ceph-blockpool\n\ncephblockpool.ceph.rook.io \"ceph-blockpool\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephobjectstore ceph-objectstore\n\ncephobjectstore.ceph.rook.io \"ceph-objectstore\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephfilesystem ceph-filesystem\n\ncephfilesystem.ceph.rook.io \"ceph-filesystem\" deleted\n\n\nOnce that is complete, the Ceph cluster itself can be removed, along with the Rook Ceph cluster Helm chart installation.\n\n$ kubectl --namespace rook-ceph delete cephcluster rook-ceph\n\ncephcluster.ceph.rook.io \"rook-ceph\" deleted\n\n\n\n$ helm --namespace rook-ceph uninstall rook-ceph-cluster\n\nrelease \"rook-ceph-cluster\" uninstalled\n\n\nIf needed, the Rook Operator can also be removed along with all the Custom Resource Definitions that it created.\n\n$ helm --namespace rook-ceph uninstall rook-ceph\n\nW0328 12:41:14.998307  147203 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nThese resources were kept due to the resource policy:\n\n[CustomResourceDefinition] cephblockpools.ceph.rook.io\n\n[CustomResourceDefinition] cephbucketnotifications.ceph.rook.io\n\n[CustomResourceDefinition] cephbuckettopics.ceph.rook.io\n\n[CustomResourceDefinition] cephclients.ceph.rook.io\n\n[CustomResourceDefinition] cephclusters.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemmirrors.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystems.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemsubvolumegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephnfses.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectrealms.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstores.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstoreusers.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzonegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzones.ceph.rook.io\n\n[CustomResourceDefinition] cephrbdmirrors.ceph.rook.io\n\n[CustomResourceDefinition] objectbucketclaims.objectbucket.io\n\n[CustomResourceDefinition] objectbuckets.objectbucket.io\n\n\n\nrelease \"rook-ceph\" uninstalled\n\n\n\n$ kubectl delete crds cephblockpools.ceph.rook.io cephbucketnotifications.ceph.rook.io cephbuckettopics.ceph.rook.io \\\n\n                      cephclients.ceph.rook.io cephclusters.ceph.rook.io cephfilesystemmirrors.ceph.rook.io \\\n\n                      cephfilesystems.ceph.rook.io cephfilesystemsubvolumegroups.ceph.rook.io \\\n\n                      cephnfses.ceph.rook.io cephobjectrealms.ceph.rook.io cephobjectstores.ceph.rook.io \\\n\n                      cephobjectstoreusers.ceph.rook.io cephobjectzonegroups.ceph.rook.io cephobjectzones.ceph.rook.io \\\n\n                      cephrbdmirrors.ceph.rook.io objectbucketclaims.objectbucket.io objectbuckets.objectbucket.io\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephblockpools.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbucketnotifications.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbuckettopics.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclients.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclusters.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystems.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemsubvolumegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephnfses.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectrealms.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstores.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstoreusers.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzonegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzones.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephrbdmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbucketclaims.objectbucket.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbuckets.objectbucket.io\" deleted\n\nTalos Linux Rook Metadata Removal\n\nIf the Rook Operator is cleanly removed following the above process, the node metadata and disks should be clean and ready to be re-used. In the case of an unclean cluster removal, there may be still a few instances of metadata stored on the system disk, as well as the partition information on the storage disks. First the node metadata needs to be removed, make sure to update the nodeName with the actual name of a storage node that needs cleaning, and path with the Rook configuration dataDirHostPath set when installing the chart. The following will need to be repeated for each node used in the Rook Ceph cluster.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-clean\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  volumes:\n\n  - name: rook-data-dir\n\n    hostPath:\n\n      path: <dataDirHostPath>\n\n  containers:\n\n  - name: disk-clean\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    volumeMounts:\n\n    - name: rook-data-dir\n\n      mountPath: /node/rook-data\n\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\n\nEOF\n\npod/disk-clean created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\n\npod/disk-clean condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-clean\" deleted\n\n\nLastly, the disks themselves need the partition and filesystem data wiped before they can be reused. Again, the following as to be repeated for each node and disk used in the Rook Ceph cluster, updating nodeName and of= in the command as needed.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-wipe\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  containers:\n\n  - name: disk-wipe\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=<device>\"]\n\nEOF\n\npod/disk-wipe created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\n\npod/disk-wipe condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-wipe\" deleted\n\n1.2 - Deploying Metrics Server\nIn this guide you will learn how to set up metrics-server.\n\nMetrics Server enables use of the Horizontal Pod Autoscaler and Vertical Pod Autoscaler. It does this by gathering metrics data from the kubelets in a cluster. By default, the certificates in use by the kubelets will not be recognized by metrics-server. This can be solved by either configuring metrics-server to do no validation of the TLS certificates, or by modifying the kubelet configuration to rotate its certificates and use ones that will be recognized by metrics-server.\n\nNode Configuration\n\nTo enable kubelet certificate rotation, all nodes should have the following Machine Config snippet:\n\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      rotate-server-certificates: true\n\nInstall During Bootstrap\n\nWe will want to ensure that new certificates for the kubelets are approved automatically. This can easily be done with the Kubelet Serving Certificate Approver, which will automatically approve the Certificate Signing Requests generated by the kubelets.\n\nWe can have Kubelet Serving Certificate Approver and metrics-server installed on the cluster automatically during bootstrap by adding the following snippet to the Cluster Config of the node that will be handling the bootstrap process:\n\ncluster:\n\n  extraManifests:\n\n    - https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\n    - https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nInstall After Bootstrap\n\nIf you choose not to use extraManifests to install Kubelet Serving Certificate Approver and metrics-server during bootstrap, you can install them once the cluster is online using kubectl:\n\nkubectl apply -f https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n1.3 - iSCSI Storage with Synology CSI\nAutomatically provision iSCSI volumes on a Synology NAS with the synology-csi driver.\nBackground\n\nSynology is a company that specializes in Network Attached Storage (NAS) devices. They provide a number of features within a simple web OS, including an LDAP server, Docker support, and (perhaps most relevant to this guide) function as an iSCSI host. The focus of this guide is to allow a Kubernetes cluster running on Talos to provision Kubernetes storage (both dynamic or static) on a Synology NAS using a direct integration, rather than relying on an intermediary layer like Rook/Ceph or Maystor.\n\nThis guide assumes a very basic familiarity with iSCSI terminology (LUN, iSCSI target, etc.).\n\nPrerequisites\nSynology NAS running DSM 7.0 or above\nProvisioned Talos cluster running Kubernetes v1.20 or above\n(Optional) Both Volume Snapshot CRDs and the common snapshot controller must be installed in your Kubernetes cluster if you want to use the Snapshot feature\nSetting up the Synology user account\n\nThe synology-csi controller interacts with your NAS in two different ways: via the API and via the iSCSI protocol. Actions such as creating a new iSCSI target or deleting an old one are accomplished via the Synology API, and require administrator access. On the other hand, mounting the disk to a pod and reading from / writing to it will utilize iSCSI. Because you can only authenticate with one account per DSM configured, that account needs to have admin privileges. In order to minimize access in the case of these credentials being compromised, you should configure the account with the lease possible amount of access – explicitly specify “No Access” on all volumes when configuring the user permissions.\n\nSetting up the Synology CSI\n\nNote: this guide is paraphrased from the Synology CSI readme. Please consult the readme for more in-depth instructions and explanations.\n\nClone the git repository.\n\ngit clone https://github.com/zebernst/synology-csi-talos.git\n\n\nWhile Synology provides some automated scripts to deploy the CSI driver, they can be finicky especially when making changes to the source code. We will be configuring and deploying things manually in this guide.\n\nThe relevant files we will be touching are in the following locations:\n\n.\n\n├── Dockerfile\n\n├── Makefile\n\n├── config\n\n│   └── client-info-template.yml\n\n└── deploy\n\n    └── kubernetes\n\n        └── v1.20\n\n            ├── controller.yml\n\n            ├── csi-driver.yml\n\n            ├── namespace.yml\n\n            ├── node.yml\n\n            ├── snapshotter\n\n            │   ├── snapshotter.yaml\n\n            │   └── volume-snapshot-class.yml\n\n            └── storage-class.yml\n\nConfigure connection info\n\nUse config/client-info-template.yml as an example to configure the connection information for DSM. You can specify one or more storage systems on which the CSI volumes will be created. See below for an example:\n\n---\n\nclients:\n\n- host: 192.168.1.1   # ipv4 address or domain of the DSM\n\n  port: 5000          # port for connecting to the DSM\n\n  https: false        # set this true to use https. you need to specify the port to DSM HTTPS port as well\n\n  username: username  # username\n\n  password: password  # password\n\n\nCreate a Kubernetes secret using the client information config file.\n\nkubectl create secret -n synology-csi generic client-info-secret --from-file=config/client-info.yml\n\n\nNote that if you rename the secret to something other than client-info-secret, make sure you update the corresponding references in the deployment manifests as well.\n\nBuild the Talos-compatible image\n\nModify the Makefile so that the image is built and tagged under your GitHub Container Registry username:\n\nREGISTRY_NAME=ghcr.io/<username>\n\n\nWhen you run make docker-build or make docker-build-multiarch, it will push the resulting image to ghcr.io/<username>/synology-csi:v1.1.0. Ensure that you find and change any reference to synology/synology-csi:v1.1.0 to point to your newly-pushed image within the deployment manifests.\n\nConfigure the CSI driver\n\nBy default, the deployment manifests include one storage class and one volume snapshot class. See below for examples:\n\n---\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\n  name: syno-storage\n\nprovisioner: csi.san.synology.com\n\nparameters:\n\n  fsType: 'ext4'\n\n  dsm: '192.168.1.1'\n\n  location: '/volume1'\n\nreclaimPolicy: Retain\n\nallowVolumeExpansion: true\n\n---\n\napiVersion: snapshot.storage.k8s.io/v1\n\nkind: VolumeSnapshotClass\n\nmetadata:\n\n  name: syno-snapshot\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\ndriver: csi.san.synology.com\n\ndeletionPolicy: Delete\n\nparameters:\n\n  description: 'Kubernetes CSI'\n\n\nIt can be useful to configure multiple different StorageClasses. For example, a popular strategy is to create two nearly identical StorageClasses, with one configured with reclaimPolicy: Retain and the other with reclaimPolicy: Delete. Alternately, a workload may require a specific filesystem, such as ext4. If a Synology NAS is going to be the most common way to configure storage on your cluster, it can be convenient to add the storageclass.kubernetes.io/is-default-class: \"true\" annotation to one of your StorageClasses.\n\nThe following table details the configurable parameters for the Synology StorageClass.\n\nName\tType\tDescription\tDefault\tSupported protocols\ndsm\tstring\tThe IPv4 address of your DSM, which must be included in the client-info.yml for the CSI driver to log in to DSM\t-\tiSCSI, SMB\nlocation\tstring\tThe location (/volume1, /volume2, …) on DSM where the LUN for PersistentVolume will be created\t-\tiSCSI, SMB\nfsType\tstring\tThe formatting file system of the PersistentVolumes when you mount them on the pods. This parameter only works with iSCSI. For SMB, the fsType is always ‘cifs‘.\text4\tiSCSI\nprotocol\tstring\tThe backing storage protocol. Enter ‘iscsi’ to create LUNs or ‘smb‘ to create shared folders on DSM.\tiscsi\tiSCSI, SMB\ncsi.storage.k8s.io/node-stage-secret-name\tstring\tThe name of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\ncsi.storage.k8s.io/node-stage-secret-namespace\tstring\tThe namespace of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\n\nThe VolumeSnapshotClass can be similarly configured with the following parameters:\n\nName\tType\tDescription\tDefault\tSupported protocols\ndescription\tstring\tThe description of the snapshot on DSM\t-\tiSCSI\nis_locked\tstring\tWhether you want to lock the snapshot on DSM\tfalse\tiSCSI, SMB\nApply YAML manifests\n\nOnce you have created the desired StorageClass(es) and VolumeSnapshotClass(es), the final step is to apply the Kubernetes manifests against the cluster. The easiest way to apply them all at once is to create a kustomization.yaml file in the same directory as the manifests and use Kustomize to apply:\n\nkubectl apply -k path/to/manifest/directory\n\n\nAlternately, you can apply each manifest one-by-one:\n\nkubectl apply -f <file>\n\nRun performance tests\n\nIn order to test the provisioning, mounting, and performance of using a Synology NAS as Kubernetes persistent storage, use the following command:\n\nkubectl apply -f speedtest.yaml\n\n\nContent of speedtest.yaml (source)\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: test-claim\n\nspec:\n\n#  storageClassName: syno-storage\n\n  accessModes:\n\n  - ReadWriteMany\n\n  resources:\n\n    requests:\n\n      storage: 5G\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: read\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: read\n\n      labels:\n\n        app: speedtest\n\n        job: read\n\n    spec:\n\n      containers:\n\n      - name: read\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/mnt/pv/test.img\",\"of=/dev/null\",\"bs=8k\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: write\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: write\n\n      labels:\n\n        app: speedtest\n\n        job: write\n\n    spec:\n\n      containers:\n\n      - name: write\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/dev/zero\",\"of=/mnt/pv/test.img\",\"bs=1G\",\"count=1\",\"oflag=dsync\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n\nIf these two jobs complete successfully, use the following commands to get the results of the speed tests:\n\n# Pod logs for read test:\n\nkubectl logs -l app=speedtest,job=read\n\n\n\n# Pod logs for write test:\n\nkubectl logs -l app=speedtest,job=write\n\n\nWhen you’re satisfied with the results of the test, delete the artifacts created from the speedtest:\n\nkubectl delete -f speedtest.yaml\n\n1.4 - KubePrism\nEnabling in-cluster highly-available controlplane endpoint.\n\nKubernetes pods running in CNI mode can use the kubernetes.default.svc service endpoint to access the Kubernetes API server, while pods running in host networking mode can only use the external cluster endpoint to access the Kubernetes API server.\n\nKubernetes controlplane components run in host networking mode, and it is critical for them to be able to access the Kubernetes API server, same as CNI components (when CNI requires access to Kubernetes API).\n\nThe external cluster endpoint might be unavailable due to misconfiguration or network issues, or it might have higher latency than the internal endpoint. A failure to access the Kubernetes API server might cause a series of issues in the cluster: pods are not scheduled, service IPs stop working, etc.\n\nKubePrism feature solves this problem by enabling in-cluster highly-available controlplane endpoint on every node in the cluster.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nEnabling KubePrism\n\nAs of Talos 1.6, KubePrism is enabled by default with port 7445.\n\nNote: the port specified should be available on every node in the cluster.\n\nHow it works\n\nTalos spins up a TCP loadbalancer on every machine on the localhost on the specified port which automatically picks up one of the endpoints:\n\nthe external cluster endpoint as specified in the machine configuration\nfor controlplane machines: https://localhost:<api-server-local-port> (http://localhost:6443 in the default configuration)\nhttps://<controlplane-address>:<api-server-port> for every controlplane machine (based on the information from Cluster Discovery)\n\nKubePrism automatically filters out unhealthy (or unreachable) endpoints, and prefers lower-latency endpoints over higher-latency endpoints.\n\nTalos automatically reconfigures kubelet, kube-scheduler and kube-controller-manager to use the KubePrism endpoint. The kube-proxy manifest is also reconfigured to use the KubePrism endpoint by default, but when enabling KubePrism for a running cluster the manifest should be updated with talosctl upgrade-k8s command.\n\nWhen using CNI components that require access to the Kubernetes API server, the KubePrism endpoint should be passed to the CNI configuration (e.g. Cilium, Calico CNIs).\n\nNotes\n\nAs the list of endpoints for KubePrism includes the external cluster endpoint, KubePrism in the worst case scenario will behave the same as the external cluster endpoint. For controlplane nodes, the KubePrism should pick up the localhost endpoint of the kube-apiserver, minimizing the latency. Worker nodes might use direct address of the controlplane endpoint if the latency is lower than the latency of the external cluster endpoint.\n\nKubePrism listen endpoint is bound to localhost address, so it can’t be used outside the cluster.\n\n1.5 - Local Storage\nUsing local storage for Kubernetes workloads.\n\nUsing local storage for Kubernetes workloads implies that the pod will be bound to the node where the local storage is available. Local storage is not replicated, so in case of a machine failure contents of the local storage will be lost.\n\nNote: when using EPHEMERAL Talos partition (/var), make sure to use --preserve set while performing upgrades, otherwise you risk losing data.\n\nhostPath mounts\n\nThe simplest way to use local storage is to use hostPath mounts. When using hostPath mounts, make sure the root directory of the mount is mounted into the kubelet container:\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/mnt\n\n        type: bind\n\n        source: /var/mnt\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nBoth EPHEMERAL partition and user disks can be used for hostPath mounts.\n\nLocal Path Provisioner\n\nLocal Path Provisioner can be used to dynamically provision local storage. Make sure to update its configuration to use a path under /var, e.g. /var/local-path-provisioner as the root path for the local storage. (In Talos Linux default local path provisioner path /opt/local-path-provisioner is read-only).\n\nFor example, Local Path Provisioner can be installed using kustomize with the following configuration:\n\n# kustomization.yaml\n\napiVersion: kustomize.config.k8s.io/v1beta1\n\nkind: Kustomization\n\nresources:\n\n- github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26\n\npatches:\n\n- patch: |-\n\n    kind: ConfigMap\n\n    apiVersion: v1\n\n    metadata:\n\n      name: local-path-config\n\n      namespace: local-path-storage\n\n    data:\n\n      config.json: |-\n\n        {\n\n                \"nodePathMap\":[\n\n                {\n\n                        \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n\n                        \"paths\":[\"/var/local-path-provisioner\"]\n\n                }\n\n                ]\n\n        }    \n\n- patch: |-\n\n    apiVersion: storage.k8s.io/v1\n\n    kind: StorageClass\n\n    metadata:\n\n      name: local-path\n\n      annotations:\n\n        storageclass.kubernetes.io/is-default-class: \"true\"    \n\n- patch: |-\n\n    apiVersion: v1\n\n    kind: Namespace\n\n    metadata:\n\n      name: local-path-storage\n\n      labels:\n\n        pod-security.kubernetes.io/enforce: privileged    \n\n\nPut kustomization.yaml into a new directory, and run kustomize build | kubectl apply -f - to install Local Path Provisioner to a Talos Linux cluster. There are three patches applied:\n\nchange default /opt/local-path-provisioner path to /var/local-path-provisioner\nmake local-path storage class the default storage class (optional)\nlabel the local-path-storage namespace as privileged to allow privileged pods to be scheduled there\n1.6 - Pod Security\nEnabling Pod Security Admission plugin to configure Pod Security Standards.\n\nKubernetes deprecated Pod Security Policy as of v1.21, and it was removed in v1.25.\n\nPod Security Policy was replaced with Pod Security Admission, which is enabled by default starting with Kubernetes v1.23.\n\nTalos Linux by default enables and configures Pod Security Admission plugin to enforce Pod Security Standards with the baseline profile as the default enforced with the exception of kube-system namespace which enforces privileged profile.\n\nSome applications (e.g. Prometheus node exporter or storage solutions) require more relaxed Pod Security Standards, which can be configured by either updating the Pod Security Admission plugin configuration, or by using the pod-security.kubernetes.io/enforce label on the namespace level:\n\nkubectl label namespace NAMESPACE-NAME pod-security.kubernetes.io/enforce=privileged\n\nConfiguration\n\nTalos provides default Pod Security Admission in the machine configuration:\n\napiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\nkind: PodSecurityConfiguration\n\ndefaults:\n\n    enforce: \"baseline\"\n\n    enforce-version: \"latest\"\n\n    audit: \"restricted\"\n\n    audit-version: \"latest\"\n\n    warn: \"restricted\"\n\n    warn-version: \"latest\"\n\nexemptions:\n\n    usernames: []\n\n    runtimeClasses: []\n\n    namespaces: [kube-system]\n\n\nThis is a cluster-wide configuration for the Pod Security Admission plugin:\n\nby default baseline Pod Security Standard profile is enforced\nmore strict restricted profile is not enforced, but API server warns about found issues\n\nThis default policy can be modified by updating the generated machine configuration before the cluster is created or on the fly by using the talosctl CLI utility.\n\nVerify current admission plugin configuration with:\n\n$ talosctl get admissioncontrolconfigs.kubernetes.talos.dev admission-control -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: AdmissionControlConfigs.kubernetes.talos.dev\n\n    id: admission-control\n\n    version: 1\n\n    owner: config.K8sControlPlaneController\n\n    phase: running\n\n    created: 2022-02-22T20:28:21Z\n\n    updated: 2022-02-22T20:28:21Z\n\nspec:\n\n    config:\n\n        - name: PodSecurity\n\n          configuration:\n\n            apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n            defaults:\n\n                audit: restricted\n\n                audit-version: latest\n\n                enforce: baseline\n\n                enforce-version: latest\n\n                warn: restricted\n\n                warn-version: latest\n\n            exemptions:\n\n                namespaces:\n\n                    - kube-system\n\n                runtimeClasses: []\n\n                usernames: []\n\n            kind: PodSecurityConfiguration\n\nUsage\n\nCreate a deployment that satisfies the baseline policy but gives warnings on restricted policy:\n\n$ kubectl create deployment nginx --image=nginx\n\nWarning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndeployment.apps/nginx created\n\n$ kubectl get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nnginx-85b98978db-j68l8   1/1     Running   0          2m3s\n\n\nCreate a daemonset which fails to meet requirements of the baseline policy:\n\napiVersion: apps/v1\n\nkind: DaemonSet\n\nmetadata:\n\n  labels:\n\n    app: debug-container\n\n  name: debug-container\n\n  namespace: default\n\nspec:\n\n  revisionHistoryLimit: 10\n\n  selector:\n\n    matchLabels:\n\n      app: debug-container\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: debug-container\n\n    spec:\n\n      containers:\n\n      - args:\n\n        - \"360000\"\n\n        command:\n\n        - /bin/sleep\n\n        image: ubuntu:latest\n\n        imagePullPolicy: IfNotPresent\n\n        name: debug-container\n\n        resources: {}\n\n        securityContext:\n\n          privileged: true\n\n        terminationMessagePath: /dev/termination-log\n\n        terminationMessagePolicy: File\n\n      dnsPolicy: ClusterFirstWithHostNet\n\n      hostIPC: true\n\n      hostPID: true\n\n      hostNetwork: true\n\n      restartPolicy: Always\n\n      schedulerName: default-scheduler\n\n      securityContext: {}\n\n      terminationGracePeriodSeconds: 30\n\n  updateStrategy:\n\n    rollingUpdate:\n\n      maxSurge: 0\n\n      maxUnavailable: 1\n\n    type: RollingUpdate\n\n$ kubectl apply -f debug.yaml\n\nWarning: would violate PodSecurity \"restricted:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container \"debug-container\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"debug-container\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"debug-container\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"debug-container\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndaemonset.apps/debug-container created\n\n\nDaemonset debug-container gets created, but no pods are scheduled:\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   0         0         0       0            0           <none>          34s\n\n\nPod Security Admission plugin errors are in the daemonset events:\n\n$ kubectl describe ds debug-container\n\n...\n\n  Warning  FailedCreate  92s                daemonset-controller  Error creating: pods \"debug-container-kwzdj\" is forbidden: violates PodSecurity \"baseline:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true)\n\n\nPod Security Admission configuration can also be overridden on a namespace level:\n\n$ kubectl label ns default pod-security.kubernetes.io/enforce=privileged\n\nnamespace/default labeled\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   2         2         0       2            0           <none>          4s\n\n\nAs enforce policy was updated to the privileged for the default namespace, debug-container is now successfully running.\n\n1.7 - Replicated Local Storage\nUsing local storage with OpenEBS Jiva\n\nIf you want to use replicated storage leveraging disk space from a local disk with Talos Linux installed, OpenEBS Jiva is a great option. This requires installing the iscsi-tools system extension.\n\nSince OpenEBS Jiva is a replicated storage, it’s recommended to have at least three nodes where sufficient local disk space is available. The documentation will follow installing OpenEBS Jiva via the offical Helm chart. Since Talos is different from standard Operating Systems, the OpenEBS components need a little tweaking after the Helm installation. Refer to the OpenEBS Jiva documentation if you need further customization.\n\nNB: Also note that the Talos nodes need to be upgraded with --preserve set while running OpenEBS Jiva, otherwise you risk losing data. Even though it’s possible to recover data from other replicas if the node is wiped during an upgrade, this can require extra operational knowledge to recover, so it’s highly recommended to use --preserve to avoid data loss.\n\nPreparing the nodes\n\nCreate the boot assets which includes the iscsi-tools system extensions (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nCreate a machine config patch with the contents below and save as patch.yaml\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/openebs/local\n\n        type: bind\n\n        source: /var/openebs/local\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThe extension status can be verified by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get extensions\n\n\nAn output similar to below can be observed:\n\nNODE            NAMESPACE   TYPE              ID                                          VERSION   NAME          VERSION\n\n192.168.20.61   runtime     ExtensionStatus   000.ghcr.io-siderolabs-iscsi-tools-v0.1.1   1         iscsi-tools   v0.1.1\n\n\nThe service status can be checked by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> services\n\n\nYou should see that the ext-tgtd and the ext-iscsid services are running.\n\nNODE            SERVICE      STATE     HEALTH   LAST CHANGE     LAST EVENT\n\n192.168.20.51   apid         Running   OK       64h57m15s ago   Health check successful\n\n192.168.20.51   containerd   Running   OK       64h57m23s ago   Health check successful\n\n192.168.20.51   cri          Running   OK       64h57m20s ago   Health check successful\n\n192.168.20.51   etcd         Running   OK       64h55m29s ago   Health check successful\n\n192.168.20.51   ext-iscsid   Running   ?        64h57m19s ago   Started task ext-iscsid (PID 4040) for container ext-iscsid\n\n192.168.20.51   ext-tgtd     Running   ?        64h57m19s ago   Started task ext-tgtd (PID 3999) for container ext-tgtd\n\n192.168.20.51   kubelet      Running   OK       38h14m10s ago   Health check successful\n\n192.168.20.51   machined     Running   ?        64h57m29s ago   Service started as goroutine\n\n192.168.20.51   trustd       Running   OK       64h57m19s ago   Health check successful\n\n192.168.20.51   udevd        Running   OK       64h57m21s ago   Health check successful\n\nInstall OpenEBS Jiva\nhelm repo add openebs-jiva https://openebs.github.io/jiva-operator\n\nhelm repo update\n\nhelm upgrade --install --create-namespace --namespace openebs --version 3.2.0 openebs-jiva openebs-jiva/jiva\n\n\nThis will create a storage class named openebs-jiva-csi-default which can be used for workloads. The storage class named openebs-hostpath is used by jiva to create persistent volumes backed by local storage and then used for replicated storage by the jiva controller.\n\nPatching the Namespace\n\nwhen using the default Pod Security Admissions created by Talos you need the following labels on your namespace:\n\n    pod-security.kubernetes.io/audit: privileged\n\n    pod-security.kubernetes.io/enforce: privileged\n\n    pod-security.kubernetes.io/warn: privileged\n\n\nor via kubectl:\n\nkubectl label ns openebs pod-security.kubernetes.io/audit=privileged pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/warn=privileged\n\nNumber of Replicas\n\nBy Default Jiva uses 3 replicas if your cluster consists of lesser nodes consider setting defaultPolicy.replicas to the number of nodes in your cluster e.g. 2.\n\nPatching the jiva installation\n\nSince Jiva assumes iscisd to be running natively on the host and not as a Talos extension service, we need to modify the CSI node daemonset to enable it to find the PID of the iscsid service. The default config map used by Jiva also needs to be modified so that it can execute iscsiadm commands inside the PID namespace of the iscsid service.\n\nStart by creating a configmap definition named config.yaml as below:\n\napiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\n  labels:\n\n    app.kubernetes.io/managed-by: pulumi\n\n  name: openebs-jiva-csi-iscsiadm\n\n  namespace: openebs\n\ndata:\n\n  iscsiadm: |\n\n    #!/bin/sh\n\n    iscsid_pid=$(pgrep iscsid)\n\n\n\n    nsenter --mount=\"/proc/${iscsid_pid}/ns/mnt\" --net=\"/proc/${iscsid_pid}/ns/net\" -- /usr/local/sbin/iscsiadm \"$@\"    \n\n\nReplace the existing config map with the above config map by running the following command:\n\nkubectl --namespace openebs apply --filename config.yaml\n\n\nNow we need to update the jiva CSI daemonset to run with hostPID: true so it can find the PID of the iscsid service, by running the following command:\n\nkubectl --namespace openebs patch daemonset openebs-jiva-csi-node --type=json --patch '[{\"op\": \"add\", \"path\": \"/spec/template/spec/hostPID\", \"value\": true}]'\n\nTesting a simple workload\n\nIn order to test the Jiva installation, let’s first create a PVC referencing the openebs-jiva-csi-default storage class:\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: example-jiva-csi-pvc\n\nspec:\n\n  storageClassName: openebs-jiva-csi-default\n\n  accessModes:\n\n    - ReadWriteOnce\n\n  resources:\n\n    requests:\n\n      storage: 4Gi\n\n\nand then create a deployment using the above PVC:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: fio\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      name: fio\n\n  replicas: 1\n\n  strategy:\n\n    type: Recreate\n\n    rollingUpdate: null\n\n  template:\n\n    metadata:\n\n      labels:\n\n        name: fio\n\n    spec:\n\n      containers:\n\n      - name: perfrunner\n\n        image: openebs/tests-fio\n\n        command: [\"/bin/bash\"]\n\n        args: [\"-c\", \"while true ;do sleep 50; done\"]\n\n        volumeMounts:\n\n        - mountPath: /datadir\n\n          name: fio-vol\n\n      volumes:\n\n      - name: fio-vol\n\n        persistentVolumeClaim:\n\n          claimName: example-jiva-csi-pvc\n\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete deployment fio\n\nkubectl delete pvc example-jiva-csi-pvc\n\n1.8 - Seccomp Profiles\nUsing custom Seccomp Profiles with Kubernetes workloads.\n\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.\n\nRefer the Kubernetes Seccomp Guide for more details.\n\nIn this guide we are going to configure a custom Seccomp Profile that logs all syscalls made by the workload.\n\nPreparing the nodes\n\nCreate a machine config path with the contents below and save as patch.yaml\n\nmachine:\n\n  seccompProfiles:\n\n    - name: audit.json\n\n      value:\n\n        defaultAction: SCMP_ACT_LOG\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThis would create a seccomp profile name audit.json on the node at /var/lib/kubelet/seccomp/profiles.\n\nThe profiles can be used by Kubernetes pods by specfying the pod securityContext as below:\n\nspec:\n\n  securityContext:\n\n    seccompProfile:\n\n      type: Localhost\n\n      localhostProfile: profiles/audit.json\n\n\nNote that the localhostProfile uses the name of the profile created under profiles directory. So make sure to use path as profiles/<profile-name.json>\n\nThis can be verfied by running the below commands:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get seccompprofiles\n\n\nAn output similar to below can be observed:\n\nNODE       NAMESPACE   TYPE             ID           VERSION\n\n10.5.0.3   cri         SeccompProfile   audit.json   1\n\n\nThe content of the seccomp profile can be viewed by running the below command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> read /var/lib/kubelet/seccomp/profiles/audit.json\n\n\nAn output similar to below can be observed:\n\n{\"defaultAction\":\"SCMP_ACT_LOG\"}\n\nCreate a Kubernetes workload that uses the custom Seccomp Profile\n\nHere we’ll be using an example workload from the Kubernetes documentation.\n\nFirst open up a second terminal and run the following talosctl command so that we can view the Syscalls being logged in realtime:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> dmesg --follow --tail\n\n\nNow deploy the example workload from the Kubernetes documentation:\n\nkubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml\n\n\nOnce the pod starts running the terminal running talosctl dmesg command from above should log similar to below:\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.489473063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.490852063Z]: cni0: port 1(veth32488a86) entered disabled state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.492470063Z]: device veth32488a86 entered promiscuous mode\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503105063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503944063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): veth32488a86: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.504764063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.505423063Z]: cni0: port 1(veth32488a86) entered forwarding state\n\n10.5.0.3: kern: warning: [2022-07-28T11:49:44.873616063Z]: kauditd_printk_skb: 14 callbacks suppressed\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.873619063Z]: audit: type=1326 audit(1659008985.445:25): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.876609063Z]: audit: type=1326 audit(1659008985.445:26): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.878789063Z]: audit: type=1326 audit(1659008985.449:27): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=257 compat=0 ip=0x55ec0657bdaa code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.886693063Z]: audit: type=1326 audit(1659008985.461:28): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.888764063Z]: audit: type=1326 audit(1659008985.461:29): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.891009063Z]: audit: type=1326 audit(1659008985.461:30): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=1 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.893162063Z]: audit: type=1326 audit(1659008985.461:31): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.895365063Z]: audit: type=1326 audit(1659008985.461:32): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=39 compat=0 ip=0x55ec066eb68b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.898306063Z]: audit: type=1326 audit(1659008985.461:33): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=59 compat=0 ip=0x55ec0657be16 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.901518063Z]: audit: type=1326 audit(1659008985.473:34): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"http-echo\" exe=\"/http-echo\" sig=0 arch=c000003e syscall=158 compat=0 ip=0x455f35 code=0x7ffc0000\n\nCleanup\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete pod audit-pod\n\n1.9 - Storage\nSetting up storage for a Kubernetes cluster\n\nIn Kubernetes, using storage in the right way is well-facilitated by the API. However, unless you are running in a major public cloud, that API may not be hooked up to anything. This frequently sends users down a rabbit hole of researching all the various options for storage backends for their platform, for Kubernetes, and for their workloads. There are a lot of options out there, and it can be fairly bewildering.\n\nFor Talos, we try to limit the options somewhat to make the decision-making easier.\n\nPublic Cloud\n\nIf you are running on a major public cloud, use their block storage. It is easy and automatic.\n\nStorage Clusters\n\nSidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\nRedundancy, scaling capabilities, reliability, speed, maintenance load, and ease of use are all factors you must consider when managing your own storage.\n\nRunning a storage cluster can be a very good choice when managing your own storage, and there are two projects we recommend, depending on your situation.\n\nIf you need vast amounts of storage composed of more than a dozen or so disks, we recommend you use Rook to manage Ceph. Also, if you need both mount-once and mount-many capabilities, Ceph is your answer. Ceph also bundles in an S3-compatible object store. The down side of Ceph is that there are a lot of moving parts.\n\nPlease note that most people should never use mount-many semantics. NFS is pervasive because it is old and easy, not because it is a good idea. While it may seem like a convenience at first, there are all manner of locking, performance, change control, and reliability concerns inherent in any mount-many situation, so we strongly recommend you avoid this method.\n\nIf your storage needs are small enough to not need Ceph, use Mayastor.\n\nRook/Ceph\n\nCeph is the grandfather of open source storage clusters. It is big, has a lot of pieces, and will do just about anything. It scales better than almost any other system out there, open source or proprietary, being able to easily add and remove storage over time with no downtime, safely and easily. It comes bundled with RadosGW, an S3-compatible object store; CephFS, a NFS-like clustered filesystem; and RBD, a block storage system.\n\nWith the help of Rook, the vast majority of the complexity of Ceph is hidden away by a very robust operator, allowing you to control almost everything about your Ceph cluster from fairly simple Kubernetes CRDs.\n\nSo if Ceph is so great, why not use it for everything?\n\nCeph can be rather slow for small clusters. It relies heavily on CPUs and massive parallelisation to provide good cluster performance, so if you don’t have much of those dedicated to Ceph, it is not going to be well-optimised for you. Also, if your cluster is small, just running Ceph may eat up a significant amount of the resources you have available.\n\nTroubleshooting Ceph can be difficult if you do not understand its architecture. There are lots of acronyms and the documentation assumes a fair level of knowledge. There are very good tools for inspection and debugging, but this is still frequently seen as a concern.\n\nMayastor\n\nMayastor is an OpenEBS project built in Rust utilising the modern NVMEoF system. (Despite the name, Mayastor does not require you to have NVME drives.) It is fast and lean but still cluster-oriented and cloud native. Unlike most of the other OpenEBS project, it is not built on the ancient iSCSI system.\n\nUnlike Ceph, Mayastor is just a block store. It focuses on block storage and does it well. It is much less complicated to set up than Ceph, but you probably wouldn’t want to use it for more than a few dozen disks.\n\nMayastor is new, maybe too new. If you’re looking for something well-tested and battle-hardened, this is not it. However, if you’re looking for something lean, future-oriented, and simpler than Ceph, it might be a great choice.\n\nVideo Walkthrough\n\nTo see a live demo of this section, see the video below:\n\nPrep Nodes\n\nEither during initial cluster creation or on running worker nodes, several machine config values should be edited. (This information is gathered from the Mayastor documentation.) We need to set the vm.nr_hugepages sysctl and add openebs.io/engine=mayastor labels to the nodes which are meant to be storage nodes. This can be done with talosctl patch machineconfig or via config patches during talosctl gen config.\n\nSome examples are shown below: modify as needed.\n\nFirst create a config patch file named mayastor-patch.yaml with the following contents:\n\n- op: add\n\n  path: /machine/sysctls\n\n  value:\n\n    vm.nr_hugepages: \"1024\"\n\n- op: add\n\n  path: /machine/nodeLabels\n\n  value:\n\n    openebs.io/engine: mayastor\n\n\nUsing gen config\n\ntalosctl gen config my-cluster https://mycluster.local:6443 --config-patch @mayastor-patch.yaml\n\n\nPatching an existing node\n\ntalosctl patch --mode=no-reboot machineconfig -n <node ip> --patch @mayastor-patch.yaml\n\n\nNote: If you are adding/updating the vm.nr_hugepages on a node which already had the openebs.io/engine=mayastor label set, you’d need to restart kubelet so that it picks up the new value, by issuing the following command\n\ntalosctl -n <node ip> service kubelet restart\n\nDeploy Mayastor\n\nContinue setting up Mayastor using the official documentation.\n\nPiraeus / LINSTOR\nPiraeus-Operator\nLINSTOR\nDRBD Extension\nInstall Piraeus Operator V2\n\nThere is already a how-to for Talos: Link\n\nCreate first storage pool and PVC\n\nBefore proceeding, install linstor plugin for kubectl: https://github.com/piraeusdatastore/kubectl-linstor\n\nOr use krew: kubectl krew install linstor\n\n# Create device pool on a blank (no partitation table!) disk on node01\n\nkubectl linstor physical-storage create-device-pool --pool-name nvme_lvm_pool LVM node01 /dev/nvme0n1 --storage-pool nvme_pool\n\n\npiraeus-sc.yml\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  name: simple-nvme\n\nparameters:\n\n  csi.storage.k8s.io/fstype: xfs\n\n  linstor.csi.linbit.com/autoPlace: \"3\"\n\n  linstor.csi.linbit.com/storagePool: nvme_pool\n\nprovisioner: linstor.csi.linbit.com\n\nvolumeBindingMode: WaitForFirstConsumer\n\n# Create storage class\n\nkubectl apply -f piraeus-sc.yml\n\nNFS\n\nNFS is an old pack animal long past its prime. NFS is slow, has all kinds of bottlenecks involving contention, distributed locking, single points of service, and more. However, it is supported by a wide variety of systems. You don’t want to use it unless you have to, but unfortunately, that “have to” is too frequent.\n\nThe NFS client is part of the kubelet image maintained by the Talos team. This means that the version installed in your running kubelet is the version of NFS supported by Talos. You can reduce some of the contention problems by parceling Persistent Volumes from separate underlying directories.\n\nObject storage\n\nCeph comes with an S3-compatible object store, but there are other options, as well. These can often be built on top of other storage backends. For instance, you may have your block storage running with Mayastor but assign a Pod a large Persistent Volume to serve your object store.\n\nOne of the most popular open source add-on object stores is MinIO.\n\nOthers (iSCSI)\n\nThe most common remaining systems involve iSCSI in one form or another. These include the original OpenEBS, Rancher’s Longhorn, and many proprietary systems. iSCSI in Linux is facilitated by open-iscsi. This system was designed long before containers caught on, and it is not well suited to the task, especially when coupled with a read-only host operating system.\n\niSCSI support in Talos is now supported via the iscsi-tools system extension installed. The extension enables compatibility with OpenEBS Jiva - refer to the local storage installation guide for more information.\n\n2 - Network\nManaging the Kubernetes cluster networking\n2.1 - Deploying Cilium CNI\nIn this guide you will learn how to set up Cilium CNI on Talos.\n\nCilium can be installed either via the cilium cli or using helm.\n\nThis documentation will outline installing Cilium CNI v1.14.0 on Talos in six different ways. Adhering to Talos principles we’ll deploy Cilium with IPAM mode set to Kubernetes, and using the cgroupv2 and bpffs mount that talos already provides. As Talos does not allow loading kernel modules by Kubernetes workloads, SYS_MODULE capability needs to be dropped from the Cilium default set of values, this override can be seen in the helm/cilium cli install commands. Each method can either install Cilium using kube proxy (default) or without: Kubernetes Without kube-proxy\n\nIn this guide we assume that KubePrism is enabled and configured to use the port 7445.\n\nMachine config preparation\n\nWhen generating the machine config for a node set the CNI to none. For example using a config patch:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nOr if you want to deploy Cilium without kube-proxy, you also need to disable kube proxy:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\nInstallation using Cilium CLI\n\nNote: It is recommended to template the cilium manifest using helm and use it as part of Talos machine config, but if you want to install Cilium using the Cilium CLI, you can follow the steps below.\n\nInstall the Cilium CLI following the steps here.\n\nWith kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=disabled \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup\n\nWithout kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=true \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --helm-set=k8sServiceHost=localhost \\\n\n    --helm-set=k8sServicePort=7445\n\nInstallation using Helm\n\nRefer to Installing with Helm for more information.\n\nFirst we’ll need to add the helm repo for Cilium.\n\nhelm repo add cilium https://helm.cilium.io/\n\nhelm repo update\n\nMethod 1: Helm install\n\nAfter applying the machine config and bootstrapping Talos will appear to hang on phase 18/19 with the message: retrying error: node not ready. This happens because nodes in Kubernetes are only marked as ready once the CNI is up. As there is no CNI defined, the boot process is pending and will reboot the node to retry after 10 minutes, this is expected behavior.\n\nDuring this window you can install Cilium manually by running the following:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup\n\n\nOr if you want to deploy Cilium without kube-proxy, also set some extra paramaters:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445\n\n\nAfter Cilium is installed the boot process should continue and complete successfully.\n\nMethod 2: Helm manifests install\n\nInstead of directly installing Cilium you can instead first generate the manifest and then apply it:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\n\nWithout kube-proxy:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445 > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\nMethod 3: Helm manifests hosted install\n\nAfter generating cilium.yaml using helm template, instead of applying this manifest directly during the Talos boot window (before the reboot timeout). You can also host this file somewhere and patch the machine config to apply this manifest automatically during bootstrap. To do this patch your machine configuration to include this config instead of the above:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: custom\n\n      urls:\n\n        - https://server.yourdomain.tld/some/path/cilium.yaml\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nHowever, beware of the fact that the helm generated Cilium manifest contains sensitive key material. As such you should definitely not host this somewhere publicly accessible.\n\nMethod 4: Helm manifests inline install\n\nA more secure option would be to include the helm template output manifest inside the machine configuration. The machine config should be generated with CNI set to none\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nif deploying Cilium with kube-proxy disabled, you can also include the following:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\nmachine:\n\n  features:\n\n    kubePrism:\n\n      enabled: true\n\n      port: 7445\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nTo do so patch this into your machine configuration:\n\ninlineManifests:\n\n    - name: cilium\n\n      contents: |\n\n        --\n\n        # Source: cilium/templates/cilium-agent/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        metadata:\n\n          name: \"cilium\"\n\n          namespace: kube-system\n\n        ---\n\n        # Source: cilium/templates/cilium-operator/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        -> Your cilium.yaml file will be pretty long....        \n\n\nThis will install the Cilium manifests at just the right time during bootstrap.\n\nBeware though:\n\nChanging the namespace when templating with Helm does not generate a manifest containing the yaml to create that namespace. As the inline manifest is processed from top to bottom make sure to manually put the namespace yaml at the start of the inline manifest.\nOnly add the Cilium inline manifest to the control plane nodes machine configuration.\nMake sure all control plane nodes have an identical configuration.\nIf you delete any of the generated resources they will be restored whenever a control plane node reboots.\nAs a safety messure Talos only creates missing resources from inline manifests, it never deletes or updates anything.\nIf you need to update a manifest make sure to first edit all control plane machine configurations and then run talosctl upgrade-k8s as it will take care of updating inline manifests.\nKnown issues\nThere are some gotchas when using Talos and Cilium on the Google cloud platform when using internal load balancers. For more details: GCP ILB support / support scope local routes to be configured\nOther things to know\nTalos has full kernel module support for eBPF, See:\nCilium System Requirements\nTalos Kernel Config AMD64\nTalos Kernel Config ARM64\n3 - Upgrading Kubernetes\nGuide on how to upgrade the Kubernetes cluster from Talos Linux.\n\nThis guide covers upgrading Kubernetes on Talos Linux clusters.\n\nFor a list of Kubernetes versions compatible with each Talos release, see the Support Matrix.\n\nFor upgrading the Talos Linux operating system, see Upgrading Talos\n\nVideo Walkthrough\n\nTo see a demo of this process, watch this video:\n\nAutomated Kubernetes Upgrade\n\nThe recommended method to upgrade Kubernetes is to use the talosctl upgrade-k8s command. This will automatically update the components needed to upgrade Kubernetes safely. Upgrading Kubernetes is non-disruptive to the cluster workloads.\n\nTo trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to ugprade to, such as:\n\ntalosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nNote that the --nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded.\n\nTo check what will be upgraded you can run talosctl upgrade-k8s with the --dry-run flag:\n\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0 --dry-run\n\nWARNING: found resources which are going to be deprecated/migrated in the version 1.29.0\n\nRESOURCE                                                               COUNT\n\nvalidatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io   4\n\nmutatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io     3\n\ncustomresourcedefinitions.v1beta1.apiextensions.k8s.io                 25\n\napiservices.v1beta1.apiregistration.k8s.io                             54\n\nleases.v1beta1.coordination.k8s.io                                     4\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.4\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\nupdating \"kube-controller-manager\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-controller-manager: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n\n\n<snip>\n\n\n\nupdating manifests\n\n > apply manifest Secret bootstrap-token-3lb63t\n\n > apply skipped in dry run\n\n > apply manifest ClusterRoleBinding system-bootstrap-approve-node-client-csr\n\n > apply skipped in dry run\n\n<snip>\n\n\nTo upgrade Kubernetes from v1.28.3 to v1.29.0 run:\n\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > \"172.20.0.2\": machine configuration patched\n\n > \"172.20.0.2\": waiting for API server state pod update\n\n < \"172.20.0.2\": successfully updated\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n<snip>\n\n\nThis command runs in several phases:\n\nImages for new Kubernetes components are pre-pulled to the nodes to minimize downtime and test for image availability.\nEvery control plane node machine configuration is patched with the new image version for each control plane component. Talos renders new static pod definitions on the configuration update which is picked up by the kubelet. The command waits for the change to propagate to the API server state.\nThe command updates the kube-proxy daemonset with the new image version.\nOn every node in the cluster, the kubelet version is updated. The command then waits for the kubelet service to be restarted and become healthy. The update is verified by checking the Node resource state.\nKubernetes bootstrap manifests are re-applied to the cluster. Updated bootstrap manifests might come with a new Talos version (e.g. CoreDNS version update), or might be the result of machine configuration change.\n\nNote: The upgrade-k8s command never deletes any resources from the cluster: they should be deleted manually.\n\nIf the command fails for any reason, it can be safely restarted to continue the upgrade process from the moment of the failure.\n\nManual Kubernetes Upgrade\n\nKubernetes can be upgraded manually by following the steps outlined below. They are equivalent to the steps performed by the talosctl upgrade-k8s command.\n\nKubeconfig\n\nIn order to edit the control plane, you need a working kubectl config. If you don’t already have one, you can get one by running:\n\ntalosctl --nodes <controlplane node> kubeconfig\n\nAPI Server\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need to be adjusted if current machine configuration is missing .cluster.apiServer.image key.\n\nAlso the machine configuration can be edited manually with talosctl -n <IP> edit mc --mode=no-reboot.\n\nCapture the new version of kube-apiserver config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-apiserver -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-apiserver\n\n    version: 5\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-apiserver:v1.29.0\n\n    cloudProvider: \"\"\n\n    controlPlaneEndpoint: https://172.20.0.1:6443\n\n    etcdServers:\n\n        - https://127.0.0.1:2379\n\n    localPort: 6443\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, the new version is 5. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n5\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-apiserver-talos-default-controlplane-1   1/1     Running   0          16m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nController Manager\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/controllerManager/image\", \"value\": \"registry.k8s.io/kube-controller-manager:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need be adjusted if current machine configuration is missing .cluster.controllerManager.image key.\n\nCapture new version of kube-controller-manager config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-controller-manager -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-controller-manager\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-controller-manager:v1.29.0\n\n    cloudProvider: \"\"\n\n    podCIDR: 10.244.0.0/16\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\nkube-controller-manager-talos-default-controlplane-1   1/1     Running   0          35m\n\n\nRepeat this process for every control plane node, verifying that state propagated successfully between each node update.\n\nScheduler\n\nPatch machine configuration using talosctl patch command:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/scheduler/image\", \"value\": \"registry.k8s.io/kube-scheduler:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nJSON patch might need be adjusted if current machine configuration is missing .cluster.scheduler.image key.\n\nCapture new version of kube-scheduler config with:\n\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-scheduler -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-scheduler\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-scheduler:v1.29.0\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-scheduler-talos-default-controlplane-1   1/1     Running   0          39m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nProxy\n\nIn the proxy’s DaemonSet, change:\n\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n\nto:\n\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n        - key: node-role.kubernetes.io/control-plane\n\n          operator: Exists\n\n          effect: NoSchedule\n\n\nTo edit the DaemonSet, run:\n\nkubectl edit daemonsets -n kube-system kube-proxy\n\nBootstrap Manifests\n\nBootstrap manifests can be retrieved in a format which works for kubectl with the following command:\n\ntalosctl -n <controlplane IP> get manifests -o yaml | yq eval-all '.spec | .[] | splitDoc' - > manifests.yaml\n\n\nDiff the manifests with the cluster:\n\nkubectl diff -f manifests.yaml\n\n\nApply the manifests:\n\nkubectl apply -f manifests.yaml\n\n\nNote: if some bootstrap resources were removed, they have to be removed from the cluster manually.\n\nkubelet\n\nFor every node, patch machine configuration with new kubelet version, wait for the kubelet to restart with new version:\n\n$ talosctl -n <IP> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nOnce kubelet restarts with the new configuration, confirm upgrade with kubectl get nodes <name>:\n\n$ kubectl get nodes talos-default-controlplane-1\n\nNAME                           STATUS   ROLES                  AGE    VERSION\n\ntalos-default-controlplane-1   Ready    control-plane          123m   v1.29.0\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "404 Page not found | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/metal-network-configuration/%7B%7B%20relref%20%22../reference/kernel%22%20%7D%7D",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\nNot found\n\nOops! This page doesn't exist. Try going back to our home page.\n\nYou can learn how to make a 404 page like this in Custom 404 Pages.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nConfiguration\nHow to configure components of the Kubernetes cluster itself.\n1: Ceph Storage cluster with Rook\n2: Deploying Metrics Server\n3: iSCSI Storage with Synology CSI\n4: KubePrism\n5: Local Storage\n6: Pod Security\n7: Replicated Local Storage\n8: Seccomp Profiles\n9: Storage\n1 - Ceph Storage cluster with Rook\nGuide on how to create a simple Ceph storage cluster with Rook for Kubernetes\nPreparation\n\nTalos Linux reserves an entire disk for the OS installation, so machines with multiple available disks are needed for a reliable Ceph cluster with Rook and Talos Linux. Rook requires that the block devices or partitions used by Ceph have no partitions or formatted filesystems before use. Rook also requires a minimum Kubernetes version of v1.16 and Helm v3.0 for installation of charts. It is highly recommended that the Rook Ceph overview is read and understood before deploying a Ceph cluster with Rook.\n\nInstallation\n\nCreating a Ceph cluster with Rook requires two steps; first the Rook Operator needs to be installed which can be done with a Helm Chart. The example below installs the Rook Operator into the rook-ceph namespace, which is the default for a Ceph cluster with Rook.\n\n$ helm repo add rook-release https://charts.rook.io/release\n\n\"rook-release\" has been added to your repositories\n\n\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph\n\nW0327 17:52:44.277830   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nW0327 17:52:44.612243   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nNAME: rook-ceph\n\nLAST DEPLOYED: Sun Mar 27 17:52:42 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Rook Operator has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get pods -l \"app=rook-ceph-operator\"\n\n\n\nVisit https://rook.io/docs/rook/latest for instructions on how to create and configure Rook clusters\n\n\n\nImportant Notes:\n\n- You must customize the 'CephCluster' resource in the sample manifests for your cluster.\n\n- Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the namespace.\n\n- The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.\n\n- The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.\n\n- Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).\n\n\nOnce that is complete, the Ceph cluster can be installed with the official Helm Chart. The Chart can be installed with default values, which will attempt to use all nodes in the Kubernetes cluster, and all unused disks on each node for Ceph storage, and make available block storage, object storage, as well as a shared filesystem. Generally more specific node/device/cluster configuration is used, and the Rook documentation explains all the available options in detail. For this example the defaults will be adequate.\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster\n\nNAME: rook-ceph-cluster\n\nLAST DEPLOYED: Sun Mar 27 18:12:46 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Ceph Cluster has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get cephcluster\n\n\n\nVisit https://rook.github.io/docs/rook/latest/ceph-cluster-crd.html for more information about the Ceph CRD.\n\n\n\nImportant Notes:\n\n- You can only deploy a single cluster per namespace\n\n- If you wish to delete this cluster and start fresh, you will also have to wipe the OSD disks using `sfdisk`\n\n\nNow the Ceph cluster configuration has been created, the Rook operator needs time to install the Ceph cluster and bring all the components online. The progression of the Ceph cluster state can be followed with the following command.\n\n$ watch kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nEvery 2.0s: kubectl --namespace rook-ceph get cephcluster rook-ceph\n\n\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                 HEALTH   EXTERNAL\n\nrook-ceph   /var/lib/rook     3          57s   Progressing   Configuring Ceph Mons\n\n\nDepending on the size of the Ceph cluster and the availability of resources the Ceph cluster should become available, and with it the storage classes that can be used with Kubernetes Physical Volumes.\n\n$ kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          40m   Ready   Cluster created successfully   HEALTH_OK\n\n\n\n$ kubectl  get storageclass\n\nNAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n\nceph-block (default)   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   77m\n\nceph-bucket            rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  77m\n\nceph-filesystem        rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   77m\n\nTalos Linux Considerations\n\nIt is important to note that a Rook Ceph cluster saves cluster information directly onto the node (by default dataDirHostPath is set to /var/lib/rook). If running only a single mon instance, cluster management is little bit more involved, as any time a Talos Linux node is reconfigured or upgraded, the partition that stores the /var file system is wiped, but the --preserve option of talosctl upgrade will ensure that doesn’t happen.\n\nBy default, Rook configues Ceph to have 3 mon instances, in which case the data stored in dataDirHostPath can be regenerated from the other mon instances. So when performing maintenance on a Talos Linux node with a Rook Ceph cluster (e.g. upgrading the Talos Linux version), it is imperative that care be taken to maintain the health of the Ceph cluster. Before upgrading, you should always check the health status of the Ceph cluster to ensure that it is healthy.\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          98m   Ready   Cluster created successfully   HEALTH_OK\n\n\nIf it is, you can begin the upgrade process for the Talos Linux node, during which time the Ceph cluster will become unhealthy as the node is reconfigured. Before performing any other action on the Talos Linux nodes, the Ceph cluster must return to a healthy status.\n\n$ talosctl upgrade --nodes 172.20.15.5 --image ghcr.io/talos-systems/installer:v0.14.3\n\nNODE          ACK                        STARTED\n\n172.20.15.5   Upgrade request received   2022-03-27 20:29:55.292432887 +0200 CEST m=+10.050399758\n\n\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                   HEALTH        EXTERNAL\n\nrook-ceph   /var/lib/rook     3          99m   Progressing   Configuring Ceph Mgr(s)   HEALTH_WARN\n\n\n\n$ kubectl --namespace rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n\ncephcluster.ceph.rook.io/rook-ceph condition met\n\n\nThe above steps need to be performed for each Talos Linux node undergoing maintenance, one at a time.\n\nCleaning Up\nRook Ceph Cluster Removal\n\nRemoving a Rook Ceph cluster requires a few steps, starting with signalling to Rook that the Ceph cluster is really being destroyed. Then all Persistent Volumes (and Claims) backed by the Ceph cluster must be deleted, followed by the Storage Classes and the Ceph storage types.\n\n$ kubectl --namespace rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\ncephcluster.ceph.rook.io/rook-ceph patched\n\n\n\n$ kubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nstorageclass.storage.k8s.io \"ceph-block\" deleted\n\nstorageclass.storage.k8s.io \"ceph-bucket\" deleted\n\nstorageclass.storage.k8s.io \"ceph-filesystem\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephblockpools ceph-blockpool\n\ncephblockpool.ceph.rook.io \"ceph-blockpool\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephobjectstore ceph-objectstore\n\ncephobjectstore.ceph.rook.io \"ceph-objectstore\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephfilesystem ceph-filesystem\n\ncephfilesystem.ceph.rook.io \"ceph-filesystem\" deleted\n\n\nOnce that is complete, the Ceph cluster itself can be removed, along with the Rook Ceph cluster Helm chart installation.\n\n$ kubectl --namespace rook-ceph delete cephcluster rook-ceph\n\ncephcluster.ceph.rook.io \"rook-ceph\" deleted\n\n\n\n$ helm --namespace rook-ceph uninstall rook-ceph-cluster\n\nrelease \"rook-ceph-cluster\" uninstalled\n\n\nIf needed, the Rook Operator can also be removed along with all the Custom Resource Definitions that it created.\n\n$ helm --namespace rook-ceph uninstall rook-ceph\n\nW0328 12:41:14.998307  147203 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nThese resources were kept due to the resource policy:\n\n[CustomResourceDefinition] cephblockpools.ceph.rook.io\n\n[CustomResourceDefinition] cephbucketnotifications.ceph.rook.io\n\n[CustomResourceDefinition] cephbuckettopics.ceph.rook.io\n\n[CustomResourceDefinition] cephclients.ceph.rook.io\n\n[CustomResourceDefinition] cephclusters.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemmirrors.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystems.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemsubvolumegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephnfses.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectrealms.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstores.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstoreusers.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzonegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzones.ceph.rook.io\n\n[CustomResourceDefinition] cephrbdmirrors.ceph.rook.io\n\n[CustomResourceDefinition] objectbucketclaims.objectbucket.io\n\n[CustomResourceDefinition] objectbuckets.objectbucket.io\n\n\n\nrelease \"rook-ceph\" uninstalled\n\n\n\n$ kubectl delete crds cephblockpools.ceph.rook.io cephbucketnotifications.ceph.rook.io cephbuckettopics.ceph.rook.io \\\n\n                      cephclients.ceph.rook.io cephclusters.ceph.rook.io cephfilesystemmirrors.ceph.rook.io \\\n\n                      cephfilesystems.ceph.rook.io cephfilesystemsubvolumegroups.ceph.rook.io \\\n\n                      cephnfses.ceph.rook.io cephobjectrealms.ceph.rook.io cephobjectstores.ceph.rook.io \\\n\n                      cephobjectstoreusers.ceph.rook.io cephobjectzonegroups.ceph.rook.io cephobjectzones.ceph.rook.io \\\n\n                      cephrbdmirrors.ceph.rook.io objectbucketclaims.objectbucket.io objectbuckets.objectbucket.io\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephblockpools.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbucketnotifications.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbuckettopics.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclients.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclusters.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystems.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemsubvolumegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephnfses.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectrealms.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstores.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstoreusers.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzonegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzones.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephrbdmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbucketclaims.objectbucket.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbuckets.objectbucket.io\" deleted\n\nTalos Linux Rook Metadata Removal\n\nIf the Rook Operator is cleanly removed following the above process, the node metadata and disks should be clean and ready to be re-used. In the case of an unclean cluster removal, there may be still a few instances of metadata stored on the system disk, as well as the partition information on the storage disks. First the node metadata needs to be removed, make sure to update the nodeName with the actual name of a storage node that needs cleaning, and path with the Rook configuration dataDirHostPath set when installing the chart. The following will need to be repeated for each node used in the Rook Ceph cluster.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-clean\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  volumes:\n\n  - name: rook-data-dir\n\n    hostPath:\n\n      path: <dataDirHostPath>\n\n  containers:\n\n  - name: disk-clean\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    volumeMounts:\n\n    - name: rook-data-dir\n\n      mountPath: /node/rook-data\n\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\n\nEOF\n\npod/disk-clean created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\n\npod/disk-clean condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-clean\" deleted\n\n\nLastly, the disks themselves need the partition and filesystem data wiped before they can be reused. Again, the following as to be repeated for each node and disk used in the Rook Ceph cluster, updating nodeName and of= in the command as needed.\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-wipe\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  containers:\n\n  - name: disk-wipe\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=<device>\"]\n\nEOF\n\npod/disk-wipe created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\n\npod/disk-wipe condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-wipe\" deleted\n\n2 - Deploying Metrics Server\nIn this guide you will learn how to set up metrics-server.\n\nMetrics Server enables use of the Horizontal Pod Autoscaler and Vertical Pod Autoscaler. It does this by gathering metrics data from the kubelets in a cluster. By default, the certificates in use by the kubelets will not be recognized by metrics-server. This can be solved by either configuring metrics-server to do no validation of the TLS certificates, or by modifying the kubelet configuration to rotate its certificates and use ones that will be recognized by metrics-server.\n\nNode Configuration\n\nTo enable kubelet certificate rotation, all nodes should have the following Machine Config snippet:\n\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      rotate-server-certificates: true\n\nInstall During Bootstrap\n\nWe will want to ensure that new certificates for the kubelets are approved automatically. This can easily be done with the Kubelet Serving Certificate Approver, which will automatically approve the Certificate Signing Requests generated by the kubelets.\n\nWe can have Kubelet Serving Certificate Approver and metrics-server installed on the cluster automatically during bootstrap by adding the following snippet to the Cluster Config of the node that will be handling the bootstrap process:\n\ncluster:\n\n  extraManifests:\n\n    - https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\n    - https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nInstall After Bootstrap\n\nIf you choose not to use extraManifests to install Kubelet Serving Certificate Approver and metrics-server during bootstrap, you can install them once the cluster is online using kubectl:\n\nkubectl apply -f https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n3 - iSCSI Storage with Synology CSI\nAutomatically provision iSCSI volumes on a Synology NAS with the synology-csi driver.\nBackground\n\nSynology is a company that specializes in Network Attached Storage (NAS) devices. They provide a number of features within a simple web OS, including an LDAP server, Docker support, and (perhaps most relevant to this guide) function as an iSCSI host. The focus of this guide is to allow a Kubernetes cluster running on Talos to provision Kubernetes storage (both dynamic or static) on a Synology NAS using a direct integration, rather than relying on an intermediary layer like Rook/Ceph or Maystor.\n\nThis guide assumes a very basic familiarity with iSCSI terminology (LUN, iSCSI target, etc.).\n\nPrerequisites\nSynology NAS running DSM 7.0 or above\nProvisioned Talos cluster running Kubernetes v1.20 or above\n(Optional) Both Volume Snapshot CRDs and the common snapshot controller must be installed in your Kubernetes cluster if you want to use the Snapshot feature\nSetting up the Synology user account\n\nThe synology-csi controller interacts with your NAS in two different ways: via the API and via the iSCSI protocol. Actions such as creating a new iSCSI target or deleting an old one are accomplished via the Synology API, and require administrator access. On the other hand, mounting the disk to a pod and reading from / writing to it will utilize iSCSI. Because you can only authenticate with one account per DSM configured, that account needs to have admin privileges. In order to minimize access in the case of these credentials being compromised, you should configure the account with the lease possible amount of access – explicitly specify “No Access” on all volumes when configuring the user permissions.\n\nSetting up the Synology CSI\n\nNote: this guide is paraphrased from the Synology CSI readme. Please consult the readme for more in-depth instructions and explanations.\n\nClone the git repository.\n\ngit clone https://github.com/zebernst/synology-csi-talos.git\n\n\nWhile Synology provides some automated scripts to deploy the CSI driver, they can be finicky especially when making changes to the source code. We will be configuring and deploying things manually in this guide.\n\nThe relevant files we will be touching are in the following locations:\n\n.\n\n├── Dockerfile\n\n├── Makefile\n\n├── config\n\n│   └── client-info-template.yml\n\n└── deploy\n\n    └── kubernetes\n\n        └── v1.20\n\n            ├── controller.yml\n\n            ├── csi-driver.yml\n\n            ├── namespace.yml\n\n            ├── node.yml\n\n            ├── snapshotter\n\n            │   ├── snapshotter.yaml\n\n            │   └── volume-snapshot-class.yml\n\n            └── storage-class.yml\n\nConfigure connection info\n\nUse config/client-info-template.yml as an example to configure the connection information for DSM. You can specify one or more storage systems on which the CSI volumes will be created. See below for an example:\n\n---\n\nclients:\n\n- host: 192.168.1.1   # ipv4 address or domain of the DSM\n\n  port: 5000          # port for connecting to the DSM\n\n  https: false        # set this true to use https. you need to specify the port to DSM HTTPS port as well\n\n  username: username  # username\n\n  password: password  # password\n\n\nCreate a Kubernetes secret using the client information config file.\n\nkubectl create secret -n synology-csi generic client-info-secret --from-file=config/client-info.yml\n\n\nNote that if you rename the secret to something other than client-info-secret, make sure you update the corresponding references in the deployment manifests as well.\n\nBuild the Talos-compatible image\n\nModify the Makefile so that the image is built and tagged under your GitHub Container Registry username:\n\nREGISTRY_NAME=ghcr.io/<username>\n\n\nWhen you run make docker-build or make docker-build-multiarch, it will push the resulting image to ghcr.io/<username>/synology-csi:v1.1.0. Ensure that you find and change any reference to synology/synology-csi:v1.1.0 to point to your newly-pushed image within the deployment manifests.\n\nConfigure the CSI driver\n\nBy default, the deployment manifests include one storage class and one volume snapshot class. See below for examples:\n\n---\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\n  name: syno-storage\n\nprovisioner: csi.san.synology.com\n\nparameters:\n\n  fsType: 'ext4'\n\n  dsm: '192.168.1.1'\n\n  location: '/volume1'\n\nreclaimPolicy: Retain\n\nallowVolumeExpansion: true\n\n---\n\napiVersion: snapshot.storage.k8s.io/v1\n\nkind: VolumeSnapshotClass\n\nmetadata:\n\n  name: syno-snapshot\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\ndriver: csi.san.synology.com\n\ndeletionPolicy: Delete\n\nparameters:\n\n  description: 'Kubernetes CSI'\n\n\nIt can be useful to configure multiple different StorageClasses. For example, a popular strategy is to create two nearly identical StorageClasses, with one configured with reclaimPolicy: Retain and the other with reclaimPolicy: Delete. Alternately, a workload may require a specific filesystem, such as ext4. If a Synology NAS is going to be the most common way to configure storage on your cluster, it can be convenient to add the storageclass.kubernetes.io/is-default-class: \"true\" annotation to one of your StorageClasses.\n\nThe following table details the configurable parameters for the Synology StorageClass.\n\nName\tType\tDescription\tDefault\tSupported protocols\ndsm\tstring\tThe IPv4 address of your DSM, which must be included in the client-info.yml for the CSI driver to log in to DSM\t-\tiSCSI, SMB\nlocation\tstring\tThe location (/volume1, /volume2, …) on DSM where the LUN for PersistentVolume will be created\t-\tiSCSI, SMB\nfsType\tstring\tThe formatting file system of the PersistentVolumes when you mount them on the pods. This parameter only works with iSCSI. For SMB, the fsType is always ‘cifs‘.\text4\tiSCSI\nprotocol\tstring\tThe backing storage protocol. Enter ‘iscsi’ to create LUNs or ‘smb‘ to create shared folders on DSM.\tiscsi\tiSCSI, SMB\ncsi.storage.k8s.io/node-stage-secret-name\tstring\tThe name of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\ncsi.storage.k8s.io/node-stage-secret-namespace\tstring\tThe namespace of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\n\nThe VolumeSnapshotClass can be similarly configured with the following parameters:\n\nName\tType\tDescription\tDefault\tSupported protocols\ndescription\tstring\tThe description of the snapshot on DSM\t-\tiSCSI\nis_locked\tstring\tWhether you want to lock the snapshot on DSM\tfalse\tiSCSI, SMB\nApply YAML manifests\n\nOnce you have created the desired StorageClass(es) and VolumeSnapshotClass(es), the final step is to apply the Kubernetes manifests against the cluster. The easiest way to apply them all at once is to create a kustomization.yaml file in the same directory as the manifests and use Kustomize to apply:\n\nkubectl apply -k path/to/manifest/directory\n\n\nAlternately, you can apply each manifest one-by-one:\n\nkubectl apply -f <file>\n\nRun performance tests\n\nIn order to test the provisioning, mounting, and performance of using a Synology NAS as Kubernetes persistent storage, use the following command:\n\nkubectl apply -f speedtest.yaml\n\n\nContent of speedtest.yaml (source)\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: test-claim\n\nspec:\n\n#  storageClassName: syno-storage\n\n  accessModes:\n\n  - ReadWriteMany\n\n  resources:\n\n    requests:\n\n      storage: 5G\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: read\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: read\n\n      labels:\n\n        app: speedtest\n\n        job: read\n\n    spec:\n\n      containers:\n\n      - name: read\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/mnt/pv/test.img\",\"of=/dev/null\",\"bs=8k\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: write\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: write\n\n      labels:\n\n        app: speedtest\n\n        job: write\n\n    spec:\n\n      containers:\n\n      - name: write\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/dev/zero\",\"of=/mnt/pv/test.img\",\"bs=1G\",\"count=1\",\"oflag=dsync\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n\nIf these two jobs complete successfully, use the following commands to get the results of the speed tests:\n\n# Pod logs for read test:\n\nkubectl logs -l app=speedtest,job=read\n\n\n\n# Pod logs for write test:\n\nkubectl logs -l app=speedtest,job=write\n\n\nWhen you’re satisfied with the results of the test, delete the artifacts created from the speedtest:\n\nkubectl delete -f speedtest.yaml\n\n4 - KubePrism\nEnabling in-cluster highly-available controlplane endpoint.\n\nKubernetes pods running in CNI mode can use the kubernetes.default.svc service endpoint to access the Kubernetes API server, while pods running in host networking mode can only use the external cluster endpoint to access the Kubernetes API server.\n\nKubernetes controlplane components run in host networking mode, and it is critical for them to be able to access the Kubernetes API server, same as CNI components (when CNI requires access to Kubernetes API).\n\nThe external cluster endpoint might be unavailable due to misconfiguration or network issues, or it might have higher latency than the internal endpoint. A failure to access the Kubernetes API server might cause a series of issues in the cluster: pods are not scheduled, service IPs stop working, etc.\n\nKubePrism feature solves this problem by enabling in-cluster highly-available controlplane endpoint on every node in the cluster.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nEnabling KubePrism\n\nAs of Talos 1.6, KubePrism is enabled by default with port 7445.\n\nNote: the port specified should be available on every node in the cluster.\n\nHow it works\n\nTalos spins up a TCP loadbalancer on every machine on the localhost on the specified port which automatically picks up one of the endpoints:\n\nthe external cluster endpoint as specified in the machine configuration\nfor controlplane machines: https://localhost:<api-server-local-port> (http://localhost:6443 in the default configuration)\nhttps://<controlplane-address>:<api-server-port> for every controlplane machine (based on the information from Cluster Discovery)\n\nKubePrism automatically filters out unhealthy (or unreachable) endpoints, and prefers lower-latency endpoints over higher-latency endpoints.\n\nTalos automatically reconfigures kubelet, kube-scheduler and kube-controller-manager to use the KubePrism endpoint. The kube-proxy manifest is also reconfigured to use the KubePrism endpoint by default, but when enabling KubePrism for a running cluster the manifest should be updated with talosctl upgrade-k8s command.\n\nWhen using CNI components that require access to the Kubernetes API server, the KubePrism endpoint should be passed to the CNI configuration (e.g. Cilium, Calico CNIs).\n\nNotes\n\nAs the list of endpoints for KubePrism includes the external cluster endpoint, KubePrism in the worst case scenario will behave the same as the external cluster endpoint. For controlplane nodes, the KubePrism should pick up the localhost endpoint of the kube-apiserver, minimizing the latency. Worker nodes might use direct address of the controlplane endpoint if the latency is lower than the latency of the external cluster endpoint.\n\nKubePrism listen endpoint is bound to localhost address, so it can’t be used outside the cluster.\n\n5 - Local Storage\nUsing local storage for Kubernetes workloads.\n\nUsing local storage for Kubernetes workloads implies that the pod will be bound to the node where the local storage is available. Local storage is not replicated, so in case of a machine failure contents of the local storage will be lost.\n\nNote: when using EPHEMERAL Talos partition (/var), make sure to use --preserve set while performing upgrades, otherwise you risk losing data.\n\nhostPath mounts\n\nThe simplest way to use local storage is to use hostPath mounts. When using hostPath mounts, make sure the root directory of the mount is mounted into the kubelet container:\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/mnt\n\n        type: bind\n\n        source: /var/mnt\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nBoth EPHEMERAL partition and user disks can be used for hostPath mounts.\n\nLocal Path Provisioner\n\nLocal Path Provisioner can be used to dynamically provision local storage. Make sure to update its configuration to use a path under /var, e.g. /var/local-path-provisioner as the root path for the local storage. (In Talos Linux default local path provisioner path /opt/local-path-provisioner is read-only).\n\nFor example, Local Path Provisioner can be installed using kustomize with the following configuration:\n\n# kustomization.yaml\n\napiVersion: kustomize.config.k8s.io/v1beta1\n\nkind: Kustomization\n\nresources:\n\n- github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26\n\npatches:\n\n- patch: |-\n\n    kind: ConfigMap\n\n    apiVersion: v1\n\n    metadata:\n\n      name: local-path-config\n\n      namespace: local-path-storage\n\n    data:\n\n      config.json: |-\n\n        {\n\n                \"nodePathMap\":[\n\n                {\n\n                        \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n\n                        \"paths\":[\"/var/local-path-provisioner\"]\n\n                }\n\n                ]\n\n        }    \n\n- patch: |-\n\n    apiVersion: storage.k8s.io/v1\n\n    kind: StorageClass\n\n    metadata:\n\n      name: local-path\n\n      annotations:\n\n        storageclass.kubernetes.io/is-default-class: \"true\"    \n\n- patch: |-\n\n    apiVersion: v1\n\n    kind: Namespace\n\n    metadata:\n\n      name: local-path-storage\n\n      labels:\n\n        pod-security.kubernetes.io/enforce: privileged    \n\n\nPut kustomization.yaml into a new directory, and run kustomize build | kubectl apply -f - to install Local Path Provisioner to a Talos Linux cluster. There are three patches applied:\n\nchange default /opt/local-path-provisioner path to /var/local-path-provisioner\nmake local-path storage class the default storage class (optional)\nlabel the local-path-storage namespace as privileged to allow privileged pods to be scheduled there\n6 - Pod Security\nEnabling Pod Security Admission plugin to configure Pod Security Standards.\n\nKubernetes deprecated Pod Security Policy as of v1.21, and it was removed in v1.25.\n\nPod Security Policy was replaced with Pod Security Admission, which is enabled by default starting with Kubernetes v1.23.\n\nTalos Linux by default enables and configures Pod Security Admission plugin to enforce Pod Security Standards with the baseline profile as the default enforced with the exception of kube-system namespace which enforces privileged profile.\n\nSome applications (e.g. Prometheus node exporter or storage solutions) require more relaxed Pod Security Standards, which can be configured by either updating the Pod Security Admission plugin configuration, or by using the pod-security.kubernetes.io/enforce label on the namespace level:\n\nkubectl label namespace NAMESPACE-NAME pod-security.kubernetes.io/enforce=privileged\n\nConfiguration\n\nTalos provides default Pod Security Admission in the machine configuration:\n\napiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\nkind: PodSecurityConfiguration\n\ndefaults:\n\n    enforce: \"baseline\"\n\n    enforce-version: \"latest\"\n\n    audit: \"restricted\"\n\n    audit-version: \"latest\"\n\n    warn: \"restricted\"\n\n    warn-version: \"latest\"\n\nexemptions:\n\n    usernames: []\n\n    runtimeClasses: []\n\n    namespaces: [kube-system]\n\n\nThis is a cluster-wide configuration for the Pod Security Admission plugin:\n\nby default baseline Pod Security Standard profile is enforced\nmore strict restricted profile is not enforced, but API server warns about found issues\n\nThis default policy can be modified by updating the generated machine configuration before the cluster is created or on the fly by using the talosctl CLI utility.\n\nVerify current admission plugin configuration with:\n\n$ talosctl get admissioncontrolconfigs.kubernetes.talos.dev admission-control -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: AdmissionControlConfigs.kubernetes.talos.dev\n\n    id: admission-control\n\n    version: 1\n\n    owner: config.K8sControlPlaneController\n\n    phase: running\n\n    created: 2022-02-22T20:28:21Z\n\n    updated: 2022-02-22T20:28:21Z\n\nspec:\n\n    config:\n\n        - name: PodSecurity\n\n          configuration:\n\n            apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n            defaults:\n\n                audit: restricted\n\n                audit-version: latest\n\n                enforce: baseline\n\n                enforce-version: latest\n\n                warn: restricted\n\n                warn-version: latest\n\n            exemptions:\n\n                namespaces:\n\n                    - kube-system\n\n                runtimeClasses: []\n\n                usernames: []\n\n            kind: PodSecurityConfiguration\n\nUsage\n\nCreate a deployment that satisfies the baseline policy but gives warnings on restricted policy:\n\n$ kubectl create deployment nginx --image=nginx\n\nWarning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndeployment.apps/nginx created\n\n$ kubectl get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nnginx-85b98978db-j68l8   1/1     Running   0          2m3s\n\n\nCreate a daemonset which fails to meet requirements of the baseline policy:\n\napiVersion: apps/v1\n\nkind: DaemonSet\n\nmetadata:\n\n  labels:\n\n    app: debug-container\n\n  name: debug-container\n\n  namespace: default\n\nspec:\n\n  revisionHistoryLimit: 10\n\n  selector:\n\n    matchLabels:\n\n      app: debug-container\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: debug-container\n\n    spec:\n\n      containers:\n\n      - args:\n\n        - \"360000\"\n\n        command:\n\n        - /bin/sleep\n\n        image: ubuntu:latest\n\n        imagePullPolicy: IfNotPresent\n\n        name: debug-container\n\n        resources: {}\n\n        securityContext:\n\n          privileged: true\n\n        terminationMessagePath: /dev/termination-log\n\n        terminationMessagePolicy: File\n\n      dnsPolicy: ClusterFirstWithHostNet\n\n      hostIPC: true\n\n      hostPID: true\n\n      hostNetwork: true\n\n      restartPolicy: Always\n\n      schedulerName: default-scheduler\n\n      securityContext: {}\n\n      terminationGracePeriodSeconds: 30\n\n  updateStrategy:\n\n    rollingUpdate:\n\n      maxSurge: 0\n\n      maxUnavailable: 1\n\n    type: RollingUpdate\n\n$ kubectl apply -f debug.yaml\n\nWarning: would violate PodSecurity \"restricted:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container \"debug-container\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"debug-container\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"debug-container\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"debug-container\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndaemonset.apps/debug-container created\n\n\nDaemonset debug-container gets created, but no pods are scheduled:\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   0         0         0       0            0           <none>          34s\n\n\nPod Security Admission plugin errors are in the daemonset events:\n\n$ kubectl describe ds debug-container\n\n...\n\n  Warning  FailedCreate  92s                daemonset-controller  Error creating: pods \"debug-container-kwzdj\" is forbidden: violates PodSecurity \"baseline:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true)\n\n\nPod Security Admission configuration can also be overridden on a namespace level:\n\n$ kubectl label ns default pod-security.kubernetes.io/enforce=privileged\n\nnamespace/default labeled\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   2         2         0       2            0           <none>          4s\n\n\nAs enforce policy was updated to the privileged for the default namespace, debug-container is now successfully running.\n\n7 - Replicated Local Storage\nUsing local storage with OpenEBS Jiva\n\nIf you want to use replicated storage leveraging disk space from a local disk with Talos Linux installed, OpenEBS Jiva is a great option. This requires installing the iscsi-tools system extension.\n\nSince OpenEBS Jiva is a replicated storage, it’s recommended to have at least three nodes where sufficient local disk space is available. The documentation will follow installing OpenEBS Jiva via the offical Helm chart. Since Talos is different from standard Operating Systems, the OpenEBS components need a little tweaking after the Helm installation. Refer to the OpenEBS Jiva documentation if you need further customization.\n\nNB: Also note that the Talos nodes need to be upgraded with --preserve set while running OpenEBS Jiva, otherwise you risk losing data. Even though it’s possible to recover data from other replicas if the node is wiped during an upgrade, this can require extra operational knowledge to recover, so it’s highly recommended to use --preserve to avoid data loss.\n\nPreparing the nodes\n\nCreate the boot assets which includes the iscsi-tools system extensions (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nCreate a machine config patch with the contents below and save as patch.yaml\n\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/openebs/local\n\n        type: bind\n\n        source: /var/openebs/local\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThe extension status can be verified by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get extensions\n\n\nAn output similar to below can be observed:\n\nNODE            NAMESPACE   TYPE              ID                                          VERSION   NAME          VERSION\n\n192.168.20.61   runtime     ExtensionStatus   000.ghcr.io-siderolabs-iscsi-tools-v0.1.1   1         iscsi-tools   v0.1.1\n\n\nThe service status can be checked by running the following command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> services\n\n\nYou should see that the ext-tgtd and the ext-iscsid services are running.\n\nNODE            SERVICE      STATE     HEALTH   LAST CHANGE     LAST EVENT\n\n192.168.20.51   apid         Running   OK       64h57m15s ago   Health check successful\n\n192.168.20.51   containerd   Running   OK       64h57m23s ago   Health check successful\n\n192.168.20.51   cri          Running   OK       64h57m20s ago   Health check successful\n\n192.168.20.51   etcd         Running   OK       64h55m29s ago   Health check successful\n\n192.168.20.51   ext-iscsid   Running   ?        64h57m19s ago   Started task ext-iscsid (PID 4040) for container ext-iscsid\n\n192.168.20.51   ext-tgtd     Running   ?        64h57m19s ago   Started task ext-tgtd (PID 3999) for container ext-tgtd\n\n192.168.20.51   kubelet      Running   OK       38h14m10s ago   Health check successful\n\n192.168.20.51   machined     Running   ?        64h57m29s ago   Service started as goroutine\n\n192.168.20.51   trustd       Running   OK       64h57m19s ago   Health check successful\n\n192.168.20.51   udevd        Running   OK       64h57m21s ago   Health check successful\n\nInstall OpenEBS Jiva\nhelm repo add openebs-jiva https://openebs.github.io/jiva-operator\n\nhelm repo update\n\nhelm upgrade --install --create-namespace --namespace openebs --version 3.2.0 openebs-jiva openebs-jiva/jiva\n\n\nThis will create a storage class named openebs-jiva-csi-default which can be used for workloads. The storage class named openebs-hostpath is used by jiva to create persistent volumes backed by local storage and then used for replicated storage by the jiva controller.\n\nPatching the Namespace\n\nwhen using the default Pod Security Admissions created by Talos you need the following labels on your namespace:\n\n    pod-security.kubernetes.io/audit: privileged\n\n    pod-security.kubernetes.io/enforce: privileged\n\n    pod-security.kubernetes.io/warn: privileged\n\n\nor via kubectl:\n\nkubectl label ns openebs pod-security.kubernetes.io/audit=privileged pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/warn=privileged\n\nNumber of Replicas\n\nBy Default Jiva uses 3 replicas if your cluster consists of lesser nodes consider setting defaultPolicy.replicas to the number of nodes in your cluster e.g. 2.\n\nPatching the jiva installation\n\nSince Jiva assumes iscisd to be running natively on the host and not as a Talos extension service, we need to modify the CSI node daemonset to enable it to find the PID of the iscsid service. The default config map used by Jiva also needs to be modified so that it can execute iscsiadm commands inside the PID namespace of the iscsid service.\n\nStart by creating a configmap definition named config.yaml as below:\n\napiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\n  labels:\n\n    app.kubernetes.io/managed-by: pulumi\n\n  name: openebs-jiva-csi-iscsiadm\n\n  namespace: openebs\n\ndata:\n\n  iscsiadm: |\n\n    #!/bin/sh\n\n    iscsid_pid=$(pgrep iscsid)\n\n\n\n    nsenter --mount=\"/proc/${iscsid_pid}/ns/mnt\" --net=\"/proc/${iscsid_pid}/ns/net\" -- /usr/local/sbin/iscsiadm \"$@\"    \n\n\nReplace the existing config map with the above config map by running the following command:\n\nkubectl --namespace openebs apply --filename config.yaml\n\n\nNow we need to update the jiva CSI daemonset to run with hostPID: true so it can find the PID of the iscsid service, by running the following command:\n\nkubectl --namespace openebs patch daemonset openebs-jiva-csi-node --type=json --patch '[{\"op\": \"add\", \"path\": \"/spec/template/spec/hostPID\", \"value\": true}]'\n\nTesting a simple workload\n\nIn order to test the Jiva installation, let’s first create a PVC referencing the openebs-jiva-csi-default storage class:\n\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: example-jiva-csi-pvc\n\nspec:\n\n  storageClassName: openebs-jiva-csi-default\n\n  accessModes:\n\n    - ReadWriteOnce\n\n  resources:\n\n    requests:\n\n      storage: 4Gi\n\n\nand then create a deployment using the above PVC:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: fio\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      name: fio\n\n  replicas: 1\n\n  strategy:\n\n    type: Recreate\n\n    rollingUpdate: null\n\n  template:\n\n    metadata:\n\n      labels:\n\n        name: fio\n\n    spec:\n\n      containers:\n\n      - name: perfrunner\n\n        image: openebs/tests-fio\n\n        command: [\"/bin/bash\"]\n\n        args: [\"-c\", \"while true ;do sleep 50; done\"]\n\n        volumeMounts:\n\n        - mountPath: /datadir\n\n          name: fio-vol\n\n      volumes:\n\n      - name: fio-vol\n\n        persistentVolumeClaim:\n\n          claimName: example-jiva-csi-pvc\n\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete deployment fio\n\nkubectl delete pvc example-jiva-csi-pvc\n\n8 - Seccomp Profiles\nUsing custom Seccomp Profiles with Kubernetes workloads.\n\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.\n\nRefer the Kubernetes Seccomp Guide for more details.\n\nIn this guide we are going to configure a custom Seccomp Profile that logs all syscalls made by the workload.\n\nPreparing the nodes\n\nCreate a machine config path with the contents below and save as patch.yaml\n\nmachine:\n\n  seccompProfiles:\n\n    - name: audit.json\n\n      value:\n\n        defaultAction: SCMP_ACT_LOG\n\n\nApply the machine config to all the nodes using talosctl:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThis would create a seccomp profile name audit.json on the node at /var/lib/kubelet/seccomp/profiles.\n\nThe profiles can be used by Kubernetes pods by specfying the pod securityContext as below:\n\nspec:\n\n  securityContext:\n\n    seccompProfile:\n\n      type: Localhost\n\n      localhostProfile: profiles/audit.json\n\n\nNote that the localhostProfile uses the name of the profile created under profiles directory. So make sure to use path as profiles/<profile-name.json>\n\nThis can be verfied by running the below commands:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get seccompprofiles\n\n\nAn output similar to below can be observed:\n\nNODE       NAMESPACE   TYPE             ID           VERSION\n\n10.5.0.3   cri         SeccompProfile   audit.json   1\n\n\nThe content of the seccomp profile can be viewed by running the below command:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> read /var/lib/kubelet/seccomp/profiles/audit.json\n\n\nAn output similar to below can be observed:\n\n{\"defaultAction\":\"SCMP_ACT_LOG\"}\n\nCreate a Kubernetes workload that uses the custom Seccomp Profile\n\nHere we’ll be using an example workload from the Kubernetes documentation.\n\nFirst open up a second terminal and run the following talosctl command so that we can view the Syscalls being logged in realtime:\n\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> dmesg --follow --tail\n\n\nNow deploy the example workload from the Kubernetes documentation:\n\nkubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml\n\n\nOnce the pod starts running the terminal running talosctl dmesg command from above should log similar to below:\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.489473063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.490852063Z]: cni0: port 1(veth32488a86) entered disabled state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.492470063Z]: device veth32488a86 entered promiscuous mode\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503105063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503944063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): veth32488a86: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.504764063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.505423063Z]: cni0: port 1(veth32488a86) entered forwarding state\n\n10.5.0.3: kern: warning: [2022-07-28T11:49:44.873616063Z]: kauditd_printk_skb: 14 callbacks suppressed\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.873619063Z]: audit: type=1326 audit(1659008985.445:25): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.876609063Z]: audit: type=1326 audit(1659008985.445:26): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.878789063Z]: audit: type=1326 audit(1659008985.449:27): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=257 compat=0 ip=0x55ec0657bdaa code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.886693063Z]: audit: type=1326 audit(1659008985.461:28): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.888764063Z]: audit: type=1326 audit(1659008985.461:29): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.891009063Z]: audit: type=1326 audit(1659008985.461:30): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=1 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.893162063Z]: audit: type=1326 audit(1659008985.461:31): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.895365063Z]: audit: type=1326 audit(1659008985.461:32): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=39 compat=0 ip=0x55ec066eb68b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.898306063Z]: audit: type=1326 audit(1659008985.461:33): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=59 compat=0 ip=0x55ec0657be16 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.901518063Z]: audit: type=1326 audit(1659008985.473:34): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"http-echo\" exe=\"/http-echo\" sig=0 arch=c000003e syscall=158 compat=0 ip=0x455f35 code=0x7ffc0000\n\nCleanup\n\nYou can clean up the test resources by running the following command:\n\nkubectl delete pod audit-pod\n\n9 - Storage\nSetting up storage for a Kubernetes cluster\n\nIn Kubernetes, using storage in the right way is well-facilitated by the API. However, unless you are running in a major public cloud, that API may not be hooked up to anything. This frequently sends users down a rabbit hole of researching all the various options for storage backends for their platform, for Kubernetes, and for their workloads. There are a lot of options out there, and it can be fairly bewildering.\n\nFor Talos, we try to limit the options somewhat to make the decision-making easier.\n\nPublic Cloud\n\nIf you are running on a major public cloud, use their block storage. It is easy and automatic.\n\nStorage Clusters\n\nSidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\nRedundancy, scaling capabilities, reliability, speed, maintenance load, and ease of use are all factors you must consider when managing your own storage.\n\nRunning a storage cluster can be a very good choice when managing your own storage, and there are two projects we recommend, depending on your situation.\n\nIf you need vast amounts of storage composed of more than a dozen or so disks, we recommend you use Rook to manage Ceph. Also, if you need both mount-once and mount-many capabilities, Ceph is your answer. Ceph also bundles in an S3-compatible object store. The down side of Ceph is that there are a lot of moving parts.\n\nPlease note that most people should never use mount-many semantics. NFS is pervasive because it is old and easy, not because it is a good idea. While it may seem like a convenience at first, there are all manner of locking, performance, change control, and reliability concerns inherent in any mount-many situation, so we strongly recommend you avoid this method.\n\nIf your storage needs are small enough to not need Ceph, use Mayastor.\n\nRook/Ceph\n\nCeph is the grandfather of open source storage clusters. It is big, has a lot of pieces, and will do just about anything. It scales better than almost any other system out there, open source or proprietary, being able to easily add and remove storage over time with no downtime, safely and easily. It comes bundled with RadosGW, an S3-compatible object store; CephFS, a NFS-like clustered filesystem; and RBD, a block storage system.\n\nWith the help of Rook, the vast majority of the complexity of Ceph is hidden away by a very robust operator, allowing you to control almost everything about your Ceph cluster from fairly simple Kubernetes CRDs.\n\nSo if Ceph is so great, why not use it for everything?\n\nCeph can be rather slow for small clusters. It relies heavily on CPUs and massive parallelisation to provide good cluster performance, so if you don’t have much of those dedicated to Ceph, it is not going to be well-optimised for you. Also, if your cluster is small, just running Ceph may eat up a significant amount of the resources you have available.\n\nTroubleshooting Ceph can be difficult if you do not understand its architecture. There are lots of acronyms and the documentation assumes a fair level of knowledge. There are very good tools for inspection and debugging, but this is still frequently seen as a concern.\n\nMayastor\n\nMayastor is an OpenEBS project built in Rust utilising the modern NVMEoF system. (Despite the name, Mayastor does not require you to have NVME drives.) It is fast and lean but still cluster-oriented and cloud native. Unlike most of the other OpenEBS project, it is not built on the ancient iSCSI system.\n\nUnlike Ceph, Mayastor is just a block store. It focuses on block storage and does it well. It is much less complicated to set up than Ceph, but you probably wouldn’t want to use it for more than a few dozen disks.\n\nMayastor is new, maybe too new. If you’re looking for something well-tested and battle-hardened, this is not it. However, if you’re looking for something lean, future-oriented, and simpler than Ceph, it might be a great choice.\n\nVideo Walkthrough\n\nTo see a live demo of this section, see the video below:\n\nPrep Nodes\n\nEither during initial cluster creation or on running worker nodes, several machine config values should be edited. (This information is gathered from the Mayastor documentation.) We need to set the vm.nr_hugepages sysctl and add openebs.io/engine=mayastor labels to the nodes which are meant to be storage nodes. This can be done with talosctl patch machineconfig or via config patches during talosctl gen config.\n\nSome examples are shown below: modify as needed.\n\nFirst create a config patch file named mayastor-patch.yaml with the following contents:\n\n- op: add\n\n  path: /machine/sysctls\n\n  value:\n\n    vm.nr_hugepages: \"1024\"\n\n- op: add\n\n  path: /machine/nodeLabels\n\n  value:\n\n    openebs.io/engine: mayastor\n\n\nUsing gen config\n\ntalosctl gen config my-cluster https://mycluster.local:6443 --config-patch @mayastor-patch.yaml\n\n\nPatching an existing node\n\ntalosctl patch --mode=no-reboot machineconfig -n <node ip> --patch @mayastor-patch.yaml\n\n\nNote: If you are adding/updating the vm.nr_hugepages on a node which already had the openebs.io/engine=mayastor label set, you’d need to restart kubelet so that it picks up the new value, by issuing the following command\n\ntalosctl -n <node ip> service kubelet restart\n\nDeploy Mayastor\n\nContinue setting up Mayastor using the official documentation.\n\nPiraeus / LINSTOR\nPiraeus-Operator\nLINSTOR\nDRBD Extension\nInstall Piraeus Operator V2\n\nThere is already a how-to for Talos: Link\n\nCreate first storage pool and PVC\n\nBefore proceeding, install linstor plugin for kubectl: https://github.com/piraeusdatastore/kubectl-linstor\n\nOr use krew: kubectl krew install linstor\n\n# Create device pool on a blank (no partitation table!) disk on node01\n\nkubectl linstor physical-storage create-device-pool --pool-name nvme_lvm_pool LVM node01 /dev/nvme0n1 --storage-pool nvme_pool\n\n\npiraeus-sc.yml\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  name: simple-nvme\n\nparameters:\n\n  csi.storage.k8s.io/fstype: xfs\n\n  linstor.csi.linbit.com/autoPlace: \"3\"\n\n  linstor.csi.linbit.com/storagePool: nvme_pool\n\nprovisioner: linstor.csi.linbit.com\n\nvolumeBindingMode: WaitForFirstConsumer\n\n# Create storage class\n\nkubectl apply -f piraeus-sc.yml\n\nNFS\n\nNFS is an old pack animal long past its prime. NFS is slow, has all kinds of bottlenecks involving contention, distributed locking, single points of service, and more. However, it is supported by a wide variety of systems. You don’t want to use it unless you have to, but unfortunately, that “have to” is too frequent.\n\nThe NFS client is part of the kubelet image maintained by the Talos team. This means that the version installed in your running kubelet is the version of NFS supported by Talos. You can reduce some of the contention problems by parceling Persistent Volumes from separate underlying directories.\n\nObject storage\n\nCeph comes with an S3-compatible object store, but there are other options, as well. These can often be built on top of other storage backends. For instance, you may have your block storage running with Mayastor but assign a Pod a large Persistent Volume to serve your object store.\n\nOne of the most popular open source add-on object stores is MinIO.\n\nOthers (iSCSI)\n\nThe most common remaining systems involve iSCSI in one form or another. These include the original OpenEBS, Rancher’s Longhorn, and many proprietary systems. iSCSI in Linux is facilitated by open-iscsi. This system was designed long before containers caught on, and it is not well suited to the task, especially when coupled with a read-only host operating system.\n\niSCSI support in Talos is now supported via the iscsi-tools system extension installed. The extension enables compatibility with OpenEBS Jiva - refer to the local storage installation guide for more information.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nNetwork\nSet up networking layers for Talos Linux\n1: Corporate Proxies\n2: Ingress Firewall\n3: KubeSpan\n4: Network Device Selector\n5: Predictable Interface Names\n6: Virtual (shared) IP\n7: Wireguard Network\n1 - Corporate Proxies\nHow to configure Talos Linux to use proxies in a corporate environment\nAppending the Certificate Authority of MITM Proxies\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\nConfiguring a Machine to Use the Proxy\n\nTo make use of a proxy:\n\nmachine:\n\n  env:\n\n    http_proxy: <http proxy>\n\n    https_proxy: <https proxy>\n\n    no_proxy: <no proxy>\n\n\nAdditionally, configure the DNS nameservers, and NTP servers:\n\nmachine:\n\n  env:\n\n  ...\n\n  time:\n\n    servers:\n\n      - <server 1>\n\n      - <server ...>\n\n      - <server n>\n\n  ...\n\n  network:\n\n    nameservers:\n\n      - <ip 1>\n\n      - <ip ...>\n\n      - <ip n>\n\n\nIf a proxy is required before Talos machine configuration is applied, use kernel command line arguments:\n\ntalos.environment=http_proxy=<http-proxy> talos.environment=https_proxy=<https-proxy>\n\n2 - Ingress Firewall\nLearn to use Talos Linux Ingress Firewall to limit access to the host services.\n\nTalos Linux Ingress Firewall is a simple and effective way to limit access to the services running on the host, which includes both Talos standard services (e.g. apid and kubelet), and any additional workloads that may be running on the host. Talos Linux Ingress Firewall doesn’t affect the traffic between the Kubernetes pods/services, please use CNI Network Policies for that.\n\nConfiguration\n\nIngress rules are configured as extra documents NetworkDefaultActionConfig and NetworkRuleConfig in the Talos machine configuration:\n\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 172.20.0.0/24\n\n    except: 172.20.0.1/32\n\n\nThe first document configures the default action for the ingress traffic, which can be either accept or block, with the default being accept. If the default action is set to accept, then all the ingress traffic will be allowed, unless there is a matching rule that blocks it. If the default action is set to block, then all the ingress traffic will be blocked, unless there is a matching rule that allows it.\n\nWith either accept or block, the traffic is always allowed on the following network interfaces:\n\nlo\nsiderolink\nkubespan\n\nIn the block mode:\n\nICMP and ICMPv6 traffic is also allowed with a rate limit of 5 packets per second\ntraffic between Kubernetes pod/service subnets is allowed (for native routing CNIs)\n\nThe second document defines an ingress rule for a set of ports and protocols on the host. The NetworkRuleConfig might be repeated many times to define multiple rules, but each document must have a unique name.\n\nThe ports field accepts either a single port or a port range:\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n    - 10260\n\n    - 10300-10400\n\n\nThe protocol might be either tcp or udp.\n\nThe ingress specifies the list of subnets that are allowed to access the host services, with the optional except field to exclude a set of addresses from the subnet.\n\nNote: incorrect configuration of the ingress firewall might result in the host becoming inaccessible over Talos API. The configuration might be applied in --mode=try to make sure it gets reverted in case of a mistake.\n\nRecommended Rules\n\nThe following rules improve the security of the cluster and cover only standard Talos services. If there are additional services running with host networking in the cluster, they should be covered by additional rules.\n\nIn the block mode, the ingress firewall will also block encapsulated traffic (e.g. VXLAN) between the nodes, which needs to be explicitly allowed for the Kubernetes networking to function properly. Please refer to the CNI documentation for the specifics, some default configurations are listed below:\n\nFlannel, Calico: vxlan UDP port 4789\nCilium: vxlan UDP port 8472\n\nIn the examples we assume following template variables to describe the cluster:\n\n$CLUSTER_SUBNET, e.g. 172.20.0.0/24 - the subnet which covers all machines in the cluster\n$CP1, $CP2, $CP3 - the IP addresses of the controlplane nodes\n$VXLAN_PORT - the UDP port used by the CNI for encapsulated traffic\nControlplane\napid and Kubernetes API are wide open\nkubelet and trustd API is only accessible within the cluster\netcd API is limited to controlplane nodes\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: trustd-ingress\n\nportSelector:\n\n  ports:\n\n    - 50001\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubernetes-api-ingress\n\nportSelector:\n\n  ports:\n\n    - 6443\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: etcd-ingress\n\nportSelector:\n\n  ports:\n\n    - 2379-2380\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CP1/32\n\n  - subnet: $CP2/32\n\n  - subnet: $CP3/32\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nWorker\nkubelet and apid API is only accessible within the cluster\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nLearn More\n\nTalos Linux Ingress Firewall is using the nftables to perform the filtering.\n\nWith the default action set to accept, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy accept;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ip saddr != { 172.20.0.0/24 } tcp dport { 10250 } drop\n\n    meta nfproto ipv6 tcp dport { 10250 } drop\n\n  }\n\n}\n\n\nWith the default action set to block, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy drop;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ct state { established, related } accept\n\n    ct state invalid drop\n\n    meta l4proto icmp limit rate 5/second accept\n\n    meta l4proto ipv6-icmp limit rate 5/second accept\n\n    ip saddr { 172.20.0.0/24 } tcp dport { 10250 }  accept\n\n    meta nfproto ipv4 tcp dport { 50000 } accept\n\n    meta nfproto ipv6 tcp dport { 50000 } accept\n\n  }\n\n}\n\n3 - KubeSpan\nLearn to use KubeSpan to connect Talos Linux machines securely across networks.\n\nKubeSpan is a feature of Talos that automates the setup and maintenance of a full mesh WireGuard network for your cluster, giving you the ability to operate hybrid Kubernetes clusters that can span the edge, datacenter, and cloud. Management of keys and discovery of peers can be completely automated, making it simple and easy to create hybrid clusters.\n\nKubeSpan consists of client code in Talos Linux, as well as a discovery service that enables clients to securely find each other. Sidero Labs operates a free Discovery Service, but the discovery service may, with a commercial license, be operated by your organization and can be downloaded here.\n\nVideo Walkthrough\n\nTo see a live demo of KubeSpan, see one the videos below:\n\n \nNetwork Requirements\n\nKubeSpan uses UDP port 51820 to carry all KubeSpan encrypted traffic. Because UDP traversal of firewalls is often lenient, and the Discovery Service communicates the apparent IP address of all peers to all other peers, KubeSpan will often work automatically, even when each nodes is behind their own firewall. However, when both ends of a KubeSpan connection are behind firewalls, it is possible the connection may not be established correctly - it depends on each end sending out packets in a limited time window.\n\nThus best practice is to ensure that one end of all possible node-node communication allows UDP port 51820, inbound.\n\nFor example, if control plane nodes are running in a corporate data center, behind firewalls, KubeSpan connectivity will work correctly so long as worker nodes on the public Internet can receive packets on UDP port 51820. (Note the workers will also need to receive TCP port 50000 for initial configuration via talosctl).\n\nAn alternative topology would be to run control plane nodes in a public cloud, and allow inbound UDP port 51820 to the control plane nodes. Workers could be behind firewalls, and KubeSpan connectivity will be established. Note that if workers are in different locations, behind different firewalls, the KubeSpan connectivity between workers should be correctly established, but may require opening the KubeSpan UDP port on the local firewall also.\n\nCaveats\nKubernetes API Endpoint Limitations\n\nWhen the K8s endpoint is an IP address that is not part of Kubespan, but is an address that is forwarded on to the Kubespan address of a control plane node, without changing the source address, then worker nodes will fail to join the cluster. In such a case, the control plane node has no way to determine whether the packet arrived on the private Kubespan address, or the public IP address. If the source of the packet was a Kubespan member, the reply will be Kubespan encapsulated, and thus not translated to the public IP, and so the control plane will reply to the session with the wrong address.\n\nThis situation is seen, for example, when the Kubernetes API endpoint is the public IP of a VM in GCP or Azure for a single node control plane. The control plane will receive packets on the public IP, but will reply from it’s KubeSpan address. The workaround is to create a load balancer to terminate the Kubernetes API endpoint.\n\nDigital Ocean Limitations\n\nDigital Ocean assigns an “Anchor IP” address to each droplet. Talos Linux correctly identifies this as a link-local address, and configures KubeSpan correctly, but this address will often be selected by Flannel or other CNIs as a node’s private IP. Because this address is not routable, nor advertised via KubeSpan, it will break pod-pod communication between nodes. This can be worked-around by assigning a non-Anchor private IP:\n\nkubectl annotate node do-worker flannel.alpha.coreos.com/public-ip-overwrite=10.116.X.X\n\nThen restarting flannel: kubectl delete pods -n kube-system -l k8s-app=flannel\n\nEnabling\nCreating a New Cluster\n\nTo enable KubeSpan for a new cluster, we can use the --with-kubespan flag in talosctl gen config. This will enable peer discovery and KubeSpan.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\n\ncluster:\n\n    discovery:\n\n        enabled: true\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            kubernetes: # Kubernetes registry is problematic with KubeSpan, if the control plane endpoint is routeable itself via KubeSpan.\n\n              disabled: true\n\n            service: {}\n\n\nThe default discovery service is an external service hosted by Sidero Labs at https://discovery.talos.dev/. Contact Sidero Labs if you need to run this service privately.\n\nEnabling for an Existing Cluster\n\nIn order to enable KubeSpan on an existing cluster, enable kubespan and discovery settings in the machine config for each machine in the cluster (discovery is enabled by default):\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: true\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\nConfiguration\n\nKubeSpan will automatically discovery all cluster members, exchange Wireguard public keys and establish a full mesh network.\n\nThere are configuration options available which are not usually required:\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: false\n\n      advertiseKubernetesNetworks: false\n\n      allowDownPeerBypass: false\n\n      mtu: 1420\n\n      filters:\n\n        endpoints:\n\n          - 0.0.0.0/0\n\n          - ::/0\n\n\nThe setting advertiseKubernetesNetworks controls whether the node will advertise Kubernetes service and pod networks to other nodes in the cluster over KubeSpan. It defaults to being disabled, which means KubeSpan only controls the node-to-node traffic, while pod-to-pod traffic is routed and encapsulated by CNI. This setting should not be enabled with Calico and Cilium CNI plugins, as they do their own pod IP allocation which is not visible to KubeSpan.\n\nThe setting allowDownPeerBypass controls whether the node will allow traffic to bypass WireGuard if the destination is not connected over KubeSpan. If enabled, there is a risk that traffic will be routed unencrypted if the destination is not connected over KubeSpan, but it allows a workaround for the case where a node is not connected to the KubeSpan network, but still needs to access the cluster.\n\nThe mtu setting configures the Wireguard MTU, which defaults to 1420. This default value of 1420 is safe to use when the underlying network MTU is 1500, but if the underlying network MTU is smaller, the KubeSpanMTU should be adjusted accordingly: KubeSpanMTU = UnderlyingMTU - 80.\n\nThe filters setting allows hiding some endpoints from being advertised over KubeSpan. This is useful when some endpoints are known to be unreachable between the nodes, so that KubeSpan doesn’t try to establish a connection to them. Another use-case is hiding some endpoints if nodes can connect on multiple networks, and some of the networks are more preferable than others.\n\nResource Definitions\nKubeSpanIdentities\n\nA node’s WireGuard identities can be obtained with:\n\n$ talosctl get kubespanidentities -o yaml\n\n...\n\nspec:\n\n    address: fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94/128\n\n    subnet: fd83:b1f7:fcb5:2802::/64\n\n    privateKey: gNoasoKOJzl+/B+uXhvsBVxv81OcVLrlcmQ5jQwZO08=\n\n    publicKey: NzW8oeIH5rJyY5lefD9WRoHWWRr/Q6DwsDjMX+xKjT4=\n\n\nTalos automatically configures unique IPv6 address for each node in the cluster-specific IPv6 ULA prefix.\n\nThe Wireguard private key is generated and never leaves the node, while the public key is published through the cluster discovery.\n\nKubeSpanIdentity is persisted across reboots and upgrades in STATE partition in the file kubespan-identity.yaml.\n\nKubeSpanPeerSpecs\n\nA node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerspecs\n\nID                                             VERSION   LABEL                          ENDPOINTS\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   2         talos-default-controlplane-2   [\"172.20.0.3:51820\"]\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   2         talos-default-controlplane-3   [\"172.20.0.4:51820\"]\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   2         talos-default-worker-2         [\"172.20.0.6:51820\"]\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   2         talos-default-worker-1         [\"172.20.0.5:51820\"]\n\n\nThe peer ID is the Wireguard public key. KubeSpanPeerSpecs are built from the cluster discovery data.\n\nKubeSpanPeerStatuses\n\nThe status of a node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerstatuses\n\nID                                             VERSION   LABEL                          ENDPOINT           STATE   RX         TX\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   63        talos-default-controlplane-2   172.20.0.3:51820   up      15043220   17869488\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   62        talos-default-controlplane-3   172.20.0.4:51820   up      14573208   18157680\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   60        talos-default-worker-2         172.20.0.6:51820   up      130072     46888\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   60        talos-default-worker-1         172.20.0.5:51820   up      130044     46556\n\n\nKubeSpan peer status includes following information:\n\nthe actual endpoint used for peer communication\nlink state:\nunknown: the endpoint was just changed, link state is not known yet\nup: there is a recent handshake from the peer\ndown: there is no handshake from the peer\nnumber of bytes sent/received over the Wireguard link with the peer\n\nIf the connection state goes down, Talos will be cycling through the available endpoints until it finds the one which works.\n\nPeer status information is updated every 30 seconds.\n\nKubeSpanEndpoints\n\nA node’s WireGuard endpoints (peer addresses) can be obtained with:\n\n$ talosctl get kubespanendpoints\n\nID                                             VERSION   ENDPOINT           AFFILIATE ID\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   1         172.20.0.3:51820   2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   1         172.20.0.4:51820   b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   1         172.20.0.6:51820   NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   1         172.20.0.5:51820   6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA\n\n\nThe endpoint ID is the base64 encoded WireGuard public key.\n\nThe observed endpoints are submitted back to the discovery service (if enabled) so that other peers can try additional endpoints to establish the connection.\n\n4 - Network Device Selector\nHow to configure network devices by selecting them using hardware information\nConfiguring Network Device Using Device Selector\n\ndeviceSelector is an alternative method of configuring a network device:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          driver: virtio\n\n          hardwareAddr: \"00:00:*\"\n\n        address: 192.168.88.21\n\n\nSelector has the following traits:\n\nqualifiers match a device by reading the hardware information in /sys/class/net/...\nqualifiers are applied using logical AND\nmachine.network.interfaces.deviceConfig option is mutually exclusive with machine.network.interfaces.interface\nif the selector matches multiple devices, the controller will apply config to all of them\n\nThe available hardware information used in the selector can be observed in the LinkStatus resource (works in maintenance mode):\n\n# talosctl get links eth0 -o yaml\n\nspec:\n\n  ...\n\n  hardwareAddr: 4e:95:8e:8f:e4:47\n\n  busPath: 0000:06:00.0\n\n  driver: alx\n\n  pciID: 1969:E0B1\n\nUsing Device Selector for Bonding\n\nDevice selectors can be used to configure bonded interfaces:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        bond:\n\n          mode: balance-rr\n\n          deviceSelectors:\n\n            - hardwareAddr: '00:50:56:8e:8f:e4'\n\n            - hardwareAddr: '00:50:57:9c:2c:2d'\n\n\nIn this example, the bond0 interface will be created and bonded using two devices with the specified hardware addresses.\n\n5 - Predictable Interface Names\nHow to use predictable interface naming.\n\nStarting with version Talos 1.5, network interfaces are renamed to predictable names same way as systemd does that in other Linux distributions.\n\nThe naming schema enx78e7d1ea46da (based on MAC addresses) is enabled by default, the order of interface naming decisions is:\n\nfirmware/BIOS provided index numbers for on-board devices (example: eno1)\nfirmware/BIOS provided PCI Express hotplug slot index numbers (example: ens1)\nphysical/geographical location of the connector of the hardware (example: enp2s0)\ninterfaces’s MAC address (example: enx78e7d1ea46da)\n\nThe predictable network interface names features can be disabled by specifying net.ifnames=0 in the kernel command line.\n\nNote: Talos automatically adds the net.ifnames=0 kernel argument when upgrading from Talos versions before 1.5, so upgrades to 1.5 don’t require any manual intervention.\n\n“Cloud” platforms, like AWS, still use old eth0 naming scheme as Talos automatically adds net.ifnames=0 to the kernel command line.\n\nSingle Network Interface\n\nWhen running Talos on a machine with a single network interface, predictable interface names might be confusing, as it might come up as enxSOMETHING which is hard to address. There are two ways to solve this:\n\ndisable the feature by supplying net.ifnames=0 to the initial boot of Talos, Talos will persist net.ifnames=0 over installs/upgrades.\n\nuse device selectors:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n        # any configuration can follow, e.g:\n\n        addresses: [10.3.4.5/24]\n\n6 - Virtual (shared) IP\nUsing Talos Linux to set up a floating virtual IP address for cluster access.\n\nOne of the pain points when building a high-availability controlplane is giving clients a single IP or URL at which they can reach any of the controlplane nodes. The most common approaches - reverse proxy, load balancer, BGP, and DNS - all require external resources, and add complexity in setting up Kubernetes.\n\nTo simplify cluster creation, Talos Linux supports a “Virtual” IP (VIP) address to access the Kubernetes API server, providing high availability with no other resources required.\n\nWhat happens is that the controlplane machines vie for control of the shared IP address using etcd elections. There can be only one owner of the IP address at any given time. If that owner disappears or becomes non-responsive, another owner will be chosen, and it will take up the IP address.\n\nRequirements\n\nThe controlplane nodes must share a layer 2 network, and the virtual IP must be assigned from that shared network subnet. In practical terms, this means that they are all connected via a switch, with no router in between them. Note that the virtual IP election depends on etcd being up, as Talos uses etcd for elections and leadership (control) of the IP address.\n\nThe virtual IP is not restricted by ports - you can access any port that the control plane nodes are listening on, on that IP address. Thus it is possible to access the Talos API over the VIP, but it is not recommended, as you cannot access the VIP when etcd is down - and then you could not access the Talos API to recover etcd.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nChoose your Shared IP\n\nThe Virtual IP should be a reserved, unused IP address in the same subnet as your controlplane nodes. It should not be assigned or assignable by your DHCP server.\n\nFor our example, we will assume that the controlplane nodes have the following IP addresses:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nWe then choose our shared IP to be:\n\n192.168.0.15\nConfigure your Talos Machines\n\nThe shared IP setting is only valid for controlplane nodes.\n\nFor the example above, each of the controlplane nodes should have the following Machine Config snippet:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n\nVirtual IP’s can also be configured on a VLAN interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n      vlans:\n\n        - vlanId: 100\n\n          dhcp: true\n\n          vip:\n\n            ip: 192.168.1.15\n\n\nFor your own environment, the interface and the DHCP setting may differ, or you may use static addressing (adresses) instead of DHCP.\n\nWhen using predictable interface names, the interface name might not be eth0.\n\nIf the machine has a single network interface, it can be selected using a dummy device selector:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\nCaveats\n\nSince VIP functionality relies on etcd for elections, the shared IP will not come alive until after you have bootstrapped Kubernetes.\n\nDon’t use the VIP as the endpoint in the talosconfig, as the VIP is bound to etcd and kube-apiserver health, and you will not be able to recover from a failure of either of those components using Talos API.\n\n7 - Wireguard Network\nA guide on how to set up Wireguard network using Kernel module.\nConfiguring Wireguard Network\nQuick Start\n\nThe quickest way to try out Wireguard is to use talosctl cluster create command:\n\ntalosctl cluster create --wireguard-cidr 10.1.0.0/24\n\n\nIt will automatically generate Wireguard network configuration for each node with the following network topology:\n\nWhere all controlplane nodes will be used as Wireguard servers which listen on port 51111. All controlplanes and workers will connect to all controlplanes. It also sets PersistentKeepalive to 5 seconds to establish controlplanes to workers connection.\n\nAfter the cluster is deployed it should be possible to verify Wireguard network connectivity. It is possible to deploy a container with hostNetwork enabled, then do kubectl exec <container> /bin/bash and either do:\n\nping 10.1.0.2\n\n\nOr install wireguard-tools package and run:\n\nwg show\n\n\nWireguard show should output something like this:\n\ninterface: wg0\n\n  public key: OMhgEvNIaEN7zeCLijRh4c+0Hwh3erjknzdyvVlrkGM=\n\n  private key: (hidden)\n\n  listening port: 47946\n\n\n\npeer: 1EsxUygZo8/URWs18tqB5FW2cLVlaTA+lUisKIf8nh4=\n\n  endpoint: 10.5.0.2:51111\n\n  allowed ips: 10.1.0.0/24\n\n  latest handshake: 1 minute, 55 seconds ago\n\n  transfer: 3.17 KiB received, 3.55 KiB sent\n\n  persistent keepalive: every 5 seconds\n\n\nIt is also possible to use generated configuration as a reference by pulling generated config files using:\n\ntalosctl read -n 10.5.0.2 /system/state/config.yaml > controlplane.yaml\n\ntalosctl read -n 10.5.0.3 /system/state/config.yaml > worker.yaml\n\nManual Configuration\n\nAll Wireguard configuration can be done by changing Talos machine config files. As an example we will use this official Wireguard quick start tutorial.\n\nKey Generation\n\nThis part is exactly the same:\n\nwg genkey | tee privatekey | wg pubkey > publickey\n\nSetting up Device\n\nInline comments show relations between configs and wg quickstart tutorial commands:\n\n...\n\nnetwork:\n\n  interfaces:\n\n    ...\n\n      # ip link add dev wg0 type wireguard\n\n    - interface: wg0\n\n      mtu: 1500\n\n      # ip address add dev wg0 192.168.2.1/24\n\n      addresses:\n\n        - 192.168.2.1/24\n\n      # wg set wg0 listen-port 51820 private-key /path/to/private-key peer ABCDEF... allowed-ips 192.168.88.0/24 endpoint 209.202.254.14:8172\n\n      wireguard:\n\n        privateKey: <privatekey file contents>\n\n        listenPort: 51820\n\n        peers:\n\n          allowedIPs:\n\n            - 192.168.88.0/24\n\n          endpoint: 209.202.254.14.8172\n\n          publicKey: ABCDEF...\n\n...\n\n\nWhen networkd gets this configuration it will create the device, configure it and will bring it up (equivalent to ip link set up dev wg0).\n\nAll supported config parameters are described in the Machine Config Reference.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Advanced Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nAdvanced Guides\n1: Advanced Networking\n2: Air-gapped Environments\n3: Building Custom Talos Images\n4: Customizing the Kernel\n5: Customizing the Root Filesystem\n6: Developing Talos\n7: Disaster Recovery\n8: etcd Maintenance\n9: Extension Services\n10: Machine Configuration OAuth2 Authentication\n11: Metal Network Configuration\n12: Migrating from Kubeadm\n13: Proprietary Kernel Modules\n14: Static Pods\n15: Talos API access from Kubernetes\n16: Verifying Images\n1 - Advanced Networking\nHow to configure advanced networking options on Talos Linux.\nStatic Addressing\n\nStatic addressing is comprised of specifying addresses, routes ( remember to add your default gateway ), and interface. Most likely you’ll also want to define the nameservers so you have properly functioning DNS.\n\nmachine:\n\n  network:\n\n    hostname: talos\n\n    nameservers:\n\n      - 10.0.0.1\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.201/8\n\n        mtu: 8765\n\n        routes:\n\n          - network: 0.0.0.0/0\n\n            gateway: 10.0.0.1\n\n      - interface: eth1\n\n        ignore: true\n\n  time:\n\n    servers:\n\n      - time.cloudflare.com\n\nAdditional Addresses for an Interface\n\nIn some environments you may need to set additional addresses on an interface. In the following example, we set two additional addresses on the loopback interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: lo\n\n        addresses:\n\n          - 192.168.0.21/24\n\n          - 10.2.2.2/24\n\nBonding\n\nThe following example shows how to create a bonded interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        dhcp: true\n\n        bond:\n\n          mode: 802.3ad\n\n          lacpRate: fast\n\n          xmitHashPolicy: layer3+4\n\n          miimon: 100\n\n          updelay: 200\n\n          downdelay: 200\n\n          interfaces:\n\n            - eth0\n\n            - eth1\n\nSetting Up a Bridge\n\nThe following example shows how to set up a bridge between two interfaces with an assigned static address.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: br0\n\n        addresses:\n\n          - 192.168.0.42/24\n\n        bridge:\n\n          stp:\n\n            enabled: true\n\n          interfaces:\n\n              - eth0\n\n              - eth1\n\nVLANs\n\nTo setup vlans on a specific device use an array of VLANs to add. The master device may be configured without addressing by setting dhcp to false.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        vlans:\n\n          - vlanId: 100\n\n            addresses:\n\n              - \"192.168.2.10/28\"\n\n            routes:\n\n              - network: 0.0.0.0/0\n\n                gateway: 192.168.2.1\n\n2 - Air-gapped Environments\nSetting up Talos Linux to work in environments with no internet access.\n\nIn this guide we will create a Talos cluster running in an air-gapped environment with all the required images being pulled from an internal registry. We will use the QEMU provisioner available in talosctl to create a local cluster, but the same approach could be used to deploy Talos in bigger air-gapped networks.\n\nRequirements\n\nThe follow are requirements for this guide:\n\nDocker 18.03 or greater\nRequirements for the Talos QEMU cluster\nIdentifying Images\n\nIn air-gapped environments, access to the public Internet is restricted, so Talos can’t pull images from public Docker registries (docker.io, ghcr.io, etc.) We need to identify the images required to install and run Talos. The same strategy can be used for images required by custom workloads running on the cluster.\n\nThe talosctl image default command provides a list of default images used by the Talos cluster (with default configuration settings). To print the list of images, run:\n\ntalosctl image default\n\n\nThis list contains images required by a default deployment of Talos. There might be additional images required for the workloads running on this cluster, and those should be added to this list.\n\nPreparing the Internal Registry\n\nAs access to the public registries is restricted, we have to run an internal Docker registry. In this guide, we will launch the registry on the same machine using Docker:\n\n$ docker run -d -p 6000:5000 --restart always --name registry-airgapped registry:2\n\n1bf09802bee1476bc463d972c686f90a64640d87dacce1ac8485585de69c91a5\n\n\nThis registry will be accepting connections on port 6000 on the host IPs. The registry is empty by default, so we have fill it with the images required by Talos.\n\nFirst, we pull all the images to our local Docker daemon:\n\n$ for image in `talosctl image default`; do docker pull $image; done\n\nv0.15.1: Pulling from coreos/flannel\n\nDigest: sha256:9a296fbb67790659adc3701e287adde3c59803b7fcefe354f1fc482840cdb3d9\n\n...\n\n\nAll images are now stored in the Docker daemon store:\n\n$ docker images\n\nREPOSITORY                               TAG                                        IMAGE ID       CREATED         SIZE\n\ngcr.io/etcd-development/etcd             v3.5.3                                     604d4f022632   6 days ago      181MB\n\nghcr.io/siderolabs/install-cni           v1.0.0-2-gc5d3ab0                          4729e54f794d   6 days ago      76MB\n\n...\n\n\nNow we need to re-tag them so that we can push them to our local registry. We are going to replace the first component of the image name (before the first slash) with our registry endpoint 127.0.0.1:6000:\n\n$ for image in `talosctl image default`; do \\\n\n    docker tag $image `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nAs the next step, we push images to the internal registry:\n\n$ for image in `talosctl image default`; do \\\n\n    docker push `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nWe can now verify that the images are pushed to the registry:\n\n$ curl http://127.0.0.1:6000/v2/_catalog\n\n{\"repositories\":[\"coredns/coredns\",\"coreos/flannel\",\"etcd-development/etcd\",\"kube-apiserver\",\"kube-controller-manager\",\"kube-proxy\",\"kube-scheduler\",\"pause\",\"siderolabs/install-cni\",\"siderolabs/installer\",\"siderolabs/kubelet\"]}\n\n\nNote: images in the registry don’t have the registry endpoint prefix anymore.\n\nLaunching Talos in an Air-gapped Environment\n\nFor Talos to use the internal registry, we use the registry mirror feature to redirect all image pull requests to the internal registry. This means that the registry endpoint (as the first component of the image reference) gets ignored, and all pull requests are sent directly to the specified endpoint.\n\nWe are going to use a QEMU-based Talos cluster for this guide, but the same approach works with Docker-based clusters as well. As QEMU-based clusters go through the Talos install process, they can be used better to model a real air-gapped environment.\n\nIdentify all registry prefixes from talosctl image default, for example:\n\ndocker.io\ngcr.io\nghcr.io\nregistry.k8s.io\n\nThe talosctl cluster create command provides conveniences for common configuration options. The only required flag for this guide is --registry-mirror <endpoint>=http://10.5.0.1:6000 which redirects every pull request to the internal registry, this flag needs to be repeated for each of the identified registry prefixes above. The endpoint being used is 10.5.0.1, as this is the default bridge interface address which will be routable from the QEMU VMs (127.0.0.1 IP will be pointing to the VM itself).\n\n$ sudo --preserve-env=HOME talosctl cluster create --provisioner=qemu --install-image=ghcr.io/siderolabs/installer:v1.6.2 \\\n\n  --registry-mirror docker.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror gcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror ghcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror registry.k8s.io=http://10.5.0.1:6000 \\\n\nvalidating CIDR and reserving IPs\n\ngenerating PKI and tokens\n\ncreating state directory in \"/home/user/.talos/clusters/talos-default\"\n\ncreating network talos-default\n\ncreating load balancer\n\ncreating dhcpd\n\ncreating master nodes\n\ncreating worker nodes\n\nwaiting for API\n\n...\n\n\nNote: --install-image should match the image which was copied into the internal registry in the previous step.\n\nYou can be verify that the cluster is air-gapped by inspecting the registry logs: docker logs -f registry-airgapped.\n\nClosing Notes\n\nRunning in an air-gapped environment might require additional configuration changes, for example using custom settings for DNS and NTP servers.\n\nWhen scaling this guide to the bare-metal environment, following Talos config snippet could be used as an equivalent of the --registry-mirror flag above:\n\nmachine:\n\n  ...\n\n  registries:\n\n      mirrors:\n\n        docker.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        gcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        ghcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        registry.k8s.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n...\n\n\nOther implementations of Docker registry can be used in place of the Docker registry image used above to run the registry. If required, auth can be configured for the internal registry (and custom TLS certificates if needed).\n\nPlease see pull-through cache guide for an example using Harbor container registry with Talos.\n\n3 - Building Custom Talos Images\nHow to build a custom Talos image from source.\n\nThere might be several reasons to build Talos images from source:\n\nverifying the image integrity\nbuilding an image with custom configuration\nCheckout Talos Source\ngit clone https://github.com/siderolabs/talos.git\n\n\nIf building for a specific release, checkout the corresponding tag:\n\ngit checkout v1.6.2\n\nSet up the Build Environment\n\nSee Developing Talos for details on setting up the buildkit builder.\n\nArchitectures\n\nBy default, Talos builds for linux/amd64, but you can customize that by passing PLATFORM variable to make:\n\nmake <target> PLATFORM=linux/arm64 # build for arm64 only\n\nmake <target> PLATFORM=linux/arm64,linux/amd64 # build for arm64 and amd64, container images will be multi-arch\n\nCustomizations\n\nSome of the build parameters can be customized by passing environment variables to make, e.g. GOAMD64=v1 can be used to build Talos images compatible with old AMD64 CPUs:\n\nmake <target> GOAMD64=v1\n\nBuilding Kernel and Initramfs\n\nThe most basic boot assets can be built with:\n\nmake kernel initramfs\n\n\nBuild result will be stored as _out/vmlinuz-<arch> and _out/initramfs-<arch>.xz.\n\nBuilding Container Images\n\nTalos container images should be pushed to the registry as the result of the build process.\n\nThe default settings are:\n\nIMAGE_REGISTRY is set to ghcr.io\nUSERNAME is set to the siderolabs (or value of environment variable USERNAME if it is set)\n\nThe image can be pushed to any registry you have access to, but the access credentials should be stored in ~/.docker/config.json file (e.g. with docker login).\n\nBuilding and pushing the image can be done with:\n\nmake installer PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nmake imager PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nBuilding ISO\n\nThe ISO image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nmake iso\n\n\nThe ISO image will be stored as _out/talos-<arch>.iso.\n\nIf ISO image should be built with the custom imager image, it can be specified with IMAGE_REGISTRY/USERNAME variables:\n\nmake iso IMAGE_REGISTRY=docker.io USERNAME=<username>\n\nBuilding Disk Images\n\nThe disk image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nmake image-metal\n\n\nAvailable disk images are encoded in the image-% target, e.g. make image-aws. Same as with ISO image, the custom imager image can be specified with IMAGE_REGISTRY/USERNAME variables.\n\n4 - Customizing the Kernel\nGuide on how to customize the kernel used by Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nBuild and push your own kernel:\n\ngit clone https://github.com/talos-systems/pkgs.git\n\ncd pkgs\n\nmake kernel-menuconfig USERNAME=_your_github_user_name_\n\n\n\ndocker login ghcr.io --username _your_github_user_name_\n\nmake kernel USERNAME=_your_github_user_name_ PUSH=true\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nFROM scratch AS customization\n\n# this is needed so that Talos copies base kernel modules info and default modules shipped with Talos\n\nCOPY --from=<custom kernel image> /lib/modules /kernel/lib/modules\n\n# this copies over the custom modules\n\nCOPY --from=<custom kernel image> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\nCOPY --from=<custom kernel image> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nTo build the image, run:\n\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t installer:kernel .\n\n\nNote: buildkit has a bug #816, to disable it use DOCKER_BUILDKIT=0\n\nNow that we have a custom installer we can build Talos for the specific platform we wish to deploy to.\n\n5 - Customizing the Root Filesystem\nHow to add your own content to the immutable root file system of Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nFor example, say we have an image that contains the contents of a library we wish to add to the Talos rootfs. We need to define a stage with the name customization:\n\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nNote: <dest> is the path relative to the rootfs that you wish to place the contents of <src>.\n\nTo build the image, run:\n\ndocker build --squash -t <organization>/installer:latest .\n\n\nIn the case that you need to perform some cleanup before adding additional files to the rootfs, you can specify the RM build-time variable:\n\ndocker build --squash --build-arg RM=\"[<path> ...]\" -t <organization>/installer:latest .\n\n\nThis will perform a rm -rf on the specified paths relative to the rootfs.\n\nNote: RM must be a whitespace delimited list.\n\nThe resulting image can be used to:\n\ngenerate an image for any of the supported providers\nperform bare-metall installs\nperform upgrades\n\nWe will step through common customizations in the remainder of this section.\n\n6 - Developing Talos\nLearn how to set up a development environment for local testing and hacking on Talos itself!\n\nThis guide outlines steps and tricks to develop Talos operating systems and related components. The guide assumes Linux operating system on the development host. Some steps might work under Mac OS X, but using Linux is highly advised.\n\nPrepare\n\nCheck out the Talos repository.\n\nTry running make help to see available make commands. You would need Docker and buildx installed on the host.\n\nNote: Usually it is better to install up to date Docker from Docker apt repositories, e.g. Ubuntu instructions.\n\nIf buildx plugin is not available with OS docker packages, it can be installed as a plugin from GitHub releases.\n\nSet up a builder with access to the host network:\n\n docker buildx create --driver docker-container  --driver-opt network=host --name local1 --buildkitd-flags '--allow-insecure-entitlement security.insecure' --use\n\n\nNote: network=host allows buildx builder to access host network, so that it can push to a local container registry (see below).\n\nMake sure the following steps work:\n\nmake talosctl\nmake initramfs kernel\n\nSet up a local docker registry:\n\ndocker run -d -p 5005:5000 \\\n\n    --restart always \\\n\n    --name local registry:2\n\n\nTry to build and push to local registry an installer image:\n\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRecord the image name output in the step above.\n\nNote: it is also possible to force a stable image tag by using TAG variable: make installer IMAGE_REGISTRY=127.0.0.1:5005 TAG=v1.0.0-alpha.1 PUSH=true.\n\nRunning Talos cluster\n\nSet up local caching docker registries (this speeds up Talos cluster boot a lot), script is in the Talos repo:\n\nbash hack/start-registry-proxies.sh\n\n\nStart your local cluster with:\n\nsudo --preserve-env=HOME _out/talosctl-linux-amd64 cluster create \\\n\n    --provisioner=qemu \\\n\n    --cidr=172.20.0.0/24 \\\n\n    --registry-mirror docker.io=http://172.20.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.20.0.1:5001  \\\n\n    --registry-mirror gcr.io=http://172.20.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.20.0.1:5004 \\\n\n    --registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005 \\\n\n    --install-image=127.0.0.1:5005/siderolabs/installer:<RECORDED HASH from the build step> \\\n\n    --controlplanes 3 \\\n\n    --workers 2 \\\n\n    --with-bootloader=false\n\n--provisioner selects QEMU vs. default Docker\ncustom --cidr to make QEMU cluster use different network than default Docker setup (optional)\n--registry-mirror uses the caching proxies set up above to speed up boot time a lot, last one adds your local registry (installer image was pushed to it)\n--install-image is the image you built with make installer above\n--controlplanes & --workers configure cluster size, choose to match your resources; 3 controlplanes give you HA control plane; 1 controlplane is enough, never do 2 controlplanes\n--with-bootloader=false disables boot from disk (Talos will always boot from _out/vmlinuz-amd64 and _out/initramfs-amd64.xz). This speeds up development cycle a lot - no need to rebuild installer and perform install, rebooting is enough to get new code.\n\nNote: as boot loader is not used, it’s not necessary to rebuild installer each time (old image is fine), but sometimes it’s needed (when configuration changes are done and old installer doesn’t validate the config).\n\ntalosctl cluster create derives Talos machine configuration version from the install image tag, so sometimes early in the development cycle (when new minor tag is not released yet), machine config version can be overridden with --talos-version=v1.6.\n\nIf the --with-bootloader=false flag is not enabled, for Talos cluster to pick up new changes to the code (in initramfs), it will require a Talos upgrade (so new installer should be built). With --with-bootloader=false flag, Talos always boots from initramfs in _out/ directory, so simple reboot is enough to pick up new code changes.\n\nIf the installation flow needs to be tested, --with-bootloader=false shouldn’t be used.\n\nConsole Logs\n\nWatching console logs is easy with tail:\n\ntail -F ~/.talos/clusters/talos-default/talos-default-*.log\n\nInteracting with Talos\n\nOnce talosctl cluster create finishes successfully, talosconfig and kubeconfig will be set up automatically to point to your cluster.\n\nStart playing with talosctl:\n\ntalosctl -n 172.20.0.2 version\n\ntalosctl -n 172.20.0.3,172.20.0.4 dashboard\n\ntalosctl -n 172.20.0.4 get members\n\n\nSame with kubectl:\n\nkubectl get nodes -o wide\n\n\nYou can deploy some Kubernetes workloads to the cluster.\n\nYou can edit machine config on the fly with talosctl edit mc --immediate, config patches can be applied via --config-patch flags, also many features have specific flags in talosctl cluster create.\n\nQuick Reboot\n\nTo reboot whole cluster quickly (e.g. to pick up a change made in the code):\n\nfor socket in ~/.talos/clusters/talos-default/talos-default-*.monitor; do echo \"q\" | sudo socat - unix-connect:$socket; done\n\n\nSending q to a single socket allows to reboot a single node.\n\nNote: This command performs immediate reboot (as if the machine was powered down and immediately powered back up), for normal Talos reboot use talosctl reboot.\n\nDevelopment Cycle\n\nFast development cycle:\n\nbring up a cluster\nmake code changes\nrebuild initramfs with make initramfs\nreboot a node to pick new initramfs\nverify code changes\nmore code changes…\n\nSome aspects of Talos development require to enable bootloader (when working on installer itself), in that case quick development cycle is no longer possible, and cluster should be destroyed and recreated each time.\n\nRunning Integration Tests\n\nIf integration tests were changed (or when running them for the first time), first rebuild the integration test binary:\n\nrm -f  _out/integration-test-linux-amd64; make _out/integration-test-linux-amd64\n\n\nRunning short tests against QEMU provisioned cluster:\n\n_out/integration-test-linux-amd64 \\\n\n    -talos.provisioner=qemu \\\n\n    -test.v \\\n\n    -talos.crashdump=false \\\n\n    -test.short \\\n\n    -talos.talosctlpath=$PWD/_out/talosctl-linux-amd64\n\n\nWhole test suite can be run removing -test.short flag.\n\nSpecfic tests can be run with -test.run=TestIntegration/api.ResetSuite.\n\nBuild Flavors\n\nmake <something> WITH_RACE=1 enables Go race detector, Talos runs slower and uses more memory, but memory races are detected.\n\nmake <something> WITH_DEBUG=1 enables Go profiling and other debug features, useful for local development.\n\nDestroying Cluster\nsudo --preserve-env=HOME ../talos/_out/talosctl-linux-amd64 cluster destroy --provisioner=qemu\n\n\nThis command stops QEMU and helper processes, tears down bridged network on the host, and cleans up cluster state in ~/.talos/clusters.\n\nNote: if the host machine is rebooted, QEMU instances and helpers processes won’t be started back. In that case it’s required to clean up files in ~/.talos/clusters/<cluster-name> directory manually.\n\nOptional\n\nSet up cross-build environment with:\n\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n\n\nNote: the static qemu binaries which come with Ubuntu 21.10 seem to be broken.\n\nUnit tests\n\nUnit tests can be run in buildx with make unit-tests, on Ubuntu systems some tests using loop devices will fail because Ubuntu uses low-index loop devices for snaps.\n\nMost of the unit-tests can be run standalone as well, with regular go test, or using IDE integration:\n\ngo test -v ./internal/pkg/circular/\n\n\nThis provides much faster feedback loop, but some tests require either elevated privileges (running as root) or additional binaries available only in Talos rootfs (containerd tests).\n\nRunning tests as root can be done with -exec flag to go test, but this is risky, as test code has root access and can potentially make undesired changes:\n\ngo test -exec sudo  -v ./internal/app/machined/pkg/controllers/network/...\n\nGo Profiling\n\nBuild initramfs with debug enabled: make initramfs WITH_DEBUG=1.\n\nLaunch Talos cluster with bootloader disabled, and use go tool pprof to capture the profile and show the output in your browser:\n\ngo tool pprof http://172.20.0.2:9982/debug/pprof/heap\n\n\nThe IP address 172.20.0.2 is the address of the Talos node, and port :9982 depends on the Go application to profile:\n\n9981: apid\n9982: machined\n9983: trustd\nTesting Air-gapped Environments\n\nThere is a hidden talosctl debug air-gapped command which launches two components:\n\nHTTP proxy capable of proxying HTTP and HTTPS requests\nHTTPS server with a self-signed certificate\n\nThe command also writes down Talos machine configuration patch to enable the HTTP proxy and add a self-signed certificate to the list of trusted certificates:\n\n$ talosctl debug air-gapped --advertised-address 172.20.0.1\n\n2022/08/04 16:43:14 writing config patch to air-gapped-patch.yaml\n\n2022/08/04 16:43:14 starting HTTP proxy on :8002\n\n2022/08/04 16:43:14 starting HTTPS server with self-signed cert on :8001\n\n\nThe --advertised-address should match the bridge IP of the Talos node.\n\nGenerated machine configuration patch looks like:\n\nmachine:\n\n    files:\n\n        - content: |\n\n            -----BEGIN CERTIFICATE-----\n\n            MIIBijCCAS+gAwIBAgIBATAKBggqhkjOPQQDAjAUMRIwEAYDVQQKEwlUZXN0IE9u\n\n            bHkwHhcNMjIwODA0MTI0MzE0WhcNMjIwODA1MTI0MzE0WjAUMRIwEAYDVQQKEwlU\n\n            ZXN0IE9ubHkwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQfOJdaOFSOI1I+EeP1\n\n            RlMpsDZJaXjFdoo5zYM5VYs3UkLyTAXAmdTi7JodydgLhty0pwLEWG4NUQAEvip6\n\n            EmzTo3IwcDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsG\n\n            AQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFCwxL+BjG0pDwaH8QgKW\n\n            Ex0J2mVXMA8GA1UdEQQIMAaHBKwUAAEwCgYIKoZIzj0EAwIDSQAwRgIhAJoW0z0D\n\n            JwpjFcgCmj4zT1SbBFhRBUX64PHJpAE8J+LgAiEAvfozZG8Or6hL21+Xuf1x9oh4\n\n            /4Hx3jozbSjgDyHOLk4=\n\n            -----END CERTIFICATE-----            \n\n          permissions: 0o644\n\n          path: /etc/ssl/certs/ca-certificates\n\n          op: append\n\n    env:\n\n        http_proxy: http://172.20.0.1:8002\n\n        https_proxy: http://172.20.0.1:8002\n\n        no_proxy: 172.20.0.1/24\n\ncluster:\n\n    extraManifests:\n\n        - https://172.20.0.1:8001/debug.yaml\n\n\nThe first section appends a self-signed certificate of the HTTPS server to the list of trusted certificates, followed by the HTTP proxy setup (in-cluster traffic is excluded from the proxy). The last section adds an extra Kubernetes manifest hosted on the HTTPS server.\n\nThe machine configuration patch can now be used to launch a test Talos cluster:\n\ntalosctl cluster create ... --config-patch @air-gapped-patch.yaml\n\n\nThe following lines should appear in the output of the talosctl debug air-gapped command:\n\nCONNECT discovery.talos.dev:443: the HTTP proxy is used to talk to the discovery service\nhttp: TLS handshake error from 172.20.0.2:53512: remote error: tls: bad certificate: an expected error on Talos side, as self-signed cert is not written yet to the file\nGET /debug.yaml: Talos successfully fetches the extra manifest successfully\n\nThere might be more output depending on the registry caches being used or not.\n\nRunning Upgrade Integration Tests\n\nTalos has a separate set of provision upgrade tests, which create a cluster on older versions of Talos, perform an upgrade, and verify that the cluster is still functional.\n\nBuild the test binary:\n\nrm -f  _out/integration-test-provision-linux-amd64; make _out/integration-test-provision-linux-amd64\n\n\nPrepare the test artifacts for the upgrade test:\n\nmake release-artifacts\n\n\nBuild and push an installer image for the development version of Talos:\n\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRun the tests (the tests will create the cluster on the older version of Talos, perform an upgrade, and verify that the cluster is still functional):\n\nsudo --preserve-env=HOME _out/integration-test-provision-linux-amd64 \\\n\n    -test.v \\\n\n    -talos.talosctlpath _out/talosctl-linux-amd64 \\\n\n    -talos.provision.target-installer-registry=127.0.0.1:5005 \\\n\n    -talos.provision.registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005,docker.io=http://172.20.0.1:5000,registry.k8s.io=http://172.20.0.1:5001,quay.io=http://172.20.0.1:5002,gcr.io=http://172.20.0.1:5003,ghcr.io=http://172.20.0.1:5004 \\\n\n    -talos.provision.cidr 172.20.0.0/24\n\n7 - Disaster Recovery\nProcedure for snapshotting etcd database and recovering from catastrophic control plane failure.\n\netcd database backs Kubernetes control plane state, so if the etcd service is unavailable, the Kubernetes control plane goes down, and the cluster is not recoverable until etcd is recovered. etcd builds around the consensus protocol Raft, so highly-available control plane clusters can tolerate the loss of nodes so long as more than half of the members are running and reachable. For a three control plane node Talos cluster, this means that the cluster tolerates a failure of any single node, but losing more than one node at the same time leads to complete loss of service. Because of that, it is important to take routine backups of etcd state to have a snapshot to recover the cluster from in case of catastrophic failure.\n\nBackup\nSnapshotting etcd Database\n\nCreate a consistent snapshot of etcd database with talosctl etcd snapshot command:\n\n$ talosctl -n <IP> etcd snapshot db.snapshot\n\netcd snapshot saved to \"db.snapshot\" (2015264 bytes)\n\nsnapshot info: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: filename db.snapshot is arbitrary.\n\nThis database snapshot can be taken on any healthy control plane node (with IP address <IP> in the example above), as all etcd instances contain exactly same data. It is recommended to configure etcd snapshots to be created on some schedule to allow point-in-time recovery using the latest snapshot.\n\nDisaster Database Snapshot\n\nIf the etcd cluster is not healthy (for example, if quorum has already been lost), the talosctl etcd snapshot command might fail. In that case, copy the database snapshot directly from the control plane node:\n\ntalosctl -n <IP> cp /var/lib/etcd/member/snap/db .\n\n\nThis snapshot might not be fully consistent (if the etcd process is running), but it allows for disaster recovery when latest regular snapshot is not available.\n\nMachine Configuration\n\nMachine configuration might be required to recover the node after hardware failure. Backup Talos node machine configuration with the command:\n\ntalosctl -n IP get mc v1alpha1 -o yaml | yq eval '.spec' -\n\nRecovery\n\nBefore starting a disaster recovery procedure, make sure that etcd cluster can’t be recovered:\n\nget etcd cluster member list on all healthy control plane nodes with talosctl -n IP etcd members command and compare across all members.\nquery etcd health across control plane nodes with talosctl -n IP service etcd.\n\nIf the quorum can be restored, restoring quorum might be a better strategy than performing full disaster recovery procedure.\n\nLatest Etcd Snapshot\n\nGet hold of the latest etcd database snapshot. If a snapshot is not fresh enough, create a database snapshot (see above), even if the etcd cluster is unhealthy.\n\nInit Node\n\nMake sure that there are no control plane nodes with machine type init:\n\n$ talosctl -n <IP1>,<IP2>,... get machinetype\n\nNODE         NAMESPACE   TYPE          ID             VERSION   TYPE\n\n172.20.0.2   config      MachineType   machine-type   2         controlplane\n\n172.20.0.4   config      MachineType   machine-type   2         controlplane\n\n172.20.0.3   config      MachineType   machine-type   2         controlplane\n\n\nInit node type is deprecated, and are incompatible with etcd recovery procedure. init node can be converted to controlplane type with talosctl edit mc --mode=staged command followed by node reboot with talosctl reboot command.\n\nPreparing Control Plane Nodes\n\nIf some control plane nodes experienced hardware failure, replace them with new nodes.\n\nUse machine configuration backup to re-create the nodes with the same secret material and control plane settings to allow workers to join the recovered control plane.\n\nIf a control plane node is up but etcd isn’t, wipe the node’s EPHEMERAL partition to remove the etcd data directory (make sure a database snapshot is taken before doing this):\n\ntalosctl -n <IP> reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL\n\n\nAt this point, all control plane nodes should boot up, and etcd service should be in the Preparing state.\n\nThe Kubernetes control plane endpoint should be pointed to the new control plane nodes if there were changes to the node addresses.\n\nRecovering from the Backup\n\nMake sure all etcd service instances are in Preparing state:\n\n$ talosctl -n <IP> service etcd\n\nNODE     172.20.0.2\n\nID       etcd\n\nSTATE    Preparing\n\nHEALTH   ?\n\nEVENTS   [Preparing]: Running pre state (17s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", time sync (18s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", service \"networkd\" to be \"up\", time sync (20s ago)\n\n\nExecute the bootstrap command against any control plane node passing the path to the etcd database snapshot:\n\n$ talosctl -n <IP> bootstrap --recover-from=./db.snapshot\n\nrecovering from snapshot \"./db.snapshot\": hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: if database snapshot was copied out directly from the etcd data directory using talosctl cp, add flag --recover-skip-hash-check to skip integrity check on restore.\n\nTalos node should print matching information in the kernel log:\n\nrecovering etcd from snapshot: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n{\"level\":\"info\",\"msg\":\"restoring snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/li}\n{\"level\":\"info\",\"msg\":\"restored last compact revision\",\"meta-bucket-name\":\"meta\",\"meta-bucket-name-key\":\"finishedCompactRev\",\"restored-compact-revision\":3360}\n{\"level\":\"info\",\"msg\":\"added member\",\"cluster-id\":\"a3390e43eb5274e2\",\"local-member-id\":\"0\",\"added-peer-id\":\"eb4f6f534361855e\",\"added-peer-peer-urls\":[\"https:/}\n{\"level\":\"info\",\"msg\":\"restored snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/lib/etcd/member/snap\"}\n\n\nNow etcd service should become healthy on the bootstrap node, Kubernetes control plane components should start and control plane endpoint should become available. Remaining control plane nodes join etcd cluster once control plane endpoint is up.\n\nSingle Control Plane Node Cluster\n\nThis guide applies to the single control plane clusters as well. In fact, it is much more important to take regular snapshots of the etcd database in single control plane node case, as loss of the control plane node might render the whole cluster irrecoverable without a backup.\n\n8 - etcd Maintenance\nOperational instructions for etcd database.\n\netcd database backs Kubernetes control plane state, so etcd health is critical for Kubernetes availability.\n\nSpace Quota\n\netcd default database space quota is set to 2 GiB by default. If the database size exceeds the quota, etcd will stop operations until the issue is resolved.\n\nThis condition can be checked with talosctl etcd alarm list command:\n\n$ talosctl -n <IP> etcd alarm list\n\nNODE         MEMBER             ALARM\n\n172.20.0.2   a49c021e76e707db   NOSPACE\n\n\nIf the Kubernetes database contains lots of resources, space quota can be increased to match the actual usage. The recommended maximum size is 8 GiB.\n\nTo increase the space quota, edit the etcd section in the machine configuration:\n\nmachine:\n\n  etcd:\n\n    extraArgs:\n\n      quota-backend-bytes: 4294967296 # 4 GiB\n\n\nOnce the node is rebooted with the new configuration, use talosctl etcd alarm disarm to clear the NOSPACE alarm.\n\nDefragmentation\n\netcd database can become fragmented over time if there are lots of writes and deletes. Kubernetes API server performs automatic compaction of the etcd database, which marks deleted space as free and ready to be reused. However, the space is not actually freed until the database is defragmented.\n\nIf the database is heavily fragmented (in use/db size ratio is less than 0.5), defragmentation might increase the performance. If the database runs over the space quota (see above), but the actual in use database size is small, defragmentation is required to bring the on-disk database size below the limit.\n\nCurrent database size can be checked with talosctl etcd status command:\n\n$ talosctl -n <CP1>,<CP2>,<CP3> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE            LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.3   ecebb05b59a776f1   21 MB     6.0 MB (29.08%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.2   a49c021e76e707db   17 MB     4.5 MB (26.10%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.4   eb47fb33e59bf0e2   20 MB     5.9 MB (28.96%)   ecebb05b59a776f1   53391        4           53391                false\n\n\nIf any of the nodes are over database size quota, alarms will be printed in the ERRORS column.\n\nTo defragment the database, run talosctl etcd defrag command:\n\ntalosctl -n <CP1> etcd defrag\n\n\nNote: defragmentation is a resource-intensive operation, so it is recommended to run it on a single node at a time. Defragmentation to a live member blocks the system from reading and writing data while rebuilding its state.\n\nOnce the defragmentation is complete, the database size will match closely to the in use size:\n\n$ talosctl -n <CP1> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE             LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.2   a49c021e76e707db   4.5 MB    4.5 MB (100.00%)   ecebb05b59a776f1   56065        4           56065                false\n\nSnapshotting\n\nRegular backups of etcd database should be performed to ensure that the cluster can be restored in case of a failure. This procedure is described in the disaster recovery guide.\n\n9 - Extension Services\nUse extension services in Talos Linux.\n\nTalos provides a way to run additional system services early in the Talos boot process. Extension services should be included into the Talos root filesystem (e.g. using system extensions). Extension services run as privileged containers with ephemeral root filesystem located in the Talos root filesystem.\n\nExtension services can be used to use extend core features of Talos in a way that is not possible via static pods or Kubernetes DaemonSets.\n\nPotential extension services use-cases:\n\nstorage: Open iSCSI, software RAID, etc.\nnetworking: BGP FRR, etc.\nplatform integration: VMWare open VM tools, etc.\nConfiguration\n\nTalos on boot scans directory /usr/local/etc/containers for *.yaml files describing the extension services to run. Format of the extension service config:\n\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello-world\n\n  # an optional path to a file containing environment variables\n\n  environmentFile: /var/etc/hello-world/env\n\n  environment:\n\n    - XDG_RUNTIME_DIR=/run\n\n  args:\n\n     - -f\n\n  mounts:\n\n     - # OCI Mount Spec\n\ndepends:\n\n   - service: cri\n\n   - path: /run/machined/machined.sock\n\n   - network:\n\n       - addresses\n\n       - connectivity\n\n       - hostname\n\n       - etcfiles\n\n   - time: true\n\nrestart: never|always|untilSuccess\n\nname\n\nField name sets the service name, valid names are [a-z0-9-_]+. The service container root filesystem path is derived from the name: /usr/local/lib/containers/<name>. The extension service will be registered as a Talos service under an ext-<name> identifier.\n\ncontainer\nentrypoint defines the container entrypoint relative to the container root filesystem (/usr/local/lib/containers/<name>)\nenvironmentFile defines the path to a file containing environment variables, the service waits for the file to exist before starting\nenvironment defines the container environment variables, overrides the variables from environmentFile\nargs defines the additional arguments to pass to the entrypoint\nmounts defines the volumes to be mounted into the container root\ncontainer.mounts\n\nThe section mounts uses the standard OCI spec:\n\n- source: /var/log/audit\n\n  destination: /var/log/audit\n\n  type: bind\n\n  options:\n\n    - rshared\n\n    - bind\n\n    - ro\n\n\nAll requested directories will be mounted into the extension service container mount namespace. If the source directory doesn’t exist in the host filesystem, it will be created (only for writable paths in the Talos root filesystem).\n\ncontainer.security\n\nThe section security follows this example:\n\nmaskedPaths:\n\n  - \"/should/be/masked\"\n\nreadonlyPaths:\n\n  - \"/path/that/should/be/readonly\"\n\n  - \"/another/readonly/path\"\n\nwriteableRootfs: true\n\nwriteableSysfs: true\n\nrootfsPropagation: shared\n\nThe rootfs is readonly by default unless writeableRootfs: true is set.\nThe sysfs is readonly by default unless writeableSysfs: true is set.\nMasked paths if not set defaults to containerd defaults. Masked paths will be mounted to /dev/null. To set empty masked paths use:\ncontainer:\n\n  security:\n\n    maskedPaths: []\n\nRead Only paths if not set defaults to containerd defaults. Read-only paths will be mounted to /dev/null. To set empty read only paths use:\ncontainer:\n\n  security:\n\n    readonlyPaths: []\n\nRootfs propagation is not set by default (container mounts are private).\ndepends\n\nThe depends section describes extension service start dependencies: the service will not be started until all dependencies are met.\n\nAvailable dependencies:\n\nservice: <name>: wait for the service <name> to be running and healthy\npath: <path>: wait for the <path> to exist\nnetwork: [addresses, connectivity, hostname, etcfiles]: wait for the specified network readiness checks to succeed\ntime: true: wait for the NTP time sync\nrestart\n\nField restart defines the service restart policy, it allows to either configure an always running service or a one-shot service:\n\nalways: restart service always\nnever: start service only once and never restart\nuntilSuccess: restart failing service, stop restarting on successful run\nExample\n\nExample layout of the Talos root filesystem contents for the extension service:\n\n/\n\n└── usr\n\n    └── local\n\n        ├── etc\n\n        │   └── containers\n\n        │       └── hello-world.yaml\n\n        └── lib\n\n            └── containers\n\n                └── hello-world\n\n                    ├── hello\n\n                    └── config.ini\n\n\nTalos discovers the extension service configuration in /usr/local/etc/containers/hello-world.yaml:\n\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello\n\n  args:\n\n    - --config\n\n    - config.ini\n\ndepends:\n\n  - network:\n\n    - addresses\n\nrestart: always\n\n\nTalos starts the container for the extension service with container root filesystem at /usr/local/lib/containers/hello-world:\n\n/\n\n├── hello\n\n└── config.ini\n\n\nExtension service is registered as ext-hello-world in talosctl services:\n\n$ talosctl service ext-hello-world\n\nNODE     172.20.0.5\n\nID       ext-hello-world\n\nSTATE    Running\n\nHEALTH   ?\n\nEVENTS   [Running]: Started task ext-hello-world (PID 1100) for container ext-hello-world (2m47s ago)\n\n         [Preparing]: Creating service runner (2m47s ago)\n\n         [Preparing]: Running pre state (2m47s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\" (2m48s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\", network (2m49s ago)\n\n\nAn extension service can be started, restarted and stopped using talosctl service ext-hello-world start|restart|stop. Use talosctl logs ext-hello-world to get the logs of the service.\n\nComplete example of the extension service can be found in the extensions repository.\n\n10 - Machine Configuration OAuth2 Authentication\nHow to authenticate Talos machine configuration download (talos.config=) on metal platform using OAuth.\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow. The machine configuration is fetched from the URL specified with talos.config kernel argument, and by default this HTTP request is not authenticated. When the OAuth2 authentication is enabled, Talos will authenticate the request using OAuth device flow first, and then pass the token to the machine configuration download endpoint.\n\nPrerequisites\n\nObtain the following information:\n\nOAuth client ID (mandatory)\nOAuth client secret (optional)\nOAuth device endpoint\nOAuth token endpoint\nOAuth scopes, audience (optional)\nOAuth client secret (optional)\nextra Talos variables to send to the device auth endpoint (optional)\nConfiguration\n\nSet the following kernel parameters on the initial Talos boot to enable the OAuth flow:\n\ntalos.config set to the URL of the machine configuration endpoint (which will be authenticated using OAuth)\ntalos.config.oauth.client_id set to the OAuth client ID (required)\ntalos.config.oauth.client_secret set to the OAuth client secret (optional)\ntalos.config.oauth.scope set to the OAuth scopes (optional, repeat the parameter for multiple scopes)\ntalos.config.oauth.audience set to the OAuth audience (optional)\ntalos.config.oauth.device_auth_url set to the OAuth device endpoint (if not set defaults to talos.config URL with the path /device/code)\ntalos.config.oauth.token_url set to the OAuth token endpoint (if not set defaults to talos.config URL with the path /token)\ntalos.config.oauth.extra_variable set to the extra Talos variables to send to the device auth endpoint (optional, repeat the parameter for multiple variables)\n\nThe list of variables supported by the talos.config.oauth.extra_variable parameter is same as the list of variables supported by the talos.config parameter.\n\nFlow\n\nOn the initial Talos boot, when machine configuration is not available, Talos will print the following messages:\n\n[talos] downloading config {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"platform\": \"metal\"}\n\n[talos] waiting for network to be ready\n\n[talos] [OAuth] starting the authentication device flow with the following settings:\n\n[talos] [OAuth]  - client ID: \"<REDACTED>\"\n\n[talos] [OAuth]  - device auth URL: \"https://oauth2.googleapis.com/device/code\"\n\n[talos] [OAuth]  - token URL: \"https://oauth2.googleapis.com/token\"\n\n[talos] [OAuth]  - extra variables: [\"uuid\" \"mac\"]\n\n[talos] waiting for variables: [uuid mac]\n\n[talos] waiting for variables: [mac]\n\n[talos] [OAuth] please visit the URL https://www.google.com/device and enter the code <REDACTED>\n\n[talos] [OAuth] waiting for the device to be authorized (expires at 14:46:55)...\n\n\nIf the OAuth service provides the complete verification URL, the QR code to scan is also printed to the console:\n\n[talos] [OAuth] or scan the following QR code:\n\n█████████████████████████████████\n\n█████████████████████████████████\n\n████ ▄▄▄▄▄ ██▄▀▀    ▀█ ▄▄▄▄▄ ████\n\n████ █   █ █▄  ▀▄██▄██ █   █ ████\n\n████ █▄▄▄█ ██▀▄██▄  ▀█ █▄▄▄█ ████\n\n████▄▄▄▄▄▄▄█ ▀ █ ▀ █▄█▄▄▄▄▄▄▄████\n\n████   ▀ ▄▄ ▄█  ██▄█   ███▄█▀████\n\n████▀█▄  ▄▄▀▄▄█▀█▄██ ▄▀▄██▄ ▄████\n\n████▄██▀█▄▄▄███▀ ▀█▄▄  ██ █▄ ████\n\n████▄▀▄▄▄ ▄███ ▄ ▀ ▀▀▄▀▄▀█▄ ▄████\n\n████▄█████▄█  █ ██ ▀ ▄▄▄  █▀▀████\n\n████ ▄▄▄▄▄ █ █ ▀█▄█▄ █▄█  █▄ ████\n\n████ █   █ █▄ ▄▀ ▀█▀▄▄▄   ▀█▄████\n\n████ █▄▄▄█ █ ██▄ ▀  ▀███ ▀█▀▄████\n\n████▄▄▄▄▄▄▄█▄▄█▄██▄▄▄▄█▄███▄▄████\n\n█████████████████████████████████\n\n\nOnce the authentication flow is complete on the OAuth provider side, Talos will print the following message:\n\n[talos] [OAuth] device authorized\n\n[talos] fetching machine config from: \"http://example.com/config.yaml\"\n\n[talos] machine config loaded successfully {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"sources\": [\"metal\"]}\n\n11 - Metal Network Configuration\nHow to use META-based network configuration on Talos metal platform.\n\nNote: This is an advanced feature which requires deep understanding of Talos and Linux network configuration.\n\nTalos Linux when running on a cloud platform (e.g. AWS or Azure), uses the platform-provided metadata server to provide initial network configuration to the node. When running on bare-metal, there is no metadata server, so there are several options to provide initial network configuration (before machine configuration is acquired):\n\nuse automatic network configuration via DHCP (Talos default)\nuse initial boot kernel command line parameters to configure networking\nuse automatic network configuration via DHCP just enough to fetch machine configuration and then use machine configuration to set desired advanced configuration.\n\nIf DHCP option is available, it is by far the easiest way to configure networking. The initial boot kernel command line parameters are not very flexible, and they are not persisted after initial Talos installation.\n\nTalos starting with version 1.4.0 offers a new option to configure networking on bare-metal: META-based network configuration.\n\nNote: META-based network configuration is only available on Talos Linux metal platform.\n\nTalos dashboard provides a way to configure META-based network configuration for a machine using the console, but it doesn’t support all kinds of network configuration.\n\nNetwork Configuration Format\n\nTalos META-based network configuration is a YAML file with the following format:\n\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 10.68.182.1/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nlinks:\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      mtu: 0\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\nroutes:\n\n    - family: inet4\n\n      gateway: 147.75.61.42\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 1024\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nhostnames:\n\n    - hostname: ci-blue-worker-amd64-2\n\n      layer: platform\n\nresolvers: []\n\ntimeServers: []\n\n\nEvery section is optional, so you can configure only the parts you need. The format of each section matches the respective network *Spec resource .spec part, e.g the addresses: section matches the .spec of AddressSpec resource:\n\n# talosctl get addressspecs bond0/10.68.182.1/31 -o yaml | yq .spec\n\naddress: 10.68.182.1/31\n\nlinkName: bond0\n\nfamily: inet4\n\nscope: global\n\nflags: permanent\n\nlayer: platform\n\n\nSo one way to prepare the network configuration file is to boot Talos Linux, apply necessary network configuration using Talos machine configuration, and grab the resulting resources from the running Talos instance.\n\nIn this guide we will briefly cover the most common examples of the network configuration.\n\nAddresses\n\nThe addresses configured are usually routable IP addresses assigned to the machine, so the scope: should be set to global and flags: to permanent. Additionally, family: should be set to either inet4 or init6 depending on the address family.\n\nThe linkName: property should match the name of the link the address is assigned to, it might be a physical link, e.g. en9sp0, or the name of a logical link, e.g. bond0, created in the links: section.\n\nExample, IPv4 address:\n\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n\nExample, IPv6 address:\n\naddresses:\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nLinks\n\nFor physical network interfaces (links), the most usual configuration is to bring the link up:\n\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      layer: platform\n\n\nThis will bring the link up, and it will also disable Talos auto-configuration (disables running DHCP on the link).\n\nAnother common case is to set a custom MTU:\n\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      mtu: 9000\n\n      layer: platform\n\n\nThe order of the links in the links: section is not important.\n\nBonds\n\nFor bonded links, there should be a link resource for the bond itself, and a link resource for each enslaved link:\n\nlinks:\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n\nThe name of the bond can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: bond - this is a bonded link\ntype: ether - this is an Ethernet link\nbondMaster: - defines bond configuration, please see Linux documentation on the available options\n\nFor each enslaved link, the following properties are important:\n\nmasterName: bond0 - the name of the bond this link is enslaved to\nslaveIndex: 0 - the index of the enslaved link, starting from 0, controls the order of bond slaves\nVLANs\n\nVLANs are logical links which have a parent link, and a VLAN ID and protocol:\n\nlinks:\n\n    - name: bond0.35\n\n      logical: true\n\n      up: true\n\n      kind: vlan\n\n      type: ether\n\n      parentName: bond0\n\n      vlan:\n\n        vlanID: 35\n\n        vlanProtocol: 802.1ad\n\n\nThe name of the VLAN link can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: vlan - this is a VLAN link\ntype: ether - this is an Ethernet link\nparentName: bond0 - the name of the parent link\nvlan: - defines VLAN configuration: vlanID and vlanProtocol\nRoutes\n\nFor route configuration, most of the time table: main, scope: global, type: unicast and protocol: static are used.\n\nThe route most important fields are:\n\ndst: defines the destination network, if left empty means “default gateway”\ngateway: defines the gateway address\npriority: defines the route priority (metric), lower values are preferred for the same dst: network\noutLinkName: defines the name of the link the route is associated with\nsrc: sets the source address for the route (optional)\n\nAdditionally, family: should be set to either inet4 or init6 depending on the address family.\n\nExample, IPv6 default gateway:\n\nroutes:\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n\nExample, IPv4 route to 10/8 via 10.68.182.0 gateway:\n\nroutes:\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nHostnames\n\nEven though the section supports multiple hostnames, only a single one should be used:\n\nhostnames:\n\n    - hostname: host\n\n      domainname: some.org\n\n      layer: platform\n\n\nThe domainname: is optional.\n\nIf the hostname is not set, Talos will use default generated hostname.\n\nResolvers\n\nThe resolvers: section is used to configure DNS resolvers, only single entry should be used:\n\nresolvers:\n\n    - dnsServers:\n\n        - 8.8.8.8\n\n        - 1.1.1.1\n\n      layer: platform\n\n\nIf the dnsServers: is not set, Talos will use default DNS servers.\n\nTime Servers\n\nThe timeServers: section is used to configure NTP time servers, only single entry should be used:\n\ntimeServers:\n\n    - timeServers:\n\n        - 169.254.169.254\n\n      layer: platform\n\n\nIf the timeServers: is not set, Talos will use default NTP servers.\n\nSupplying META Network Configuration\n\nOnce the network configuration YAML document is ready, it can be supplied to Talos in one of the following ways:\n\nfor a running Talos machine, using Talos API (requires already established network connectivity)\nfor Talos disk images, it can be embedded into the image\nfor ISO/PXE boot methods, it can be supplied via kernel command line parameters as an environment variable\n\nThe metal network configuration is stored in Talos META partition under the key 0xa (decimal 10).\n\nIn this guide we will assume that the prepared network configuration is stored in the file network.yaml.\n\nNote: as JSON is a subset of YAML, the network configuration can be also supplied as a JSON document.\n\nSupplying Network Configuration to a Running Talos Machine\n\nUse the talosctl to write a network configuration to a running Talos machine:\n\ntalosctl meta write 0xa \"$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos Disk Image\n\nFollowing the boot assets guide, create a disk image passing the network configuration as a --meta flag:\n\ndocker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 metal --meta \"0xa=$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos ISO/PXE Boot\n\nAs there is no META partition created yet before Talos Linux is installed, META values can be set as an environment variable INSTALLER_META_BASE64 passed to the initial boot of Talos. The supplied value will be used immediately, and also it will be written to the META partition once Talos is installed.\n\nWhen using imager to create the ISO, the INSTALLER_META_BASE64 environment variable will be automatically generated from the --meta flag:\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --meta \"0xa=$(cat network.yaml)\"\n\n...\n\nkernel command line: ... talos.environment=INSTALLER_META_BASE64=MHhhPWZvbw==\n\n\nWhen PXE booting, the value of INSTALLER_META_BASE64 should be set manually:\n\necho -n \"0xa=$(cat network.yaml)\" | base64\n\n\nThe resulting base64 string should be passed as an environment variable INSTALLER_META_BASE64 to the initial boot of Talos: talos.environment=INSTALLER_META_BASE64=<base64-encoded value>.\n\nGetting Current META Network Configuration\n\nTalos exports META keys as resources:\n\n# talosctl get meta 0x0a -o yaml\n\n...\n\nspec:\n\n    value: '{\"addresses\": ...}'\n\n12 - Migrating from Kubeadm\nMigrating Kubeadm-based clusters to Talos.\n\nIt is possible to migrate Talos from a cluster that is created using kubeadm to Talos.\n\nHigh-level steps are the following:\n\nCollect CA certificates and a bootstrap token from a control plane node.\nCreate a Talos machine config with the CA certificates with the ones you collected.\nUpdate control plane endpoint in the machine config to point to the existing control plane (i.e. your load balancer address).\nBoot a new Talos machine and apply the machine config.\nVerify that the new control plane node is ready.\nRemove one of the old control plane nodes.\nRepeat the same steps for all control plane nodes.\nVerify that all control plane nodes are ready.\nRepeat the same steps for all worker nodes, using the machine config generated for the workers.\nRemarks on kube-apiserver load balancer\n\nWhile migrating to Talos, you need to make sure that your kube-apiserver load balancer is in place and keeps pointing to the correct set of control plane nodes.\n\nThis process depends on your load balancer setup.\n\nIf you are using an LB that is external to the control plane nodes (e.g. cloud provider LB, F5 BIG-IP, etc.), you need to make sure that you update the backend IPs of the load balancer to point to the control plane nodes as you add Talos nodes and remove kubeadm-based ones.\n\nIf your load balancing is done on the control plane nodes (e.g. keepalived + haproxy on the control plane nodes), you can do the following:\n\nAdd Talos nodes and remove kubeadm-based ones while updating the haproxy backends to point to the newly added nodes except the last kubeadm-based control plane node.\nTurn off keepalived to drop the virtual IP used by the kubeadm-based nodes (introduces kube-apiserver downtime).\nSet up a virtual-IP based new load balancer on the new set of Talos control plane nodes. Use the previous LB IP as the LB virtual IP.\nVerify apiserver connectivity over the Talos-managed virtual IP.\nMigrate the last control-plane node.\nPrerequisites\nAdmin access to the kubeadm-based cluster\nAccess to the /etc/kubernetes/pki directory (e.g. SSH & root permissions) on the control plane nodes of the kubeadm-based cluster\nAccess to kube-apiserver load-balancer configuration\nStep-by-step guide\n\nDownload /etc/kubernetes/pki directory from a control plane node of the kubeadm-based cluster.\n\nCreate a new join token for the new control plane nodes:\n\n# inside a control plane node\n\nkubeadm token create --ttl 0\n\n\nCreate Talos secrets from the PKI directory you downloaded on step 1 and the token you generated on step 2:\n\ntalosctl gen secrets --kubernetes-bootstrap-token <TOKEN> --from-kubernetes-pki <PKI_DIR>\n\n\nCreate a new Talos config from the secrets:\n\ntalosctl gen config --with-secrets secrets.yaml <CLUSTER_NAME> https://<EXISTING_CLUSTER_LB_IP>\n\n\nCollect the information about the kubeadm-based cluster from the kubeadm configmap:\n\nkubectl get configmap -n kube-system kubeadm-config -oyaml\n\n\nTake note of the following information in the ClusterConfiguration:\n\n.controlPlaneEndpoint\n.networking.dnsDomain\n.networking.podSubnet\n.networking.serviceSubnet\n\nReplace the following information in the generated controlplane.yaml:\n\n.cluster.network.cni.name with none\n.cluster.network.podSubnets[0] with the value of the networking.podSubnet from the previous step\n.cluster.network.serviceSubnets[0] with the value of the networking.serviceSubnet from the previous step\n.cluster.network.dnsDomain with the value of the networking.dnsDomain from the previous step\n\nGo through the rest of controlplane.yaml and worker.yaml to customize them according to your needs, especially :\n\n.cluster.secretboxEncryptionSecret should be either removed if you don’t currently use EncryptionConfig on your kube-apiserver or set to the correct value\n\nMake sure that, on your current Kubeadm cluster, the first --service-account-issuer= parameter in /etc/kubernetes/manifests/kube-apiserver.yaml is equal to the value of .cluster.controlPlane.endpoint in controlplane.yaml. If it’s not, add a new --service-account-issuer= parameter with the correct value before your current one in /etc/kubernetes/manifests/kube-apiserver.yaml on all of your control planes nodes, and restart the kube-apiserver containers.\n\nBring up a Talos node to be the initial Talos control plane node.\n\nApply the generated controlplane.yaml to the Talos control plane node:\n\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file controlplane.yaml\n\n\nWait until the new control plane node joins the cluster and is ready.\n\nkubectl get node -owide --watch\n\n\nUpdate your load balancer to point to the new control plane node.\n\nDrain the old control plane node you are replacing:\n\nkubectl drain <OLD_NODE> --delete-emptydir-data --force --ignore-daemonsets --timeout=10m\n\n\nRemove the old control plane node from the cluster:\n\nkubectl delete node <OLD_NODE>\n\n\nDestroy the old node:\n\n# inside the node\n\nsudo kubeadm reset --force\n\n\nRepeat the same steps, starting from step 7, for all control plane nodes.\n\nRepeat the same steps, starting from step 7, for all worker nodes while applying the worker.yaml instead and skipping the LB step:\n\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file worker.yaml\n\n\nYour kubeadm kube-proxy configuration may not be compatible with the one generated by Talos, which will make the Talos Kubernetes upgrades impossible (labels may not be the same, and selector.matchLabels is an immutable field). To be sure, export your current kube-proxy daemonset manifest, check the labels, they have to be:\n\ntier: node\n\nk8s-app: kube-proxy\n\n\nIf the are not, modify all the labels fields, save the file, delete your current kube-proxy daemonset, and apply the one you modified.\n\n13 - Proprietary Kernel Modules\nAdding a proprietary kernel module to Talos Linux\n\nPatching and building the kernel image\n\nClone the pkgs repository from Github and check out the revision corresponding to your version of Talos Linux\n\ngit clone https://github.com/talos-systems/pkgs pkgs && cd pkgs\n\ngit checkout v0.8.0\n\n\nClone the Linux kernel and check out the revision that pkgs uses (this can be found in kernel/kernel-prepare/pkg.yaml and it will be something like the following: https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-x.xx.x.tar.xz)\n\ngit clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git && cd linux\n\ngit checkout v5.15\n\n\nYour module will need to be converted to be in-tree. The steps for this are different depending on the complexity of the module to port, but generally it would involve moving the module source code into the drivers tree and creating a new Makefile and Kconfig.\n\nStage your changes in Git with git add -A.\n\nRun git diff --cached --no-prefix > foobar.patch to generate a patch from your changes.\n\nCopy this patch to kernel/kernel/patches in the pkgs repo.\n\nAdd a patch line in the prepare segment of kernel/kernel/pkg.yaml:\n\npatch -p0 < /pkg/patches/foobar.patch\n\n\nBuild the kernel image. Make sure you are logged in to ghcr.io before running this command, and you can change or omit PLATFORM depending on what you want to target.\n\nmake kernel PLATFORM=linux/amd64 USERNAME=your-username PUSH=true\n\n\nMake a note of the image name the make command outputs.\n\nBuilding the installer image\n\nCopy the following into a new Dockerfile:\n\nFROM scratch AS customization\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:<talos version>\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nRun to build and push the installer:\n\nINSTALLER_VERSION=<talos version>\n\nIMAGE_NAME=\"ghcr.io/your-username/talos-installer:$INSTALLER_VERSION\"\n\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t \"$IMAGE_NAME\" . && docker push \"$IMAGE_NAME\"\n\n\nDeploying to your cluster\n\ntalosctl upgrade --image ghcr.io/your-username/talos-installer:<talos version> --preserve=true\n\n14 - Static Pods\nUsing Talos Linux to set up static pods in Kubernetes.\nStatic Pods\n\nStatic pods are run directly by the kubelet bypassing the Kubernetes API server checks and validations. Most of the time DaemonSet is a better alternative to static pods, but some workloads need to run before the Kubernetes API server is available or might need to bypass security restrictions imposed by the API server.\n\nSee Kubernetes documentation for more information on static pods.\n\nConfiguration\n\nStatic pod definitions are specified in the Talos machine configuration:\n\nmachine:\n\n  pods:\n\n    - apiVersion: v1\n\n       kind: Pod\n\n       metadata:\n\n         name: nginx\n\n       spec:\n\n         containers:\n\n           - name: nginx\n\n             image: nginx\n\n\nTalos renders static pod definitions to the kubelet manifest directory (/etc/kubernetes/manifests), kubelet picks up the definition and launches the pod.\n\nTalos accepts changes to the static pod configuration without a reboot.\n\nUsage\n\nKubelet mirrors pod definition to the API server state, so static pods can be inspected with kubectl get pods, logs can be retrieved with kubectl logs, etc.\n\n$ kubectl get pods\n\nNAME                           READY   STATUS    RESTARTS   AGE\n\nnginx-talos-default-controlplane-2   1/1     Running   0          17s\n\n\nIf the API server is not available, status of the static pod can also be inspected with talosctl containers --kubernetes:\n\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                      IMAGE                                                   PID    STATUS\n\n172.20.0.3   k8s.io      default/nginx-talos-default-controlplane-2                                              registry.k8s.io/pause:3.6                               4886   SANDBOX_READY\n\n172.20.0.3   k8s.io      └─ default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771                        docker.io/library/nginx:latest\n\n...\n\n\nLogs of static pods can be retrieved with talosctl logs --kubernetes:\n\n$ talosctl logs --kubernetes default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771\n\n172.20.0.3: 2022-02-10T15:26:01.289208227Z stderr F 2022/02/10 15:26:01 [notice] 1#1: using the \"epoll\" event method\n\n172.20.0.3: 2022-02-10T15:26:01.2892466Z stderr F 2022/02/10 15:26:01 [notice] 1#1: nginx/1.21.6\n\n172.20.0.3: 2022-02-10T15:26:01.28925723Z stderr F 2022/02/10 15:26:01 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)\n\nTroubleshooting\n\nTalos doesn’t perform any validation on the static pod definitions. If the pod isn’t running, use kubelet logs (talosctl logs kubelet) to find the problem:\n\n$ talosctl logs kubelet\n\n172.20.0.2: {\"ts\":1644505520281.427,\"caller\":\"config/file.go:187\",\"msg\":\"Could not process manifest file\",\"path\":\"/etc/kubernetes/manifests/talos-default-nginx-gvisor.yaml\",\"err\":\"invalid pod: [spec.containers: Required value]\"}\n\nResource Definitions\n\nStatic pod definitions are available as StaticPod resources combined with Talos-generated control plane static pods:\n\n$ talosctl get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.3   k8s         StaticPod   default-nginx             1\n\n172.20.0.3   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.3   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.3   k8s         StaticPod   kube-scheduler            1\n\n\nTalos assigns ID <namespace>-<name> to the static pods specified in the machine configuration.\n\nOn control plane nodes status of the running static pods is available in the StaticPodStatus resource:\n\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE   TYPE              ID                                                           VERSION   READY\n\n172.20.0.3   k8s         StaticPodStatus   default/nginx-talos-default-controlplane-2                         2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-2            2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-2   3         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-2            3         True\n\n15 - Talos API access from Kubernetes\nHow to access Talos API from within Kubernetes.\n\nIn this guide, we will enable the Talos feature to access the Talos API from within Kubernetes.\n\nEnabling the Feature\n\nEdit the machine configuration to enable the feature, specifying the Kubernetes namespaces from which Talos API can be accessed and the allowed Talos API roles.\n\ntalosctl -n 172.20.0.2 edit machineconfig\n\n\nConfigure the kubernetesTalosAPIAccess like the following:\n\nspec:\n\n  machine:\n\n    features:\n\n      kubernetesTalosAPIAccess:\n\n        enabled: true\n\n        allowedRoles:\n\n          - os:reader\n\n        allowedKubernetesNamespaces:\n\n          - default\n\nInjecting Talos ServiceAccount into manifests\n\nCreate the following manifest file deployment.yaml:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  template:\n\n    metadata:\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n        - name: talos-api-access\n\n          image: alpine:3\n\n          command:\n\n            - sh\n\n            - -c\n\n            - |\n\n              wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n              chmod +x /usr/local/bin/talosctl\n\n              while true; talosctl -n 172.20.0.2 version; do sleep 1; done              \n\n\nNote: make sure that you replace the IP 172.20.0.2 with a valid Talos node IP.\n\nUse talosctl inject serviceaccount command to inject the Talos ServiceAccount into the manifest.\n\ntalosctl inject serviceaccount -f deployment.yaml > deployment-injected.yaml\n\n\nInspect the generated manifest:\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  creationTimestamp: null\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  strategy: {}\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n      - command:\n\n        - sh\n\n        - -c\n\n        - |\n\n          wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n          chmod +x /usr/local/bin/talosctl\n\n          while true; talosctl -n 172.20.0.2 version; do sleep 1; done          \n\n        image: alpine:3\n\n        name: talos-api-access\n\n        resources: {}\n\n        volumeMounts:\n\n        - mountPath: /var/run/secrets/talos.dev\n\n          name: talos-secrets\n\n      tolerations:\n\n      - operator: Exists\n\n      volumes:\n\n      - name: talos-secrets\n\n        secret:\n\n          secretName: talos-api-access-talos-secrets\n\nstatus: {}\n\n---\n\napiVersion: talos.dev/v1alpha1\n\nkind: ServiceAccount\n\nmetadata:\n\n    name: talos-api-access-talos-secrets\n\nspec:\n\n    roles:\n\n        - os:reader\n\n---\n\n\nAs you can notice, your deployment manifest is now injected with the Talos ServiceAccount.\n\nTesting API Access\n\nApply the new manifest into default namespace:\n\nkubectl apply -n default -f deployment-injected.yaml\n\n\nFollow the logs of the pods belong to the deployment:\n\nkubectl logs -n default -f -l app=talos-api-access\n\n\nYou’ll see a repeating output similar to the following:\n\nClient:\n\n    Tag:         <talos version>\n\n    SHA:         ....\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\nServer:\n\n    NODE:        172.20.0.2\n\n    Tag:         <talos version>\n\n    SHA:         ...\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\n    Enabled:     RBAC\n\n\nThis means that the pod can talk to Talos API of node 172.20.0.2 successfully.\n\n16 - Verifying Images\nVerifying Talos container image signatures.\n\nSidero Labs signs the container images generated for the Talos release with cosign:\n\nghcr.io/siderolabs/installer (Talos installer)\nghcr.io/siderolabs/talos (Talos image for container runtime)\nghcr.io/siderolabs/talosctl (talosctl client packaged as a container image)\nghcr.io/siderolabs/imager (Talos install image generator)\nall system extension images\nVerifying Container Image Signatures\n\nThe cosign tool can be used to verify the signatures of the Talos container images:\n\n$ cosign verify --certificate-identity-regexp '@siderolabs\\.com$' --certificate-oidc-issuer https://accounts.google.com ghcr.io/siderolabs/installer:v1.4.0\n\n\n\nVerification for ghcr.io/siderolabs/installer:v1.4.0 --\n\nThe following checks were performed on each of these signatures:\n\n  - The cosign claims were validated\n\n  - Existence of the claims in the transparency log was verified offline\n\n  - The code-signing certificate was verified using trusted certificate authority certificates\n\n\n\n[{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/siderolabs/installer\"},\"image\":{\"docker-manifest-digest\":\"sha256:f41795cc88f40eb1bc6b3c638c4a3123f6ef3c90627bfc35c04ebab82581e3ee\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://accounts.google.com\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEQCIERkQpgEnPWnfjUHIWO9QxC9Ute3/xJOc7TO5GUnu59xAiBKcFvrDWHoUYChT0/+gaazTrI+r0/GWSbi+Q+sEQ5AKA==\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1681843022,\"logIndex\":18304044,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://accounts.google.com\",\"Subject\":\"andrey.smirnov@siderolabs.com\"}}]\n\n\nThe image should be signed using cosing keyless flow by a Sidero Labs employee with and email from siderolabs.com domain.\n\nReproducible Builds\n\nTalos builds for kernel, initramfs, talosctl, ISO image, and container images are reproducible. So you can verify that the build is the same as the one as provided on GitHub releases page.\n\nSee building Talos images for more details.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Talos Linux Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nTalos Linux Guides\nDocumentation on how to manage Talos Linux\n1: Installation\n1.1: Bare Metal Platforms\n1.1.1: Digital Rebar\n1.1.2: Equinix Metal\n1.1.3: ISO\n1.1.4: Matchbox\n1.1.5: Network Configuration\n1.1.6: PXE\n1.1.7: SecureBoot\n1.2: Virtualized Platforms\n1.2.1: Hyper-V\n1.2.2: KVM\n1.2.3: Proxmox\n1.2.4: Vagrant & Libvirt\n1.2.5: VMware\n1.2.6: Xen\n1.3: Cloud Platforms\n1.3.1: AWS\n1.3.2: Azure\n1.3.3: DigitalOcean\n1.3.4: Exoscale\n1.3.5: GCP\n1.3.6: Hetzner\n1.3.7: Nocloud\n1.3.8: Openstack\n1.3.9: Oracle\n1.3.10: Scaleway\n1.3.11: UpCloud\n1.3.12: Vultr\n1.4: Local Platforms\n1.4.1: Docker\n1.4.2: QEMU\n1.4.3: VirtualBox\n1.5: Single Board Computers\n1.5.1: Banana Pi M64\n1.5.2: Friendlyelec Nano PI R4S\n1.5.3: Jetson Nano\n1.5.4: Libre Computer Board ALL-H3-CC\n1.5.5: Pine64\n1.5.6: Pine64 Rock64\n1.5.7: Radxa ROCK PI 4\n1.5.8: Radxa ROCK PI 4C\n1.5.9: Raspberry Pi Series\n1.6: Boot Assets\n1.7: Omni SaaS\n2: Configuration\n2.1: Configuration Patches\n2.2: Containerd\n2.3: Custom Certificate Authorities\n2.4: Disk Encryption\n2.5: Editing Machine Configuration\n2.6: Logging\n2.7: Managing Talos PKI\n2.8: NVIDIA Fabric Manager\n2.9: NVIDIA GPU (OSS drivers)\n2.10: NVIDIA GPU (Proprietary drivers)\n2.11: Pull Through Image Cache\n2.12: Role-based access control (RBAC)\n2.13: System Extensions\n3: How Tos\n3.1: How to enable workers on your control plane nodes\n3.2: How to manage certificate lifetimes with Talos Linux\n3.3: How to scale down a Talos cluster\n3.4: How to scale up a Talos cluster\n4: Network\n4.1: Corporate Proxies\n4.2: Ingress Firewall\n4.3: KubeSpan\n4.4: Network Device Selector\n4.5: Predictable Interface Names\n4.6: Virtual (shared) IP\n4.7: Wireguard Network\n5: Discovery Service\n6: Interactive Dashboard\n7: Resetting a Machine\n8: Upgrading Talos Linux\n1 - Installation\nHow to install Talos Linux on various platforms\n1.1 - Bare Metal Platforms\nInstallation of Talos Linux on various bare-metal platforms.\n1.1.1 - Digital Rebar\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\nPrerequisites\n3 nodes (please see hardware requirements)\nLoadbalancer\nDigital Rebar Server\nTalosctl access (see talosctl setup)\nCreating a Cluster\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes. We assume an existing digital rebar deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe loadbalancer is used to distribute the load across multiple controlplane nodes. This isn’t covered in detail, because we assume some loadbalancing knowledge before hand. If you think this should be added to the docs, please create a issue.\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nDigital Rebar has a built-in fileserver, which means we can use this feature to expose the talos configuration files. We will place controlplane.yaml, and worker.yaml into Digital Rebar file server by using the drpcli tools.\n\nCopy the generated files from the step above into your Digital Rebar installation.\n\ndrpcli file upload <file>.yaml as <file>.yaml\n\n\nReplacing <file> with controlplane or worker.\n\nDownload the boot files\n\nDownload a recent version of boot.tar.gz from github.\n\nUpload to DRB:\n\n$ drpcli isos upload boot.tar.gz as talos.tar.gz\n\n{\n\n  \"Path\": \"talos.tar.gz\",\n\n  \"Size\": 96470072\n\n}\n\n\nWe have some Digital Rebar example files in the Git repo you can use to provision Digital Rebar with drpcli.\n\nTo apply these configs you need to create them, and then apply them as follow:\n\n$ drpcli bootenvs create talos\n\n{\n\n  \"Available\": true,\n\n  \"BootParams\": \"\",\n\n  \"Bundle\": \"\",\n\n  \"Description\": \"\",\n\n  \"Documentation\": \"\",\n\n  \"Endpoint\": \"\",\n\n  \"Errors\": [],\n\n  \"Initrds\": [],\n\n  \"Kernel\": \"\",\n\n  \"Meta\": {},\n\n  \"Name\": \"talos\",\n\n  \"OS\": {\n\n    \"Codename\": \"\",\n\n    \"Family\": \"\",\n\n    \"IsoFile\": \"\",\n\n    \"IsoSha256\": \"\",\n\n    \"IsoUrl\": \"\",\n\n    \"Name\": \"\",\n\n    \"SupportedArchitectures\": {},\n\n    \"Version\": \"\"\n\n  },\n\n  \"OnlyUnknown\": false,\n\n  \"OptionalParams\": [],\n\n  \"ReadOnly\": false,\n\n  \"RequiredParams\": [],\n\n  \"Templates\": [],\n\n  \"Validated\": true\n\n}\n\ndrpcli bootenvs update talos - < bootenv.yaml\n\n\nYou need to do this for all files in the example directory. If you don’t have access to the drpcli tools you can also use the webinterface.\n\nIt’s important to have a corresponding SHA256 hash matching the boot.tar.gz\n\nBootenv BootParams\n\nWe’re using some of Digital Rebar built in templating to make sure the machine gets the correct role assigned.\n\ntalos.platform=metal talos.config={{ .ProvisionerURL }}/files/{{.Param \\\"talos/role\\\"}}.yaml\"\n\nThis is why we also include a params.yaml in the example directory to make sure the role is set to one of the following:\n\ncontrolplane\nworker\n\nThe {{.Param \\\"talos/role\\\"}} then gets populated with one of the above roles.\n\nBoot the Machines\n\nIn the UI of Digital Rebar you need to select the machines you want to provision. Once selected, you need to assign to following:\n\nProfile\nWorkflow\n\nThis will provision the Stage and Bootenv with the talos values. Once this is done, you can boot the machine.\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.1.2 - Equinix Metal\nCreating Talos clusters with Equinix Metal.\n\nYou can create a Talos Linux cluster on Equinix Metal in a variety of ways, such as through the EM web UI, the metal command line too, or through PXE booting. Talos Linux is a supported OS install option on Equinix Metal, so it’s an easy process.\n\nRegardless of the method, the process is:\n\nCreate a DNS entry for your Kubernetes endpoint.\nGenerate the configurations using talosctl.\nProvision your machines on Equinix Metal.\nPush the configurations to your servers (if not done as part of the machine provisioning).\nconfigure your Kubernetes endpoint to point to the newly created control plane nodes\nbootstrap the cluster\nDefine the Kubernetes Endpoint\n\nThere are a variety of ways to create an HA endpoint for the Kubernetes cluster. Some of the ways are:\n\nDNS\nLoad Balancer\nBGP\n\nWhatever way is chosen, it should result in an IP address/DNS name that routes traffic to all the control plane nodes. We do not know the control plane node IP addresses at this stage, but we should define the endpoint DNS entry so that we can use it in creating the cluster configuration. After the nodes are provisioned, we can use their addresses to create the endpoint A records, or bind them to the load balancer, etc.\n\nCreate the Machine Configuration Files\nGenerating Configurations\n\nUsing the DNS name of the loadbalancer defined above, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-em-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe port used above should be 6443, unless your load balancer maps a different port to port 6443 on the control plane nodes.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode metal\n\ntalosctl validate --config worker.yaml --mode metal\n\n\nNote: Validation of the install disk could potentially fail as validation is performed on your local machine and the specified disk may not exist.\n\nPassing in the configuration as User Data\n\nYou can use the metadata service provide by Equinix Metal to pass in the machines configuration. It is required to add a shebang to the top of the configuration file.\n\nThe convention we use is #!talos.\n\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\n\nSimply select the location and type of machines in the Equinix Metal web interface. Select Talos as the Operating System, then select the number of servers to create, and name them (in lowercase only.) Under optional settings, you can optionally paste in the contents of controlplane.yaml that was generated, above (ensuring you add a first line of #!talos).\n\nYou can repeat this process to create machines of different types for control plane and worker nodes (although you would pass in worker.yaml for the worker nodes, as user data).\n\nIf you did not pass in the machine configuration as User Data, you need to provide it to each machine, with the following command:\n\ntalosctl apply-config --insecure --nodes <Node IP> --file ./controlplane.yaml\n\nCreating a Cluster via the Equinix Metal CLI\n\nThis guide assumes the user has a working API token,and the Equinix Metal CLI installed.\n\nBecause Talos Linux is a supported operating system, Talos Linux machines can be provisioned directly via the CLI, using the -O talos_v1 parameter (for Operating System).\n\nNote: Ensure you have prepended #!talos to the controlplane.yaml file.\n\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --operating-system \"talos_v1\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\ne.g. metal device create -p <projectID> -f da11 -O talos_v1 -P c3.small.x86 -H steve.test.11 --userdata-file ./controlplane.yaml\n\nRepeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nNetwork Booting via iPXE\n\nTalos Linux can be PXE-booted on Equinix Metal using Image Factory, using the equinixMetal platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/equinixMetal-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate the Control Plane Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\nNote: Repeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nCreate the Worker Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file worker.yaml\n\nUpdate the Kubernetes endpoint\n\nNow our control plane nodes have been created, and we know their IP addresses, we can associate them with the Kubernetes endpoint. Configure your load balancer to route traffic to these nodes, or add A records to your DNS entry for the endpoint, for each control plane node. e.g.\n\nhost endpoint.mydomain.com\n\nendpoint.mydomain.com has address 145.40.90.201\n\nendpoint.mydomain.com has address 147.75.109.71\n\nendpoint.mydomain.com has address 145.40.90.177\n\nBootstrap Etcd\n\nSet the endpoints and nodes for talosctl:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\n\nThis only needs to be issued to one control plane node.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.1.3 - ISO\nBooting Talos on bare-metal with ISO.\n\nTalos can be installed on bare-metal machine using an ISO image. ISO images for amd64 and arm64 architectures are available on the Talos releases page.\n\nTalos doesn’t install itself to disk when booted from an ISO until the machine configuration is applied.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from a Talos ISO. The boot order should prefer disk over ISO, or the ISO should be removed after the installation to make Talos boot from disk.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nThere are two flavors of ISO images available:\n\nmetal-<arch>.iso supports booting on BIOS and UEFI systems (for x86, UEFI only for arm64)\nmetal-<arch>-secureboot.iso supports booting on only UEFI systems in SecureBoot mode (via Image Factory)\n1.1.4 - Matchbox\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\nCreating a Cluster\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing load balancer, matchbox deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nIn bare-metal setups it is up to the user to provide the configuration files over HTTP(S). A special kernel parameter (talos.config) must be used to inform Talos about where it should retrieve its configuration file. To keep things simple we will place controlplane.yaml, and worker.yaml into Matchbox’s assets directory. This directory is automatically served by Matchbox.\n\nCreate the Matchbox Configuration Files\n\nThe profiles we will create will reference vmlinuz, and initramfs.xz. Download these files from the release of your choice, and place them in /var/lib/matchbox/assets.\n\nProfiles\nControl Plane Nodes\n{\n\n  \"id\": \"control-plane\",\n\n  \"name\": \"control-plane\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/controlplane.yaml\"\n\n    ]\n\n  }\n\n}\n\n\nNote: Be sure to change http://matchbox.talos.dev to the endpoint of your matchbox server.\n\nWorker Nodes\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/worker.yaml\"\n\n    ]\n\n  }\n\n}\n\nGroups\n\nNow, create the following groups, and ensure that the selectors are accurate for your specific setup.\n\n{\n\n  \"id\": \"control-plane-1\",\n\n  \"name\": \"control-plane-1\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-2\",\n\n  \"name\": \"control-plane-2\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-3\",\n\n  \"name\": \"control-plane-3\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"profile\": \"default\"\n\n}\n\nBoot the Machines\n\nNow that we have our configuration files in place, boot all the machines. Talos will come up on each machine, grab its configuration file, and bootstrap itself.\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.1.5 - Network Configuration\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nBy default, Talos will run DHCP client on all interfaces which have a link, and that might be enough for most of the cases. If some advanced network configuration is required, it can be done via the machine configuration file.\n\nBut sometimes it is required to apply network configuration even before the machine configuration can be fetched from the network.\n\nKernel Command Line\n\nTalos supports some kernel command line parameters to configure network before the machine configuration is fetched.\n\nNote: Kernel command line parameters are not persisted after Talos installation, so proper network configuration should be done via the machine configuration.\n\nAddress, default gateway and DNS servers can be configured via ip= kernel command line parameter:\n\nip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\n\nBonding can be configured via bond= kernel command line parameter:\n\nbond=bond0:eth0,eth1:balance-rr\n\n\nVLANs can be configured via vlan= kernel command line parameter:\n\nvlan=eth0.100:eth0\n\n\nSee kernel parameters reference for more details.\n\nPlatform Network Configuration\n\nSome platforms (e.g. AWS, Google Cloud, etc.) have their own network configuration mechanisms, which can be used to perform the initial network configuration. There is no such mechanism for bare-metal platforms, so Talos provides a way to use platform network config on the metal platform to submit the initial network configuration.\n\nThe platform network configuration is a YAML document which contains resource specifications for various network resources. For the metal platform, the interactive dashboard can be used to edit the platform network configuration, also the configuration can be created manually.\n\nThe current value of the platform network configuration can be retrieved using the MetaKeys resource (key 0xa):\n\ntalosctl get meta 0xa\n\n\nThe platform network configuration can be updated using the talosctl meta command for the running node:\n\ntalosctl meta write 0xa '{\"externalIPs\": [\"1.2.3.4\"]}'\n\ntalosctl meta delete 0xa\n\n\nThe initial platform network configuration for the metal platform can be also included into the generated Talos image:\n\ndocker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\ndocker run --rm -i --privileged ghcr.io/siderolabs/imager:v1.6.2 image --platform metal --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\n\nThe platform network configuration gets merged with other sources of network configuration, the details can be found in the network resources guide.\n\n1.1.6 - PXE\nBooting Talos over the network on bare-metal with PXE.\n\nTalos can be installed on bare-metal using PXE service. There are two more detailed guides for PXE booting using Matchbox and Digital Rebar.\n\nThis guide describes generic steps for PXE booting Talos on bare-metal.\n\nFirst, download the vmlinuz and initramfs assets from the Talos releases page. Set up the machines to PXE boot from the network (usually by setting the boot order in the BIOS). There might be options specific to the hardware being used, booting in BIOS or UEFI mode, using iPXE, etc.\n\nTalos requires the following kernel parameters to be set on the initial boot:\n\ntalos.platform=metal\nslab_nomerge\npti=on\n\nWhen booted from the network without machine configuration, Talos will start in maintenance mode.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from network. The boot order should prefer disk over network.\n\nTalos can automatically fetch the machine configuration from the network on the initial boot using talos.config kernel parameter. A metadata service (HTTP service) can be implemented to deliver customized configuration to each node for example by using the MAC address of the node:\n\ntalos.config=https://metadata.service/talos/config?mac=${mac}\n\n\nNote: The talos.config kernel parameter supports other substitution variables, see kernel parameters reference for the full list.\n\nPXE booting can be also performed via Image Factory.\n\n1.1.7 - SecureBoot\nBooting Talos in SecureBoot mode on UEFI platforms.\n\nTalos now supports booting on UEFI systems in SecureBoot mode. When combined with TPM-based disk encryption, this provides Trusted Boot experience.\n\nNote: SecureBoot is not supported on x86 platforms in BIOS mode.\n\nThe implementation is using systemd-boot as a boot menu implementation, while the Talos kernel, initramfs and cmdline arguments are combined into the Unified Kernel Image (UKI) format. UEFI firmware loads the systemd-boot bootloader, which then loads the UKI image. Both systemd-boot and Talos UKI image are signed with the key, which is enrolled into the UEFI firmware.\n\nAs Talos Linux is fully contained in the UKI image, the full operating system is verified and booted by the UEFI firmware.\n\nNote: There is no support at the moment to upgrade non-UKI (GRUB-based) Talos installation to use UKI/SecureBoot, so a fresh installation is required.\n\nSecureBoot with Sidero Labs Images\n\nSidero Labs provides Talos images signed with the Sidero Labs SecureBoot key via Image Factory.\n\nNote: The SecureBoot images are available for Talos releases starting from v1.5.0.\n\nThe easiest way to get started with SecureBoot is to download the ISO, and boot it on a UEFI-enabled system which has SecureBoot enabled in setup mode.\n\nThe ISO bootloader will enroll the keys in the UEFI firmware, and boot the Talos Linux in SecureBoot mode. The install should performed using SecureBoot installer (put it Talos machine configuration): factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2.\n\nNote: SecureBoot images can also be generated with custom keys.\n\nBooting Talos Linux in SecureBoot Mode\n\nIn this guide we will use the ISO image to boot Talos Linux in SecureBoot mode, followed by submitting machine configuration to the machine in maintenance mode. We will use one the ways to generate and submit machine configuration to the node, please refer to the Production Notes for the full guide.\n\nFirst, make sure SecureBoot is enabled in the UEFI firmware. For the first boot, the UEFI firmware should be in the setup mode, so that the keys can be enrolled into the UEFI firmware automatically. If the UEFI firmware does not support automatic enrollment, you may need to hit Esc to force the boot menu to appear, and select the Enroll Secure Boot keys: auto option.\n\nNote: There are other ways to enroll the keys into the UEFI firmware, but this is out of scope of this guide.\n\nOnce Talos is running in maintenance mode, verify that secure boot is enabled:\n\n$ talosctl -n <IP> get securitystate --insecure\n\nNODE   NAMESPACE   TYPE            ID              VERSION   SECUREBOOT\n\n       runtime     SecurityState   securitystate   1         true\n\n\nNow we will generate the machine configuration for the node supplying the installer-secureboot container image, and applying the patch to enable TPM-based disk encryption (requires TPM 2.0):\n\n# tpm-disk-encryption.yaml\n\nmachine:\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n\nGenerate machine configuration:\n\ntalosctl gen config <cluster-name> https://<endpoint>:6443 --install-image=factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2 --install-disk=/dev/sda --config-patch @tpm-disk-encryption.yaml\n\n\nApply machine configuration to the node:\n\ntalosctl -n <IP> apply-config --insecure -f controlplane.yaml\n\n\nTalos will perform the installation to the disk and reboot the node. Please make sure that the ISO image is not attached to the node anymore, otherwise the node will boot from the ISO image again.\n\nOnce the node is rebooted, verify that the node is running in secure boot mode:\n\ntalosctl -n <IP> --talosconfig=talosconfig get securitystate\n\nUpgrading Talos Linux\n\nAny change to the boot asset (kernel, initramfs, kernel command line) requires the UKI to be regenerated and the installer image to be rebuilt. Follow the steps above to generate new installer image updating the boot assets: use new Talos version, add a system extension, or modify the kernel command line. Once the new installer image is pushed to the registry, upgrade the node using the new installer image.\n\nIt is important to preserve the UKI signing key and the PCR signing key, otherwise the node will not be able to boot with the new UKI and unlock the encrypted partitions.\n\nDisk Encryption with TPM\n\nWhen encrypting the disk partition for the first time, Talos Linux generates a random disk encryption key and seals (encrypts) it with the TPM device. The TPM unlock policy is configured to trust the expected policy signed by the PCR signing key. This way TPM unlocking doesn’t depend on the exact PCR measurements, but rather on the expected policy signed by the PCR signing key and the state of SecureBoot (PCR 7 measurement, including secureboot status and the list of enrolled keys).\n\nWhen the UKI image is generated, the UKI is measured and expected measurements are combined into TPM unlock policy and signed with the PCR signing key. During the boot process, systemd-stub component of the UKI performs measurements of the UKI sections into the TPM device. Talos Linux during the boot appends to the PCR register the measurements of the boot phases, and once the boot reaches the point of mounting the encrypted disk partition, the expected signed policy from the UKI is matched against measured values to unlock the TPM, and TPM unseals the disk encryption key which is then used to unlock the disk partition.\n\nDuring the upgrade, as long as the new UKI is contains PCR policy signed with the same PCR signing key, and SecureBoot state has not changed the disk partition will be unlocked successfully.\n\nDisk encryption is also tied to the state of PCR register 7, so that it unlocks only if SecureBoot is enabled and the set of enrolled keys hasn’t changed.\n\nOther Boot Options\n\nUnified Kernel Image (UKI) is a UEFI-bootable image which can be booted directly from the UEFI firmware skipping the systemd-boot bootloader. In network boot mode, the UKI can be used directly as well, as it contains the full set of boot assets required to boot Talos Linux.\n\nWhen SecureBoot is enabled, the UKI image ignores any kernel command line arguments passed to it, but rather uses the kernel command line arguments embedded into the UKI image itself. If kernel command line arguments need to be changed, the UKI image needs to be rebuilt with the new kernel command line arguments.\n\nSecureBoot with Custom Keys\nGenerating the Keys\n\nTalos requires two set of keys to be used for the SecureBoot process:\n\nSecureBoot key is used to sign the boot assets and it is enrolled into the UEFI firmware.\nPCR Signing Key is used to sign the TPM policy, which is used to seal the disk encryption key.\n\nThe same key might be used for both, but it is recommended to use separate keys for each purpose.\n\nTalos provides a utility to generate the keys, but existing PKI infrastructure can be used as well:\n\n$ talosctl gen secureboot uki --common-name \"SecureBoot Key\"\n\nwriting _out/uki-signing-cert.pem\n\nwriting _out/uki-signing-cert.der\n\nwriting _out/uki-signing-key.pem\n\n\nThe generated certificate and private key are written to disk in PEM-encoded format (RSA 4096-bit key). The certificate is also written in DER format for the systems which expect the certificate in DER format.\n\nPCR signing key can be generated with:\n\n$ talosctl gen secureboot pcr\n\nwriting _out/pcr-signing-key.pem\n\n\nThe file containing the private key is written to disk in PEM-encoded format (RSA 2048-bit key).\n\nOptionally, UEFI automatic key enrollment database can be generated using the _out/uki-signing-* files as input:\n\n$ talosctl gen secureboot database\n\nwriting _out/db.auth\n\nwriting _out/KEK.auth\n\nwriting _out/PK.auth\n\n\nThese files can be used to enroll the keys into the UEFI firmware automatically when booting from a SecureBoot ISO while UEFI firmware is in the setup mode.\n\nGenerating the SecureBoot Assets\n\nOnce the keys are generated, they can be used to sign the Talos boot assets to generate required ISO images, PXE boot assets, disk images, installer containers, etc. In this guide we will generate a SecureBoot ISO image and an installer image.\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-iso\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.5.0-alpha.3-35-ge0f383598-dirty\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\nISO ready\n\noutput asset path: /out/metal-amd64-secureboot.iso\n\n\nNext, the installer image should be generated to install Talos to disk on a SecureBoot-enabled system:\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-installer\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: installer\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\ninstaller container image ready\n\noutput asset path: /out/installer-amd64-secureboot.tar\n\n\nThe generated container image should be pushed to some container registry which Talos can access during the installation, e.g.:\n\ncrane push _out/installer-amd64-secureboot.tar ghcr.io/<user>/installer-amd64-secureboot:v1.6.2\n\n\nThe generated ISO and installer images might be further customized with system extensions, extra kernel command line arguments, etc.\n\n1.2 - Virtualized Platforms\nInstallation of Talos Linux for virtualization platforms.\n1.2.1 - Hyper-V\nCreating a Talos Kubernetes cluster using Hyper-V.\nPre-requisities\nDownload the latest metal-amd64.iso ISO from github releases page\nCreate a New-TalosVM folder in any of your PS Module Path folders $env:PSModulePath -split ';' and save the New-TalosVM.psm1 there\nPlan Overview\n\nHere we will create a basic 3 node cluster with a single control-plane node and two worker nodes. The only difference between control plane and worker node is the amount of RAM and an additional storage VHD. This is personal preference and can be configured to your liking.\n\nWe are using a VMNamePrefix argument for a VM Name prefix and not the full hostname. This command will find any existing VM with that prefix and “+1” the highest suffix it finds. For example, if VMs talos-cp01 and talos-cp02 exist, this will create VMs starting from talos-cp03, depending on NumberOfVMs argument.\n\nSetup a Control Plane Node\n\nUse the following command to create a single control plane node:\n\nNew-TalosVM -VMNamePrefix talos-cp -CPUCount 2 -StartupMemory 4GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 1 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos'\n\n\nThis will create talos-cp01 VM and power it on.\n\nSetup Worker Nodes\n\nUse the following command to create 2 worker nodes:\n\nNew-TalosVM -VMNamePrefix talos-worker -CPUCount 4 -StartupMemory 8GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 2 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos' -StorageVHDSize 50GB\n\n\nThis will create two VMs: talos-worker01 and talos-wworker02 and attach an additional VHD of 50GB for storage (which in my case will be passed to Mayastor).\n\nPushing Config to the Nodes\n\nNow that our VMs are ready, find their IP addresses from console of VM. With that information, push config to the control plane node with:\n\n# set control plane IP variable\n\n$CONTROL_PLANE_IP='10.10.10.x'\n\n\n\n# Generate talos config\n\ntalosctl gen config talos-cluster https://$($CONTROL_PLANE_IP):6443 --output-dir .\n\n\n\n# Apply config to control plane node\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file .\\controlplane.yaml\n\nPushing Config to Worker Nodes\n\nSimilarly, for the workers:\n\ntalosctl apply-config --insecure --nodes 10.10.10.x --file .\\worker.yaml\n\n\nApply the config to both nodes.\n\nBootstrap Cluster\n\nNow that our nodes are ready, we are ready to bootstrap the Kubernetes cluster.\n\n# Use following command to set node and endpoint permanantly in config so you dont have to type it everytime\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\n\n\n# Bootstrap cluster\n\ntalosctl bootstrap\n\n\n\n# Generate kubeconfig\n\ntalosctl kubeconfig .\n\n\nThis will generate the kubeconfig file, you can use to connect to the cluster.\n\n1.2.2 - KVM\n\nTalos is known to work on KVM.\n\nWe don’t yet have a documented guide specific to KVM; however, you can have a look at our Vagrant & Libvirt guide which uses KVM for virtualization.\n\nIf you run into any issues, our community can probably help!\n\n1.2.3 - Proxmox\nCreating Talos Kubernetes cluster using Proxmox.\n\nIn this guide we will create a Kubernetes cluster using Proxmox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get Proxmox\n\nIt is assumed that you have already installed Proxmox onto the server you wish to create Talos VMs on. Visit the Proxmox downloads page if necessary.\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nIn order to install Talos in Proxmox, you will need the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nUpload ISO\n\nFrom the Proxmox UI, select the “local” storage and enter the “Content” section. Click the “Upload” button:\n\nSelect the ISO you downloaded previously, then hit “Upload”\n\nCreate VMs\n\nBefore starting, familiarise yourself with the system requirements for Talos and assign VM resources accordingly.\n\nCreate a new VM by clicking the “Create VM” button in the Proxmox UI:\n\nFill out a name for the new VM:\n\nIn the OS tab, select the ISO we uploaded earlier:\n\nKeep the defaults set in the “System” tab.\n\nKeep the defaults in the “Hard Disk” tab as well, only changing the size if desired.\n\nIn the “CPU” section, give at least 2 cores to the VM:\n\nNote: As of Talos v1.0 (which requires the x86-64-v2 microarchitecture), prior to Proxmox V8.0, booting with the default Processor Type kvm64 will not work. You can enable the required CPU features after creating the VM by adding the following line in the corresponding /etc/pve/qemu-server/<vmid>.conf file:\n\nargs: -cpu kvm64,+cx16,+lahf_lm,+popcnt,+sse3,+ssse3,+sse4.1,+sse4.2\n\n\nAlternatively, you can set the Processor Type to host if your Proxmox host supports these CPU features, this however prevents using live VM migration.\n\nVerify that the RAM is set to at least 2GB:\n\nKeep the default values for networking, verifying that the VM is set to come up on the bridge interface:\n\nFinish creating the VM by clicking through the “Confirm” tab and then “Finish”.\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nNote: Talos doesn’t support memory hot plugging, if creating the VM programmatically don’t enable memory hotplug on your Talos VM’s. Doing so will cause Talos to be unable to see all available memory and have insufficient memory to complete installation of the cluster.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”.\n\nWith DHCP server\n\nOnce the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nWithout DHCP server\n\nTo apply the machine configurations in maintenance mode, VM has to have IP on the network. So you can set it on boot time manually.\n\nPress e on the boot time. And set the IP parameters for the VM. Format is:\n\nip=<client-ip>:<srv-ip>:<gw-ip>:<netmask>:<host>:<device>:<autoconf>\n\n\nFor example $CONTROL_PLANE_IP will be 192.168.0.100 and gateway 192.168.0.1\n\nlinux /boot/vmlinuz init_on_alloc=1 slab_nomerge pti=on panic=0 consoleblank=0 printk.devkmsg=on earlyprintk=ttyS0 console=tty0 console=ttyS0 talos.platform=metal ip=192.168.0.100::192.168.0.1:255.255.255.0::eth0:off\n\n\nThen press Ctrl-x or F10\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nNote: The Talos config by default will install to /dev/sda. Depending on your setup the virtual disk may be mounted differently Eg: /dev/vda. You can check for disks running the following command:\n\ntalosctl disks --insecure --nodes $CONTROL_PLANE_IP\n\n\nUpdate controlplane.yaml and worker.yaml config files to point to the correct disk location.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the Proxmox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\ntalosctl bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig .\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the Proxmox UI.\n\n1.2.4 - Vagrant & Libvirt\nPre-requisities\nLinux OS\nVagrant installed\nvagrant-libvirt plugin installed\ntalosctl installed\nkubectl installed\nOverview\n\nWe will use Vagrant and its libvirt plugin to create a KVM-based cluster with 3 control plane nodes and 1 worker node.\n\nFor this, we will mount Talos ISO into the VMs using a virtual CD-ROM, and configure the VMs to attempt to boot from the disk first with the fallback to the CD-ROM.\n\nWe will also configure a virtual IP address on Talos to achieve high-availability on kube-apiserver.\n\nPreparing the environment\n\nFirst, we download the latest metal-amd64.iso ISO from GitHub releases into the /tmp directory.\n\nwget --timestamping https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -O /tmp/metal-amd64.iso\n\n\nCreate a Vagrantfile with the following contents:\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define \"control-plane-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-2\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-2.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-3\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-3.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"worker-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 1\n\n      domain.memory = 1024\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/worker-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\nend\n\nBring up the nodes\n\nCheck the status of vagrant VMs:\n\nvagrant status\n\n\nYou should see the VMs in “not created” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      not created (libvirt)\n\ncontrol-plane-node-2      not created (libvirt)\n\ncontrol-plane-node-3      not created (libvirt)\n\nworker-node-1             not created (libvirt)\n\n\nBring up the vagrant environment:\n\nvagrant up --provider=libvirt\n\n\nCheck the status again:\n\nvagrant status\n\n\nNow you should see the VMs in “running” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      running (libvirt)\n\ncontrol-plane-node-2      running (libvirt)\n\ncontrol-plane-node-3      running (libvirt)\n\nworker-node-1             running (libvirt)\n\n\nFind out the IP addresses assigned by the libvirt DHCP by running:\n\nvirsh list | grep vagrant | awk '{print $2}' | xargs -t -L1 virsh domifaddr\n\n\nOutput will look like the following:\n\nvirsh domifaddr vagrant_control-plane-node-2\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet0      52:54:00:f9:10:e5    ipv4         192.168.121.119/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet1      52:54:00:0f:ae:59    ipv4         192.168.121.203/24\n\n\n\nvirsh domifaddr vagrant_worker-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet2      52:54:00:6f:28:95    ipv4         192.168.121.69/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-3\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet3      52:54:00:03:45:10    ipv4         192.168.121.125/24\n\n\nOur control plane nodes have the IPs: 192.168.121.203, 192.168.121.119, 192.168.121.125 and the worker node has the IP 192.168.121.69.\n\nNow you should be able to interact with Talos nodes that are in maintenance mode:\n\ntalosctl -n 192.168.121.203 disks --insecure\n\n\nSample output:\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID   MODALIAS                    NAME   SIZE     BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      8.6 GB   /pci0000:00/0000:00:03.0/virtio0/\n\nInstalling Talos\n\nPick an endpoint IP in the vagrant-libvirt subnet but not used by any nodes, for example 192.168.121.100.\n\nGenerate a machine configuration:\n\ntalosctl gen config my-cluster https://192.168.121.100:6443 --install-disk /dev/vda\n\n\nEdit controlplane.yaml to add the virtual IP you picked to a network interface under .machine.network.interfaces, for example:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.121.100\n\n\nApply the configuration to the initial control plane node:\n\ntalosctl -n 192.168.121.203 apply-config --insecure --file controlplane.yaml\n\n\nYou can tail the logs of the node:\n\nsudo tail -f /tmp/control-plane-node-1.log\n\n\nSet up your shell to use the generated talosconfig and configure its endpoints (use the IPs of the control plane nodes):\n\nexport TALOSCONFIG=$(realpath ./talosconfig)\n\ntalosctl config endpoint 192.168.121.203 192.168.121.119 192.168.121.125\n\n\nBootstrap the Kubernetes cluster from the initial control plane node:\n\ntalosctl -n 192.168.121.203 bootstrap\n\n\nFinally, apply the machine configurations to the remaining nodes:\n\ntalosctl -n 192.168.121.119 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.125 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.69 apply-config --insecure --file worker.yaml\n\n\nAfter a while, you should see that all the members have joined:\n\ntalosctl -n 192.168.121.203 get members\n\n\nThe output will be like the following:\n\nNODE              NAMESPACE   TYPE     ID                      VERSION   HOSTNAME                MACHINE TYPE   OS               ADDRESSES\n\n192.168.121.203   cluster     Member   talos-192-168-121-119   1         talos-192-168-121-119   controlplane   Talos (v1.1.0)   [\"192.168.121.119\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-69    1         talos-192-168-121-69    worker         Talos (v1.1.0)   [\"192.168.121.69\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-203   6         talos-192-168-121-203   controlplane   Talos (v1.1.0)   [\"192.168.121.100\",\"192.168.121.203\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-125   1         talos-192-168-121-125   controlplane   Talos (v1.1.0)   [\"192.168.121.125\"]\n\nInteracting with Kubernetes cluster\n\nRetrieve the kubeconfig from the cluster:\n\ntalosctl -n 192.168.121.203 kubeconfig ./kubeconfig\n\n\nList the nodes in the cluster:\n\nkubectl --kubeconfig ./kubeconfig get node -owide\n\n\nYou will see an output similar to:\n\nNAME                    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-192-168-121-203   Ready    control-plane,master   3m10s   v1.24.2   192.168.121.203   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-69    Ready    <none>                 2m25s   v1.24.2   192.168.121.69    <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-119   Ready    control-plane,master   8m46s   v1.24.2   192.168.121.119   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-125   Ready    control-plane,master   3m11s   v1.24.2   192.168.121.125   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\n\nCongratulations, you have a highly-available Talos cluster running!\n\nCleanup\n\nYou can destroy the vagrant environment by running:\n\nvagrant destroy -f\n\n\nAnd remove the ISO image you downloaded:\n\nsudo rm -f /tmp/metal-amd64.iso\n\n1.2.5 - VMware\nCreating Talos Kubernetes cluster using VMware.\nCreating a Cluster via the govc CLI\n\nIn this guide we will create an HA Kubernetes cluster with 2 worker nodes. We will use the govc cli which can be downloaded here.\n\nPrereqs/Assumptions\n\nThis guide will use the virtual IP (“VIP”) functionality that is built into Talos in order to provide a stable, known IP for the Kubernetes control plane. This simply means the user should pick an IP on their “VM Network” to designate for this purpose and keep it handy for future steps.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the VIP chosen in the prereq steps, we will now generate the base configuration files for the Talos machines. This can be done with the talosctl gen config ... command. Take note that we will also use a JSON6902 patch when creating the configs so that the control plane nodes get some special information about the VIP we chose earlier, as well as a daemonset to install vmware tools on talos nodes.\n\nFirst, download cp.patch.yaml to your local machine and edit the VIP to match your chosen IP. You can do this by issuing: curl -fsSLO https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/cp.patch.yaml. It’s contents should look like the following:\n\n- op: add\n\n  path: /machine/network\n\n  value:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: <VIP>\n\n- op: replace\n\n  path: /cluster/extraManifests\n\n  value:\n\n    - \"https://raw.githubusercontent.com/mologie/talos-vmtoolsd/master/deploy/unstable.yaml\"\n\n\nWith the patch in hand, generate machine configs with:\n\n$ talosctl gen config vmware-test https://<VIP>:<port> --config-patch-control-plane @cp.patch.yaml\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking if needed. Optionally, you can specify additional patches by adding to the cp.patch.yaml file downloaded earlier, or create your own patch files.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nSet Environment Variables\n\ngovc makes use of the following environment variables\n\nexport GOVC_URL=<vCenter url>\n\nexport GOVC_USERNAME=<vCenter username>\n\nexport GOVC_PASSWORD=<vCenter password>\n\n\nNote: If your vCenter installation makes use of self signed certificates, you’ll want to export GOVC_INSECURE=true.\n\nThere are some additional variables that you may need to set:\n\nexport GOVC_DATACENTER=<vCenter datacenter>\n\nexport GOVC_RESOURCE_POOL=<vCenter resource pool>\n\nexport GOVC_DATASTORE=<vCenter datastore>\n\nexport GOVC_NETWORK=<vCenter network>\n\nChoose Install Approach\n\nAs part of this guide, we have a more automated install script that handles some of the complexity of importing OVAs and creating VMs. If you wish to use this script, we will detail that next. If you wish to carry out the manual approach, simply skip ahead to the “Manual Approach” section.\n\nScripted Install\n\nDownload the vmware.sh script to your local machine. You can do this by issuing curl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/vmware.sh\". This script has default variables for things like Talos version and cluster name that may be interesting to tweak before deploying.\n\nImport OVA\n\nTo create a content library and import the Talos OVA corresponding to the mentioned Talos version, simply issue:\n\n./vmware.sh upload_ova\n\nCreate Cluster\n\nWith the OVA uploaded to the content library, you can create a 5 node (by default) cluster with 3 control plane and 2 worker nodes:\n\n./vmware.sh create\n\n\nThis step will create a VM from the OVA, edit the settings based on the env variables used for VM size/specs, then power on the VMs.\n\nYou may now skip past the “Manual Approach” section down to “Bootstrap Cluster”.\n\nManual Approach\nImport the OVA into vCenter\n\nA talos.ova asset is published with each release. We will refer to the version of the release as $TALOS_VERSION below. It can be easily exported with export TALOS_VERSION=\"v0.3.0-alpha.10\" or similar.\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/$TALOS_VERSION/talos.ova\n\n\nCreate a content library (if needed) with:\n\ngovc library.create <library name>\n\n\nImport the OVA to the library with:\n\ngovc library.import -n talos-${TALOS_VERSION} <library name> /path/to/downloaded/talos.ova\n\nCreate the Bootstrap Node\n\nWe’ll clone the OVA to create the bootstrap node (our first control plane node).\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-1\n\n\nTalos makes use of the guestinfo facility of VMware to provide the machine/cluster configuration. This can be set using the govc vm.change command. To facilitate persistent storage using the vSphere cloud provider integration with Kubernetes, disk.enableUUID=1 is used.\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(cat controlplane.yaml | base64)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-1\n\nUpdate Hardware Resources for the Bootstrap Node\n-c is used to configure the number of cpus\n-m is used to configure the amount of memory (in MB)\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-1\n\n\nThe following can be used to adjust the EPHEMERAL disk size.\n\ngovc vm.disk.change -vm control-plane-1 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-1\n\nCreate the Remaining Control Plane Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-2\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-3\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-3\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-2\n\n\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-3\n\ngovc vm.disk.change -vm control-plane-2 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm control-plane-3 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-2\n\n\n\ngovc vm.power -on control-plane-3\n\nUpdate Settings for the Worker Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-1\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-1\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-2\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-1\n\n\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-2\n\ngovc vm.disk.change -vm worker-1 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm worker-2 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on worker-1\n\n\n\ngovc vm.power -on worker-2\n\nBootstrap Cluster\n\nIn the vSphere UI, open a console to one of the control plane nodes. You should see some output stating that etcd should be bootstrapped. This text should look like:\n\n\"etcd is waiting to join the cluster, if this node is the first node in the cluster, please run `talosctl bootstrap` against one of the following IPs:\n\n\nTake note of the IP mentioned here and issue:\n\ntalosctl --talosconfig talosconfig bootstrap -e <control plane IP> -n <control plane IP>\n\n\nKeep this IP handy for the following steps as well.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane IP>\n\ntalosctl --talosconfig talosconfig config node <control plane IP>\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nConfigure talos-vmtoolsd\n\nThe talos-vmtoolsd application was deployed as a daemonset as part of the cluster creation; however, we must now provide a talos credentials file for it to use.\n\nCreate a new talosconfig with:\n\ntalosctl --talosconfig talosconfig -n <control plane IP> config new vmtoolsd-secret.yaml --roles os:admin\n\n\nCreate a secret from the talosconfig:\n\nkubectl -n kube-system create secret generic talos-vmtoolsd-config \\\n\n  --from-file=talosconfig=./vmtoolsd-secret.yaml\n\n\nClean up the generated file from local system:\n\nrm vmtoolsd-secret.yaml\n\n\nOnce configured, you should now see these daemonset pods go into “Running” state and in vCenter, you will now see IPs and info from the Talos nodes present in the UI.\n\n1.2.6 - Xen\n\nTalos is known to work on Xen. We don’t yet have a documented guide specific to Xen; however, you can follow the General Getting Started Guide. If you run into any issues, our community can probably help!\n\n1.3 - Cloud Platforms\nInstallation of Talos Linux on many cloud platforms.\n1.3.1 - AWS\nCreating a cluster via the AWS CLI.\nCreating a Cluster via the AWS CLI\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing VPC, and some familiarity with AWS. If you need more information on AWS specifics, please see the official AWS documentation.\n\nSet the needed info\n\nChange to your desired region:\n\nREGION=\"us-west-2\"\n\naws ec2 describe-vpcs --region $REGION\n\n\n\nVPC=\"(the VpcId from the above command)\"\n\nCreate the Subnet\n\nUse a CIDR block that is present on the VPC specified above.\n\naws ec2 create-subnet \\\n\n    --region $REGION \\\n\n    --vpc-id $VPC \\\n\n    --cidr-block ${CIDR_BLOCK}\n\n\nNote the subnet ID that was returned, and assign it to a variable for ease of later use:\n\nSUBNET=\"(the subnet ID of the created subnet)\"\n\nOfficial AMI Images\n\nOfficial AMI image ID can be found in the cloud-images.json file attached to the Talos release:\n\nAMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.6.2/cloud-images.json | \\\n\n    jq -r '.[] | select(.region == \"'$REGION'\") | select (.arch == \"amd64\") | .id'`\n\necho $AMI\n\n\nReplace amd64 in the line above with the desired architecture. Note the AMI id that is returned is assigned to an environment variable: it will be used later when booting instances.\n\nIf using the official AMIs, you can skip to Creating the Security group\n\nCreate your own AMIs\n\nThe use of the official Talos AMIs are recommended, but if you wish to build your own AMIs, follow the procedure below.\n\nCreate the S3 Bucket\naws s3api create-bucket \\\n\n    --bucket $BUCKET \\\n\n    --create-bucket-configuration LocationConstraint=$REGION \\\n\n    --acl private\n\nCreate the vmimport Role\n\nIn order to create an AMI, ensure that the vmimport role exists as described in the official AWS documentation.\n\nNote that the role should be associated with the S3 bucket we created above.\n\nCreate the Image Snapshot\n\nFirst, download the AWS image from a Talos release:\n\ncurl -L https://github.com/siderolabs/talos/releases/download/v1.6.2/aws-amd64.raw.xz | xz -d > disk.raw\n\n\nCopy the RAW disk to S3 and import it as a snapshot:\n\naws s3 cp disk.raw s3://$BUCKET/talos-aws-tutorial.raw\n\naws ec2 import-snapshot \\\n\n    --region $REGION \\\n\n    --description \"Talos kubernetes tutorial\" \\\n\n    --disk-container \"Format=raw,UserBucket={S3Bucket=$BUCKET,S3Key=talos-aws-tutorial.raw}\"\n\n\nSave the SnapshotId, as we will need it once the import is done. To check on the status of the import, run:\n\naws ec2 describe-import-snapshot-tasks \\\n\n    --region $REGION \\\n\n    --import-task-ids\n\n\nOnce the SnapshotTaskDetail.Status indicates completed, we can register the image.\n\nRegister the Image\naws ec2 register-image \\\n\n    --region $REGION \\\n\n    --block-device-mappings \"DeviceName=/dev/xvda,VirtualName=talos,Ebs={DeleteOnTermination=true,SnapshotId=$SNAPSHOT,VolumeSize=4,VolumeType=gp2}\" \\\n\n    --root-device-name /dev/xvda \\\n\n    --virtualization-type hvm \\\n\n    --architecture x86_64 \\\n\n    --ena-support \\\n\n    --name talos-aws-tutorial-ami\n\n\nWe now have an AMI we can use to create our cluster. Save the AMI ID, as we will need it when we create EC2 instances.\n\nAMI=\"(AMI ID of the register image command)\"\n\nCreate a Security Group\naws ec2 create-security-group \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --description \"Security Group for EC2 instances to allow ports required by Talos\"\n\n\n\nSECURITY_GROUP=\"(security group id that is returned)\"\n\n\nUsing the security group from above, allow all internal traffic within the same security group:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol all \\\n\n    --port 0 \\\n\n    --source-group talos-aws-tutorial-sg\n\n\nand expose the Talos and Kubernetes APIs:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 6443 \\\n\n    --cidr 0.0.0.0/0\n\n\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 50000-50001 \\\n\n    --cidr 0.0.0.0/0\n\n\nIf you are using KubeSpan and will be adding workers outside of AWS, you need to allow inbound UDP for the Wireguard port:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol udp --port 51820 --cidr 0.0.0.0/0\n\nCreate a Load Balancer\naws elbv2 create-load-balancer \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-lb \\\n\n    --type network --subnets $SUBNET\n\n\nTake note of the DNS name and ARN. We will need these soon.\n\nLOAD_BALANCER_ARN=\"(arn of the load balancer)\"\n\naws elbv2 create-target-group \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-tg \\\n\n    --protocol TCP \\\n\n    --port 6443 \\\n\n    --target-type ip \\\n\n    --vpc-id $VPC\n\n\nAlso note the TargetGroupArn that is returned.\n\nTARGET_GROUP_ARN=\"(target group arn)\"\n\nCreate the Machine Configuration Files\n\nUsing the DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines.\n\nNote that the port used here is the externally accessible port configured on the load balancer - 443 - not the internal port of 6443:\n\n$ talosctl gen config talos-k8s-aws-tutorial https://<load balancer DNS>:<port> --with-examples=false --with-docs=false\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nNote that the generated configs are too long for AWS userdata field if the --with-examples and --with-docs flags are not passed.\n\nAt this point, you can modify the generated configs to your liking.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the EC2 Instances\n\nchange the instance type if desired. Note: There is a known issue that prevents Talos from running on T2 instance types. Please use T3 if you need burstable instance types.\n\nCreate the Control Plane Nodes\nCP_COUNT=1\n\nwhile [[ \"$CP_COUNT\" -lt 4 ]]; do\n\n  aws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 1 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://controlplane.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP \\\n\n    --associate-public-ip-address \\\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\"\n\n  ((CP_COUNT++))\n\ndone\n\n\nMake a note of the resulting PrivateIpAddress from the controlplane nodes for later use.\n\nCreate the Worker Nodes\naws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 3 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://worker.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\"\n\nConfigure the Load Balancer\n\nNow, using the load balancer target group’s ARN, and the PrivateIpAddress from the controlplane instances that you created :\n\naws elbv2 register-targets \\\n\n    --region $REGION \\\n\n    --target-group-arn $TARGET_GROUP_ARN \\\n\n    --targets Id=$CP_NODE_1_IP  Id=$CP_NODE_2_IP  Id=$CP_NODE_3_IP\n\n\nUsing the ARNs of the load balancer and target group from previous steps, create the listener:\n\naws elbv2 create-listener \\\n\n    --region $REGION \\\n\n    --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n    --protocol TCP \\\n\n    --port 443 \\\n\n    --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN\n\nBootstrap Etcd\n\nSet the endpoints (the control plane node to which talosctl commands are sent) and nodes (the nodes that the command operates on):\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 PUBLIC IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 PUBLIC IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nThe different control plane nodes should sendi/receive traffic via the load balancer, notice that one of the control plane has intiated the etcd cluster, and the others should join. You can now watch as your cluster bootstraps, by using\n\ntalosctl --talosconfig talosconfig  health\n\n\nYou can also watch the performance of a node, via:\n\ntalosctl  --talosconfig talosconfig dashboard\n\n\nAnd use standard kubectl commands.\n\n1.3.2 - Azure\nCreating a cluster via the CLI on Azure.\nCreating a Cluster via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node. We assume existing Blob Storage, and some familiarity with Azure. If you need more information on Azure specifics, please see the official Azure documentation.\n\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_ACCOUNT=\"StorageAccountName\"\n\n\n\n# Storage container to upload to\n\nexport STORAGE_CONTAINER=\"StorageContainerName\"\n\n\n\n# Resource group name\n\nexport GROUP=\"ResourceGroupName\"\n\n\n\n# Location\n\nexport LOCATION=\"centralus\"\n\n\n\n# Get storage account connection string based on info above\n\nexport CONNECTION=$(az storage account show-connection-string \\\n\n                    -n $STORAGE_ACCOUNT \\\n\n                    -g $GROUP \\\n\n                    -o tsv)\n\nChoose an Image\n\nThere are two methods of deployment in this tutorial.\n\nIf you would like to use the official Talos image uploaded to Azure Community Galleries by SideroLabs, you may skip ahead to setting up your network infrastructure.\n\nNetwork Infrastructure\n\nOtherwise, if you would like to upload your own image to Azure and use it to deploy Talos, continue to Creating an Image.\n\nCreate the Image\n\nFirst, download the Azure image from a Talos release. Once downloaded, untar with tar -xvf /path/to/azure-amd64.tar.gz\n\nUpload the VHD\n\nOnce you have pulled down the image, you can upload it to blob storage with:\n\naz storage blob upload \\\n\n  --connection-string $CONNECTION \\\n\n  --container-name $STORAGE_CONTAINER \\\n\n  -f /path/to/extracted/talos-azure.vhd \\\n\n  -n talos-azure.vhd\n\nRegister the Image\n\nNow that the image is present in our blob storage, we’ll register it.\n\naz image create \\\n\n  --name talos \\\n\n  --source https://$STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER/talos-azure.vhd \\\n\n  --os-type linux \\\n\n  -g $GROUP\n\nNetwork Infrastructure\nVirtual Networks and Security Groups\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a network security group and add rules to it.\n\n# Create vnet\n\naz network vnet create \\\n\n  --resource-group $GROUP \\\n\n  --location $LOCATION \\\n\n  --name talos-vnet \\\n\n  --subnet-name talos-subnet\n\n\n\n# Create network security group\n\naz network nsg create -g $GROUP -n talos-sg\n\n\n\n# Client -> apid\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n apid \\\n\n  --priority 1001 \\\n\n  --destination-port-ranges 50000 \\\n\n  --direction inbound\n\n\n\n# Trustd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n trustd \\\n\n  --priority 1002 \\\n\n  --destination-port-ranges 50001 \\\n\n  --direction inbound\n\n\n\n# etcd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n etcd \\\n\n  --priority 1003 \\\n\n  --destination-port-ranges 2379-2380 \\\n\n  --direction inbound\n\n\n\n# Kubernetes API Server\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n kube \\\n\n  --priority 1004 \\\n\n  --destination-port-ranges 6443 \\\n\n  --direction inbound\n\nLoad Balancer\n\nWe will create a public ip, load balancer, and a health check that we will use for our control plane.\n\n# Create public ip\n\naz network public-ip create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-public-ip \\\n\n  --allocation-method static\n\n\n\n# Create lb\n\naz network lb create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-lb \\\n\n  --public-ip-address talos-public-ip \\\n\n  --frontend-ip-name talos-fe \\\n\n  --backend-pool-name talos-be-pool\n\n\n\n# Create health check\n\naz network lb probe create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-lb-health \\\n\n  --protocol tcp \\\n\n  --port 6443\n\n\n\n# Create lb rule for 6443\n\naz network lb rule create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-6443 \\\n\n  --protocol tcp \\\n\n  --frontend-ip-name talos-fe \\\n\n  --frontend-port 6443 \\\n\n  --backend-pool-name talos-be-pool \\\n\n  --backend-port 6443 \\\n\n  --probe-name talos-lb-health\n\nNetwork Interfaces\n\nIn Azure, we have to pre-create the NICs for our control plane so that they can be associated with our load balancer.\n\nfor i in $( seq 0 1 2 ); do\n\n  # Create public IP for each nic\n\n  az network public-ip create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-public-ip-$i \\\n\n    --allocation-method static\n\n\n\n\n\n  # Create nic\n\n  az network nic create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-nic-$i \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --network-security-group talos-sg \\\n\n    --public-ip-address talos-controlplane-public-ip-$i\\\n\n    --lb-name talos-lb \\\n\n    --lb-address-pools talos-be-pool\n\ndone\n\n\n\n# NOTES:\n\n# Talos can detect PublicIPs automatically if PublicIP SKU is Basic.\n\n# Use `--sku Basic` to set SKU to Basic.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(az network public-ip show \\\n\n              --resource-group $GROUP \\\n\n              --name talos-public-ip \\\n\n              --query \"ipAddress\" \\\n\n              --output tsv)\n\n\n\ntalosctl gen config talos-k8s-azure-tutorial https://${LB_PUBLIC_IP}:6443\n\nCompute Creation\n\nWe are now ready to create our azure nodes. Azure allows you to pass Talos machine configuration to the virtual machine at bootstrap time via user-data or custom-data methods.\n\nTalos supports only custom-data method, machine configuration is available to the VM only on the first boot.\n\nUse the steps below depending on whether you have manually uploaded a Talos image or if you are using the Community Gallery image.\n\nManual Image Upload\nAzure Community Gallery Image\nManual Image Upload\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image talos \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image talos \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nAzure Community Gallery Image\n\nTalos is updated in Azure’s Community Galleries (Preview) on every release.\n\nTo use the Talos image for the current release create the following environment variables.\n\nEdit the variables below if you would like to use a different architecture or version.\n\n# The architecture you would like to use. Options are \"talos-x64\" or \"talos-arm64\"\n\nARCHITECTURE=\"talos-x64\"\n\n\n\n# This will use the latest version of Talos. The version must be \"latest\" or in the format Major(int).Minor(int).Patch(int), e.g. 1.5.0\n\nVERSION=\"latest\"\n\n\nCreate the Virtual Machines.\n\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(az network public-ip show \\\n\n                    --resource-group $GROUP \\\n\n                    --name talos-controlplane-public-ip-0 \\\n\n                    --query \"ipAddress\" \\\n\n                    --output tsv)\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.3.3 - DigitalOcean\nCreating a cluster via the CLI on DigitalOcean.\nCreating a Talos Linux Cluster on Digital Ocean via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node, in the NYC region. We assume an existing Space, and some familiarity with DigitalOcean. If you need more information on DigitalOcean specifics, please see the official DigitalOcean documentation.\n\nCreate the Image\n\nDownload the DigitalOcean image digital-ocean-amd64.raw.gz from the latest Talos release.\n\nNote: the minimum version of Talos required to support Digital Ocean is v1.3.3.\n\nUsing an upload method of your choice (doctl does not have Spaces support), upload the image to a space. (It’s easy to drag the image file to the space using DigitalOcean’s web console.)\n\nNote: Make sure you upload the file as public.\n\nNow, create an image using the URL of the uploaded image:\n\nexport REGION=nyc3\n\n\n\ndoctl compute image create \\\n\n    --region $REGION \\\n\n    --image-description talos-digital-ocean-tutorial \\\n\n    --image-url https://$SPACENAME.$REGION.digitaloceanspaces.com/digital-ocean-amd64.raw.gz \\\n\n    Talos\n\n\nSave the image ID. We will need it when creating droplets.\n\nCreate a Load Balancer\ndoctl compute load-balancer create \\\n\n    --region $REGION \\\n\n    --name talos-digital-ocean-tutorial-lb \\\n\n    --tag-name talos-digital-ocean-tutorial-control-plane \\\n\n    --health-check protocol:tcp,port:6443,check_interval_seconds:10,response_timeout_seconds:5,healthy_threshold:5,unhealthy_threshold:3 \\\n\n    --forwarding-rules entry_protocol:tcp,entry_port:443,target_protocol:tcp,target_port:6443\n\n\nNote the returned ID of the load balancer.\n\nWe will need the IP of the load balancer. Using the ID of the load balancer, run:\n\ndoctl compute load-balancer get --format IP <load balancer ID>\n\n\nNote that it may take a few minutes before the load balancer is provisioned, so repeat this command until it returns with the IP address.\n\nCreate the Machine Configuration Files\n\nUsing the IP address (or DNS name, if you have created one) of the loadbalancer, generate the base configuration files for the Talos machines. Also note that the load balancer forwards port 443 to port 6443 on the associated nodes, so we should use 443 as the port in the config definition:\n\n$ talosctl gen config talos-k8s-digital-ocean-tutorial https://<load balancer IP or DNS>:443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCreate the Droplets\nCreate a dummy SSH key\n\nAlthough SSH is not used by Talos, DigitalOcean requires that an SSH key be associated with a droplet during creation. We will create a dummy key that can be used to satisfy this requirement.\n\ndoctl compute ssh-key create --public-key \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDbl0I1s/yOETIKjFr7mDLp8LmJn6OIZ68ILjVCkoN6lzKmvZEqEm1YYeWoI0xgb80hQ1fKkl0usW6MkSqwrijoUENhGFd6L16WFL53va4aeJjj2pxrjOr3uBFm/4ATvIfFTNVs+VUzFZ0eGzTgu1yXydX8lZMWnT4JpsMraHD3/qPP+pgyNuI51LjOCG0gVCzjl8NoGaQuKnl8KqbSCARIpETg1mMw+tuYgaKcbqYCMbxggaEKA0ixJ2MpFC/kwm3PcksTGqVBzp3+iE5AlRe1tnbr6GhgT839KLhOB03j7lFl1K9j1bMTOEj5Io8z7xo/XeF2ZQKHFWygAJiAhmKJ dummy@dummy.local\" dummy\n\n\nNote the ssh key ID that is returned - we will use it in creating the droplets.\n\nCreate the Control Plane Nodes\n\nRun the following commands to create three control plane nodes:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-1\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-2\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-3\n\n\nNote the droplet ID returned for the first control plane node.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --user-data-file worker.yaml \\\n\n    --ssh-keys <ssh key ID>  \\\n\n    talos-worker-1\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\ndoctl compute droplet get --format PublicIPv4 <droplet ID>\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nWe can also watch the cluster bootstrap via:\n\ntalosctl --talosconfig talosconfig health\n\n1.3.4 - Exoscale\nCreating a cluster via the CLI using exoscale.com\n\nTalos is known to work on exoscale.com; however, it is currently undocumented.\n\n1.3.5 - GCP\nCreating a cluster via the CLI on Google Cloud Platform.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in GCP with 1 worker node. We will assume an existing Cloud Storage bucket, and some familiarity with Google Cloud. If you need more information on Google Cloud specifics, please see the official Google documentation.\n\njq and talosctl also needs to be installed\n\nManual Setup\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_BUCKET=\"StorageBucketName\"\n\n# Region\n\nexport REGION=\"us-central1\"\n\nCreate the Image\n\nFirst, download the Google Cloud image from a Talos release. These images are called gcp-$ARCH.tar.gz.\n\nUpload the Image\n\nOnce you have downloaded the image, you can upload it to your storage bucket with:\n\ngsutil cp /path/to/gcp-amd64.tar.gz gs://$STORAGE_BUCKET\n\nRegister the image\n\nNow that the image is present in our bucket, we’ll register it.\n\ngcloud compute images create talos \\\n\n --source-uri=gs://$STORAGE_BUCKET/gcp-amd64.tar.gz \\\n\n --guest-os-features=VIRTIO_SCSI_MULTIQUEUE\n\nNetwork Infrastructure\nLoad Balancers and Firewalls\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a firewall, load balancer, and their required components.\n\n130.211.0.0/22 and 35.191.0.0/16 are the GCP Load Balancer IP ranges\n\n# Create Instance Group\n\ngcloud compute instance-groups unmanaged create talos-ig \\\n\n  --zone $REGION-b\n\n\n\n# Create port for IG\n\ngcloud compute instance-groups set-named-ports talos-ig \\\n\n    --named-ports tcp6443:6443 \\\n\n    --zone $REGION-b\n\n\n\n# Create health check\n\ngcloud compute health-checks create tcp talos-health-check --port 6443\n\n\n\n# Create backend\n\ngcloud compute backend-services create talos-be \\\n\n    --global \\\n\n    --protocol TCP \\\n\n    --health-checks talos-health-check \\\n\n    --timeout 5m \\\n\n    --port-name tcp6443\n\n\n\n# Add instance group to backend\n\ngcloud compute backend-services add-backend talos-be \\\n\n    --global \\\n\n    --instance-group talos-ig \\\n\n    --instance-group-zone $REGION-b\n\n\n\n# Create tcp proxy\n\ngcloud compute target-tcp-proxies create talos-tcp-proxy \\\n\n    --backend-service talos-be \\\n\n    --proxy-header NONE\n\n\n\n# Create LB IP\n\ngcloud compute addresses create talos-lb-ip --global\n\n\n\n# Forward 443 from LB IP to tcp proxy\n\ngcloud compute forwarding-rules create talos-fwd-rule \\\n\n    --global \\\n\n    --ports 443 \\\n\n    --address talos-lb-ip \\\n\n    --target-tcp-proxy talos-tcp-proxy\n\n\n\n# Create firewall rule for health checks\n\ngcloud compute firewall-rules create talos-controlplane-firewall \\\n\n     --source-ranges 130.211.0.0/22,35.191.0.0/16 \\\n\n     --target-tags talos-controlplane \\\n\n     --allow tcp:6443\n\n\n\n# Create firewall rule to allow talosctl access\n\ngcloud compute firewall-rules create talos-controlplane-talosctl \\\n\n  --source-ranges 0.0.0.0/0 \\\n\n  --target-tags talos-controlplane \\\n\n  --allow tcp:50000\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(gcloud compute forwarding-rules describe talos-fwd-rule \\\n\n               --global \\\n\n               --format json \\\n\n               | jq -r .IPAddress)\n\n\n\ntalosctl gen config talos-k8s-gcp-tutorial https://${LB_PUBLIC_IP}:443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our GCP nodes.\n\n# Create the control plane nodes.\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instances create talos-controlplane-$i \\\n\n    --image talos \\\n\n    --zone $REGION-b \\\n\n    --tags talos-controlplane \\\n\n    --boot-disk-size 20GB \\\n\n    --metadata-from-file=user-data=./controlplane.yaml\n\n    --tags talos-controlplane-$i\n\ndone\n\n\n\n# Add control plane nodes to instance group\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instance-groups unmanaged add-instances talos-ig \\\n\n      --zone $REGION-b \\\n\n      --instances talos-controlplane-$i\n\ndone\n\n\n\n# Create worker\n\ngcloud compute instances create talos-worker-0 \\\n\n  --image talos \\\n\n  --zone $REGION-b \\\n\n  --boot-disk-size 20GB \\\n\n  --metadata-from-file=user-data=./worker.yaml\n\n  --tags talos-worker-$i\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(gcloud compute instances describe talos-controlplane-0 \\\n\n                     --zone $REGION-b \\\n\n                     --format json \\\n\n                     | jq -r '.networkInterfaces[0].accessConfigs[0].natIP')\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nCleanup\n# cleanup VM's\n\ngcloud compute instances delete \\\n\n  talos-worker-0 \\\n\n  talos-controlplane-0 \\\n\n  talos-controlplane-1 \\\n\n  talos-controlplane-2\n\n\n\n# cleanup firewall rules\n\ngcloud compute firewall-rules delete \\\n\n  talos-controlplane-talosctl \\\n\n  talos-controlplane-firewall\n\n\n\n# cleanup forwarding rules\n\ngcloud compute forwarding-rules delete \\\n\n  talos-fwd-rule\n\n\n\n# cleanup addresses\n\ngcloud compute addresses delete \\\n\n  talos-lb-ip\n\n\n\n# cleanup proxies\n\ngcloud compute target-tcp-proxies delete \\\n\n  talos-tcp-proxy\n\n\n\n# cleanup backend services\n\ngcloud compute backend-services delete \\\n\n  talos-be\n\n\n\n# cleanup health checks\n\ngcloud compute health-checks delete \\\n\n  talos-health-check\n\n\n\n# cleanup unmanaged instance groups\n\ngcloud compute instance-groups unmanaged delete \\\n\n  talos-ig\n\n\n\n# cleanup Talos image\n\ngcloud compute images delete \\\n\n  talos\n\nUsing GCP Deployment manager\n\nUsing GCP deployment manager automatically creates a Google Storage bucket and uploads the Talos image to it. Once the deployment is complete the generated talosconfig and kubeconfig files are uploaded to the bucket.\n\nBy default this setup creates a three node control plane and a single worker in us-west1-b\n\nFirst we need to create a folder to store our deployment manifests and perform all subsequent operations from that folder.\n\nmkdir -p talos-gcp-deployment\n\ncd talos-gcp-deployment\n\nGetting the deployment manifests\n\nWe need to download two deployment manifests for the deployment from the Talos github repository.\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/config.yaml\"\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/talos-ha.jinja\"\n\n# if using ccm\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/gcp-ccm.yaml\"\n\nUpdating the config\n\nNow we need to update the local config.yaml file with any required changes such as changing the default zone, Talos version, machine sizes, nodes count etc.\n\nAn example config.yaml file is shown below:\n\nimports:\n\n  - path: talos-ha.jinja\n\n\n\nresources:\n\n  - name: talos-ha\n\n    type: talos-ha.jinja\n\n    properties:\n\n      zone: us-west1-b\n\n      talosVersion: v1.6.2\n\n      externalCloudProvider: false\n\n      controlPlaneNodeCount: 5\n\n      controlPlaneNodeType: n1-standard-1\n\n      workerNodeCount: 3\n\n      workerNodeType: n1-standard-1\n\noutputs:\n\n  - name: bucketName\n\n    value: $(ref.talos-ha.bucketName)\n\nEnabling external cloud provider\n\nNote: The externalCloudProvider property is set to false by default. The manifest used for deploying the ccm (cloud controller manager) is currently using the GCP ccm provided by openshift since there are no public images for the ccm yet.\n\nSince the routes controller is disabled while deploying the CCM, the CNI pods needs to be restarted after the CCM deployment is complete to remove the node.kubernetes.io/network-unavailable taint. See Nodes network-unavailable taint not removed after installing ccm for more information\n\nUse a custom built image for the ccm deployment if required.\n\nCreating the deployment\n\nNow we are ready to create the deployment. Confirm with y for any prompts. Run the following command to create the deployment:\n\n# use a unique name for the deployment, resources are prefixed with the deployment name\n\nexport DEPLOYMENT_NAME=\"<deployment name>\"\n\ngcloud deployment-manager deployments create \"${DEPLOYMENT_NAME}\" --config config.yaml\n\nRetrieving the outputs\n\nFirst we need to get the deployment outputs.\n\n# first get the outputs\n\nOUTPUTS=$(gcloud deployment-manager deployments describe \"${DEPLOYMENT_NAME}\" --format json | jq '.outputs[]')\n\n\n\nBUCKET_NAME=$(jq -r '. | select(.name == \"bucketName\").finalValue' <<< \"${OUTPUTS}\")\n\n# used when cloud controller is enabled\n\nSERVICE_ACCOUNT=$(jq -r '. | select(.name == \"serviceAccount\").finalValue' <<< \"${OUTPUTS}\")\n\nPROJECT=$(jq -r '. | select(.name == \"project\").finalValue' <<< \"${OUTPUTS}\")\n\n\nNote: If cloud controller manager is enabled, the below command needs to be run to allow the controller custom role to access cloud resources\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\nDownloading talos and kube config\n\nIn addition to the talosconfig and kubeconfig files, the storage bucket contains the controlplane.yaml and worker.yaml files used to join additional nodes to the cluster.\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/talosconfig\" .\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/kubeconfig\" .\n\nDeploying the cloud controller manager\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  apply \\\n\n  --filename gcp-ccm.yaml\n\n#  wait for the ccm to be up\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset cloud-controller-manager\n\n\nIf the cloud controller manager is enabled, we need to restart the CNI pods to remove the node.kubernetes.io/network-unavailable taint.\n\n# restart the CNI pods, in this case flannel\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout restart \\\n\n  daemonset kube-flannel\n\n# wait for the pods to be restarted\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset kube-flannel\n\nCheck cluster status\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  get nodes\n\nCleanup deployment\n\nWarning: This will delete the deployment and all resources associated with it.\n\nRun below if cloud controller manager is enabled\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\n\nNow we can finally remove the deployment\n\n# delete the objects in the bucket first\n\ngsutil -m rm -r \"gs://${BUCKET_NAME}\"\n\ngcloud deployment-manager deployments delete \"${DEPLOYMENT_NAME}\" --quiet\n\n1.3.6 - Hetzner\nCreating a cluster via the CLI (hcloud) on Hetzner.\nUpload image\n\nHetzner Cloud does not support uploading custom images. You can email their support to get a Talos ISO uploaded by following issues:3599 or you can prepare image snapshot by yourself.\n\nThere are two options to upload your own.\n\nRun an instance in rescue mode and replace the system OS with the Talos image\nUse Hashicorp packer to prepare an image\nRescue mode\n\nCreate a new Server in the Hetzner console. Enable the Hetzner Rescue System for this server and reboot. Upon a reboot, the server will boot a special minimal Linux distribution designed for repair and reinstall. Once running, login to the server using ssh to prepare the system disk by doing the following:\n\n# Check that you in Rescue mode\n\ndf\n\n\n\n### Result is like:\n\n# udev                   987432         0    987432   0% /dev\n\n# 213.133.99.101:/nfs 308577696 247015616  45817536  85% /root/.oldroot/nfs\n\n# overlay                995672      8340    987332   1% /\n\n# tmpfs                  995672         0    995672   0% /dev/shm\n\n# tmpfs                  398272       572    397700   1% /run\n\n# tmpfs                    5120         0      5120   0% /run/lock\n\n# tmpfs                  199132         0    199132   0% /run/user/0\n\n\n\n# Download the Talos image\n\ncd /tmp\n\nwget -O /tmp/talos.raw.xz https://github.com/siderolabs/talos/releases/download/v1.6.2/hcloud-amd64.raw.xz\n\n# Replace system\n\nxz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\n\n# shutdown the instance\n\nshutdown -h now\n\n\nTo make sure disk content is consistent, it is recommended to shut the server down before taking an image (snapshot). Once shutdown, simply create an image (snapshot) from the console. You can now use this snapshot to run Talos on the cloud.\n\nPacker\n\nInstall packer to the local machine.\n\nCreate a config file for packer to use:\n\n# hcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    hcloud = {\n\n      source  = \"github.com/hetznercloud/hcloud\"\n\n      version = \"~> 1\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nvariable \"arch\" {\n\n  type    = string\n\n  default = \"amd64\"\n\n}\n\n\n\nvariable \"server_type\" {\n\n  type    = string\n\n  default = \"cx11\"\n\n}\n\n\n\nvariable \"server_location\" {\n\n  type    = string\n\n  default = \"hel1\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/hcloud-${var.arch}.raw.xz\"\n\n}\n\n\n\nsource \"hcloud\" \"talos\" {\n\n  rescue       = \"linux64\"\n\n  image        = \"debian-11\"\n\n  location     = \"${var.server_location}\"\n\n  server_type  = \"${var.server_type}\"\n\n  ssh_username = \"root\"\n\n\n\n  snapshot_name   = \"talos system disk - ${var.arch} - ${var.talos_version}\"\n\n  snapshot_labels = {\n\n    type    = \"infra\",\n\n    os      = \"talos\",\n\n    version = \"${var.talos_version}\",\n\n    arch    = \"${var.arch}\",\n\n  }\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.hcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget\",\n\n      \"wget -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\",\n\n    ]\n\n  }\n\n}\n\n\nAdditionally you could create a file containing\n\narch            = \"arm64\"\n\nserver_type     = \"cax11\"\n\nserver_location = \"fsn1\"\n\n\nand build the snapshot for arm64.\n\nCreate a new image by issuing the commands shown below. Note that to create a new API token for your Project, switch into the Hetzner Cloud Console choose a Project, go to Access → Security, and create a new token.\n\n# First you need set API Token\n\nexport HCLOUD_TOKEN=${TOKEN}\n\n\n\n# Upload image\n\npacker init .\n\npacker build .\n\n# Save the image ID\n\nexport IMAGE_ID=<image-id-in-packer-output>\n\n\nAfter doing this, you can find the snapshot in the console interface.\n\nCreating a Cluster via the CLI\n\nThis section assumes you have the hcloud console utility on your local machine.\n\n# Set hcloud context and api key\n\nhcloud context create talos-tutorial\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nhcloud load-balancer create --name controlplane --network-zone eu-central --type lb11 --label 'type=controlplane'\n\n\n\n### Result is like:\n\n# LoadBalancer 484487 created\n\n# IPv4: 49.12.X.X\n\n# IPv6: 2a01:4f8:X:X::1\n\n\n\nhcloud load-balancer add-service controlplane \\\n\n    --listen-port 6443 --destination-port 6443 --protocol tcp\n\nhcloud load-balancer add-target controlplane \\\n\n    --label-selector 'type=controlplane'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-hcloud-tutorial https://<load balancer IP or DNS>:6443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\n\nWe can now create our servers. Note that you can find IMAGE_ID in the snapshot section of the console: https://console.hetzner.cloud/projects/$PROJECT_ID/servers/snapshots.\n\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport IMAGE_ID=<your-image-id>\n\n\n\nhcloud server create --name talos-control-plane-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-2 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location fsn1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-3 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location nbg1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nhcloud server create --name talos-worker-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=worker' \\\n\n    --user-data-from-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nhcloud server list | grep talos-control-plane\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <control-plane-1-IP>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.3.7 - Nocloud\nCreating a cluster via the CLI using qemu.\n\nTalos supports nocloud data source implementation.\n\nThere are two ways to configure Talos server with nocloud platform:\n\nvia SMBIOS “serial number” option\nusing CDROM or USB-flash filesystem\n\nNote: This requires the nocloud image which can be found on the Github Releases page.\n\nSMBIOS Serial Number\n\nThis method requires the network connection to be up (e.g. via DHCP). Configuration is delivered from the HTTP server.\n\nds=nocloud-net;s=http://10.10.0.1/configs/;h=HOSTNAME\n\n\nAfter the network initialization is complete, Talos fetches:\n\nthe machine config from http://10.10.0.1/configs/user-data\nthe network config (if available) from http://10.10.0.1/configs/network-config\nSMBIOS: QEMU\n\nAdd the following flag to qemu command line when starting a VM:\n\nqemu-system-x86_64 \\\n\n  ...\\\n\n  -smbios type=1,serial=ds=nocloud-net;s=http://10.10.0.1/configs/\n\nSMBIOS: Proxmox\n\nSet the source machine config through the serial number on Proxmox GUI.\n\nThe Proxmox stores the VM config at /etc/pve/qemu-server/$ID.conf ($ID - VM ID number of virtual machine), you will see something like:\n\n...\nsmbios1: uuid=ceae4d10,serial=ZHM9bm9jbG91ZC1uZXQ7cz1odHRwOi8vMTAuMTAuMC4xL2NvbmZpZ3Mv,base64=1\n...\n\n\nWhere serial holds the base64-encoded string version of ds=nocloud-net;s=http://10.10.0.1/configs/.\n\nCDROM/USB\n\nTalos can also get machine config from local attached storage without any prior network connection being established.\n\nYou can provide configs to the server via files on a VFAT or ISO9660 filesystem. The filesystem volume label must be cidata or CIDATA.\n\nExample: QEMU\n\nCreate and prepare Talos machine config:\n\nexport CONTROL_PLANE_IP=192.168.1.10\n\n\n\ntalosctl gen config talos-nocloud https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nPrepare cloud-init configs:\n\nmkdir -p iso\n\nmv _out/controlplane.yaml iso/user-data\n\necho \"local-hostname: controlplane-1\" > iso/meta-data\n\ncat > iso/network-config << EOF\n\nversion: 1\n\nconfig:\n\n   - type: physical\n\n     name: eth0\n\n     mac_address: \"52:54:00:12:34:00\"\n\n     subnets:\n\n        - type: static\n\n          address: 192.168.1.10\n\n          netmask: 255.255.255.0\n\n          gateway: 192.168.1.254\n\nEOF\n\n\nCreate cloud-init iso image\n\ncd iso && genisoimage -output cidata.iso -V cidata -r -J user-data meta-data network-config\n\n\nStart the VM\n\nqemu-system-x86_64 \\\n\n    ...\n\n    -cdrom iso/cidata.iso \\\n\n    ...\n\nExample: Proxmox\n\nProxmox can create cloud-init disk for you. Edit the cloud-init config information in Proxmox as follows, substitute your own information as necessary:\n\nand then update cicustom param at /etc/pve/qemu-server/$ID.conf.\n\ncicustom: user=local:snippets/controlplane-1.yml\nipconfig0: ip=192.168.1.10/24,gw=192.168.10.254\nnameserver: 1.1.1.1\nsearchdomain: local\n\n\nNote: snippets/controlplane-1.yml is Talos machine config. It is usually located at /var/lib/vz/snippets/controlplane-1.yml. This file must be placed to this path manually, as Proxmox does not support snippet uploading via API/GUI.\n\nClick on Regenerate Image button after the above changes are made.\n\n1.3.8 - Openstack\nCreating a cluster via the CLI on Openstack.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in Openstack with 1 worker node. We will assume an existing some familiarity with Openstack. If you need more information on Openstack specifics, please see the official Openstack documentation.\n\nEnvironment Setup\n\nYou should have an existing openrc file. This file will provide environment variables necessary to talk to your Openstack cloud. See here for instructions on fetching this file.\n\nCreate the Image\n\nFirst, download the Openstack image from a Talos release. These images are called openstack-$ARCH.tar.gz. Untar this file with tar -xvf openstack-$ARCH.tar.gz. The resulting file will be called disk.raw.\n\nUpload the Image\n\nOnce you have the image, you can upload to Openstack with:\n\nopenstack image create --public --disk-format raw --file disk.raw talos\n\nNetwork Infrastructure\nLoad Balancer and Network Ports\n\nOnce the image is prepared, you will need to work through setting up the network. Issue the following to create a load balancer, the necessary network ports for each control plane node, and associations between the two.\n\nCreating loadbalancer:\n\n# Create load balancer, updating vip-subnet-id if necessary\n\nopenstack loadbalancer create --name talos-control-plane --vip-subnet-id public\n\n\n\n# Create listener\n\nopenstack loadbalancer listener create --name talos-control-plane-listener --protocol TCP --protocol-port 6443 talos-control-plane\n\n\n\n# Pool and health monitoring\n\nopenstack loadbalancer pool create --name talos-control-plane-pool --lb-algorithm ROUND_ROBIN --listener talos-control-plane-listener --protocol TCP\n\nopenstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP talos-control-plane-pool\n\n\nCreating ports:\n\n# Create ports for control plane nodes, updating network name if necessary\n\nopenstack port create --network shared talos-control-plane-1\n\nopenstack port create --network shared talos-control-plane-2\n\nopenstack port create --network shared talos-control-plane-3\n\n\n\n# Create floating IPs for the ports, so that you will have talosctl connectivity to each control plane\n\nopenstack floating ip create --port talos-control-plane-1 public\n\nopenstack floating ip create --port talos-control-plane-2 public\n\nopenstack floating ip create --port talos-control-plane-3 public\n\n\nNote: Take notice of the private and public IPs associated with each of these ports, as they will be used in the next step. Additionally, take node of the port ID, as it will be used in server creation.\n\nAssociate port’s private IPs to loadbalancer:\n\n# Create members for each port IP, updating subnet-id and address as necessary.\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-1 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-2 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-3 PORT> --protocol-port 6443 talos-control-plane-pool\n\nSecurity Groups\n\nThis example uses the default security group in Openstack. Ports have been opened to ensure that connectivity from both inside and outside the group is possible. You will want to allow, at a minimum, ports 6443 (Kubernetes API server) and 50000 (Talos API) from external sources. It is also recommended to allow communication over all ports from within the subnet.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(openstack loadbalancer show talos-control-plane -f json | jq -r .vip_address)\n\n\n\ntalosctl gen config talos-k8s-openstack-tutorial https://${LB_PUBLIC_IP}:6443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our Openstack nodes.\n\nCreate control plane:\n\n# Create control planes 2 and 3, substituting the same info.\n\nfor i in $( seq 1 3 ); do\n\n  openstack server create talos-control-plane-$i --flavor m1.small --nic port-id=talos-control-plane-$i --image talos --user-data /path/to/controlplane.yaml\n\ndone\n\n\nCreate worker:\n\n# Update network name as necessary.\n\nopenstack server create talos-worker-1 --flavor m1.small --network shared --image talos --user-data /path/to/worker.yaml\n\n\nNote: This step can be repeated to add more workers.\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will use one of the floating IPs we allocated earlier. It does not matter which one.\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.3.9 - Oracle\nCreating a cluster via the CLI (oci) on OracleCloud.com.\nUpload image\n\nOracle Cloud at the moment does not have a Talos official image. So you can use Bring Your Own Image (BYOI) approach.\n\nOnce the image is uploaded, set the Boot volume type to Paravirtualized mode.\n\nOracleCloud has highly available NTP service, it can be enabled in Talos machine config with:\n\nmachine:\n\n  time:\n\n    servers:\n\n      - 169.254.169.254\n\nCreating a Cluster via the CLI\n\nLogin to the console. And open the Cloud Shell.\n\nCreate a network\nexport cidr_block=10.0.0.0/16\n\nexport subnet_block=10.0.0.0/24\n\nexport compartment_id=<substitute-value-of-compartment_id> # https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/latest/oci_cli_docs/cmdref/network/vcn/create.html#cmdoption-compartment-id\n\n\n\nexport vcn_id=$(oci network vcn create --cidr-block $cidr_block --display-name talos-example --compartment-id $compartment_id --query data.id --raw-output)\n\nexport rt_id=$(oci network subnet create --cidr-block $subnet_block --display-name kubernetes --compartment-id $compartment_id --vcn-id $vcn_id --query data.route-table-id --raw-output)\n\nexport ig_id=$(oci network internet-gateway create --compartment-id $compartment_id --is-enabled true --vcn-id $vcn_id --query data.id --raw-output)\n\n\n\noci network route-table update --rt-id $rt_id --route-rules \"[{\\\"cidrBlock\\\":\\\"0.0.0.0/0\\\",\\\"networkEntityId\\\":\\\"$ig_id\\\"}]\" --force\n\n\n\n# disable firewall\n\nexport sl_id=$(oci network vcn list --compartment-id $compartment_id --query 'data[0].\"default-security-list-id\"' --raw-output)\n\n\n\noci network security-list update --security-list-id $sl_id --egress-security-rules '[{\"destination\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --ingress-security-rules '[{\"source\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --force\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer create --compartment-id $compartment_id --display-name controlplane-lb --subnet-id $subnet_id --is-preserve-source-destination false --is-private false --query data.id --raw-output)\n\n\n\ncat <<EOF > talos-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 50000,\n\n  \"protocol\": \"TCP\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://talos-health-checker.json --name talos --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name talos --name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --protocol TCP\n\n\n\ncat <<EOF > controlplane-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 6443,\n\n  \"protocol\": \"HTTPS\",\n\n  \"returnCode\": 401,\n\n  \"urlPath\": \"/readyz\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://controlplane-health-checker.json --name controlplane --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name controlplane --name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --protocol TCP\n\n\n\n# Save the external IP\n\noci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].\"ip-addresses\"'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-oracle-tutorial https://<load balancer IP or DNS>:6443 --additional-sans <load balancer IP or DNS>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport shape='VM.Standard.A1.Flex'\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --shape $shape --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].id' --raw-output)\n\n\n\ncat <<EOF > shape.json\n\n{\n\n  \"memoryInGBs\": 4,\n\n  \"ocpus\": 1\n\n}\n\nEOF\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-1 --private-ip 10.0.0.11 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-2 --private-ip 10.0.0.12 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-3 --private-ip 10.0.0.13 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport shape='VM.Standard.E2.1.Micro'\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-1 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-2 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-3 --assign-public-ip true --user-data-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nexport instance_id=$(oci compute instance list --compartment-id $compartment_id --display-name controlplane-1 --query 'data[0].id' --raw-output)\n\n\n\noci compute instance list-vnics --instance-id $instance_id --query 'data[0].\"private-ip\"' --raw-output\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <load balancer IP or DNS>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.3.10 - Scaleway\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nTalos is known to work on scaleway.com; however, it is currently undocumented.\n\n1.3.11 - UpCloud\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nIn this guide we will create an HA Kubernetes cluster 3 control plane nodes and 1 worker node. We assume some familiarity with UpCloud. If you need more information on UpCloud specifics, please see the official UpCloud documentation.\n\nCreate the Image\n\nThe best way to create an image for UpCloud, is to build one using Hashicorp packer, with the upcloud-amd64.raw.xz image found on the Talos Releases. Using the general ISO is also possible, but the UpCloud image has some UpCloud specific features implemented, such as the fetching of metadata and user data to configure the nodes.\n\nTo create the cluster, you need a few things locally installed:\n\nUpCloud CLI\nHashicorp Packer\n\nNOTE: Make sure your account allows API connections. To do so, log into UpCloud control panel and go to People -> Account -> Permissions -> Allow API connections checkbox. It is recommended to create a separate subaccount for your API access and only set the API permission.\n\nTo use the UpCloud CLI, you need to create a config in $HOME/.config/upctl.yaml\n\nusername: your_upcloud_username\n\npassword: your_upcloud_password\n\n\nTo use the UpCloud packer plugin, you need to also export these credentials to your environment variables, by e.g. putting the following in your .bashrc or .zshrc\n\nexport UPCLOUD_USERNAME=\"<username>\"\n\nexport UPCLOUD_PASSWORD=\"<password>\"\n\n\nNext create a config file for packer to use:\n\n# upcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    upcloud = {\n\n      version = \">=v1.0.0\"\n\n      source  = \"github.com/UpCloudLtd/upcloud\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/upcloud-amd64.raw.xz\"\n\n}\n\n\n\nvariable \"username\" {\n\n  type        = string\n\n  description = \"UpCloud API username\"\n\n  default     = \"${env(\"UPCLOUD_USERNAME\")}\"\n\n}\n\n\n\nvariable \"password\" {\n\n  type        = string\n\n  description = \"UpCloud API password\"\n\n  default     = \"${env(\"UPCLOUD_PASSWORD\")}\"\n\n  sensitive   = true\n\n}\n\n\n\nsource \"upcloud\" \"talos\" {\n\n  username        = \"${var.username}\"\n\n  password        = \"${var.password}\"\n\n  zone            = \"us-nyc1\"\n\n  storage_name    = \"Debian GNU/Linux 11 (Bullseye)\"\n\n  template_name   = \"Talos (${var.talos_version})\"\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.upcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget xz-utils\",\n\n      \"wget -q -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/vda\",\n\n    ]\n\n  }\n\n\n\n  provisioner \"shell-local\" {\n\n      inline = [\n\n      \"upctl server stop --type hard custom\",\n\n      ]\n\n  }\n\n}\n\n\nNow create a new image by issuing the commands shown below.\n\npacker init .\n\npacker build .\n\n\nAfter doing this, you can find the custom image in the console interface under storage.\n\nCreating a Cluster via the CLI\nCreate an Endpoint\n\nTo communicate with the Talos cluster you will need a single endpoint that is used to access the cluster. This can either be a loadbalancer that will sit in front of all your control plane nodes, a DNS name with one or more A or AAAA records pointing to the control plane nodes, or directly the IP of a control plane node.\n\nWhich option is best for you will depend on your needs. Endpoint selection has been further documented here.\n\nAfter you decide on which endpoint to use, note down the domain name or IP, as we will need it in the next step.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the endpoint created earlier, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-upcloud-tutorial https://<load balancer IP or DNS>:<port> --install-disk /dev/vda\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Depending on the Kubernetes version you want to run, you might need to select a different Talos version, as not all versions are compatible. You can find the support matrix here.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch or yamlpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nRun the following to create three total control plane nodes:\n\nfor ID in $(seq 3); do\n\n    upctl server create \\\n\n      --zone us-nyc1 \\\n\n      --title talos-us-nyc1-master-$ID \\\n\n      --hostname talos-us-nyc1-master-$ID \\\n\n      --plan 2xCPU-4GB \\\n\n      --os \"Talos (v1.6.2)\" \\\n\n      --user-data \"$(cat controlplane.yaml)\" \\\n\n      --enable-metada\n\ndone\n\n\nNote: modify the zone and OS depending on your preferences. The OS should match the template name generated with packer in the previous step.\n\nNote the IP address of the first control plane node, as we will need it later.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nupctl server create \\\n\n  --zone us-nyc1 \\\n\n  --title talos-us-nyc1-worker-1 \\\n\n  --hostname talos-us-nyc1-worker-1 \\\n\n  --plan 2xCPU-4GB \\\n\n  --os \"Talos (v1.6.2)\" \\\n\n  --user-data \"$(cat worker.yaml)\" \\\n\n  --enable-metada\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP, as noted earlier. We only add one node IP, as that is the entry into our cluster against which our commands will be run. All requests to other nodes are proxied through the endpoint, and therefore not all nodes need to be manually added to the config. You don’t want to run your commands against all nodes, as this can destroy your cluster if you are not careful (further documentation).\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig\n\n\nIt will take a few minutes before Kubernetes has been fully bootstrapped, and is accessible.\n\nYou can check if the nodes are registered in Talos by running\n\ntalosctl --talosconfig talosconfig get members\n\n\nTo check if your nodes are ready, run\n\nkubectl get nodes\n\n1.3.12 - Vultr\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\nCreating a Cluster using the Vultr CLI\n\nThis guide will demonstrate how to create a highly-available Kubernetes cluster with one worker using the Vultr cloud provider.\n\nVultr have a very well documented REST API, and an open-source CLI tool to interact with the API which will be used in this guide. Make sure to follow installation and authentication instructions for the vultr-cli tool.\n\nBoot Options\nUpload an ISO Image\n\nFirst step is to make the Talos ISO available to Vultr by uploading the latest release of the ISO to the Vultr ISO server.\n\nvultr-cli iso create --url https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\n\n\nMake a note of the ID in the output, it will be needed later when creating the instances.\n\nPXE Booting via Image Factory\n\nTalos Linux can be PXE-booted on Vultr using Image Factory, using the vultr platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/vultr-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate a Load Balancer\n\nA load balancer is needed to serve as the Kubernetes endpoint for the cluster.\n\nvultr-cli load-balancer create \\\n\n   --region $REGION \\\n\n   --label \"Talos Kubernetes Endpoint\" \\\n\n   --port 6443 \\\n\n   --protocol tcp \\\n\n   --check-interval 10 \\\n\n   --response-timeout 5 \\\n\n   --healthy-threshold 5 \\\n\n   --unhealthy-threshold 3 \\\n\n   --forwarding-rules frontend_protocol:tcp,frontend_port:443,backend_protocol:tcp,backend_port:6443\n\n\nMake a note of the ID of the load balancer from the output of the above command, it will be needed after the control plane instances are created.\n\nvultr-cli load-balancer get $LOAD_BALANCER_ID | grep ^IP\n\n\nMake a note of the IP address, it will be needed later when generating the configuration.\n\nCreate the Machine Configuration\nGenerate Base Configuration\n\nUsing the IP address (or DNS name if one was created) of the load balancer created above, generate the machine configuration files for the new cluster.\n\ntalosctl gen config talos-kubernetes-vultr https://$LOAD_BALANCER_ADDRESS\n\n\nOnce generated, the machine configuration can be modified as necessary for the new cluster, for instance updating disk installation, or adding SANs for the certificates.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode cloud\n\ntalosctl validate --config worker.yaml --mode cloud\n\nCreate the Nodes\nCreate the Control Plane Nodes\n\nFirst a control plane needs to be created, with the example below creating 3 instances in a loop. The instance type (noted by the --plan vc2-2c-4gb argument) in the example is for a minimum-spec control plane node, and should be updated to suit the cluster being created.\n\nfor id in $(seq 3); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-2c-4gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-cp${id} \\\n\n        --label \"Talos Kubernetes Control Plane\" \\\n\n        --tags talos,kubernetes,control-plane\n\ndone\n\n\nMake a note of the instance IDs, as they are needed to attach to the load balancer created earlier.\n\nvultr-cli load-balancer update $LOAD_BALANCER_ID --instances $CONTROL_PLANE_1_ID,$CONTROL_PLANE_2_ID,$CONTROL_PLANE_3_ID\n\n\nOnce the nodes are booted and waiting in maintenance mode, the machine configuration can be applied to each one in turn.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_1_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_2_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_3_ADDRESS --file controlplane.yaml\n\nCreate the Worker Nodes\n\nNow worker nodes can be created and configured in a similar way to the control plane nodes, the difference being mainly in the machine configuration file. Note that like with the control plane nodes, the instance type (here set by --plan vc2-1-1gb) should be changed for the actual cluster requirements.\n\nfor id in $(seq 1); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-1c-1gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-worker${id} \\\n\n        --label \"Talos Kubernetes Worker\" \\\n\n        --tags talos,kubernetes,worker\n\ndone\n\n\nOnce the worker is booted and in maintenance mode, the machine configuration can be applied in the following manner.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $WORKER_1_ADDRESS --file worker.yaml\n\nBootstrap etcd\n\nOnce all the cluster nodes are correctly configured, the cluster can be bootstrapped to become functional. It is important that the talosctl bootstrap command be executed only once and against only a single control plane node.\n\ntalosctl --talosconfig talosconfig boostrap --endpoints $CONTROL_PLANE_1_ADDRESS --nodes $CONTROL_PLANE_1_ADDRESS\n\nConfigure Endpoints and Nodes\n\nWhile the cluster goes through the bootstrapping process and beings to self-manage, the talosconfig can be updated with the endpoints and nodes.\n\ntalosctl --talosconfig talosconfig config endpoints $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS\n\ntalosctl --talosconfig talosconfig config nodes $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS WORKER_1_ADDRESS\n\nRetrieve the kubeconfig\n\nFinally, with the cluster fully running, the administrative kubeconfig can be retrieved from the Talos API to be saved locally.\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nNow the kubeconfig can be used by any of the usual Kubernetes tools to interact with the Talos-based Kubernetes cluster as normal.\n\n1.4 - Local Platforms\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\n1.4.1 - Docker\nCreating Talos Kubernetes cluster using Docker.\n\nIn this guide we will create a Kubernetes cluster in Docker, using a containerized version of Talos.\n\nRunning Talos in Docker is intended to be used in CI pipelines, and local testing when you need a quick and easy cluster. Furthermore, if you are running Talos in production, it provides an excellent way for developers to develop against the same version of Talos.\n\nRequirements\n\nThe follow are requirements for running Talos in Docker:\n\nDocker 18.03 or greater\na recent version of talosctl\nCaveats\n\nDue to the fact that Talos will be running in a container, certain APIs are not available. For example upgrade, reset, and similar APIs don’t apply in container mode. Further, when running on a Mac in docker, due to networking limitations, VIPs are not supported.\n\nCreate the Cluster\n\nCreating a local cluster is as simple as:\n\ntalosctl cluster create --wait\n\n\nOnce the above finishes successfully, your talosconfig(~/.talos/config) will be configured to point to the new cluster.\n\nNote: Startup times can take up to a minute or more before the cluster is available.\n\nFinally, we just need to specify which nodes you want to communicate with using talosctl. Talosctl can operate on one or all the nodes in the cluster – this makes cluster wide commands much easier.\n\ntalosctl config nodes 10.5.0.2 10.5.0.3\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nCleaning Up\n\nTo cleanup, run:\n\ntalosctl cluster destroy\n\nRunning Talos in Docker Manually\n\nTo run Talos in a container manually, run:\n\ndocker run --rm -it \\\n\n  --name tutorial \\\n\n  --hostname talos-cp \\\n\n  --read-only \\\n\n  --privileged \\\n\n  --security-opt seccomp=unconfined \\\n\n  --mount type=tmpfs,destination=/run \\\n\n  --mount type=tmpfs,destination=/system \\\n\n  --mount type=tmpfs,destination=/tmp \\\n\n  --mount type=volume,destination=/system/state \\\n\n  --mount type=volume,destination=/var \\\n\n  --mount type=volume,destination=/etc/cni \\\n\n  --mount type=volume,destination=/etc/kubernetes \\\n\n  --mount type=volume,destination=/usr/libexec/kubernetes \\\n\n  --mount type=volume,destination=/usr/etc/udev \\\n\n  --mount type=volume,destination=/opt \\\n\n  -e PLATFORM=container \\\n\n  ghcr.io/siderolabs/talos:v1.6.2\n\n1.4.2 - QEMU\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nIn this guide we will create a Kubernetes cluster using QEMU.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\nLinux\na kernel with\nKVM enabled (/dev/kvm must exist)\nCONFIG_NET_SCH_NETEM enabled\nCONFIG_NET_SCH_INGRESS enabled\nat least CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities\nQEMU\nbridge, static and firewall CNI plugins from the standard CNI plugins, and tc-redirect-tap CNI plugin from the awslabs tc-redirect-tap installed to /opt/cni/bin (installed automatically by talosctl)\niptables\n/var/run/netns directory should exist\nInstallation\nHow to get QEMU\n\nInstall QEMU with your operating system package manager. For example, on Ubuntu for x86:\n\napt install qemu-system-x86 qemu-kvm\n\nInstall talosctl\n\nDownload talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nInstall Talos kernel and initramfs\n\nQEMU provisioner depends on Talos kernel (vmlinuz) and initramfs (initramfs.xz). These files can be downloaded from the Talos release:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/vmlinuz-<arch> -L -o _out/vmlinuz-<arch>\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/initramfs-<arch>.xz -L -o _out/initramfs-<arch>.xz\n\n\nFor example version v1.6.2:\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/vmlinuz-amd64 -L -o _out/vmlinuz-amd64\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/initramfs-amd64.xz -L -o _out/initramfs-amd64.xz\n\nCreate the Cluster\n\nFor the first time, create root state directory as your user so that you can inspect the logs as non-root user:\n\nmkdir -p ~/.talos/clusters\n\n\nCreate the cluster:\n\nsudo --preserve-env=HOME talosctl cluster create --provisioner qemu\n\n\nBefore the first cluster is created, talosctl will download the CNI bundle for the VM provisioning and install it to ~/.talos/cni directory.\n\nOnce the above finishes successfully, your talosconfig (~/.talos/config) will be configured to point to the new cluster, and kubeconfig will be downloaded and merged into default kubectl config location (~/.kube/config).\n\nCluster provisioning process can be optimized with registry pull-through caches.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl -n 10.5.0.2 containers for a list of containers in the system namespace, or talosctl -n 10.5.0.2 containers -k for the k8s.io namespace. To view the logs of a container, use talosctl -n 10.5.0.2 logs <container> or talosctl -n 10.5.0.2 logs -k <container>.\n\nA bridge interface will be created, and assigned the default IP 10.5.0.1. Each node will be directly accessible on the subnet specified at cluster creation time. A loadbalancer runs on 10.5.0.1 by default, which handles loadbalancing for the Kubernetes APIs.\n\nYou can see a summary of the cluster state by running:\n\n$ talosctl cluster show --provisioner qemu\n\nPROVISIONER       qemu\n\nNAME              talos-default\n\nNETWORK NAME      talos-default\n\nNETWORK CIDR      10.5.0.0/24\n\nNETWORK GATEWAY   10.5.0.1\n\nNETWORK MTU       1500\n\n\n\nNODES:\n\n\n\nNAME                           TYPE           IP         CPU    RAM      DISK\n\ntalos-default-controlplane-1   ControlPlane   10.5.0.2   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-2   ControlPlane   10.5.0.3   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-3   ControlPlane   10.5.0.4   1.00   1.6 GB   4.3 GB\n\ntalos-default-worker-1         Worker         10.5.0.5   1.00   1.6 GB   4.3 GB\n\nCleaning Up\n\nTo cleanup, run:\n\nsudo --preserve-env=HOME talosctl cluster destroy --provisioner qemu\n\n\nNote: In that case that the host machine is rebooted before destroying the cluster, you may need to manually remove ~/.talos/clusters/talos-default.\n\nManual Clean Up\n\nThe talosctl cluster destroy command depends heavily on the clusters state directory. It contains all related information of the cluster. The PIDs and network associated with the cluster nodes.\n\nIf you happened to have deleted the state folder by mistake or you would like to cleanup the environment, here are the steps how to do it manually:\n\nRemove VM Launchers\n\nFind the process of talosctl qemu-launch:\n\nps -elf | grep 'talosctl qemu-launch'\n\n\nTo remove the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 157615 and 157617\n\nps -elf | grep '[t]alosctl qemu-launch'\n\n0 S root      157615    2835  0  80   0 - 184934 -     07:53 ?        00:00:00 talosctl qemu-launch\n\n0 S root      157617    2835  0  80   0 - 185062 -     07:53 ?        00:00:00 talosctl qemu-launch\n\nsudo kill -s SIGTERM 157615\n\nsudo kill -s SIGTERM 157617\n\nStopping VMs\n\nFind the process of qemu-system:\n\nps -elf | grep 'qemu-system'\n\n\nTo stop the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 158065 and 158216\n\nps -elf | grep qemu-system\n\n2 S root     1061663 1061168 26  80   0 - 1786238 -    14:05 ?        01:53:56 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/home/username/.talos/clusters/talos-default/bootstrap-master.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=1e:86:c6:b4:7c:c4 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=7ec0a73c-826e-4eeb-afd1-39ff9f9160ca -machine q35,accel=kvm\n\n2 S root     1061663 1061170 67  80   0 - 621014 -     21:23 ?        00:00:07 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/homeusername/.talos/clusters/talos-default/pxe-1.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=36:f3:2f:c3:9f:06 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=ce12a0d0-29c8-490f-b935-f6073ab916a6 -machine q35,accel=kvm\n\nsudo kill -s SIGTERM 1061663\n\nsudo kill -s SIGTERM 1061663\n\nRemove load balancer\n\nFind the process of talosctl loadbalancer-launch:\n\nps -elf | grep 'talosctl loadbalancer-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl loadbalancer-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl loadbalancer-launch --loadbalancer-addr 10.5.0.1 --loadbalancer-upstreams 10.5.0.2\n\nsudo kill -s SIGTERM 157609\n\nRemove DHCP server\n\nFind the process of talosctl dhcpd-launch:\n\nps -elf | grep 'talosctl dhcpd-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl dhcpd-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl dhcpd-launch --state-path /home/username/.talos/clusters/talos-default --addr 10.5.0.1 --interface talosbd9c32bc\n\nsudo kill -s SIGTERM 157609\n\nRemove network\n\nThis is more tricky part as if you have already deleted the state folder. If you didn’t then it is written in the state.yaml in the ~/.talos/clusters/<cluster-name> directory.\n\nsudo cat ~/.talos/clusters/<cluster-name>/state.yaml | grep bridgename\n\nbridgename: talos<uuid>\n\n\nIf you only had one cluster, then it will be the interface with name talos<uuid>\n\n46: talos<uuid>: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n\n    link/ether a6:72:f4:0a:d3:9c brd ff:ff:ff:ff:ff:ff\n\n    inet 10.5.0.1/24 brd 10.5.0.255 scope global talos17c13299\n\n       valid_lft forever preferred_lft forever\n\n    inet6 fe80::a472:f4ff:fe0a:d39c/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n\nTo remove this interface:\n\nsudo ip link del talos<uuid>\n\nRemove state directory\n\nTo remove the state directory execute:\n\nsudo rm -Rf /home/$USER/.talos/clusters/<cluster-name>\n\nTroubleshooting\nLogs\n\nInspect logs directory\n\nsudo cat ~/.talos/clusters/<cluster-name>/*.log\n\n\nLogs are saved under <cluster-name>-<role>-<node-id>.log\n\nFor example in case of k8s cluster name:\n\nls -la ~/.talos/clusters/k8s | grep log\n\n-rw-r--r--. 1 root root      69415 Apr 26 20:58 k8s-master-1.log\n\n-rw-r--r--. 1 root root      68345 Apr 26 20:58 k8s-worker-1.log\n\n-rw-r--r--. 1 root root      24621 Apr 26 20:59 lb.log\n\n\nInspect logs during the installation\n\ntail -f ~/.talos/clusters/<cluster-name>/*.log\n\n1.4.3 - VirtualBox\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\nIn this guide we will create a Kubernetes cluster using VirtualBox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get VirtualBox\n\nInstall VirtualBox with your operating system package manager or from the website. For example, on Ubuntu for x86:\n\napt install virtualbox\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nDownload the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nCreate VMs\n\nStart by creating a new VM by clicking the “New” button in the VirtualBox UI:\n\nSupply a name for this VM, and specify the Type and Version:\n\nEdit the memory to supply at least 2GB of RAM for the VM:\n\nProceed through the disk settings, keeping the defaults. You can increase the disk space if desired.\n\nOnce created, select the VM and hit “Settings”:\n\nIn the “System” section, supply at least 2 CPUs:\n\nIn the “Network” section, switch the network “Attached To” section to “Bridged Adapter”:\n\nFinally, in the “Storage” section, select the optical drive and, on the right, select the ISO by browsing your filesystem:\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”. Once the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-vbox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the VirtualBox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig $TALOSCONFIG config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig $TALOSCONFIG config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig $TALOSCONFIG bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig $TALOSCONFIG kubeconfig .\n\n\nYou can then use kubectl in this fashion:\n\nkubectl get nodes\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the VirtualBox UI.\n\n1.5 - Single Board Computers\nInstallation of Talos Linux on single-board computers.\n1.5.1 - Banana Pi M64\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-bananapi_m64-arm64.raw.xz\n\nxz -d metal-bananapi_m64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-bananapi_m64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.2 - Friendlyelec Nano PI R4S\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-nanopi_r4s-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-nanopi_r4s-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.3 - Jetson Nano\nInstalling Talos on Jetson Nano SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card/USB drive\ncrane CLI\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nFlashing the firmware to on-board SPI flash\n\nFlashing the firmware only needs to be done once.\n\nWe will use the R32.7.2 release for the Jetson Nano. Most of the instructions is similar to this doc except that we’d be using a upstream version of u-boot with patches from NVIDIA u-boot so that USB boot also works.\n\nBefore flashing we need the following:\n\nA USB-A to micro USB cable\nA jumper wire to enable recovery mode\nA HDMI monitor to view the logs if the USB serial adapter is not available\nA USB to Serial adapter with 3.3V TTL (optional)\nA 5V DC barrel jack\n\nIf you’re planning to use the serial console follow the documentation here\n\nFirst start by downloading the Jetson Nano L4T release.\n\ncurl -SLO https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/t210/jetson-210_linux_r32.7.2_aarch64.tbz2\n\n\nNext we will extract the L4T release and replace the u-boot binary with the patched version.\n\ntar xf jetson-210_linux_r32.6.1_aarch64.tbz2\n\ncd Linux_for_Tegra\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C bootloader/t210ref/p3450-0000/ jetson_nano/u-boot.bin\n\n\nNext we will flash the firmware to the Jetson Nano SPI flash. In order to do that we need to put the Jetson Nano into Force Recovery Mode (FRC). We will use the instructions from here\n\nEnsure that the Jetson Nano is powered off. There is no need for the SD card/USB storage/network cable to be connected\nConnect the micro USB cable to the micro USB port on the Jetson Nano, don’t plug the other end to the PC yet\nEnable Force Recovery Mode (FRC) by placing a jumper across the FRC pins on the Jetson Nano\nFor board revision A02, these are pins 3 and 4 of header J40\nFor board revision B01, these are pins 9 and 10 of header J50\nPlace another jumper across J48 to enable power from the DC jack and connect the Jetson Nano to the DC jack J25\nNow connect the other end of the micro USB cable to the PC and remove the jumper wire from the FRC pins\n\nNow the Jetson Nano is in Force Recovery Mode (FRC) and can be confirmed by running the following command\n\nlsusb | grep -i \"nvidia\"\n\n\nNow we can move on the flashing the firmware.\n\nsudo ./flash p3448-0000-max-spi external\n\n\nThis will flash the firmware to the Jetson Nano SPI flash and you’ll see a lot of output. If you’ve connected the serial console you’ll also see the progress there. Once the flashing is done you can disconnect the USB cable and power off the Jetson Nano.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-jetson_nano-arm64.raw.xz\n\nxz -d metal-jetson_nano-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card/USB storage:\n\nsudo dd if=metal-jetson_nano-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M status=progress\n\n\n| Replace /dev/mmcblk0 with the name of your SD card/USB storage.\n\nBootstrapping the Node\n\nInsert the SD card/USB storage to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.4 - Libre Computer Board ALL-H3-CC\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nxz -d metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-libretech_all_h3_cc_h5-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.5 - Pine64\nInstalling Talos on a Pine64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-pine64-arm64.raw.xz\n\nxz -d metal-pine64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-pine64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.6 - Pine64 Rock64\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rock64-arm64.raw.xz\n\nxz -d metal-rock64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rock64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.7 - Radxa ROCK PI 4\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.8 - Radxa ROCK PI 4C\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4c-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4c/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4c-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n1.5.9 - Raspberry Pi Series\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\nTalos disk image for the Raspberry Pi generic should in theory work for the boards supported by u-boot rpi_arm64_defconfig. This has only been officialy tested on the Raspberry Pi 4 and community tested on one variant of the Compute Module 4 using Super 6C boards. If you have tested this on other Raspberry Pi boards, please let us know.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nUpdating the EEPROM\n\nUse Raspberry Pi Imager to write an EEPROM update image to a spare SD card. Select Misc utility images under the Operating System tab.\n\nRemove the SD card from your local machine and insert it into the Raspberry Pi. Power the Raspberry Pi on, and wait at least 10 seconds. If successful, the green LED light will blink rapidly (forever), otherwise an error pattern will be displayed. If an HDMI display is attached to the port closest to the power/USB-C port, the screen will display green for success or red if a failure occurs. Power off the Raspberry Pi and remove the SD card from it.\n\nNote: Updating the bootloader only needs to be done once.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rpi_generic-arm64.raw.xz\n\nxz -d metal-rpi_generic-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rpi_generic-arm64.raw of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nNote: if you have an HDMI display attached and it shows only a rainbow splash, please use the other HDMI port, the one closest to the power/USB-C port.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\nTroubleshooting\n\nThe following table can be used to troubleshoot booting issues:\n\nLong Flashes\tShort Flashes\tStatus\n0\t3\tGeneric failure to boot\n0\t4\tstart*.elf not found\n0\t7\tKernel image not found\n0\t8\tSDRAM failure\n0\t9\tInsufficient SDRAM\n0\t10\tIn HALT state\n2\t1\tPartition not FAT\n2\t2\tFailed to read from partition\n2\t3\tExtended partition not FAT\n2\t4\tFile signature/hash mismatch - Pi 4\n4\t4\tUnsupported board type\n4\t5\tFatal firmware error\n4\t6\tPower failure type A\n4\t7\tPower failure type B\n1.6 - Boot Assets\nCreating customized Talos boot assets, disk images, ISO and installer images.\n\nTalos Linux provides a set of pre-built images on the release page, but these images can be customized further for a specific use case:\n\nadding system extensions\nupdating kernel command line arguments\nusing custom META contents, e.g. for metal network configuration\ngenerating SecureBoot images signed with a custom key\n\nThere are two ways to generate Talos boot assets:\n\nusing Image Factory service (recommended)\nmanually using imager container image (advanced)\n\nImage Factory is easier to use, but it only produces images for official Talos Linux releases and official Talos Linux system extensions. The imager container can be used to generate images from main branch, with local changes, or with custom system extensions.\n\nImage Factory\n\nImage Factory is a service that generates Talos boot assets on-demand. Image Factory allows to generate boot assets for the official Talos Linux releases and official Talos Linux system extensions.\n\nThe main concept of the Image Factory is a schematic which defines the customization of the boot asset. Once the schematic is configured, Image Factory can be used to pull various Talos Linux images, ISOs, installer images, PXE booting bare-metal machines across different architectures, versions of Talos and platforms.\n\nSidero Labs maintains a public Image Factory instance at https://factory.talos.dev. Image Factory provides a simple UI to prepare schematics and retrieve asset links.\n\nExample: Bare-metal with Image Factory\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument.\n\nFirst, let’s create the schematic file bare-metal.yaml:\n\n# bare-metal.yaml\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n      - siderolabs/intel-ucode\n\n\nThe schematic doesn’t contain system extension versions, Image Factory will pick the correct version matching Talos Linux release.\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @bare-metal.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176\"}\n\n\nThe returned schematic ID b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176 we will use to generate the boot assets.\n\nThe schematic ID is based on the schematic contents, so uploading the same schematic will return the same ID.\n\nNow we have two options to boot our bare-metal machine:\n\nusing ISO image: https://factory.talos.dev/image/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64.iso (download it and burn to a CD/DVD or USB stick)\nPXE booting via iPXE script: https://factory.talos.dev/pxe/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64\n\nThe Image Factory URL contains both schematic ID and Talos version, and both can be changed to generate different boot assets.\n\nOnce the bare-metal machine is booted up for the first time, it will require Talos Linux installer image to be installed on the disk. The installer image will be produced by the Image Factory as well:\n\n# Talos machine configuration patch\n\nmachine:\n\n  install:\n\n    image: factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:v1.6.2\n\n\nOnce installed, the machine can be upgraded to a new version of Talos by referencing new installer image:\n\ntalosctl upgrade --image factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:<new_version>\n\n\nSame way upgrade process can be used to transition to a new set of system extensions: generate new schematic with the new set of system extensions, and upgrade the machine to the new schematic ID:\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nExample: AWS with Image Factory\n\nTalos Linux is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required. Let’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s create the schematic file aws.yaml:\n\n# aws.yaml\n\ncustomization:\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @aws.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5\"}\n\n\nThe returned schematic ID d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5 we will use to generate the boot assets.\n\nNow we can download the AWS disk image from the Image Factory:\n\ncurl -LO https://factory.talos.dev/image/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5/v1.6.2/aws-amd64.raw.xz\n\n\nNow the aws-amd64.raw.xz file contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nOnce the AWS VM is created from the AMI, it can be upgraded to a different Talos version or a different schematic using talosctl upgrade:\n\n# upgrade to a new Talos version\n\ntalosctl upgrade --image factory.talos.dev/installer/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5:<new_version>\n\n# upgrade to a new schematic\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nImager\n\nA custom disk image, boot asset can be generated by using the Talos Linux imager container: ghcr.io/siderolabs/imager:v1.6.2. The imager container image can be checked by verifying its signature.\n\nThe generation process can be run with a simple docker run command:\n\ndocker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 <image-kind> [optional: customization]\n\n\nA quick guide to the flags used for docker run:\n\n--rm flag removes the container after the run (as it’s not going to be used anymore)\n-t attaches a terminal for colorized output, it can be removed if used in scripts\n-v $PWD/_out:/secureboot:ro mounts the SecureBoot keys into the container (can be skipped if not generating SecureBoot image)\n-v $PWD/_out:/out mounts the output directory (where the generated image will be placed) into the container\n-v /dev:/dev --privileged is required to generate disk images (loop devices are used), but not required for ISOs, installer container images\n\nThe <image-kind> argument to the imager defines the base profile to be used for the image generation. There are several built-in profiles:\n\niso builds a Talos ISO image (see ISO)\nsecureboot-iso builds a Talos ISO image with SecureBoot (see SecureBoot)\nmetal builds a generic disk image for bare-metal machines\nsecureboot-metal builds a generic disk image for bare-metal machines with SecureBoot\nsecureboot-installer builds an installer container image with SecureBoot (see SecureBoot)\naws, gcp, azure, etc. builds a disk image for a specific Talos platform\n\nThe base profile can be customized with the additional flags to the imager:\n\n--arch specifies the architecture of the image to be generated (default: host architecture)\n--meta allows to set initial META values\n--extra-kernel-arg allows to customize the kernel command line arguments. Default kernel arg can be removed by prefixing the argument with a -. For example -console removes all console=<value> arguments, whereas -console=tty0 removes the console=tty0 default argument.\n--system-extension-image allows to install a system extension into the image\nExtension Image Reference\n\nWhile Image Factory automatically resolves the extension name into a matching container image for a specific version of Talos, imager requires the full explicit container image reference. The imager also allows to install custom extensions which are not part of the official Talos Linux system extensions.\n\nTo get the official Talos Linux system extension container image reference matching a Talos release, use the following command:\n\ncrane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep EXTENSION-NAME\n\n\nNote: this command is using crane tool, but any other tool which allows to export the image contents can be used.\n\nFor each Talos release, the ghcr.io/siderolabs/extensions:VERSION image contains a pinned reference to each system extension container image.\n\nExample: Bare-metal with Imager\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument and replace the Talos default console arguments and add a custom console arg.\n\nFirst, let’s lookup extension images for Intel CPU microcode updates and gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep -E 'gvisor|intel-ucode'\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\nghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow we can generate the ISO image with the following command:\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d --extra-kernel-arg net.ifnames=0 --extra-kernel-arg=-console --extra-kernel-arg=console=ttyS1\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: false\n\nversion: v1.6.2\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  systemExtensions:\n\n    - imageRef: ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n    - imageRef: ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\ninitramfs ready\n\nkernel command line: talos.platform=metal console=ttyS1 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 net.ifnames=0\n\nISO ready\n\noutput asset path: /out/metal-amd64.iso\n\n\nNow the _out/metal-amd64.iso contains the customized Talos ISO image.\n\nIf the machine is going to be booted using PXE, we can instead generate kernel and initramfs images:\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind kernel\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind initramfs --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow the _out/kernel-amd64 and _out/initramfs-amd64 contain the customized Talos kernel and initramfs images.\n\nNote: the extra kernel args are not used now, as they are set via the PXE boot process, and can’t be embedded into the kernel or initramfs.\n\nAs the next step, we should generate a custom installer image which contains all required system extensions (kernel args can’t be specified with the installer image, but they are set in the machine configuration):\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 installer --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n...\n\noutput asset path: /out/metal-amd64-installer.tar\n\n\nThe installer container image should be pushed to the container registry:\n\ncrane push _out/metal-amd64-installer.tar ghcr.io/<username></username>/installer:v1.6.2\n\n\nNow we can use the customized installer image to install Talos on the bare-metal machine.\n\nWhen it’s time to upgrade a machine, a new installer image can be generated using the new version of imager, and updating the system extension images to the matching versions. The custom installer image can now be used to upgrade Talos machine.\n\nExample: AWS with Imager\n\nTalos is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required.\n\nLet’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s lookup extension images for the gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep gvisor\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n\nNext, let’s generate AWS disk image with that system extension:\n\n$ docker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 aws --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n...\n\noutput asset path: /out/aws-amd64.raw\n\ncompression done: /out/aws-amd64.raw.xz\n\n\nNow the _out/aws-amd64.raw.xz contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nIf the AWS machine is later going to be upgraded to a new version of Talos (or a new set of system extensions), generate a customized installer image following the steps above, and upgrade Talos to that installer image.\n\n1.7 - Omni SaaS\nOmni is a project created by the Talos team that has native support for Talos Linux.\n\nOmni allows you to start with bare metal, virtual machines or a cloud provider, and create clusters spanning all of your locations, with a few clicks.\n\nYou provide the machines – edge compute, bare metal, VMs, or in your cloud account. Boot from an Omni Talos Linux image. Click to allocate to a cluster. That’s it!\n\nVanilla Kubernetes, on your machines, under your control.\nElegant UI for management and operations\nSecurity taken care of – ties into your Enterprise ID provider\nHighly Available Kubernetes API end point built in\nFirewall friendly: manage Edge nodes securely\nFrom single-node clusters to the largest scale\nSupport for GPUs and most CSIs.\n\nThe Omni SaaS is available to run locally, to support air-gapped security and data sovereignty concerns.\n\nOmni handles the lifecycle of Talos Linux machines, provides unified access to the Talos and Kubernetes API tied to the identity provider of your choice, and provides a UI for cluster management and operations. Omni automates scaling the clusters up and down, and provides a unified view of the state of your clusters.\n\nSee more in the Omni documentation.\n\n2 - Configuration\nGuides on how to configure Talos Linux machines\n2.1 - Configuration Patches\nIn this guide, we’ll patch the generated machine configuration.\n\nTalos generates machine configuration for two types of machines: controlplane and worker machines. Many configuration options can be adjusted using talosctl gen config but not all of them. Configuration patching allows modifying machine configuration to fit it for the cluster or a specific machine.\n\nConfiguration Patch Formats\n\nTalos supports two configuration patch formats:\n\nstrategic merge patches\nRFC6902 (JSON patches)\n\nStrategic merge patches are the easiest to use, but JSON patches allow more precise configuration adjustments.\n\nNote: Talos 1.5+ supports multi-document machine configuration. JSON patches don’t support multi-document machine configuration, while strategic merge patches do.\n\nStrategic Merge patches\n\nStrategic merge patches look like incomplete machine configuration files:\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nWhen applied to the machine configuration, the patch gets merged with the respective section of the machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.2/24\n\n    hostname: worker1\n\n\nIn general, machine configuration contents are merged with the contents of the strategic merge patch, with strategic merge patch values overriding machine configuration values. There are some special rules:\n\nIf the field value is a list, the patch value is appended to the list, with the following exceptions:\nvalues of the fields cluster.network.podSubnets and cluster.network.serviceSubnets are overwritten on merge\nnetwork.interfaces section is merged with the value in the machine config if there is a match on interface: or deviceSelector: keys\nnetwork.interfaces.vlans section is merged with the value in the machine config if there is a match on the vlanId: key\ncluster.apiServer.auditPolicy value is replaced on merge\n\nWhen patching a multi-document machine configuration, following rules apply:\n\nfor each document in the patch, the document is merged with the respective document in the machine configuration (matching by kind, apiVersion and name for named documents)\nif the patch document doesn’t exist in the machine configuration, it is appended to the machine configuration\n\nThe strategic merge patch itself might be a multi-document YAML, and each document will be applied as a patch to the base machine configuration.\n\nRFC6902 (JSON Patches)\n\nJSON patches can be written either in JSON or YAML format. A proper JSON patch requires an op field that depends on the machine configuration contents: whether the path already exists or not.\n\nFor example, the strategic merge patch from the previous section can be written either as:\n\n- op: replace\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nor:\n\n- op: add\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nThe correct op depends on whether the /machine/network/hostname section exists already in the machine config or not.\n\nExamples\nMachine Network\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n\nThe goal is to add a virtual IP 192.168.10.50 to the eth0 interface and add another interface eth1 with DHCP enabled.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nPatched machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nCluster Network\n\nBase machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 10.244.0.0/16\n\n    serviceSubnets:\n\n      - 10.96.0.0/12\n\n\nThe goal is to update pod and service subnets and disable default CNI (Flannel).\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  network:\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nPatched machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nKubelet\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  kubelet: {}\n\n\nThe goal is to set the kubelet node IP to come from the subnet 192.168.10.0/24.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nPatched machine configuration:\n\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nAdmission Control: Pod Security Policy\n\nBase machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\n\nThe goal is to add an exemption for the namespace rook-ceph.\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          exemptions:\n\n            namespaces:\n\n              - rook-ceph\n\nPatched machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n              - rook-ceph\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\nConfiguration Patching with talosctl CLI\n\nSeveral talosctl commands accept config patches as command-line flags. Config patches might be passed either as an inline value or as a reference to a file with @file.patch syntax:\n\ntalosctl ... --patch '[{\"op\": \"add\", \"path\": \"/machine/network/hostname\", \"value\": \"worker1\"}]' --patch @file.patch\n\n\nIf multiple config patches are specified, they are applied in the order of appearance. The format of the patch (JSON patch or strategic merge patch) is detected automatically.\n\nTalos machine configuration can be patched at the moment of generation with talosctl gen config:\n\ntalosctl gen config test-cluster https://172.20.0.1:6443 --config-patch @all.yaml --config-patch-control-plane @cp.yaml --config-patch-worker @worker.yaml\n\n\nGenerated machine configuration can also be patched after the fact with talosctl machineconfig patch\n\ntalosctl machineconfig patch worker.yaml --patch @patch.yaml -o worker1.yaml\n\n\nMachine configuration on the running Talos node can be patched with talosctl patch:\n\ntalosctl patch mc --nodes 172.20.0.2 --patch @patch.yaml\n\n2.2 - Containerd\nCustomize Containerd Settings\n\nThe base containerd configuration expects to merge in any additional configs present in /etc/cri/conf.d/20-customization.part.\n\nExamples\nExposing Metrics\n\nPatch the machine config by adding the following:\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [metrics]\n\n          address = \"0.0.0.0:11234\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nOnce the server reboots, metrics are now available:\n\n$ curl ${IP}:11234/v1/metrics\n\n# HELP container_blkio_io_service_bytes_recursive_bytes The blkio io service bytes recursive\n\n# TYPE container_blkio_io_service_bytes_recursive_bytes gauge\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Async\"} 0\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Discard\"} 0\n\n...\n\n...\n\nPause Image\n\nThis change is often required for air-gapped environments, as containerd CRI plugin has a reference to the pause image which is used to create pods, and it can’t be controlled with Kubernetes pod definitions.\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            sandbox_image = \"registry.k8s.io/pause:3.8\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow the pause image is set to registry.k8s.io/pause:3.8:\n\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                      PID    STATUS\n\n172.20.0.5   k8s.io      kube-system/kube-flannel-6hfck                                  registry.k8s.io/pause:3.8                                  1773   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-cni:bc39fec3cbac      ghcr.io/siderolabs/install-cni:v1.3.0-alpha.0-2-gb155fa0   0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-config:5c3989353b98   ghcr.io/siderolabs/flannel:v0.20.1                         0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:kube-flannel:116c67b50da8     ghcr.io/siderolabs/flannel:v0.20.1                         2092   CONTAINER_RUNNING\n\n172.20.0.5   k8s.io      kube-system/kube-proxy-xp7jq                                    registry.k8s.io/pause:3.8                                  1780   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-proxy-xp7jq:kube-proxy:84fc77c59e17         registry.k8s.io/kube-proxy:v1.26.0-alpha.3                 1843   CONTAINER_RUNNING\n\n2.3 - Custom Certificate Authorities\nHow to supply custom certificate authorities\nAppending the Certificate Authority\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\n2.4 - Disk Encryption\nGuide on using system disk encryption\n\nIt is possible to enable encryption for system disks at the OS level. Currently, only STATE and EPHEMERAL partitions can be encrypted. STATE contains the most sensitive node data: secrets and certs. The EPHEMERAL partition may contain sensitive workload data. Data is encrypted using LUKS2, which is provided by the Linux kernel modules and cryptsetup utility. The operating system will run additional setup steps when encryption is enabled.\n\nIf the disk encryption is enabled for the STATE partition, the system will:\n\nSave STATE encryption config as JSON in the META partition.\nBefore mounting the STATE partition, load encryption configs either from the machine config or from the META partition. Note that the machine config is always preferred over the META one.\nBefore mounting the STATE partition, format and encrypt it. This occurs only if the STATE partition is empty and has no filesystem.\n\nIf the disk encryption is enabled for the EPHEMERAL partition, the system will:\n\nGet the encryption config from the machine config.\nBefore mounting the EPHEMERAL partition, encrypt and format it.\n\nThis occurs only if the EPHEMERAL partition is empty and has no filesystem.\n\nTalos Linux supports four encryption methods, which can be combined together for a single partition:\n\nstatic - encrypt with the static passphrase (weakest protection, for STATE partition encryption it means that the passphrase will be stored in the META partition).\nnodeID - encrypt with the key derived from the node UUID (weak, it is designed to protect against data being leaked or recovered from a drive that has been removed from a Talos Linux node).\nkms - encrypt using key sealed with network KMS (strong, but requires network access to decrypt the data.)\ntpm - encrypt with the key derived from the TPM (strong, when used with SecureBoot).\n\nNote: nodeID encryption is not designed to protect against attacks where physical access to the machine, including the drive, is available. It uses the hardware characteristics of the machine in order to decrypt the data, so drives that have been removed, or recycled from a cloud environment or attached to a different virtual machine, will maintain their protection and encryption.\n\nConfiguration\n\nDisk encryption is disabled by default. To enable disk encryption you should modify the machine configuration with the following options:\n\nmachine:\n\n  ...\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\nEncryption Keys\n\nNote: What the LUKS2 docs call “keys” are, in reality, a passphrase. When this passphrase is added, LUKS2 runs argon2 to create an actual key from that passphrase.\n\nLUKS2 supports up to 32 encryption keys and it is possible to specify all of them in the machine configuration. Talos always tries to sync the keys list defined in the machine config with the actual keys defined for the LUKS2 partition. So if you update the keys list, keep at least one key that is not changed to be used for key management.\n\nWhen you define a key you should specify the key kind and the slot:\n\nmachine:\n\n  ...\n\n  state:\n\n    keys:\n\n      - nodeID: {} # key kind\n\n        slot: 1\n\n\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: supersecret\n\n        slot: 0\n\n\nTake a note that key order does not play any role on which key slot is used. Every key must always have a slot defined.\n\nEncryption Key Kinds\n\nTalos supports two kinds of keys:\n\nnodeID which is generated using the node UUID and the partition label (note that if the node UUID is not really random it will fail the entropy check).\nstatic which you define right in the configuration.\nkms which is sealed with the network KMS.\ntpm which is sealed using the TPM and protected with SecureBoot.\n\nNote: Use static keys only if your STATE partition is encrypted and only for the EPHEMERAL partition. For the STATE partition it will be stored in the META partition, which is not encrypted.\n\nKey Rotation\n\nIn order to completely rotate keys, it is necessary to do talosctl apply-config a couple of times, since there is a need to always maintain a single working key while changing the other keys around it.\n\nSo, for example, first add a new key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: oldkey\n\n        slot: 0\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\n\nThen remove the old key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\nGoing from Unencrypted to Encrypted and Vice Versa\nEphemeral Partition\n\nThere is no in-place encryption support for the partitions right now, so to avoid losing data only empty partitions can be encrypted.\n\nAs such, migration from unencrypted to encrypted needs some additional handling, especially around explicitly wiping partitions.\n\napply-config should be called with --mode=staged.\nPartition should be wiped after apply-config, but before the reboot.\n\nEdit your machine config and add the encryption configuration:\n\nvim config.yaml\n\n\nApply the configuration with --mode=staged:\n\ntalosctl apply-config -f config.yaml -n <node ip> --mode=staged\n\n\nWipe the partition you’re going to encrypt:\n\ntalosctl reset --system-labels-to-wipe EPHEMERAL -n <node ip> --reboot=true\n\n\nThat’s it! After you run the last command, the partition will be wiped and the node will reboot. During the next boot the system will encrypt the partition.\n\nState Partition\n\nCalling wipe against the STATE partition will make the node lose the config, so the previous flow is not going to work.\n\nThe flow should be to first wipe the STATE partition:\n\ntalosctl reset  --system-labels-to-wipe STATE -n <node ip> --reboot=true\n\n\nNode will enter into maintenance mode, then run apply-config with --insecure flag:\n\ntalosctl apply-config --insecure -n <node ip> -f config.yaml\n\n\nAfter installation is complete the node should encrypt the STATE partition.\n\n2.5 - Editing Machine Configuration\nHow to edit and patch Talos machine configuration, with reboot, immediately, or stage update on reboot.\n\nTalos node state is fully defined by machine configuration. Initial configuration is delivered to the node at bootstrap time, but configuration can be updated while the node is running.\n\nThere are three talosctl commands which facilitate machine configuration updates:\n\ntalosctl apply-config to apply configuration from the file\ntalosctl edit machineconfig to launch an editor with existing node configuration, make changes and apply configuration back\ntalosctl patch machineconfig to apply automated machine configuration via JSON patch\n\nEach of these commands can operate in one of four modes:\n\napply change in automatic mode (default): reboot if the change can’t be applied without a reboot, otherwise apply the change immediately\napply change with a reboot (--mode=reboot): update configuration, reboot Talos node to apply configuration change\napply change immediately (--mode=no-reboot flag): change is applied immediately without a reboot, fails if the change contains any fields that can not be updated without a reboot\napply change on next reboot (--mode=staged): change is staged to be applied after a reboot, but node is not rebooted\napply change with automatic revert (--mode=try): change is applied immediately (if not possible, returns an error), and reverts it automatically in 1 minute if no configuration update is applied\napply change in the interactive mode (--mode=interactive; only for talosctl apply-config): launches TUI based interactive installer\n\nNote: applying change on next reboot (--mode=staged) doesn’t modify current node configuration, so next call to talosctl edit machineconfig --mode=staged will not see changes\n\nAdditionally, there is also talosctl get machineconfig -o yaml, which retrieves the current node configuration API resource and contains the machine configuration in the .spec field. It can be used to modify the configuration locally before being applied to the node.\n\nThe list of config changes allowed to be applied immediately in Talos v1.6.2:\n\n.debug\n.cluster\n.machine.time\n.machine.certCANs\n.machine.install (configuration is only applied during install/upgrade)\n.machine.network\n.machine.nodeLabels\n.machine.sysfs\n.machine.sysctls\n.machine.logging\n.machine.controlplane\n.machine.kubelet\n.machine.pods\n.machine.kernel\n.machine.registries (CRI containerd plugin will not pick up the registry authentication settings without a reboot)\n.machine.features.kubernetesTalosAPIAccess\ntalosctl apply-config\n\nThis command is traditionally used to submit initial machine configuration generated by talosctl gen config to the node.\n\nIt can also be used to apply configuration to running nodes. The initial YAML for this is typically obtained using talosctl get machineconfig -o yaml | yq eval .spec >machs.yaml. (We must use yq because for historical reasons, get returns the configuration as a full resource, while apply-config only accepts the raw machine config directly.)\n\nExample:\n\ntalosctl -n <IP> apply-config -f config.yaml\n\n\nCommand apply-config can also be invoked as apply machineconfig:\n\ntalosctl -n <IP> apply machineconfig -f config.yaml\n\n\nApplying machine configuration immediately (without a reboot):\n\ntalosctl -n IP apply machineconfig -f config.yaml --mode=no-reboot\n\n\nStarting the interactive installer:\n\ntalosctl -n IP apply machineconfig --mode=interactive\n\n\nNote: when a Talos node is running in the maintenance mode it’s necessary to provide --insecure (-i) flag to connect to the API and apply the config.\n\ntaloctl edit machineconfig\n\nCommand talosctl edit loads current machine configuration from the node and launches configured editor to modify the config. If config hasn’t been changed in the editor (or if updated config is empty), update is not applied.\n\nNote: Talos uses environment variables TALOS_EDITOR, EDITOR to pick up the editor preference. If environment variables are missing, vi editor is used by default.\n\nExample:\n\ntalosctl -n <IP> edit machineconfig\n\n\nConfiguration can be edited for multiple nodes if multiple IP addresses are specified:\n\ntalosctl -n <IP1>,<IP2>,... edit machineconfig\n\n\nApplying machine configuration change immediately (without a reboot):\n\ntalosctl -n <IP> edit machineconfig --mode=no-reboot\n\ntalosctl patch machineconfig\n\nCommand talosctl patch works similar to talosctl edit command - it loads current machine configuration, but instead of launching configured editor it applies a set of JSON patches to the configuration and writes the result back to the node.\n\nExample, updating kubelet version (in auto mode):\n\n$ talosctl -n <IP> patch machineconfig -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nUpdating kube-apiserver version in immediate mode (without a reboot):\n\n$ talosctl -n <IP> patch machineconfig --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nA patch might be applied to multiple nodes when multiple IPs are specified:\n\ntalosctl -n <IP1>,<IP2>,... patch machineconfig -p '[{...}]'\n\n\nPatches can also be sourced from files using @file syntax:\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.json -p @manifest-patch.json\n\n\nIt might be easier to store patches in YAML format vs. the default JSON format. Talos can detect file format automatically:\n\n# kubelet-patch.yaml\n\n- op: replace\n\n  path: /machine/kubelet/image\n\n  value: ghcr.io/siderolabs/kubelet:v1.29.0\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.yaml\n\nRecovering from Node Boot Failures\n\nIf a Talos node fails to boot because of wrong configuration (for example, control plane endpoint is incorrect), configuration can be updated to fix the issue.\n\n2.6 - Logging\nDealing with Talos Linux logs.\nViewing logs\n\nKernel messages can be retrieved with talosctl dmesg command:\n\n$ talosctl -n 172.20.1.2 dmesg\n\n\n\n172.20.1.2: kern:    info: [2021-11-10T10:09:37.662764956Z]: Command line: init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 console=ttyS0 reboot=k panic=1 talos.shutdown=halt talos.platform=metal talos.config=http://172.20.1.1:40101/config.yaml\n\n[...]\n\n\nService logs can be retrieved with talosctl logs command:\n\n$ talosctl -n 172.20.1.2 services\n\n\n\nNODE         SERVICE      STATE     HEALTH   LAST CHANGE   LAST EVENT\n\n172.20.1.2   apid         Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   containerd   Running   OK       19m29s ago    Health check successful\n\n172.20.1.2   cri          Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   etcd         Running   OK       19m22s ago    Health check successful\n\n172.20.1.2   kubelet      Running   OK       19m20s ago    Health check successful\n\n172.20.1.2   machined     Running   ?        19m30s ago    Service started as goroutine\n\n172.20.1.2   trustd       Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   udevd        Running   OK       19m28s ago    Health check successful\n\n\n\n$ talosctl -n 172.20.1.2 logs machined\n\n\n\n172.20.1.2: [talos] task setupLogger (1/1): done, 106.109µs\n\n172.20.1.2: [talos] phase logger (1/7): done, 564.476µs\n\n[...]\n\n\nContainer logs for Kubernetes pods can be retrieved with talosctl logs -k command:\n\n$ talosctl -n 172.20.1.2 containers -k\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                         PID    STATUS\n\n172.20.1.2   k8s.io      kube-system/kube-flannel-dk6d5                                  registry.k8s.io/pause:3.6                                     1329   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-cni:f1d4cf68feb9      ghcr.io/siderolabs/install-cni:v0.7.0-alpha.0-1-g2bb2efc      0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-config:bc39fec3cbac   quay.io/coreos/flannel:v0.13.0                                0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:kube-flannel:5c3989353b98     quay.io/coreos/flannel:v0.13.0                                1610   CONTAINER_RUNNING\n\n172.20.1.2   k8s.io      kube-system/kube-proxy-gfkqj                                    registry.k8s.io/pause:3.5                                     1311   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f         registry.k8s.io/kube-proxy:v1.29.0                            1379   CONTAINER_RUNNING\n\n\n\n$ talosctl -n 172.20.1.2 logs -k kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f\n\n172.20.1.2: 2021-11-30T19:13:20.567825192Z stderr F I1130 19:13:20.567737       1 server_others.go:138] \"Detected node IP\" address=\"172.20.0.3\"\n\n172.20.1.2: 2021-11-30T19:13:20.599684397Z stderr F I1130 19:13:20.599613       1 server_others.go:206] \"Using iptables Proxier\"\n\n[...]\n\nSending logs\nService logs\n\nYou can enable logs sendings in machine configuration:\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"udp://127.0.0.1:12345/\"\n\n        format: \"json_lines\"\n\n      - endpoint: \"tcp://host:5044/\"\n\n        format: \"json_lines\"\n\n\nSeveral destinations can be specified. Supported protocols are UDP and TCP. The only currently supported format is json_lines:\n\n{\n\n  \"msg\": \"[talos] apply config request: immediate true, on reboot false\",\n\n  \"talos-level\": \"info\",\n\n  \"talos-service\": \"machined\",\n\n  \"talos-time\": \"2021-11-10T10:48:49.294858021Z\"\n\n}\n\n\nMessages are newline-separated when sent over TCP. Over UDP messages are sent with one message per packet. msg, talos-level, talos-service, and talos-time fields are always present; there may be additional fields.\n\nKernel logs\n\nKernel log delivery can be enabled with the talos.logging.kernel kernel command line argument, which can be specified in the .machine.installer.extraKernelArgs:\n\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - talos.logging.kernel=tcp://host:5044/\n\n\nAlso kernel logs delivery can be configured using the document in machine configuration:\n\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log\n\nurl: tcp://host:5044/\n\n\nKernel log destination is specified in the same way as service log endpoint. The only supported format is json_lines.\n\nSample message:\n\n{\n\n  \"clock\":6252819, // time relative to the kernel boot time\n\n  \"facility\":\"user\",\n\n  \"msg\":\"[talos] task startAllServices (1/1): waiting for 6 services\\n\",\n\n  \"priority\":\"warning\",\n\n  \"seq\":711,\n\n  \"talos-level\":\"warn\", // Talos-translated `priority` into common logging level\n\n  \"talos-time\":\"2021-11-26T16:53:21.3258698Z\" // Talos-translated `clock` using current time\n\n}\n\n\nextraKernelArgs in the machine configuration are only applied on Talos upgrades, not just by applying the config. (Upgrading to the same version is fine).\n\nFilebeat example\n\nTo forward logs to other Log collection services, one way to do this is sending them to a Filebeat running in the cluster itself (in the host network), which takes care of forwarding it to other endpoints (and the necessary transformations).\n\nIf Elastic Cloud on Kubernetes is being used, the following Beat (custom resource) configuration might be helpful:\n\napiVersion: beat.k8s.elastic.co/v1beta1\n\nkind: Beat\n\nmetadata:\n\n  name: talos\n\nspec:\n\n  type: filebeat\n\n  version: 7.15.1\n\n  elasticsearchRef:\n\n    name: talos\n\n  config:\n\n    filebeat.inputs:\n\n      - type: \"udp\"\n\n        host: \"127.0.0.1:12345\"\n\n        processors:\n\n          - decode_json_fields:\n\n              fields: [\"message\"]\n\n              target: \"\"\n\n          - timestamp:\n\n              field: \"talos-time\"\n\n              layouts:\n\n                - \"2006-01-02T15:04:05.999999999Z07:00\"\n\n          - drop_fields:\n\n              fields: [\"message\", \"talos-time\"]\n\n          - rename:\n\n              fields:\n\n                - from: \"msg\"\n\n                  to: \"message\"\n\n\n\n  daemonSet:\n\n    updateStrategy:\n\n      rollingUpdate:\n\n        maxUnavailable: 100%\n\n    podTemplate:\n\n      spec:\n\n        dnsPolicy: ClusterFirstWithHostNet\n\n        hostNetwork: true\n\n        securityContext:\n\n          runAsUser: 0\n\n        containers:\n\n          - name: filebeat\n\n            ports:\n\n              - protocol: UDP\n\n                containerPort: 12345\n\n                hostPort: 12345\n\n\nThe input configuration ensures that messages and timestamps are extracted properly. Refer to the Filebeat documentation on how to forward logs to other outputs.\n\nAlso note the hostNetwork: true in the daemonSet configuration.\n\nThis ensures filebeat uses the host network, and listens on 127.0.0.1:12345 (UDP) on every machine, which can then be specified as a logging endpoint in the machine configuration.\n\nFluent-bit example\n\nFirst, we’ll create a value file for the fluentd-bit Helm chart.\n\n# fluentd-bit.yaml\n\n\n\npodAnnotations:\n\n  fluentbit.io/exclude: 'true'\n\n\n\nextraPorts:\n\n  - port: 12345\n\n    containerPort: 12345\n\n    protocol: TCP\n\n    name: talos\n\n\n\nconfig:\n\n  service: |\n\n    [SERVICE]\n\n      Flush         5\n\n      Daemon        Off\n\n      Log_Level     warn\n\n      Parsers_File  custom_parsers.conf    \n\n\n\n  inputs: |\n\n    [INPUT]\n\n      Name          tcp\n\n      Listen        0.0.0.0\n\n      Port          12345\n\n      Format        json\n\n      Tag           talos.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         kubernetes\n\n      Path          /var/log/containers/*.log\n\n      Parser        containerd\n\n      Tag           kubernetes.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         audit\n\n      Path          /var/log/audit/kube/*.log\n\n      Parser        audit\n\n      Tag           audit.*    \n\n\n\n  filters: |\n\n    [FILTER]\n\n      Name                kubernetes\n\n      Alias               kubernetes\n\n      Match               kubernetes.*\n\n      Kube_Tag_Prefix     kubernetes.var.log.containers.\n\n      Use_Kubelet         Off\n\n      Merge_Log           On\n\n      Merge_Log_Trim      On\n\n      Keep_Log            Off\n\n      K8S-Logging.Parser  Off\n\n      K8S-Logging.Exclude On\n\n      Annotations         Off\n\n      Labels              On\n\n\n\n    [FILTER]\n\n      Name          modify\n\n      Match         kubernetes.*\n\n      Add           source kubernetes\n\n      Remove        logtag    \n\n\n\n  customParsers: |\n\n    [PARSER]\n\n      Name          audit\n\n      Format        json\n\n      Time_Key      requestReceivedTimestamp\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z\n\n\n\n    [PARSER]\n\n      Name          containerd\n\n      Format        regex\n\n      Regex         ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$\n\n      Time_Key      time\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z    \n\n\n\n  outputs: |\n\n    [OUTPUT]\n\n      Name    stdout\n\n      Alias   stdout\n\n      Match   *\n\n      Format  json_lines    \n\n\n\n  # If you wish to ship directly to Loki from Fluentbit,\n\n  # Uncomment the following output, updating the Host with your Loki DNS/IP info as necessary.\n\n  # [OUTPUT]\n\n  # Name loki\n\n  # Match *\n\n  # Host loki.loki.svc\n\n  # Port 3100\n\n  # Labels job=fluentbit\n\n  # Auto_Kubernetes_Labels on\n\n\n\ndaemonSetVolumes:\n\n  - name: varlog\n\n    hostPath:\n\n      path: /var/log\n\n\n\ndaemonSetVolumeMounts:\n\n  - name: varlog\n\n    mountPath: /var/log\n\n\n\ntolerations:\n\n  - operator: Exists\n\n    effect: NoSchedule\n\n\nNext, we will add the helm repo for FluentBit, and deploy it to the cluster.\n\nhelm repo add fluent https://fluent.github.io/helm-charts\n\nhelm upgrade -i --namespace=kube-system -f fluentd-bit.yaml fluent-bit fluent/fluent-bit\n\n\nNow we need to find the service IP.\n\n$ kubectl -n kube-system get svc -l app.kubernetes.io/name=fluent-bit\n\n\n\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n\nfluent-bit   ClusterIP   10.200.0.138   <none>        2020/TCP,5170/TCP   108m\n\n\nFinally, we will change talos log destination with the command talosctl edit mc.\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"tcp://10.200.0.138:5170\"\n\n        format: \"json_lines\"\n\n\nThis example configuration was well tested with Cilium CNI, and it should work with iptables/ipvs based CNI plugins too.\n\nVector example\n\nVector is a lightweight observability pipeline ideal for a Kubernetes environment. It can ingest (source) logs from multiple sources, perform remapping on the logs (transform), and forward the resulting pipeline to multiple destinations (sinks). As it is an end to end platform, it can be run as a single-deployment ‘aggregator’ as well as a replicaSet of ‘Agents’ that run on each node.\n\nAs Talos can be set as above to send logs to a destination, we can run Vector as an Aggregator, and forward both kernel and service to a UDP socket in-cluster.\n\nBelow is an excerpt of a source/sink setup for Talos, with a ‘sink’ destination of an in-cluster Grafana Loki log aggregation service. As Loki can create labels from the log input, we have set up the Loki sink to create labels based on the host IP, service and facility of the inbound logs.\n\nNote that a method of exposing the Vector service will be required which may vary depending on your setup - a LoadBalancer is a good option.\n\nrole: \"Stateless-Aggregator\"\n\n\n\n# Sources\n\nsources:\n\n  talos_kernel_logs:\n\n    address: 0.0.0.0:6050\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n  talos_service_logs:\n\n    address: 0.0.0.0:6051\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n# Sinks\n\nsinks:\n\n  talos_kernel:\n\n    type: loki\n\n    inputs:\n\n      - talos_kernel_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 1048576\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      facility: >-\n\n                {{`{{ facility }}`}}\n\n\n\n  talos_service:\n\n    type: loki\n\n    inputs:\n\n      - talos_service_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 400000\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      service: >-\n\n                {{`{{ \"talos-service\" }}`}}\n\n2.7 - Managing Talos PKI\nHow to manage Public Key Infrastructure\nGenerating New Client Configuration\nUsing Controlplane Node\n\nIf you have a valid (not expired) talosconfig with os:admin role, a new client configuration file can be generated with talosctl config new against any controlplane node:\n\ntalosctl -n CP1 config new talosconfig-reader --roles os:reader --crt-ttl 24h\n\n\nA specific role and certificate lifetime can be specified.\n\nFrom Secrets Bundle\n\nIf a secrets bundle (secrets.yaml from talosctl gen secrets) was saved while generating machine configuration:\n\ntalosctl gen config --with-secrets secrets.yaml --output-types talosconfig -o talosconfig <cluster-name> https://<cluster-endpoint>\n\n\nNote: <cluster-name> and <cluster-endpoint> arguments don’t matter, as they are not used for talosconfig.\n\nFrom Control Plane Machine Configuration\n\nIn order to create a new key pair for client configuration, you will need the root Talos API CA. The base64 encoded CA can be found in the control plane node’s configuration file. Save the the CA public key, and CA private key as ca.crt, and ca.key respectively:\n\nyq eval .machine.ca.crt controlplane.yaml | base64 -d > ca.crt\n\nyq eval .machine.ca.key controlplane.yaml | base64 -d > ca.key\n\n\nNow, run the following commands to generate a certificate:\n\ntalosctl gen key --name admin\n\ntalosctl gen csr --key admin.key --ip 127.0.0.1\n\ntalosctl gen crt --ca ca --csr admin.csr --name admin\n\n\nPut the base64-encoded files to the respective location to the talosconfig:\n\ncontext: mycluster\n\ncontexts:\n\n    mycluster:\n\n        endpoints:\n\n            - CP1\n\n            - CP2\n\n        ca: <base64-encoded ca.crt>\n\n        crt: <base64-encoded admin.crt>\n\n        key: <base64-encoded admin.key>\n\nRenewing an Expired Administrator Certificate\n\nBy default admin talosconfig certificate is valid for 365 days, while cluster CAs are valid for 10 years. In order to prevent admin talosconfig from expiring, renew the client config before expiration using talosctl config new command described above.\n\nIf the talosconfig is expired or lost, you can still generate a new one using either the secrets.yaml secrets bundle or the control plane node’s configuration file using methods described above.\n\n2.8 - NVIDIA Fabric Manager\nIn this guide we’ll follow the procedure to enable NVIDIA Fabric Manager.\n\nNVIDIA GPUs that have nvlink support (for eg: A100) will need the nvidia-fabricmanager system extension also enabled in addition to the NVIDIA drivers. For more information on Fabric Manager refer https://docs.nvidia.com/datacenter/tesla/fabric-manager-user-guide/index.html\n\nThe published versions of the NVIDIA fabricmanager system extensions is available here\n\nThe nvidia-fabricmanager extension version has to match with the NVIDIA driver version in use.\n\nEnabling the NVIDIA fabricmanager system extension\n\nCreate the boot assets or a custom installer and perform a machine upgrade which include the following system extensions:\n\nghcr.io/siderolabs/nvidia-open-gpu-kernel-modules:535.129.03-v1.6.2\n\nghcr.io/siderolabs/nvidia-container-toolkit:535.129.03-v1.13.5\n\nghcr.io/siderolabs/nvidia-fabricmanager:535.129.03\n\n\nPatch the machine configuration to load the required modules:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n2.9 - NVIDIA GPU (OSS drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using OSS drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA OSS drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA OSS system extensions:\n\nnvidia-open-gpu-kernel-modules\nnvidia-container-toolkit\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nvidia-open-gpu-kernel-modules and nvidia-container-toolkit extensions. The nvidia-open-gpu-kernel-modules extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA OSS modules\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                           VERSION   NAME                             VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-container-toolkit-515.65.01-v1.10.0            1         nvidia-container-toolkit         515.65.01-v1.10.0\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-open-gpu-kernel-modules-515.65.01-v1.2.0       1         nvidia-open-gpu-kernel-modules   515.65.01-v1.2.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  515.65.01  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 12.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n2.10 - NVIDIA GPU (Proprietary drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using proprietary drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA system extensions:\n\nnonfree-kmod-nvidia\nnvidia-container-toolkit\n\nTo build a NVIDIA driver version not published by SideroLabs follow the instructions here\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nonfree-kmod-nvidia and nvidia-container-toolkit extensions. The nonfree-kmod-nvidia extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA modules and the system extension\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                 VERSION   NAME                       VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-frezbo-nvidia-container-toolkit-510.60.02-v1.9.0       1         nvidia-container-toolkit   510.60.02-v1.9.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  510.60.02  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 11.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n2.11 - Pull Through Image Cache\nHow to set up local transparent container images caches.\n\nIn this guide we will create a set of local caching Docker registry proxies to minimize local cluster startup time.\n\nWhen running Talos locally, pulling images from container registries might take a significant amount of time. We spin up local caching pass-through registries to cache images and configure a local Talos cluster to use those proxies. A similar approach might be used to run Talos in production in air-gapped environments. It can be also used to verify that all the images are available in local registries.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\n\nThe follow are requirements for creating the set of caching proxies:\n\nDocker 18.03 or greater\nLocal cluster requirements for either docker or QEMU.\nLaunch the Caching Docker Registry Proxies\n\nTalos pulls from docker.io, registry.k8s.io, gcr.io, and ghcr.io by default. If your configuration is different, you might need to modify the commands below:\n\ndocker run -d -p 5000:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \\\n\n    --restart always \\\n\n    --name registry-docker.io registry:2\n\n\n\ndocker run -d -p 5001:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry.k8s.io \\\n\n    --restart always \\\n\n    --name registry-registry.k8s.io registry:2\n\n\n\ndocker run -d -p 5003:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://gcr.io \\\n\n    --restart always \\\n\n    --name registry-gcr.io registry:2\n\n\n\ndocker run -d -p 5004:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \\\n\n    --restart always \\\n\n    --name registry-ghcr.io registry:2\n\n\nNote: Proxies are started as docker containers, and they’re automatically configured to start with Docker daemon.\n\nAs a registry container can only handle a single upstream Docker registry, we launch a container per upstream, each on its own host port (5000, 5001, 5002, 5003 and 5004).\n\nUsing Caching Registries with QEMU Local Cluster\n\nWith a QEMU local cluster, a bridge interface is created on the host. As registry containers expose their ports on the host, we can use bridge IP to direct proxy requests.\n\nsudo talosctl cluster create --provisioner qemu \\\n\n    --registry-mirror docker.io=http://10.5.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://10.5.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://10.5.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://10.5.0.1:5004\n\n\nThe Talos local cluster should now start pulling via caching registries. This can be verified via registry logs, e.g. docker logs -f registry-docker.io. The first time cluster boots, images are pulled and cached, so next cluster boot should be much faster.\n\nNote: 10.5.0.1 is a bridge IP with default network (10.5.0.0/24), if using custom --cidr, value should be adjusted accordingly.\n\nUsing Caching Registries with docker Local Cluster\n\nWith a docker local cluster we can use docker bridge IP, default value for that IP is 172.17.0.1. On Linux, the docker bridge address can be inspected with ip addr show docker0.\n\ntalosctl cluster create --provisioner docker \\\n\n    --registry-mirror docker.io=http://172.17.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.17.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://172.17.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.17.0.1:5004\n\nMachine Configuration\n\nThe caching registries can be configured via machine configuration patch, equivalent to the command line flags above:\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5000\n\n      gcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5003\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5004\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5001\n\nCleaning Up\n\nTo cleanup, run:\n\ndocker rm -f registry-docker.io\n\ndocker rm -f registry-registry.k8s.io\n\ndocker rm -f registry-gcr.io\n\ndocker rm -f registry-ghcr.io\n\n\nNote: Removing docker registry containers also removes the image cache. So if you plan to use caching registries, keep the containers running.\n\nUsing Harbor as a Caching Registry\n\nHarbor is an open source container registry that can be used as a caching proxy. Harbor supports configuring multiple upstream registries, so it can be used to cache multiple registries at once behind a single endpoint.\n\nAs Harbor puts a registry name in the pull image path, we need to set overridePath: true to prevent Talos and containerd from appending /v2 to the path.\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-docker.io\n\n        overridePath: true\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-ghcr.io\n\n        overridePath: true\n\n      gcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-gcr.io\n\n        overridePath: true\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-registry.k8s.io\n\n        overridePath: true\n\n\nThe Harbor external endpoint (http://harbor in this example) can be configured with authentication or custom TLS:\n\nmachine:\n\n  registries:\n\n    config:\n\n      harbor:\n\n        auth:\n\n          username: admin\n\n          password: password\n\n2.12 - Role-based access control (RBAC)\nSet up RBAC on the Talos Linux API.\n\nTalos v0.11 introduced initial support for role-based access control (RBAC). This guide will explain what that is and how to enable it without losing access to the cluster.\n\nRBAC in Talos\n\nTalos uses certificates to authorize users. The certificate subject’s organization field is used to encode user roles. There is a set of predefined roles that allow access to different API methods:\n\nos:admin grants access to all methods;\nos:operator grants everything os:reader role does, plus additional methods: rebooting, shutting down, etcd backup, etcd alarm management, and so on;\nos:reader grants access to “safe” methods (for example, that includes the ability to list files, but does not include the ability to read files content);\nos:etcd:backup grants access to /machine.MachineService/EtcdSnapshot method.\n\nRoles in the current talosconfig can be checked with the following command:\n\n$ talosctl config info\n\n\n\n[...]\n\nRoles:               os:admin\n\n[...]\n\n\nRBAC is enabled by default in new clusters created with talosctl v0.11+ and disabled otherwise.\n\nEnabling RBAC\n\nFirst, both the Talos cluster and talosctl tool should be upgraded. Then the talosctl config new command should be used to generate a new client configuration with the os:admin role. Additional configurations and certificates for different roles can be generated by passing --roles flag:\n\ntalosctl config new --roles=os:reader reader\n\n\nThat command will create a new client configuration file reader with a new certificate with os:reader role.\n\nAfter that, RBAC should be enabled in the machine configuration:\n\nmachine:\n\n  features:\n\n    rbac: true\n\n2.13 - System Extensions\nCustomizing the Talos Linux immutable root file system.\n\nSystem extensions allow extending the Talos root filesystem, which enables a variety of features, such as including custom container runtimes, loading additional firmware, etc.\n\nSystem extensions are only activated during the installation or upgrade of Talos Linux. With system extensions installed, the Talos root filesystem is still immutable and read-only.\n\nInstalling System Extensions\n\nNote: the way to install system extensions in the .machine.install section of the machine configuration is now deprecated.\n\nStarting with Talos v1.5.0, Talos supports generation of boot media with system extensions included, this removes the need to rebuild the initramfs.xz on the machine itself during the installation or upgrade.\n\nThere are two kinds of boot assets that Talos can generate:\n\ninitial boot assets (ISO, PXE, etc.) that are used to boot the machine\ndisk images that have Talos pre-installed\ninstaller container images that can be used to install or upgrade Talos on a machine (installation happens when booted from ISO or PXE)\n\nDepending on the nature of the system extension (e.g. network device driver or containerd plugin), it may be necessary to include the extension in both initial boot assets and disk images/installer, or just the installer.\n\nThe process of generating boot assets with extensions included is described in the boot assets guide.\n\nExample: Booting from an ISO\n\nLet’s assume NVIDIA extension is required on a bare metal machine which is going to be booted from an ISO. As NVIDIA extension is not required for the initial boot and install step, it is sufficient to include the extension in the installer image only.\n\nUse a generic Talos ISO to boot the machine.\nPrepare a custom installer container image with NVIDIA extension included, push the image to a registry.\nEnsure that machine configuration field .machine.install.image points to the custom installer image.\nBoot the machine using the ISO, apply the machine configuration.\nTalos pulls a custom installer image from the registry (containing NVIDIA extension), installs Talos on the machine, and reboots.\n\nWhen it’s time to upgrade Talos, generate a custom installer container for a new version of Talos, push it to a registry, and perform upgrade pointing to the custom installer image.\n\nExample: Disk Image\n\nLet’s assume NVIDIA extension is required on AWS VM.\n\nPrepare an AWS disk image with NVIDIA extension included.\nUpload the image to AWS, register it as an AMI.\nUse the AMI to launch a VM.\nTalos boots with NVIDIA extension included.\n\nWhen it’s time to upgrade Talos, either repeat steps 1-4 to replace the VM with a new AMI, or like in the previous example, generate a custom installer and use it to upgrade Talos in-place.\n\nAuthoring System Extensions\n\nA Talos system extension is a container image with the specific folder structure. System extensions can be built and managed using any tool that produces container images, e.g. docker build.\n\nSidero Labs maintains a repository of system extensions.\n\nResource Definitions\n\nUse talosctl get extensions to get a list of system extensions:\n\n$ talosctl get extensions\n\nNODE         NAMESPACE   TYPE              ID                                              VERSION   NAME          VERSION\n\n172.20.0.2   runtime     ExtensionStatus   000.ghcr.io-talos-systems-gvisor-54b831d        1         gvisor        20220117.0-v1.0.0\n\n172.20.0.2   runtime     ExtensionStatus   001.ghcr.io-talos-systems-intel-ucode-54b831d   1         intel-ucode   microcode-20210608-v1.0.0\n\n\nUse YAML or JSON format to see additional details about the extension:\n\n$ talosctl -n 172.20.0.2 get extensions 001.ghcr.io-talos-systems-intel-ucode-54b831d -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: runtime\n\n    type: ExtensionStatuses.runtime.talos.dev\n\n    id: 001.ghcr.io-talos-systems-intel-ucode-54b831d\n\n    version: 1\n\n    owner: runtime.ExtensionStatusController\n\n    phase: running\n\n    created: 2022-02-10T18:25:04Z\n\n    updated: 2022-02-10T18:25:04Z\n\nspec:\n\n    image: 001.ghcr.io-talos-systems-intel-ucode-54b831d.sqsh\n\n    metadata:\n\n        name: intel-ucode\n\n        version: microcode-20210608-v1.0.0\n\n        author: Spencer Smith\n\n        description: |\n\n            This system extension provides Intel microcode binaries.\n\n        compatibility:\n\n            talos:\n\n                version: '>= v1.0.0'\n\nExample: gVisor\n\nSee readme of the gVisor extension.\n\n3 - How Tos\nHow to guide for common tasks in Talos Linux\n3.1 - How to enable workers on your control plane nodes\nHow to enable workers on your control plane nodes.\n\nBy default, Talos Linux taints control plane nodes so that workloads are not schedulable on them.\n\nIn order to allow workloads to run on the control plane nodes (useful for single node clusters, or non-production clusters), follow the procedure below.\n\nModify the MachineConfig for the controlplane nodes to add allowSchedulingOnControlPlanes: true:\n\ncluster:\n\n    allowSchedulingOnControlPlanes: true\n\n\nThis may be done via editing the controlplane.yaml file before it is applied to the control plane nodes, by editing the machine config, or by patching the machine config.\n\n3.2 - How to manage certificate lifetimes with Talos Linux\n\nTalos Linux automatically manages and rotates all server side certs for etcd, Kubernetes, and the Talos API. Note however that the kubelet needs to be restarted at least once a year in order for the certificates to be rotated. Any upgrade/reboot of the node will suffice for this effect.\n\nClient certs (talosconfig and kubeconfig) are the user’s responsibility. Each time you download the kubeconfig file from a Talos Linux cluster, the client certificate is regenerated giving you a kubeconfig which is valid for a year.\n\nThe talosconfig file should be renewed at least once a year, using the talosctl config new command.\n\n3.3 - How to scale down a Talos cluster\nHow to remove nodes from a Talos Linux cluster.\n\nTo remove nodes from a Talos Linux cluster:\n\ntalosctl -n <IP.of.node.to.remove> reset\nkubectl delete node <nodename>\n\nThe command talosctl reset will cordon and drain the node, leaving etcd if required, and then erase its disks and power down the system.\n\nThis command will also remove the node from registration with the discovery service, so it will no longer show up in talosctl get members.\n\nIt is still necessary to remove the node from Kubernetes, as noted above.\n\n3.4 - How to scale up a Talos cluster\nHow to add more nodes to a Talos Linux cluster.\n\nTo add more nodes to a Talos Linux cluster, follow the same procedure as when initially creating the cluster:\n\nboot the new machines to install Talos Linux\napply the worker.yaml or controlplane.yaml configuration files to the new machines\n\nYou need the controlplane.yaml and worker.yaml that were created when you initially deployed your cluster. These contain the certificates that enable new machines to join.\n\nOnce you have the IP address, you can then apply the correct configuration for each machine you are adding, either worker or controlplane.\n\n  talosctl apply-config --insecure \\\n\n    --nodes [NODE IP] \\\n\n    --file controlplane.yaml\n\n\nThe insecure flag is necessary because the PKI infrastructure has not yet been made available to the node.\n\nYou do not need to bootstrap the new node. Regardless of whether you are adding a control plane or worker node, it will now join the cluster in its role.\n\n4 - Network\nSet up networking layers for Talos Linux\n4.1 - Corporate Proxies\nHow to configure Talos Linux to use proxies in a corporate environment\nAppending the Certificate Authority of MITM Proxies\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\nConfiguring a Machine to Use the Proxy\n\nTo make use of a proxy:\n\nmachine:\n\n  env:\n\n    http_proxy: <http proxy>\n\n    https_proxy: <https proxy>\n\n    no_proxy: <no proxy>\n\n\nAdditionally, configure the DNS nameservers, and NTP servers:\n\nmachine:\n\n  env:\n\n  ...\n\n  time:\n\n    servers:\n\n      - <server 1>\n\n      - <server ...>\n\n      - <server n>\n\n  ...\n\n  network:\n\n    nameservers:\n\n      - <ip 1>\n\n      - <ip ...>\n\n      - <ip n>\n\n\nIf a proxy is required before Talos machine configuration is applied, use kernel command line arguments:\n\ntalos.environment=http_proxy=<http-proxy> talos.environment=https_proxy=<https-proxy>\n\n4.2 - Ingress Firewall\nLearn to use Talos Linux Ingress Firewall to limit access to the host services.\n\nTalos Linux Ingress Firewall is a simple and effective way to limit access to the services running on the host, which includes both Talos standard services (e.g. apid and kubelet), and any additional workloads that may be running on the host. Talos Linux Ingress Firewall doesn’t affect the traffic between the Kubernetes pods/services, please use CNI Network Policies for that.\n\nConfiguration\n\nIngress rules are configured as extra documents NetworkDefaultActionConfig and NetworkRuleConfig in the Talos machine configuration:\n\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 172.20.0.0/24\n\n    except: 172.20.0.1/32\n\n\nThe first document configures the default action for the ingress traffic, which can be either accept or block, with the default being accept. If the default action is set to accept, then all the ingress traffic will be allowed, unless there is a matching rule that blocks it. If the default action is set to block, then all the ingress traffic will be blocked, unless there is a matching rule that allows it.\n\nWith either accept or block, the traffic is always allowed on the following network interfaces:\n\nlo\nsiderolink\nkubespan\n\nIn the block mode:\n\nICMP and ICMPv6 traffic is also allowed with a rate limit of 5 packets per second\ntraffic between Kubernetes pod/service subnets is allowed (for native routing CNIs)\n\nThe second document defines an ingress rule for a set of ports and protocols on the host. The NetworkRuleConfig might be repeated many times to define multiple rules, but each document must have a unique name.\n\nThe ports field accepts either a single port or a port range:\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n    - 10260\n\n    - 10300-10400\n\n\nThe protocol might be either tcp or udp.\n\nThe ingress specifies the list of subnets that are allowed to access the host services, with the optional except field to exclude a set of addresses from the subnet.\n\nNote: incorrect configuration of the ingress firewall might result in the host becoming inaccessible over Talos API. The configuration might be applied in --mode=try to make sure it gets reverted in case of a mistake.\n\nRecommended Rules\n\nThe following rules improve the security of the cluster and cover only standard Talos services. If there are additional services running with host networking in the cluster, they should be covered by additional rules.\n\nIn the block mode, the ingress firewall will also block encapsulated traffic (e.g. VXLAN) between the nodes, which needs to be explicitly allowed for the Kubernetes networking to function properly. Please refer to the CNI documentation for the specifics, some default configurations are listed below:\n\nFlannel, Calico: vxlan UDP port 4789\nCilium: vxlan UDP port 8472\n\nIn the examples we assume following template variables to describe the cluster:\n\n$CLUSTER_SUBNET, e.g. 172.20.0.0/24 - the subnet which covers all machines in the cluster\n$CP1, $CP2, $CP3 - the IP addresses of the controlplane nodes\n$VXLAN_PORT - the UDP port used by the CNI for encapsulated traffic\nControlplane\napid and Kubernetes API are wide open\nkubelet and trustd API is only accessible within the cluster\netcd API is limited to controlplane nodes\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: trustd-ingress\n\nportSelector:\n\n  ports:\n\n    - 50001\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubernetes-api-ingress\n\nportSelector:\n\n  ports:\n\n    - 6443\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: etcd-ingress\n\nportSelector:\n\n  ports:\n\n    - 2379-2380\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CP1/32\n\n  - subnet: $CP2/32\n\n  - subnet: $CP3/32\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nWorker\nkubelet and apid API is only accessible within the cluster\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nLearn More\n\nTalos Linux Ingress Firewall is using the nftables to perform the filtering.\n\nWith the default action set to accept, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy accept;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ip saddr != { 172.20.0.0/24 } tcp dport { 10250 } drop\n\n    meta nfproto ipv6 tcp dport { 10250 } drop\n\n  }\n\n}\n\n\nWith the default action set to block, the following rules are applied (example):\n\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy drop;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ct state { established, related } accept\n\n    ct state invalid drop\n\n    meta l4proto icmp limit rate 5/second accept\n\n    meta l4proto ipv6-icmp limit rate 5/second accept\n\n    ip saddr { 172.20.0.0/24 } tcp dport { 10250 }  accept\n\n    meta nfproto ipv4 tcp dport { 50000 } accept\n\n    meta nfproto ipv6 tcp dport { 50000 } accept\n\n  }\n\n}\n\n4.3 - KubeSpan\nLearn to use KubeSpan to connect Talos Linux machines securely across networks.\n\nKubeSpan is a feature of Talos that automates the setup and maintenance of a full mesh WireGuard network for your cluster, giving you the ability to operate hybrid Kubernetes clusters that can span the edge, datacenter, and cloud. Management of keys and discovery of peers can be completely automated, making it simple and easy to create hybrid clusters.\n\nKubeSpan consists of client code in Talos Linux, as well as a discovery service that enables clients to securely find each other. Sidero Labs operates a free Discovery Service, but the discovery service may, with a commercial license, be operated by your organization and can be downloaded here.\n\nVideo Walkthrough\n\nTo see a live demo of KubeSpan, see one the videos below:\n\n \nNetwork Requirements\n\nKubeSpan uses UDP port 51820 to carry all KubeSpan encrypted traffic. Because UDP traversal of firewalls is often lenient, and the Discovery Service communicates the apparent IP address of all peers to all other peers, KubeSpan will often work automatically, even when each nodes is behind their own firewall. However, when both ends of a KubeSpan connection are behind firewalls, it is possible the connection may not be established correctly - it depends on each end sending out packets in a limited time window.\n\nThus best practice is to ensure that one end of all possible node-node communication allows UDP port 51820, inbound.\n\nFor example, if control plane nodes are running in a corporate data center, behind firewalls, KubeSpan connectivity will work correctly so long as worker nodes on the public Internet can receive packets on UDP port 51820. (Note the workers will also need to receive TCP port 50000 for initial configuration via talosctl).\n\nAn alternative topology would be to run control plane nodes in a public cloud, and allow inbound UDP port 51820 to the control plane nodes. Workers could be behind firewalls, and KubeSpan connectivity will be established. Note that if workers are in different locations, behind different firewalls, the KubeSpan connectivity between workers should be correctly established, but may require opening the KubeSpan UDP port on the local firewall also.\n\nCaveats\nKubernetes API Endpoint Limitations\n\nWhen the K8s endpoint is an IP address that is not part of Kubespan, but is an address that is forwarded on to the Kubespan address of a control plane node, without changing the source address, then worker nodes will fail to join the cluster. In such a case, the control plane node has no way to determine whether the packet arrived on the private Kubespan address, or the public IP address. If the source of the packet was a Kubespan member, the reply will be Kubespan encapsulated, and thus not translated to the public IP, and so the control plane will reply to the session with the wrong address.\n\nThis situation is seen, for example, when the Kubernetes API endpoint is the public IP of a VM in GCP or Azure for a single node control plane. The control plane will receive packets on the public IP, but will reply from it’s KubeSpan address. The workaround is to create a load balancer to terminate the Kubernetes API endpoint.\n\nDigital Ocean Limitations\n\nDigital Ocean assigns an “Anchor IP” address to each droplet. Talos Linux correctly identifies this as a link-local address, and configures KubeSpan correctly, but this address will often be selected by Flannel or other CNIs as a node’s private IP. Because this address is not routable, nor advertised via KubeSpan, it will break pod-pod communication between nodes. This can be worked-around by assigning a non-Anchor private IP:\n\nkubectl annotate node do-worker flannel.alpha.coreos.com/public-ip-overwrite=10.116.X.X\n\nThen restarting flannel: kubectl delete pods -n kube-system -l k8s-app=flannel\n\nEnabling\nCreating a New Cluster\n\nTo enable KubeSpan for a new cluster, we can use the --with-kubespan flag in talosctl gen config. This will enable peer discovery and KubeSpan.\n\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\n\ncluster:\n\n    discovery:\n\n        enabled: true\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            kubernetes: # Kubernetes registry is problematic with KubeSpan, if the control plane endpoint is routeable itself via KubeSpan.\n\n              disabled: true\n\n            service: {}\n\n\nThe default discovery service is an external service hosted by Sidero Labs at https://discovery.talos.dev/. Contact Sidero Labs if you need to run this service privately.\n\nEnabling for an Existing Cluster\n\nIn order to enable KubeSpan on an existing cluster, enable kubespan and discovery settings in the machine config for each machine in the cluster (discovery is enabled by default):\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: true\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\nConfiguration\n\nKubeSpan will automatically discovery all cluster members, exchange Wireguard public keys and establish a full mesh network.\n\nThere are configuration options available which are not usually required:\n\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: false\n\n      advertiseKubernetesNetworks: false\n\n      allowDownPeerBypass: false\n\n      mtu: 1420\n\n      filters:\n\n        endpoints:\n\n          - 0.0.0.0/0\n\n          - ::/0\n\n\nThe setting advertiseKubernetesNetworks controls whether the node will advertise Kubernetes service and pod networks to other nodes in the cluster over KubeSpan. It defaults to being disabled, which means KubeSpan only controls the node-to-node traffic, while pod-to-pod traffic is routed and encapsulated by CNI. This setting should not be enabled with Calico and Cilium CNI plugins, as they do their own pod IP allocation which is not visible to KubeSpan.\n\nThe setting allowDownPeerBypass controls whether the node will allow traffic to bypass WireGuard if the destination is not connected over KubeSpan. If enabled, there is a risk that traffic will be routed unencrypted if the destination is not connected over KubeSpan, but it allows a workaround for the case where a node is not connected to the KubeSpan network, but still needs to access the cluster.\n\nThe mtu setting configures the Wireguard MTU, which defaults to 1420. This default value of 1420 is safe to use when the underlying network MTU is 1500, but if the underlying network MTU is smaller, the KubeSpanMTU should be adjusted accordingly: KubeSpanMTU = UnderlyingMTU - 80.\n\nThe filters setting allows hiding some endpoints from being advertised over KubeSpan. This is useful when some endpoints are known to be unreachable between the nodes, so that KubeSpan doesn’t try to establish a connection to them. Another use-case is hiding some endpoints if nodes can connect on multiple networks, and some of the networks are more preferable than others.\n\nResource Definitions\nKubeSpanIdentities\n\nA node’s WireGuard identities can be obtained with:\n\n$ talosctl get kubespanidentities -o yaml\n\n...\n\nspec:\n\n    address: fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94/128\n\n    subnet: fd83:b1f7:fcb5:2802::/64\n\n    privateKey: gNoasoKOJzl+/B+uXhvsBVxv81OcVLrlcmQ5jQwZO08=\n\n    publicKey: NzW8oeIH5rJyY5lefD9WRoHWWRr/Q6DwsDjMX+xKjT4=\n\n\nTalos automatically configures unique IPv6 address for each node in the cluster-specific IPv6 ULA prefix.\n\nThe Wireguard private key is generated and never leaves the node, while the public key is published through the cluster discovery.\n\nKubeSpanIdentity is persisted across reboots and upgrades in STATE partition in the file kubespan-identity.yaml.\n\nKubeSpanPeerSpecs\n\nA node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerspecs\n\nID                                             VERSION   LABEL                          ENDPOINTS\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   2         talos-default-controlplane-2   [\"172.20.0.3:51820\"]\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   2         talos-default-controlplane-3   [\"172.20.0.4:51820\"]\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   2         talos-default-worker-2         [\"172.20.0.6:51820\"]\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   2         talos-default-worker-1         [\"172.20.0.5:51820\"]\n\n\nThe peer ID is the Wireguard public key. KubeSpanPeerSpecs are built from the cluster discovery data.\n\nKubeSpanPeerStatuses\n\nThe status of a node’s WireGuard peers can be obtained with:\n\n$ talosctl get kubespanpeerstatuses\n\nID                                             VERSION   LABEL                          ENDPOINT           STATE   RX         TX\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   63        talos-default-controlplane-2   172.20.0.3:51820   up      15043220   17869488\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   62        talos-default-controlplane-3   172.20.0.4:51820   up      14573208   18157680\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   60        talos-default-worker-2         172.20.0.6:51820   up      130072     46888\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   60        talos-default-worker-1         172.20.0.5:51820   up      130044     46556\n\n\nKubeSpan peer status includes following information:\n\nthe actual endpoint used for peer communication\nlink state:\nunknown: the endpoint was just changed, link state is not known yet\nup: there is a recent handshake from the peer\ndown: there is no handshake from the peer\nnumber of bytes sent/received over the Wireguard link with the peer\n\nIf the connection state goes down, Talos will be cycling through the available endpoints until it finds the one which works.\n\nPeer status information is updated every 30 seconds.\n\nKubeSpanEndpoints\n\nA node’s WireGuard endpoints (peer addresses) can be obtained with:\n\n$ talosctl get kubespanendpoints\n\nID                                             VERSION   ENDPOINT           AFFILIATE ID\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   1         172.20.0.3:51820   2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   1         172.20.0.4:51820   b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   1         172.20.0.6:51820   NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   1         172.20.0.5:51820   6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA\n\n\nThe endpoint ID is the base64 encoded WireGuard public key.\n\nThe observed endpoints are submitted back to the discovery service (if enabled) so that other peers can try additional endpoints to establish the connection.\n\n4.4 - Network Device Selector\nHow to configure network devices by selecting them using hardware information\nConfiguring Network Device Using Device Selector\n\ndeviceSelector is an alternative method of configuring a network device:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          driver: virtio\n\n          hardwareAddr: \"00:00:*\"\n\n        address: 192.168.88.21\n\n\nSelector has the following traits:\n\nqualifiers match a device by reading the hardware information in /sys/class/net/...\nqualifiers are applied using logical AND\nmachine.network.interfaces.deviceConfig option is mutually exclusive with machine.network.interfaces.interface\nif the selector matches multiple devices, the controller will apply config to all of them\n\nThe available hardware information used in the selector can be observed in the LinkStatus resource (works in maintenance mode):\n\n# talosctl get links eth0 -o yaml\n\nspec:\n\n  ...\n\n  hardwareAddr: 4e:95:8e:8f:e4:47\n\n  busPath: 0000:06:00.0\n\n  driver: alx\n\n  pciID: 1969:E0B1\n\nUsing Device Selector for Bonding\n\nDevice selectors can be used to configure bonded interfaces:\n\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        bond:\n\n          mode: balance-rr\n\n          deviceSelectors:\n\n            - hardwareAddr: '00:50:56:8e:8f:e4'\n\n            - hardwareAddr: '00:50:57:9c:2c:2d'\n\n\nIn this example, the bond0 interface will be created and bonded using two devices with the specified hardware addresses.\n\n4.5 - Predictable Interface Names\nHow to use predictable interface naming.\n\nStarting with version Talos 1.5, network interfaces are renamed to predictable names same way as systemd does that in other Linux distributions.\n\nThe naming schema enx78e7d1ea46da (based on MAC addresses) is enabled by default, the order of interface naming decisions is:\n\nfirmware/BIOS provided index numbers for on-board devices (example: eno1)\nfirmware/BIOS provided PCI Express hotplug slot index numbers (example: ens1)\nphysical/geographical location of the connector of the hardware (example: enp2s0)\ninterfaces’s MAC address (example: enx78e7d1ea46da)\n\nThe predictable network interface names features can be disabled by specifying net.ifnames=0 in the kernel command line.\n\nNote: Talos automatically adds the net.ifnames=0 kernel argument when upgrading from Talos versions before 1.5, so upgrades to 1.5 don’t require any manual intervention.\n\n“Cloud” platforms, like AWS, still use old eth0 naming scheme as Talos automatically adds net.ifnames=0 to the kernel command line.\n\nSingle Network Interface\n\nWhen running Talos on a machine with a single network interface, predictable interface names might be confusing, as it might come up as enxSOMETHING which is hard to address. There are two ways to solve this:\n\ndisable the feature by supplying net.ifnames=0 to the initial boot of Talos, Talos will persist net.ifnames=0 over installs/upgrades.\n\nuse device selectors:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n        # any configuration can follow, e.g:\n\n        addresses: [10.3.4.5/24]\n\n4.6 - Virtual (shared) IP\nUsing Talos Linux to set up a floating virtual IP address for cluster access.\n\nOne of the pain points when building a high-availability controlplane is giving clients a single IP or URL at which they can reach any of the controlplane nodes. The most common approaches - reverse proxy, load balancer, BGP, and DNS - all require external resources, and add complexity in setting up Kubernetes.\n\nTo simplify cluster creation, Talos Linux supports a “Virtual” IP (VIP) address to access the Kubernetes API server, providing high availability with no other resources required.\n\nWhat happens is that the controlplane machines vie for control of the shared IP address using etcd elections. There can be only one owner of the IP address at any given time. If that owner disappears or becomes non-responsive, another owner will be chosen, and it will take up the IP address.\n\nRequirements\n\nThe controlplane nodes must share a layer 2 network, and the virtual IP must be assigned from that shared network subnet. In practical terms, this means that they are all connected via a switch, with no router in between them. Note that the virtual IP election depends on etcd being up, as Talos uses etcd for elections and leadership (control) of the IP address.\n\nThe virtual IP is not restricted by ports - you can access any port that the control plane nodes are listening on, on that IP address. Thus it is possible to access the Talos API over the VIP, but it is not recommended, as you cannot access the VIP when etcd is down - and then you could not access the Talos API to recover etcd.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nChoose your Shared IP\n\nThe Virtual IP should be a reserved, unused IP address in the same subnet as your controlplane nodes. It should not be assigned or assignable by your DHCP server.\n\nFor our example, we will assume that the controlplane nodes have the following IP addresses:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nWe then choose our shared IP to be:\n\n192.168.0.15\nConfigure your Talos Machines\n\nThe shared IP setting is only valid for controlplane nodes.\n\nFor the example above, each of the controlplane nodes should have the following Machine Config snippet:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n\nVirtual IP’s can also be configured on a VLAN interface.\n\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n      vlans:\n\n        - vlanId: 100\n\n          dhcp: true\n\n          vip:\n\n            ip: 192.168.1.15\n\n\nFor your own environment, the interface and the DHCP setting may differ, or you may use static addressing (adresses) instead of DHCP.\n\nWhen using predictable interface names, the interface name might not be eth0.\n\nIf the machine has a single network interface, it can be selected using a dummy device selector:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\nCaveats\n\nSince VIP functionality relies on etcd for elections, the shared IP will not come alive until after you have bootstrapped Kubernetes.\n\nDon’t use the VIP as the endpoint in the talosconfig, as the VIP is bound to etcd and kube-apiserver health, and you will not be able to recover from a failure of either of those components using Talos API.\n\n4.7 - Wireguard Network\nA guide on how to set up Wireguard network using Kernel module.\nConfiguring Wireguard Network\nQuick Start\n\nThe quickest way to try out Wireguard is to use talosctl cluster create command:\n\ntalosctl cluster create --wireguard-cidr 10.1.0.0/24\n\n\nIt will automatically generate Wireguard network configuration for each node with the following network topology:\n\nWhere all controlplane nodes will be used as Wireguard servers which listen on port 51111. All controlplanes and workers will connect to all controlplanes. It also sets PersistentKeepalive to 5 seconds to establish controlplanes to workers connection.\n\nAfter the cluster is deployed it should be possible to verify Wireguard network connectivity. It is possible to deploy a container with hostNetwork enabled, then do kubectl exec <container> /bin/bash and either do:\n\nping 10.1.0.2\n\n\nOr install wireguard-tools package and run:\n\nwg show\n\n\nWireguard show should output something like this:\n\ninterface: wg0\n\n  public key: OMhgEvNIaEN7zeCLijRh4c+0Hwh3erjknzdyvVlrkGM=\n\n  private key: (hidden)\n\n  listening port: 47946\n\n\n\npeer: 1EsxUygZo8/URWs18tqB5FW2cLVlaTA+lUisKIf8nh4=\n\n  endpoint: 10.5.0.2:51111\n\n  allowed ips: 10.1.0.0/24\n\n  latest handshake: 1 minute, 55 seconds ago\n\n  transfer: 3.17 KiB received, 3.55 KiB sent\n\n  persistent keepalive: every 5 seconds\n\n\nIt is also possible to use generated configuration as a reference by pulling generated config files using:\n\ntalosctl read -n 10.5.0.2 /system/state/config.yaml > controlplane.yaml\n\ntalosctl read -n 10.5.0.3 /system/state/config.yaml > worker.yaml\n\nManual Configuration\n\nAll Wireguard configuration can be done by changing Talos machine config files. As an example we will use this official Wireguard quick start tutorial.\n\nKey Generation\n\nThis part is exactly the same:\n\nwg genkey | tee privatekey | wg pubkey > publickey\n\nSetting up Device\n\nInline comments show relations between configs and wg quickstart tutorial commands:\n\n...\n\nnetwork:\n\n  interfaces:\n\n    ...\n\n      # ip link add dev wg0 type wireguard\n\n    - interface: wg0\n\n      mtu: 1500\n\n      # ip address add dev wg0 192.168.2.1/24\n\n      addresses:\n\n        - 192.168.2.1/24\n\n      # wg set wg0 listen-port 51820 private-key /path/to/private-key peer ABCDEF... allowed-ips 192.168.88.0/24 endpoint 209.202.254.14:8172\n\n      wireguard:\n\n        privateKey: <privatekey file contents>\n\n        listenPort: 51820\n\n        peers:\n\n          allowedIPs:\n\n            - 192.168.88.0/24\n\n          endpoint: 209.202.254.14.8172\n\n          publicKey: ABCDEF...\n\n...\n\n\nWhen networkd gets this configuration it will create the device, configure it and will bring it up (equivalent to ip link set up dev wg0).\n\nAll supported config parameters are described in the Machine Config Reference.\n\n5 - Discovery Service\nTalos Linux Node discovery services\n\nTalos Linux includes node-discovery capabilities that depend on a discovery registry. This allows you to see the members of your cluster, and the associated IP addresses of the nodes.\n\ntalosctl get members\n\nNODE       NAMESPACE   TYPE     ID                             VERSION   HOSTNAME                       MACHINE TYPE   OS               ADDRESSES\n\n10.5.0.2   cluster     Member   talos-default-controlplane-1   1         talos-default-controlplane-1   controlplane   Talos (v1.2.3)   [\"10.5.0.2\"]\n\n10.5.0.2   cluster     Member   talos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.2.3)   [\"10.5.0.3\"]\n\n\nThere are currently two supported discovery services: a Kubernetes registry (which stores data in the cluster’s etcd service) and an external registry service. Sidero Labs runs a public external registry service, which is enabled by default. The Kubernetes registry service is disabled by default. The advantage of the external registry service is that it is not dependent on etcd, and thus can inform you of cluster membership even when Kubernetes is down.\n\nVideo Walkthrough\n\nTo see a live demo of Cluster Discovery, see the video below:\n\nRegistries\n\nPeers are aggregated from enabled registries. By default, Talos will use the service registry, while the kubernetes registry is disabled. To disable a registry, set disabled to true (this option is the same for all registries): For example, to disable the service registry:\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\n    registries:\n\n      service:\n\n        disabled: true\n\n\nDisabling all registries effectively disables member discovery.\n\nNote: An enabled discovery service is required for KubeSpan to function correctly.\n\nThe Kubernetes registry uses Kubernetes Node resource data and additional Talos annotations:\n\n$ kubectl describe node <nodename>\n\nAnnotations:        cluster.talos.dev/node-id: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n                    networking.talos.dev/assigned-prefixes: 10.244.0.0/32,10.244.0.1/24\n\n                    networking.talos.dev/self-ips: 172.20.0.2,fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\n\n...\n\n\nThe Service registry by default uses a public external Discovery Service to exchange encrypted information about cluster members.\n\nNote: Talos supports operations when Discovery Service is disabled, but some features will rely on Kubernetes API availability to discover controlplane endpoints, so in case of a failure disabled Discovery Service makes troubleshooting much harder.\n\nDiscovery Service\n\nSidero Labs maintains a public discovery service at https://discovery.talos.dev/ whereby cluster members use a shared key that is globally unique to coordinate basic connection information (i.e. the set of possible “endpoints”, or IP:port pairs). We call this data “affiliate data.”\n\nNote: If KubeSpan is enabled the data has the addition of the WireGuard public key.\n\nData sent to the discovery service is encrypted with AES-GCM encryption and endpoint data is separately encrypted with AES in ECB mode so that endpoints coming from different sources can be deduplicated server-side. Each node submits its own data, plus the endpoints it sees from other peers, to the discovery service. The discovery service aggregates the data, deduplicates the endpoints, and sends updates to each connected peer. Each peer receives information back from the discovery service, decrypts it and uses it to drive KubeSpan and cluster discovery.\n\nData is stored in memory only. The cluster ID is used as a key to select the affiliates (so that different clusters see different affiliates).\n\nTo summarize, the discovery service knows the client version, cluster ID, the number of affiliates, some encrypted data for each affiliate, and a list of encrypted endpoints. The discovery service doesn’t see actual node information – it only stores and updates encrypted blobs. Discovery data is encrypted/decrypted by the clients – the cluster members. The discovery service does not have the encryption key.\n\nThe discovery service may, with a commercial license, be operated by your organization and can be downloaded here. In order for nodes to communicate to the discovery service, they must be able to connect to it on TCP port 443.\n\nResource Definitions\n\nTalos provides resources that can be used to introspect the discovery and KubeSpan features.\n\nDiscovery\nIdentities\n\nThe node’s unique identity (base62 encoded random 32 bytes) can be obtained with:\n\nNote: Using base62 allows the ID to be URL encoded without having to use the ambiguous URL-encoding version of base64.\n\n$ talosctl get identities -o yaml\n\n...\n\nspec:\n\n    nodeId: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n\nNode identity is used as the unique Affiliate identifier.\n\nNode identity resource is preserved in the STATE partition in node-identity.yaml file. Node identity is preserved across reboots and upgrades, but it is regenerated if the node is reset (wiped).\n\nAffiliates\n\nAn affiliate is a proposed member: the node has the same cluster ID and secret.\n\n$ talosctl get affiliates\n\nID                                             VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\n2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    2         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\n6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nNVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nUtoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd    4         talos-default-controlplane-1   controlplane   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\nb3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   2         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nOne of the Affiliates with the ID matching node identity is populated from the node data, other Affiliates are pulled from the registries. Enabled discovery registries run in parallel and discovered data is merged to build the list presented above.\n\nDetails about data coming from each registry can be queried from the cluster-raw namespace:\n\n$ talosctl get affiliates --namespace=cluster-raw\n\nID                                                     VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\nk8s/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF        3         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nk8s/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA       2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nk8s/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB       2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nk8s/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C       3         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\nservice/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    23        talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nservice/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   26        talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nservice/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   20        talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nservice/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   14        talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nEach Affiliate ID is prefixed with k8s/ for data coming from the Kubernetes registry and with service/ for data coming from the discovery service.\n\nMembers\n\nA member is an affiliate that has been approved to join the cluster. The members of the cluster can be obtained with:\n\n$ talosctl get members\n\nID                             VERSION   HOSTNAME                       MACHINE TYPE   OS                ADDRESSES\n\ntalos-default-controlplane-1   2         talos-default-controlplane-1   controlplane   Talos (v1.6.2)   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\ntalos-default-controlplane-2   1         talos-default-controlplane-2   controlplane   Talos (v1.6.2)   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\ntalos-default-controlplane-3   1         talos-default-controlplane-3   controlplane   Talos (v1.6.2)   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\ntalos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.6.2)   [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\ntalos-default-worker-2         1         talos-default-worker-2         worker         Talos (v1.6.2)   [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\n6 - Interactive Dashboard\nA tool to inspect the running Talos machine state on the physical video console.\n\nInteractive dashboard is enabled for all Talos platforms except for SBC images. The dashboard can be disabled with kernel parameter talos.dashboard.disabled=1.\n\nThe dashboard runs only on the physical video console (not serial console) on the 2nd virtual TTY. The first virtual TTY shows kernel logs same as in Talos <1.4.0. The virtual TTYs can be switched with <Alt+F1> and <Alt+F2> keys.\n\nKeys <F1> - <Fn> can be used to switch between different screens of the dashboard.\n\nThe dashboard is using either UEFI framebuffer or VGA/VESA framebuffer (for legacy BIOS boot). For legacy BIOS boot screen resolution can be controlled with the vga= kernel parameter.\n\nSummary Screen (F1)\n\nInteractive Dashboard Summary Screen\n\nThe header shows brief information about the node:\n\nhostname\nTalos version\nuptime\nCPU and memory hardware information\nCPU and memory load, number of processes\n\nTable view presents summary information about the machine:\n\nUUID (from SMBIOS data)\nCluster name (when the machine config is available)\nMachine stage: Installing, Upgrading, Booting, Maintenance, Running, Rebooting, Shutting down, etc.\nMachine stage readiness: checks Talos service status, static pod status, etc. (for Running stage)\nMachine type: controlplane/worker\nNumber of members discovered in the cluster\nKubernetes version\nStatus of Kubernetes components: kubelet and Kubernetes controlplane components (only on controlplane machines)\nNetwork information: Hostname, Addresses, Gateway, Connectivity, DNS and NTP servers\n\nBottom part of the screen shows kernel logs, same as on the virtual TTY 1.\n\nMonitor Screen (F2)\n\nInteractive Dashboard Monitor Screen\n\nMonitor screen provides live view of the machine resource usage: CPU, memory, disk, network and processes.\n\nNetwork Config Screen (F3)\n\nNote: network config screen is only available for metal platform.\n\nInteractive Dashboard Network Config Screen\n\nNetwork config screen provides editing capabilities for the metal platform network configuration.\n\nThe screen is split into three sections:\n\nthe leftmost section provides a way to enter network configuration: hostname, DNS and NTP servers, configure the network interface either via DHCP or static IP address, etc.\nthe middle section shows the current network configuration.\nthe rightmost section shows the network configuration which will be applied after pressing “Save” button.\n\nOnce the platform network configuration is saved, it is immediately applied to the machine.\n\n7 - Resetting a Machine\nSteps on how to reset a Talos Linux machine to a clean state.\n\nFrom time to time, it may be beneficial to reset a Talos machine to its “original” state. Bear in mind that this is a destructive action for the given machine. Doing this means removing the machine from Kubernetes, etcd (if applicable), and clears any data on the machine that would normally persist a reboot.\n\nCLI\n\nWARNING: Running a talosctl reset on cloud VM’s might result in the VM being unable to boot as this wipes the entire disk. It might be more useful to just wipe the STATE and EPHEMERAL partitions on a cloud VM if not booting via iPXE. talosctl reset --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL\n\nThe API command for doing this is talosctl reset. There are a couple of flags as part of this command:\n\nFlags:\n\n      --graceful                        if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n\n      --reboot                          if true, reboot the node after resetting instead of shutting down\n\n      --system-labels-to-wipe strings   if set, just wipe selected system disk partitions by label but keep other partitions intact keep other partitions intact\n\n\nThe graceful flag is especially important when considering HA vs. non-HA Talos clusters. If the machine is part of an HA cluster, a normal, graceful reset should work just fine right out of the box as long as the cluster is in a good state. However, if this is a single node cluster being used for testing purposes, a graceful reset is not an option since Etcd cannot be “left” if there is only a single member. In this case, reset should be used with --graceful=false to skip performing checks that would normally block the reset.\n\nKernel Parameter\n\nAnother way to reset a machine is to specify talos.experimental.wipe=system kernel parameter. If the machine got stuck in the boot loop and you access to the console you can use GRUB to specify this kernel argument. Then when Talos boots for the next time it will reset system disk and reboot.\n\nNext steps can be to install Talos either using PXE boot or by mounting an ISO.\n\n8 - Upgrading Talos Linux\nGuide to upgrading a Talos Linux machine.\n\nOS upgrades are effected by an API call, which can be sent via the talosctl CLI utility.\n\nThe upgrade API call passes a node the installer image to use to perform the upgrade. Each Talos version has a corresponding installer image, listed on the release page for the version, for example v1.6.2.\n\nUpgrades use an A-B image scheme in order to facilitate rollbacks. This scheme retains the previous Talos kernel and OS image following each upgrade. If an upgrade fails to boot, Talos will roll back to the previous version. Likewise, Talos may be manually rolled back via API (or talosctl rollback), which will update the boot reference and reboot.\n\nUnless explicitly told to preserve data, an upgrade will cause the node to wipe the EPHEMERAL partition, remove itself from the etcd cluster (if it is a controlplane node), and make itself as pristine as is possible. (This is the desired behavior except in specialised use cases such as single-node clusters.)\n\nNote An upgrade of the Talos Linux OS will not (since v1.0) apply an upgrade to the Kubernetes version by default. Kubernetes upgrades should be managed separately per upgrading kubernetes.\n\nSupported Upgrade Paths\n\nBecause Talos Linux is image based, an upgrade is almost the same as installing Talos, with the difference that the system has already been initialized with a configuration. The supported configuration may change between versions. The upgrade process should handle such changes transparently, but this migration is only tested between adjacent minor releases. Thus the recommended upgrade path is to always upgrade to the latest patch release of all intermediate minor releases.\n\nFor example, if upgrading from Talos 1.0 to Talos 1.2.4, the recommended upgrade path would be:\n\nupgrade from 1.0 to latest patch of 1.0 - to v1.0.6\nupgrade from v1.0.6 to latest patch of 1.1 - to v1.1.2\nupgrade from v1.1.2 to v1.2.4\nBefore Upgrade to v1.6.2\n\nThere are no specific actions to be taken before an upgrade.\n\nPlease review the release notes for any changes that may affect your cluster.\n\nVideo Walkthrough\n\nTo see a live demo of an upgrade of Talos Linux, see the video below:\n\nAfter Upgrade to v1.6.2\n\nThere are no specific actions to be taken after an upgrade.\n\ntalosctl upgrade\n\nTo upgrade a Talos node, specify the node’s IP address and the installer container image for the version of Talos to upgrade to.\n\nFor instance, if your Talos node has the IP address 10.20.30.40 and you want to install the current version, you would enter a command such as:\n\n  $ talosctl upgrade --nodes 10.20.30.40 \\\n\n      --image ghcr.io/siderolabs/installer:v1.6.2\n\n\nThere is an option to this command: --preserve, which will explicitly tell Talos to keep ephemeral data intact. In most cases, it is correct to let Talos perform its default action of erasing the ephemeral data. However, for a single-node control-plane, make sure that --preserve=true.\n\nRarely, an upgrade command will fail due to a process holding a file open on disk. In these cases, you can use the --stage flag. This puts the upgrade artifacts on disk, and adds some metadata to a disk partition that gets checked very early in the boot process, then reboots the node. On the reboot, Talos sees that it needs to apply an upgrade, and will do so immediately. Because this occurs in a just rebooted system, there will be no conflict with any files being held open. After the upgrade is applied, the node will reboot again, in order to boot into the new version. Note that because Talos Linux reboots via the kexec syscall, the extra reboot adds very little time.\n\nMachine Configuration Changes\n\nNew configuration documents:\n\nIngress Firewall configuration: NetworkRuleConfig and NetworkDefaultActionConfig.\n\nUpdates in v1alpha1 Config:\n\n.persist option was removed\n.machine.nodeTaints configures Kubernetes node taints\n.machine.kubelet.extraMounts supports new fields uidMappings and gidMappings\n.machine.kubelet.credendtialProviderConfig configures kubelet credential provider\n.machine.network.kubespan.harvestExtraEndpoints to disable harvesting extra endpoints\n.cluster.cni.flannel provides customization for the default Flannel CNI manifest\n.cluster.scheduler.config provides custom kube-scheduler configuration\nUpgrade Sequence\n\nWhen a Talos node receives the upgrade command, it cordons itself in Kubernetes, to avoid receiving any new workload. It then starts to drain its existing workload.\n\nNOTE: If any of your workloads are sensitive to being shut down ungracefully, be sure to use the lifecycle.preStop Pod spec.\n\nOnce all of the workload Pods are drained, Talos will start shutting down its internal processes. If it is a control node, this will include etcd. If preserve is not enabled, Talos will leave etcd membership. (Talos ensures the etcd cluster is healthy and will remain healthy after our node leaves the etcd cluster, before allowing a control plane node to be upgraded.)\n\nOnce all the processes are stopped and the services are shut down, the filesystems will be unmounted. This allows Talos to produce a very clean upgrade, as close as possible to a pristine system. We verify the disk and then perform the actual image upgrade. We set the bootloader to boot once with the new kernel and OS image, then we reboot.\n\nAfter the node comes back up and Talos verifies itself, it will make the bootloader change permanent, rejoin the cluster, and finally uncordon itself to receive new workloads.\n\nFAQs\n\nQ. What happens if an upgrade fails?\n\nA. Talos Linux attempts to safely handle upgrade failures.\n\nThe most common failure is an invalid installer image reference. In this case, Talos will fail to download the upgraded image and will abort the upgrade.\n\nSometimes, Talos is unable to successfully kill off all of the disk access points, in which case it cannot safely unmount all filesystems to effect the upgrade. In this case, it will abort the upgrade and reboot. (upgrade --stage can ensure that upgrades can occur even when the filesytems cannot be unmounted.)\n\nIt is possible (especially with test builds) that the upgraded Talos system will fail to start. In this case, the node will be rebooted, and the bootloader will automatically use the previous Talos kernel and image, thus effectively rolling back the upgrade.\n\nLastly, it is possible that Talos itself will upgrade successfully, start up, and rejoin the cluster but your workload will fail to run on it, for whatever reason. This is when you would use the talosctl rollback command to revert back to the previous Talos version.\n\nQ. Can upgrades be scheduled?\n\nA. Because the upgrade sequence is API-driven, you can easily tie it in to your own business logic to schedule and coordinate your upgrades.\n\nQ. Can the upgrade process be observed?\n\nA. Yes, using the talosctl dmesg -f command. You can also use talosctl upgrade --wait, and optionally talosctl upgrade --wait --debug to observe kernel logs\n\nQ. Are worker node upgrades handled differently from control plane node upgrades?\n\nA. Short answer: no.\n\nLong answer: Both node types follow the same set procedure. From the user’s standpoint, however, the processes are identical. However, since control plane nodes run additional services, such as etcd, there are some extra steps and checks performed on them. For instance, Talos will refuse to upgrade a control plane node if that upgrade would cause a loss of quorum for etcd. If multiple control plane nodes are asked to upgrade at the same time, Talos will protect the Kubernetes cluster by ensuring only one control plane node actively upgrades at any time, via checking etcd quorum. If running a single-node cluster, and you want to force an upgrade despite the loss of quorum, you can set preserve to true.\n\nQ. Can I break my cluster by upgrading everything at once?\n\nA. Possibly - it’s not recommended.\n\nNothing prevents the user from sending near-simultaneous upgrades to each node of the cluster - and while Talos Linux and Kubernetes can generally deal with this situation, other components of the cluster may not be able to recover from more than one node rebooting at a time. (e.g. any software that maintains a quorum or state across nodes, such as Rook/Ceph)\n\nQ. Which version of talosctl should I use to update a cluster?\n\nA. We recommend using the version that matches the current running version of the cluster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/network/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nNetwork\nManaging the Kubernetes cluster networking\n1: Deploying Cilium CNI\n1 - Deploying Cilium CNI\nIn this guide you will learn how to set up Cilium CNI on Talos.\n\nCilium can be installed either via the cilium cli or using helm.\n\nThis documentation will outline installing Cilium CNI v1.14.0 on Talos in six different ways. Adhering to Talos principles we’ll deploy Cilium with IPAM mode set to Kubernetes, and using the cgroupv2 and bpffs mount that talos already provides. As Talos does not allow loading kernel modules by Kubernetes workloads, SYS_MODULE capability needs to be dropped from the Cilium default set of values, this override can be seen in the helm/cilium cli install commands. Each method can either install Cilium using kube proxy (default) or without: Kubernetes Without kube-proxy\n\nIn this guide we assume that KubePrism is enabled and configured to use the port 7445.\n\nMachine config preparation\n\nWhen generating the machine config for a node set the CNI to none. For example using a config patch:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nOr if you want to deploy Cilium without kube-proxy, you also need to disable kube proxy:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\nInstallation using Cilium CLI\n\nNote: It is recommended to template the cilium manifest using helm and use it as part of Talos machine config, but if you want to install Cilium using the Cilium CLI, you can follow the steps below.\n\nInstall the Cilium CLI following the steps here.\n\nWith kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=disabled \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup\n\nWithout kube-proxy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=true \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --helm-set=k8sServiceHost=localhost \\\n\n    --helm-set=k8sServicePort=7445\n\nInstallation using Helm\n\nRefer to Installing with Helm for more information.\n\nFirst we’ll need to add the helm repo for Cilium.\n\nhelm repo add cilium https://helm.cilium.io/\n\nhelm repo update\n\nMethod 1: Helm install\n\nAfter applying the machine config and bootstrapping Talos will appear to hang on phase 18/19 with the message: retrying error: node not ready. This happens because nodes in Kubernetes are only marked as ready once the CNI is up. As there is no CNI defined, the boot process is pending and will reboot the node to retry after 10 minutes, this is expected behavior.\n\nDuring this window you can install Cilium manually by running the following:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup\n\n\nOr if you want to deploy Cilium without kube-proxy, also set some extra paramaters:\n\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445\n\n\nAfter Cilium is installed the boot process should continue and complete successfully.\n\nMethod 2: Helm manifests install\n\nInstead of directly installing Cilium you can instead first generate the manifest and then apply it:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\n\nWithout kube-proxy:\n\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445 > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\nMethod 3: Helm manifests hosted install\n\nAfter generating cilium.yaml using helm template, instead of applying this manifest directly during the Talos boot window (before the reboot timeout). You can also host this file somewhere and patch the machine config to apply this manifest automatically during bootstrap. To do this patch your machine configuration to include this config instead of the above:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: custom\n\n      urls:\n\n        - https://server.yourdomain.tld/some/path/cilium.yaml\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nHowever, beware of the fact that the helm generated Cilium manifest contains sensitive key material. As such you should definitely not host this somewhere publicly accessible.\n\nMethod 4: Helm manifests inline install\n\nA more secure option would be to include the helm template output manifest inside the machine configuration. The machine config should be generated with CNI set to none\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nif deploying Cilium with kube-proxy disabled, you can also include the following:\n\nCreate a patch.yaml file with the following contents:\n\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\nmachine:\n\n  features:\n\n    kubePrism:\n\n      enabled: true\n\n      port: 7445\n\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nTo do so patch this into your machine configuration:\n\ninlineManifests:\n\n    - name: cilium\n\n      contents: |\n\n        --\n\n        # Source: cilium/templates/cilium-agent/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        metadata:\n\n          name: \"cilium\"\n\n          namespace: kube-system\n\n        ---\n\n        # Source: cilium/templates/cilium-operator/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        -> Your cilium.yaml file will be pretty long....        \n\n\nThis will install the Cilium manifests at just the right time during bootstrap.\n\nBeware though:\n\nChanging the namespace when templating with Helm does not generate a manifest containing the yaml to create that namespace. As the inline manifest is processed from top to bottom make sure to manually put the namespace yaml at the start of the inline manifest.\nOnly add the Cilium inline manifest to the control plane nodes machine configuration.\nMake sure all control plane nodes have an identical configuration.\nIf you delete any of the generated resources they will be restored whenever a control plane node reboots.\nAs a safety messure Talos only creates missing resources from inline manifests, it never deletes or updates anything.\nIf you need to update a manifest make sure to first edit all control plane machine configurations and then run talosctl upgrade-k8s as it will take care of updating inline manifests.\nKnown issues\nThere are some gotchas when using Talos and Cilium on the Google cloud platform when using internal load balancers. For more details: GCP ILB support / support scope local routes to be configured\nOther things to know\nTalos has full kernel module support for eBPF, See:\nCilium System Requirements\nTalos Kernel Config AMD64\nTalos Kernel Config ARM64\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How Tos | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nHow Tos\nHow to guide for common tasks in Talos Linux\n1: How to enable workers on your control plane nodes\n2: How to manage certificate lifetimes with Talos Linux\n3: How to scale down a Talos cluster\n4: How to scale up a Talos cluster\n1 - How to enable workers on your control plane nodes\nHow to enable workers on your control plane nodes.\n\nBy default, Talos Linux taints control plane nodes so that workloads are not schedulable on them.\n\nIn order to allow workloads to run on the control plane nodes (useful for single node clusters, or non-production clusters), follow the procedure below.\n\nModify the MachineConfig for the controlplane nodes to add allowSchedulingOnControlPlanes: true:\n\ncluster:\n\n    allowSchedulingOnControlPlanes: true\n\n\nThis may be done via editing the controlplane.yaml file before it is applied to the control plane nodes, by editing the machine config, or by patching the machine config.\n\n2 - How to manage certificate lifetimes with Talos Linux\n\nTalos Linux automatically manages and rotates all server side certs for etcd, Kubernetes, and the Talos API. Note however that the kubelet needs to be restarted at least once a year in order for the certificates to be rotated. Any upgrade/reboot of the node will suffice for this effect.\n\nClient certs (talosconfig and kubeconfig) are the user’s responsibility. Each time you download the kubeconfig file from a Talos Linux cluster, the client certificate is regenerated giving you a kubeconfig which is valid for a year.\n\nThe talosconfig file should be renewed at least once a year, using the talosctl config new command.\n\n3 - How to scale down a Talos cluster\nHow to remove nodes from a Talos Linux cluster.\n\nTo remove nodes from a Talos Linux cluster:\n\ntalosctl -n <IP.of.node.to.remove> reset\nkubectl delete node <nodename>\n\nThe command talosctl reset will cordon and drain the node, leaving etcd if required, and then erase its disks and power down the system.\n\nThis command will also remove the node from registration with the discovery service, so it will no longer show up in talosctl get members.\n\nIt is still necessary to remove the node from Kubernetes, as noted above.\n\n4 - How to scale up a Talos cluster\nHow to add more nodes to a Talos Linux cluster.\n\nTo add more nodes to a Talos Linux cluster, follow the same procedure as when initially creating the cluster:\n\nboot the new machines to install Talos Linux\napply the worker.yaml or controlplane.yaml configuration files to the new machines\n\nYou need the controlplane.yaml and worker.yaml that were created when you initially deployed your cluster. These contain the certificates that enable new machines to join.\n\nOnce you have the IP address, you can then apply the correct configuration for each machine you are adding, either worker or controlplane.\n\n  talosctl apply-config --insecure \\\n\n    --nodes [NODE IP] \\\n\n    --file controlplane.yaml\n\n\nThe insecure flag is necessary because the PKI infrastructure has not yet been made available to the node.\n\nYou do not need to bootstrap the new node. Regardless of whether you are adding a control plane or worker node, it will now join the cluster in its role.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nConfiguration\nGuides on how to configure Talos Linux machines\n1: Configuration Patches\n2: Containerd\n3: Custom Certificate Authorities\n4: Disk Encryption\n5: Editing Machine Configuration\n6: Logging\n7: Managing Talos PKI\n8: NVIDIA Fabric Manager\n9: NVIDIA GPU (OSS drivers)\n10: NVIDIA GPU (Proprietary drivers)\n11: Pull Through Image Cache\n12: Role-based access control (RBAC)\n13: System Extensions\n1 - Configuration Patches\nIn this guide, we’ll patch the generated machine configuration.\n\nTalos generates machine configuration for two types of machines: controlplane and worker machines. Many configuration options can be adjusted using talosctl gen config but not all of them. Configuration patching allows modifying machine configuration to fit it for the cluster or a specific machine.\n\nConfiguration Patch Formats\n\nTalos supports two configuration patch formats:\n\nstrategic merge patches\nRFC6902 (JSON patches)\n\nStrategic merge patches are the easiest to use, but JSON patches allow more precise configuration adjustments.\n\nNote: Talos 1.5+ supports multi-document machine configuration. JSON patches don’t support multi-document machine configuration, while strategic merge patches do.\n\nStrategic Merge patches\n\nStrategic merge patches look like incomplete machine configuration files:\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nWhen applied to the machine configuration, the patch gets merged with the respective section of the machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.2/24\n\n    hostname: worker1\n\n\nIn general, machine configuration contents are merged with the contents of the strategic merge patch, with strategic merge patch values overriding machine configuration values. There are some special rules:\n\nIf the field value is a list, the patch value is appended to the list, with the following exceptions:\nvalues of the fields cluster.network.podSubnets and cluster.network.serviceSubnets are overwritten on merge\nnetwork.interfaces section is merged with the value in the machine config if there is a match on interface: or deviceSelector: keys\nnetwork.interfaces.vlans section is merged with the value in the machine config if there is a match on the vlanId: key\ncluster.apiServer.auditPolicy value is replaced on merge\n\nWhen patching a multi-document machine configuration, following rules apply:\n\nfor each document in the patch, the document is merged with the respective document in the machine configuration (matching by kind, apiVersion and name for named documents)\nif the patch document doesn’t exist in the machine configuration, it is appended to the machine configuration\n\nThe strategic merge patch itself might be a multi-document YAML, and each document will be applied as a patch to the base machine configuration.\n\nRFC6902 (JSON Patches)\n\nJSON patches can be written either in JSON or YAML format. A proper JSON patch requires an op field that depends on the machine configuration contents: whether the path already exists or not.\n\nFor example, the strategic merge patch from the previous section can be written either as:\n\n- op: replace\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nor:\n\n- op: add\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nThe correct op depends on whether the /machine/network/hostname section exists already in the machine config or not.\n\nExamples\nMachine Network\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n\nThe goal is to add a virtual IP 192.168.10.50 to the eth0 interface and add another interface eth1 with DHCP enabled.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nPatched machine configuration:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nCluster Network\n\nBase machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 10.244.0.0/16\n\n    serviceSubnets:\n\n      - 10.96.0.0/12\n\n\nThe goal is to update pod and service subnets and disable default CNI (Flannel).\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  network:\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nPatched machine configuration:\n\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nKubelet\n\nBase machine configuration:\n\n# ...\n\nmachine:\n\n  kubelet: {}\n\n\nThe goal is to set the kubelet node IP to come from the subnet 192.168.10.0/24.\n\nStrategic merge patch\nJSON patch\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nPatched machine configuration:\n\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nAdmission Control: Pod Security Policy\n\nBase machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\n\nThe goal is to add an exemption for the namespace rook-ceph.\n\nStrategic merge patch\nJSON patch\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          exemptions:\n\n            namespaces:\n\n              - rook-ceph\n\nPatched machine configuration:\n\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n              - rook-ceph\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\nConfiguration Patching with talosctl CLI\n\nSeveral talosctl commands accept config patches as command-line flags. Config patches might be passed either as an inline value or as a reference to a file with @file.patch syntax:\n\ntalosctl ... --patch '[{\"op\": \"add\", \"path\": \"/machine/network/hostname\", \"value\": \"worker1\"}]' --patch @file.patch\n\n\nIf multiple config patches are specified, they are applied in the order of appearance. The format of the patch (JSON patch or strategic merge patch) is detected automatically.\n\nTalos machine configuration can be patched at the moment of generation with talosctl gen config:\n\ntalosctl gen config test-cluster https://172.20.0.1:6443 --config-patch @all.yaml --config-patch-control-plane @cp.yaml --config-patch-worker @worker.yaml\n\n\nGenerated machine configuration can also be patched after the fact with talosctl machineconfig patch\n\ntalosctl machineconfig patch worker.yaml --patch @patch.yaml -o worker1.yaml\n\n\nMachine configuration on the running Talos node can be patched with talosctl patch:\n\ntalosctl patch mc --nodes 172.20.0.2 --patch @patch.yaml\n\n2 - Containerd\nCustomize Containerd Settings\n\nThe base containerd configuration expects to merge in any additional configs present in /etc/cri/conf.d/20-customization.part.\n\nExamples\nExposing Metrics\n\nPatch the machine config by adding the following:\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [metrics]\n\n          address = \"0.0.0.0:11234\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nOnce the server reboots, metrics are now available:\n\n$ curl ${IP}:11234/v1/metrics\n\n# HELP container_blkio_io_service_bytes_recursive_bytes The blkio io service bytes recursive\n\n# TYPE container_blkio_io_service_bytes_recursive_bytes gauge\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Async\"} 0\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Discard\"} 0\n\n...\n\n...\n\nPause Image\n\nThis change is often required for air-gapped environments, as containerd CRI plugin has a reference to the pause image which is used to create pods, and it can’t be controlled with Kubernetes pod definitions.\n\nmachine:\n\n  files:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            sandbox_image = \"registry.k8s.io/pause:3.8\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow the pause image is set to registry.k8s.io/pause:3.8:\n\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                      PID    STATUS\n\n172.20.0.5   k8s.io      kube-system/kube-flannel-6hfck                                  registry.k8s.io/pause:3.8                                  1773   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-cni:bc39fec3cbac      ghcr.io/siderolabs/install-cni:v1.3.0-alpha.0-2-gb155fa0   0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-config:5c3989353b98   ghcr.io/siderolabs/flannel:v0.20.1                         0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:kube-flannel:116c67b50da8     ghcr.io/siderolabs/flannel:v0.20.1                         2092   CONTAINER_RUNNING\n\n172.20.0.5   k8s.io      kube-system/kube-proxy-xp7jq                                    registry.k8s.io/pause:3.8                                  1780   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-proxy-xp7jq:kube-proxy:84fc77c59e17         registry.k8s.io/kube-proxy:v1.26.0-alpha.3                 1843   CONTAINER_RUNNING\n\n3 - Custom Certificate Authorities\nHow to supply custom certificate authorities\nAppending the Certificate Authority\n\nPut into each machine the PEM encoded certificate:\n\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\n4 - Disk Encryption\nGuide on using system disk encryption\n\nIt is possible to enable encryption for system disks at the OS level. Currently, only STATE and EPHEMERAL partitions can be encrypted. STATE contains the most sensitive node data: secrets and certs. The EPHEMERAL partition may contain sensitive workload data. Data is encrypted using LUKS2, which is provided by the Linux kernel modules and cryptsetup utility. The operating system will run additional setup steps when encryption is enabled.\n\nIf the disk encryption is enabled for the STATE partition, the system will:\n\nSave STATE encryption config as JSON in the META partition.\nBefore mounting the STATE partition, load encryption configs either from the machine config or from the META partition. Note that the machine config is always preferred over the META one.\nBefore mounting the STATE partition, format and encrypt it. This occurs only if the STATE partition is empty and has no filesystem.\n\nIf the disk encryption is enabled for the EPHEMERAL partition, the system will:\n\nGet the encryption config from the machine config.\nBefore mounting the EPHEMERAL partition, encrypt and format it.\n\nThis occurs only if the EPHEMERAL partition is empty and has no filesystem.\n\nTalos Linux supports four encryption methods, which can be combined together for a single partition:\n\nstatic - encrypt with the static passphrase (weakest protection, for STATE partition encryption it means that the passphrase will be stored in the META partition).\nnodeID - encrypt with the key derived from the node UUID (weak, it is designed to protect against data being leaked or recovered from a drive that has been removed from a Talos Linux node).\nkms - encrypt using key sealed with network KMS (strong, but requires network access to decrypt the data.)\ntpm - encrypt with the key derived from the TPM (strong, when used with SecureBoot).\n\nNote: nodeID encryption is not designed to protect against attacks where physical access to the machine, including the drive, is available. It uses the hardware characteristics of the machine in order to decrypt the data, so drives that have been removed, or recycled from a cloud environment or attached to a different virtual machine, will maintain their protection and encryption.\n\nConfiguration\n\nDisk encryption is disabled by default. To enable disk encryption you should modify the machine configuration with the following options:\n\nmachine:\n\n  ...\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\nEncryption Keys\n\nNote: What the LUKS2 docs call “keys” are, in reality, a passphrase. When this passphrase is added, LUKS2 runs argon2 to create an actual key from that passphrase.\n\nLUKS2 supports up to 32 encryption keys and it is possible to specify all of them in the machine configuration. Talos always tries to sync the keys list defined in the machine config with the actual keys defined for the LUKS2 partition. So if you update the keys list, keep at least one key that is not changed to be used for key management.\n\nWhen you define a key you should specify the key kind and the slot:\n\nmachine:\n\n  ...\n\n  state:\n\n    keys:\n\n      - nodeID: {} # key kind\n\n        slot: 1\n\n\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: supersecret\n\n        slot: 0\n\n\nTake a note that key order does not play any role on which key slot is used. Every key must always have a slot defined.\n\nEncryption Key Kinds\n\nTalos supports two kinds of keys:\n\nnodeID which is generated using the node UUID and the partition label (note that if the node UUID is not really random it will fail the entropy check).\nstatic which you define right in the configuration.\nkms which is sealed with the network KMS.\ntpm which is sealed using the TPM and protected with SecureBoot.\n\nNote: Use static keys only if your STATE partition is encrypted and only for the EPHEMERAL partition. For the STATE partition it will be stored in the META partition, which is not encrypted.\n\nKey Rotation\n\nIn order to completely rotate keys, it is necessary to do talosctl apply-config a couple of times, since there is a need to always maintain a single working key while changing the other keys around it.\n\nSo, for example, first add a new key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: oldkey\n\n        slot: 0\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\n\nThen remove the old key:\n\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\ntalosctl apply-config -n <node> -f config.yaml\n\nGoing from Unencrypted to Encrypted and Vice Versa\nEphemeral Partition\n\nThere is no in-place encryption support for the partitions right now, so to avoid losing data only empty partitions can be encrypted.\n\nAs such, migration from unencrypted to encrypted needs some additional handling, especially around explicitly wiping partitions.\n\napply-config should be called with --mode=staged.\nPartition should be wiped after apply-config, but before the reboot.\n\nEdit your machine config and add the encryption configuration:\n\nvim config.yaml\n\n\nApply the configuration with --mode=staged:\n\ntalosctl apply-config -f config.yaml -n <node ip> --mode=staged\n\n\nWipe the partition you’re going to encrypt:\n\ntalosctl reset --system-labels-to-wipe EPHEMERAL -n <node ip> --reboot=true\n\n\nThat’s it! After you run the last command, the partition will be wiped and the node will reboot. During the next boot the system will encrypt the partition.\n\nState Partition\n\nCalling wipe against the STATE partition will make the node lose the config, so the previous flow is not going to work.\n\nThe flow should be to first wipe the STATE partition:\n\ntalosctl reset  --system-labels-to-wipe STATE -n <node ip> --reboot=true\n\n\nNode will enter into maintenance mode, then run apply-config with --insecure flag:\n\ntalosctl apply-config --insecure -n <node ip> -f config.yaml\n\n\nAfter installation is complete the node should encrypt the STATE partition.\n\n5 - Editing Machine Configuration\nHow to edit and patch Talos machine configuration, with reboot, immediately, or stage update on reboot.\n\nTalos node state is fully defined by machine configuration. Initial configuration is delivered to the node at bootstrap time, but configuration can be updated while the node is running.\n\nThere are three talosctl commands which facilitate machine configuration updates:\n\ntalosctl apply-config to apply configuration from the file\ntalosctl edit machineconfig to launch an editor with existing node configuration, make changes and apply configuration back\ntalosctl patch machineconfig to apply automated machine configuration via JSON patch\n\nEach of these commands can operate in one of four modes:\n\napply change in automatic mode (default): reboot if the change can’t be applied without a reboot, otherwise apply the change immediately\napply change with a reboot (--mode=reboot): update configuration, reboot Talos node to apply configuration change\napply change immediately (--mode=no-reboot flag): change is applied immediately without a reboot, fails if the change contains any fields that can not be updated without a reboot\napply change on next reboot (--mode=staged): change is staged to be applied after a reboot, but node is not rebooted\napply change with automatic revert (--mode=try): change is applied immediately (if not possible, returns an error), and reverts it automatically in 1 minute if no configuration update is applied\napply change in the interactive mode (--mode=interactive; only for talosctl apply-config): launches TUI based interactive installer\n\nNote: applying change on next reboot (--mode=staged) doesn’t modify current node configuration, so next call to talosctl edit machineconfig --mode=staged will not see changes\n\nAdditionally, there is also talosctl get machineconfig -o yaml, which retrieves the current node configuration API resource and contains the machine configuration in the .spec field. It can be used to modify the configuration locally before being applied to the node.\n\nThe list of config changes allowed to be applied immediately in Talos v1.6.2:\n\n.debug\n.cluster\n.machine.time\n.machine.certCANs\n.machine.install (configuration is only applied during install/upgrade)\n.machine.network\n.machine.nodeLabels\n.machine.sysfs\n.machine.sysctls\n.machine.logging\n.machine.controlplane\n.machine.kubelet\n.machine.pods\n.machine.kernel\n.machine.registries (CRI containerd plugin will not pick up the registry authentication settings without a reboot)\n.machine.features.kubernetesTalosAPIAccess\ntalosctl apply-config\n\nThis command is traditionally used to submit initial machine configuration generated by talosctl gen config to the node.\n\nIt can also be used to apply configuration to running nodes. The initial YAML for this is typically obtained using talosctl get machineconfig -o yaml | yq eval .spec >machs.yaml. (We must use yq because for historical reasons, get returns the configuration as a full resource, while apply-config only accepts the raw machine config directly.)\n\nExample:\n\ntalosctl -n <IP> apply-config -f config.yaml\n\n\nCommand apply-config can also be invoked as apply machineconfig:\n\ntalosctl -n <IP> apply machineconfig -f config.yaml\n\n\nApplying machine configuration immediately (without a reboot):\n\ntalosctl -n IP apply machineconfig -f config.yaml --mode=no-reboot\n\n\nStarting the interactive installer:\n\ntalosctl -n IP apply machineconfig --mode=interactive\n\n\nNote: when a Talos node is running in the maintenance mode it’s necessary to provide --insecure (-i) flag to connect to the API and apply the config.\n\ntaloctl edit machineconfig\n\nCommand talosctl edit loads current machine configuration from the node and launches configured editor to modify the config. If config hasn’t been changed in the editor (or if updated config is empty), update is not applied.\n\nNote: Talos uses environment variables TALOS_EDITOR, EDITOR to pick up the editor preference. If environment variables are missing, vi editor is used by default.\n\nExample:\n\ntalosctl -n <IP> edit machineconfig\n\n\nConfiguration can be edited for multiple nodes if multiple IP addresses are specified:\n\ntalosctl -n <IP1>,<IP2>,... edit machineconfig\n\n\nApplying machine configuration change immediately (without a reboot):\n\ntalosctl -n <IP> edit machineconfig --mode=no-reboot\n\ntalosctl patch machineconfig\n\nCommand talosctl patch works similar to talosctl edit command - it loads current machine configuration, but instead of launching configured editor it applies a set of JSON patches to the configuration and writes the result back to the node.\n\nExample, updating kubelet version (in auto mode):\n\n$ talosctl -n <IP> patch machineconfig -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nUpdating kube-apiserver version in immediate mode (without a reboot):\n\n$ talosctl -n <IP> patch machineconfig --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nA patch might be applied to multiple nodes when multiple IPs are specified:\n\ntalosctl -n <IP1>,<IP2>,... patch machineconfig -p '[{...}]'\n\n\nPatches can also be sourced from files using @file syntax:\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.json -p @manifest-patch.json\n\n\nIt might be easier to store patches in YAML format vs. the default JSON format. Talos can detect file format automatically:\n\n# kubelet-patch.yaml\n\n- op: replace\n\n  path: /machine/kubelet/image\n\n  value: ghcr.io/siderolabs/kubelet:v1.29.0\n\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.yaml\n\nRecovering from Node Boot Failures\n\nIf a Talos node fails to boot because of wrong configuration (for example, control plane endpoint is incorrect), configuration can be updated to fix the issue.\n\n6 - Logging\nDealing with Talos Linux logs.\nViewing logs\n\nKernel messages can be retrieved with talosctl dmesg command:\n\n$ talosctl -n 172.20.1.2 dmesg\n\n\n\n172.20.1.2: kern:    info: [2021-11-10T10:09:37.662764956Z]: Command line: init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 console=ttyS0 reboot=k panic=1 talos.shutdown=halt talos.platform=metal talos.config=http://172.20.1.1:40101/config.yaml\n\n[...]\n\n\nService logs can be retrieved with talosctl logs command:\n\n$ talosctl -n 172.20.1.2 services\n\n\n\nNODE         SERVICE      STATE     HEALTH   LAST CHANGE   LAST EVENT\n\n172.20.1.2   apid         Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   containerd   Running   OK       19m29s ago    Health check successful\n\n172.20.1.2   cri          Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   etcd         Running   OK       19m22s ago    Health check successful\n\n172.20.1.2   kubelet      Running   OK       19m20s ago    Health check successful\n\n172.20.1.2   machined     Running   ?        19m30s ago    Service started as goroutine\n\n172.20.1.2   trustd       Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   udevd        Running   OK       19m28s ago    Health check successful\n\n\n\n$ talosctl -n 172.20.1.2 logs machined\n\n\n\n172.20.1.2: [talos] task setupLogger (1/1): done, 106.109µs\n\n172.20.1.2: [talos] phase logger (1/7): done, 564.476µs\n\n[...]\n\n\nContainer logs for Kubernetes pods can be retrieved with talosctl logs -k command:\n\n$ talosctl -n 172.20.1.2 containers -k\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                         PID    STATUS\n\n172.20.1.2   k8s.io      kube-system/kube-flannel-dk6d5                                  registry.k8s.io/pause:3.6                                     1329   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-cni:f1d4cf68feb9      ghcr.io/siderolabs/install-cni:v0.7.0-alpha.0-1-g2bb2efc      0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-config:bc39fec3cbac   quay.io/coreos/flannel:v0.13.0                                0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:kube-flannel:5c3989353b98     quay.io/coreos/flannel:v0.13.0                                1610   CONTAINER_RUNNING\n\n172.20.1.2   k8s.io      kube-system/kube-proxy-gfkqj                                    registry.k8s.io/pause:3.5                                     1311   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f         registry.k8s.io/kube-proxy:v1.29.0                            1379   CONTAINER_RUNNING\n\n\n\n$ talosctl -n 172.20.1.2 logs -k kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f\n\n172.20.1.2: 2021-11-30T19:13:20.567825192Z stderr F I1130 19:13:20.567737       1 server_others.go:138] \"Detected node IP\" address=\"172.20.0.3\"\n\n172.20.1.2: 2021-11-30T19:13:20.599684397Z stderr F I1130 19:13:20.599613       1 server_others.go:206] \"Using iptables Proxier\"\n\n[...]\n\nSending logs\nService logs\n\nYou can enable logs sendings in machine configuration:\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"udp://127.0.0.1:12345/\"\n\n        format: \"json_lines\"\n\n      - endpoint: \"tcp://host:5044/\"\n\n        format: \"json_lines\"\n\n\nSeveral destinations can be specified. Supported protocols are UDP and TCP. The only currently supported format is json_lines:\n\n{\n\n  \"msg\": \"[talos] apply config request: immediate true, on reboot false\",\n\n  \"talos-level\": \"info\",\n\n  \"talos-service\": \"machined\",\n\n  \"talos-time\": \"2021-11-10T10:48:49.294858021Z\"\n\n}\n\n\nMessages are newline-separated when sent over TCP. Over UDP messages are sent with one message per packet. msg, talos-level, talos-service, and talos-time fields are always present; there may be additional fields.\n\nKernel logs\n\nKernel log delivery can be enabled with the talos.logging.kernel kernel command line argument, which can be specified in the .machine.installer.extraKernelArgs:\n\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - talos.logging.kernel=tcp://host:5044/\n\n\nAlso kernel logs delivery can be configured using the document in machine configuration:\n\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log\n\nurl: tcp://host:5044/\n\n\nKernel log destination is specified in the same way as service log endpoint. The only supported format is json_lines.\n\nSample message:\n\n{\n\n  \"clock\":6252819, // time relative to the kernel boot time\n\n  \"facility\":\"user\",\n\n  \"msg\":\"[talos] task startAllServices (1/1): waiting for 6 services\\n\",\n\n  \"priority\":\"warning\",\n\n  \"seq\":711,\n\n  \"talos-level\":\"warn\", // Talos-translated `priority` into common logging level\n\n  \"talos-time\":\"2021-11-26T16:53:21.3258698Z\" // Talos-translated `clock` using current time\n\n}\n\n\nextraKernelArgs in the machine configuration are only applied on Talos upgrades, not just by applying the config. (Upgrading to the same version is fine).\n\nFilebeat example\n\nTo forward logs to other Log collection services, one way to do this is sending them to a Filebeat running in the cluster itself (in the host network), which takes care of forwarding it to other endpoints (and the necessary transformations).\n\nIf Elastic Cloud on Kubernetes is being used, the following Beat (custom resource) configuration might be helpful:\n\napiVersion: beat.k8s.elastic.co/v1beta1\n\nkind: Beat\n\nmetadata:\n\n  name: talos\n\nspec:\n\n  type: filebeat\n\n  version: 7.15.1\n\n  elasticsearchRef:\n\n    name: talos\n\n  config:\n\n    filebeat.inputs:\n\n      - type: \"udp\"\n\n        host: \"127.0.0.1:12345\"\n\n        processors:\n\n          - decode_json_fields:\n\n              fields: [\"message\"]\n\n              target: \"\"\n\n          - timestamp:\n\n              field: \"talos-time\"\n\n              layouts:\n\n                - \"2006-01-02T15:04:05.999999999Z07:00\"\n\n          - drop_fields:\n\n              fields: [\"message\", \"talos-time\"]\n\n          - rename:\n\n              fields:\n\n                - from: \"msg\"\n\n                  to: \"message\"\n\n\n\n  daemonSet:\n\n    updateStrategy:\n\n      rollingUpdate:\n\n        maxUnavailable: 100%\n\n    podTemplate:\n\n      spec:\n\n        dnsPolicy: ClusterFirstWithHostNet\n\n        hostNetwork: true\n\n        securityContext:\n\n          runAsUser: 0\n\n        containers:\n\n          - name: filebeat\n\n            ports:\n\n              - protocol: UDP\n\n                containerPort: 12345\n\n                hostPort: 12345\n\n\nThe input configuration ensures that messages and timestamps are extracted properly. Refer to the Filebeat documentation on how to forward logs to other outputs.\n\nAlso note the hostNetwork: true in the daemonSet configuration.\n\nThis ensures filebeat uses the host network, and listens on 127.0.0.1:12345 (UDP) on every machine, which can then be specified as a logging endpoint in the machine configuration.\n\nFluent-bit example\n\nFirst, we’ll create a value file for the fluentd-bit Helm chart.\n\n# fluentd-bit.yaml\n\n\n\npodAnnotations:\n\n  fluentbit.io/exclude: 'true'\n\n\n\nextraPorts:\n\n  - port: 12345\n\n    containerPort: 12345\n\n    protocol: TCP\n\n    name: talos\n\n\n\nconfig:\n\n  service: |\n\n    [SERVICE]\n\n      Flush         5\n\n      Daemon        Off\n\n      Log_Level     warn\n\n      Parsers_File  custom_parsers.conf    \n\n\n\n  inputs: |\n\n    [INPUT]\n\n      Name          tcp\n\n      Listen        0.0.0.0\n\n      Port          12345\n\n      Format        json\n\n      Tag           talos.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         kubernetes\n\n      Path          /var/log/containers/*.log\n\n      Parser        containerd\n\n      Tag           kubernetes.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         audit\n\n      Path          /var/log/audit/kube/*.log\n\n      Parser        audit\n\n      Tag           audit.*    \n\n\n\n  filters: |\n\n    [FILTER]\n\n      Name                kubernetes\n\n      Alias               kubernetes\n\n      Match               kubernetes.*\n\n      Kube_Tag_Prefix     kubernetes.var.log.containers.\n\n      Use_Kubelet         Off\n\n      Merge_Log           On\n\n      Merge_Log_Trim      On\n\n      Keep_Log            Off\n\n      K8S-Logging.Parser  Off\n\n      K8S-Logging.Exclude On\n\n      Annotations         Off\n\n      Labels              On\n\n\n\n    [FILTER]\n\n      Name          modify\n\n      Match         kubernetes.*\n\n      Add           source kubernetes\n\n      Remove        logtag    \n\n\n\n  customParsers: |\n\n    [PARSER]\n\n      Name          audit\n\n      Format        json\n\n      Time_Key      requestReceivedTimestamp\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z\n\n\n\n    [PARSER]\n\n      Name          containerd\n\n      Format        regex\n\n      Regex         ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$\n\n      Time_Key      time\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z    \n\n\n\n  outputs: |\n\n    [OUTPUT]\n\n      Name    stdout\n\n      Alias   stdout\n\n      Match   *\n\n      Format  json_lines    \n\n\n\n  # If you wish to ship directly to Loki from Fluentbit,\n\n  # Uncomment the following output, updating the Host with your Loki DNS/IP info as necessary.\n\n  # [OUTPUT]\n\n  # Name loki\n\n  # Match *\n\n  # Host loki.loki.svc\n\n  # Port 3100\n\n  # Labels job=fluentbit\n\n  # Auto_Kubernetes_Labels on\n\n\n\ndaemonSetVolumes:\n\n  - name: varlog\n\n    hostPath:\n\n      path: /var/log\n\n\n\ndaemonSetVolumeMounts:\n\n  - name: varlog\n\n    mountPath: /var/log\n\n\n\ntolerations:\n\n  - operator: Exists\n\n    effect: NoSchedule\n\n\nNext, we will add the helm repo for FluentBit, and deploy it to the cluster.\n\nhelm repo add fluent https://fluent.github.io/helm-charts\n\nhelm upgrade -i --namespace=kube-system -f fluentd-bit.yaml fluent-bit fluent/fluent-bit\n\n\nNow we need to find the service IP.\n\n$ kubectl -n kube-system get svc -l app.kubernetes.io/name=fluent-bit\n\n\n\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n\nfluent-bit   ClusterIP   10.200.0.138   <none>        2020/TCP,5170/TCP   108m\n\n\nFinally, we will change talos log destination with the command talosctl edit mc.\n\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"tcp://10.200.0.138:5170\"\n\n        format: \"json_lines\"\n\n\nThis example configuration was well tested with Cilium CNI, and it should work with iptables/ipvs based CNI plugins too.\n\nVector example\n\nVector is a lightweight observability pipeline ideal for a Kubernetes environment. It can ingest (source) logs from multiple sources, perform remapping on the logs (transform), and forward the resulting pipeline to multiple destinations (sinks). As it is an end to end platform, it can be run as a single-deployment ‘aggregator’ as well as a replicaSet of ‘Agents’ that run on each node.\n\nAs Talos can be set as above to send logs to a destination, we can run Vector as an Aggregator, and forward both kernel and service to a UDP socket in-cluster.\n\nBelow is an excerpt of a source/sink setup for Talos, with a ‘sink’ destination of an in-cluster Grafana Loki log aggregation service. As Loki can create labels from the log input, we have set up the Loki sink to create labels based on the host IP, service and facility of the inbound logs.\n\nNote that a method of exposing the Vector service will be required which may vary depending on your setup - a LoadBalancer is a good option.\n\nrole: \"Stateless-Aggregator\"\n\n\n\n# Sources\n\nsources:\n\n  talos_kernel_logs:\n\n    address: 0.0.0.0:6050\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n  talos_service_logs:\n\n    address: 0.0.0.0:6051\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n# Sinks\n\nsinks:\n\n  talos_kernel:\n\n    type: loki\n\n    inputs:\n\n      - talos_kernel_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 1048576\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      facility: >-\n\n                {{`{{ facility }}`}}\n\n\n\n  talos_service:\n\n    type: loki\n\n    inputs:\n\n      - talos_service_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 400000\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      service: >-\n\n                {{`{{ \"talos-service\" }}`}}\n\n7 - Managing Talos PKI\nHow to manage Public Key Infrastructure\nGenerating New Client Configuration\nUsing Controlplane Node\n\nIf you have a valid (not expired) talosconfig with os:admin role, a new client configuration file can be generated with talosctl config new against any controlplane node:\n\ntalosctl -n CP1 config new talosconfig-reader --roles os:reader --crt-ttl 24h\n\n\nA specific role and certificate lifetime can be specified.\n\nFrom Secrets Bundle\n\nIf a secrets bundle (secrets.yaml from talosctl gen secrets) was saved while generating machine configuration:\n\ntalosctl gen config --with-secrets secrets.yaml --output-types talosconfig -o talosconfig <cluster-name> https://<cluster-endpoint>\n\n\nNote: <cluster-name> and <cluster-endpoint> arguments don’t matter, as they are not used for talosconfig.\n\nFrom Control Plane Machine Configuration\n\nIn order to create a new key pair for client configuration, you will need the root Talos API CA. The base64 encoded CA can be found in the control plane node’s configuration file. Save the the CA public key, and CA private key as ca.crt, and ca.key respectively:\n\nyq eval .machine.ca.crt controlplane.yaml | base64 -d > ca.crt\n\nyq eval .machine.ca.key controlplane.yaml | base64 -d > ca.key\n\n\nNow, run the following commands to generate a certificate:\n\ntalosctl gen key --name admin\n\ntalosctl gen csr --key admin.key --ip 127.0.0.1\n\ntalosctl gen crt --ca ca --csr admin.csr --name admin\n\n\nPut the base64-encoded files to the respective location to the talosconfig:\n\ncontext: mycluster\n\ncontexts:\n\n    mycluster:\n\n        endpoints:\n\n            - CP1\n\n            - CP2\n\n        ca: <base64-encoded ca.crt>\n\n        crt: <base64-encoded admin.crt>\n\n        key: <base64-encoded admin.key>\n\nRenewing an Expired Administrator Certificate\n\nBy default admin talosconfig certificate is valid for 365 days, while cluster CAs are valid for 10 years. In order to prevent admin talosconfig from expiring, renew the client config before expiration using talosctl config new command described above.\n\nIf the talosconfig is expired or lost, you can still generate a new one using either the secrets.yaml secrets bundle or the control plane node’s configuration file using methods described above.\n\n8 - NVIDIA Fabric Manager\nIn this guide we’ll follow the procedure to enable NVIDIA Fabric Manager.\n\nNVIDIA GPUs that have nvlink support (for eg: A100) will need the nvidia-fabricmanager system extension also enabled in addition to the NVIDIA drivers. For more information on Fabric Manager refer https://docs.nvidia.com/datacenter/tesla/fabric-manager-user-guide/index.html\n\nThe published versions of the NVIDIA fabricmanager system extensions is available here\n\nThe nvidia-fabricmanager extension version has to match with the NVIDIA driver version in use.\n\nEnabling the NVIDIA fabricmanager system extension\n\nCreate the boot assets or a custom installer and perform a machine upgrade which include the following system extensions:\n\nghcr.io/siderolabs/nvidia-open-gpu-kernel-modules:535.129.03-v1.6.2\n\nghcr.io/siderolabs/nvidia-container-toolkit:535.129.03-v1.13.5\n\nghcr.io/siderolabs/nvidia-fabricmanager:535.129.03\n\n\nPatch the machine configuration to load the required modules:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n9 - NVIDIA GPU (OSS drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using OSS drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA OSS drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA OSS system extensions:\n\nnvidia-open-gpu-kernel-modules\nnvidia-container-toolkit\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nvidia-open-gpu-kernel-modules and nvidia-container-toolkit extensions. The nvidia-open-gpu-kernel-modules extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA OSS modules\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                           VERSION   NAME                             VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-container-toolkit-515.65.01-v1.10.0            1         nvidia-container-toolkit         515.65.01-v1.10.0\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-open-gpu-kernel-modules-515.65.01-v1.2.0       1         nvidia-open-gpu-kernel-modules   515.65.01-v1.2.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  515.65.01  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 12.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n10 - NVIDIA GPU (Proprietary drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using proprietary drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA system extensions:\n\nnonfree-kmod-nvidia\nnvidia-container-toolkit\n\nTo build a NVIDIA driver version not published by SideroLabs follow the instructions here\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nonfree-kmod-nvidia and nvidia-container-toolkit extensions. The nonfree-kmod-nvidia extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA modules and the system extension\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nNODE           NAMESPACE   TYPE              ID                                                                 VERSION   NAME                       VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-frezbo-nvidia-container-toolkit-510.60.02-v1.9.0       1         nvidia-container-toolkit   510.60.02-v1.9.0\n\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  510.60.02  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 11.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n11 - Pull Through Image Cache\nHow to set up local transparent container images caches.\n\nIn this guide we will create a set of local caching Docker registry proxies to minimize local cluster startup time.\n\nWhen running Talos locally, pulling images from container registries might take a significant amount of time. We spin up local caching pass-through registries to cache images and configure a local Talos cluster to use those proxies. A similar approach might be used to run Talos in production in air-gapped environments. It can be also used to verify that all the images are available in local registries.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\n\nThe follow are requirements for creating the set of caching proxies:\n\nDocker 18.03 or greater\nLocal cluster requirements for either docker or QEMU.\nLaunch the Caching Docker Registry Proxies\n\nTalos pulls from docker.io, registry.k8s.io, gcr.io, and ghcr.io by default. If your configuration is different, you might need to modify the commands below:\n\ndocker run -d -p 5000:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \\\n\n    --restart always \\\n\n    --name registry-docker.io registry:2\n\n\n\ndocker run -d -p 5001:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry.k8s.io \\\n\n    --restart always \\\n\n    --name registry-registry.k8s.io registry:2\n\n\n\ndocker run -d -p 5003:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://gcr.io \\\n\n    --restart always \\\n\n    --name registry-gcr.io registry:2\n\n\n\ndocker run -d -p 5004:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \\\n\n    --restart always \\\n\n    --name registry-ghcr.io registry:2\n\n\nNote: Proxies are started as docker containers, and they’re automatically configured to start with Docker daemon.\n\nAs a registry container can only handle a single upstream Docker registry, we launch a container per upstream, each on its own host port (5000, 5001, 5002, 5003 and 5004).\n\nUsing Caching Registries with QEMU Local Cluster\n\nWith a QEMU local cluster, a bridge interface is created on the host. As registry containers expose their ports on the host, we can use bridge IP to direct proxy requests.\n\nsudo talosctl cluster create --provisioner qemu \\\n\n    --registry-mirror docker.io=http://10.5.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://10.5.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://10.5.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://10.5.0.1:5004\n\n\nThe Talos local cluster should now start pulling via caching registries. This can be verified via registry logs, e.g. docker logs -f registry-docker.io. The first time cluster boots, images are pulled and cached, so next cluster boot should be much faster.\n\nNote: 10.5.0.1 is a bridge IP with default network (10.5.0.0/24), if using custom --cidr, value should be adjusted accordingly.\n\nUsing Caching Registries with docker Local Cluster\n\nWith a docker local cluster we can use docker bridge IP, default value for that IP is 172.17.0.1. On Linux, the docker bridge address can be inspected with ip addr show docker0.\n\ntalosctl cluster create --provisioner docker \\\n\n    --registry-mirror docker.io=http://172.17.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.17.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://172.17.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.17.0.1:5004\n\nMachine Configuration\n\nThe caching registries can be configured via machine configuration patch, equivalent to the command line flags above:\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5000\n\n      gcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5003\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5004\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5001\n\nCleaning Up\n\nTo cleanup, run:\n\ndocker rm -f registry-docker.io\n\ndocker rm -f registry-registry.k8s.io\n\ndocker rm -f registry-gcr.io\n\ndocker rm -f registry-ghcr.io\n\n\nNote: Removing docker registry containers also removes the image cache. So if you plan to use caching registries, keep the containers running.\n\nUsing Harbor as a Caching Registry\n\nHarbor is an open source container registry that can be used as a caching proxy. Harbor supports configuring multiple upstream registries, so it can be used to cache multiple registries at once behind a single endpoint.\n\nAs Harbor puts a registry name in the pull image path, we need to set overridePath: true to prevent Talos and containerd from appending /v2 to the path.\n\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-docker.io\n\n        overridePath: true\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-ghcr.io\n\n        overridePath: true\n\n      gcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-gcr.io\n\n        overridePath: true\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-registry.k8s.io\n\n        overridePath: true\n\n\nThe Harbor external endpoint (http://harbor in this example) can be configured with authentication or custom TLS:\n\nmachine:\n\n  registries:\n\n    config:\n\n      harbor:\n\n        auth:\n\n          username: admin\n\n          password: password\n\n12 - Role-based access control (RBAC)\nSet up RBAC on the Talos Linux API.\n\nTalos v0.11 introduced initial support for role-based access control (RBAC). This guide will explain what that is and how to enable it without losing access to the cluster.\n\nRBAC in Talos\n\nTalos uses certificates to authorize users. The certificate subject’s organization field is used to encode user roles. There is a set of predefined roles that allow access to different API methods:\n\nos:admin grants access to all methods;\nos:operator grants everything os:reader role does, plus additional methods: rebooting, shutting down, etcd backup, etcd alarm management, and so on;\nos:reader grants access to “safe” methods (for example, that includes the ability to list files, but does not include the ability to read files content);\nos:etcd:backup grants access to /machine.MachineService/EtcdSnapshot method.\n\nRoles in the current talosconfig can be checked with the following command:\n\n$ talosctl config info\n\n\n\n[...]\n\nRoles:               os:admin\n\n[...]\n\n\nRBAC is enabled by default in new clusters created with talosctl v0.11+ and disabled otherwise.\n\nEnabling RBAC\n\nFirst, both the Talos cluster and talosctl tool should be upgraded. Then the talosctl config new command should be used to generate a new client configuration with the os:admin role. Additional configurations and certificates for different roles can be generated by passing --roles flag:\n\ntalosctl config new --roles=os:reader reader\n\n\nThat command will create a new client configuration file reader with a new certificate with os:reader role.\n\nAfter that, RBAC should be enabled in the machine configuration:\n\nmachine:\n\n  features:\n\n    rbac: true\n\n13 - System Extensions\nCustomizing the Talos Linux immutable root file system.\n\nSystem extensions allow extending the Talos root filesystem, which enables a variety of features, such as including custom container runtimes, loading additional firmware, etc.\n\nSystem extensions are only activated during the installation or upgrade of Talos Linux. With system extensions installed, the Talos root filesystem is still immutable and read-only.\n\nInstalling System Extensions\n\nNote: the way to install system extensions in the .machine.install section of the machine configuration is now deprecated.\n\nStarting with Talos v1.5.0, Talos supports generation of boot media with system extensions included, this removes the need to rebuild the initramfs.xz on the machine itself during the installation or upgrade.\n\nThere are two kinds of boot assets that Talos can generate:\n\ninitial boot assets (ISO, PXE, etc.) that are used to boot the machine\ndisk images that have Talos pre-installed\ninstaller container images that can be used to install or upgrade Talos on a machine (installation happens when booted from ISO or PXE)\n\nDepending on the nature of the system extension (e.g. network device driver or containerd plugin), it may be necessary to include the extension in both initial boot assets and disk images/installer, or just the installer.\n\nThe process of generating boot assets with extensions included is described in the boot assets guide.\n\nExample: Booting from an ISO\n\nLet’s assume NVIDIA extension is required on a bare metal machine which is going to be booted from an ISO. As NVIDIA extension is not required for the initial boot and install step, it is sufficient to include the extension in the installer image only.\n\nUse a generic Talos ISO to boot the machine.\nPrepare a custom installer container image with NVIDIA extension included, push the image to a registry.\nEnsure that machine configuration field .machine.install.image points to the custom installer image.\nBoot the machine using the ISO, apply the machine configuration.\nTalos pulls a custom installer image from the registry (containing NVIDIA extension), installs Talos on the machine, and reboots.\n\nWhen it’s time to upgrade Talos, generate a custom installer container for a new version of Talos, push it to a registry, and perform upgrade pointing to the custom installer image.\n\nExample: Disk Image\n\nLet’s assume NVIDIA extension is required on AWS VM.\n\nPrepare an AWS disk image with NVIDIA extension included.\nUpload the image to AWS, register it as an AMI.\nUse the AMI to launch a VM.\nTalos boots with NVIDIA extension included.\n\nWhen it’s time to upgrade Talos, either repeat steps 1-4 to replace the VM with a new AMI, or like in the previous example, generate a custom installer and use it to upgrade Talos in-place.\n\nAuthoring System Extensions\n\nA Talos system extension is a container image with the specific folder structure. System extensions can be built and managed using any tool that produces container images, e.g. docker build.\n\nSidero Labs maintains a repository of system extensions.\n\nResource Definitions\n\nUse talosctl get extensions to get a list of system extensions:\n\n$ talosctl get extensions\n\nNODE         NAMESPACE   TYPE              ID                                              VERSION   NAME          VERSION\n\n172.20.0.2   runtime     ExtensionStatus   000.ghcr.io-talos-systems-gvisor-54b831d        1         gvisor        20220117.0-v1.0.0\n\n172.20.0.2   runtime     ExtensionStatus   001.ghcr.io-talos-systems-intel-ucode-54b831d   1         intel-ucode   microcode-20210608-v1.0.0\n\n\nUse YAML or JSON format to see additional details about the extension:\n\n$ talosctl -n 172.20.0.2 get extensions 001.ghcr.io-talos-systems-intel-ucode-54b831d -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: runtime\n\n    type: ExtensionStatuses.runtime.talos.dev\n\n    id: 001.ghcr.io-talos-systems-intel-ucode-54b831d\n\n    version: 1\n\n    owner: runtime.ExtensionStatusController\n\n    phase: running\n\n    created: 2022-02-10T18:25:04Z\n\n    updated: 2022-02-10T18:25:04Z\n\nspec:\n\n    image: 001.ghcr.io-talos-systems-intel-ucode-54b831d.sqsh\n\n    metadata:\n\n        name: intel-ucode\n\n        version: microcode-20210608-v1.0.0\n\n        author: Spencer Smith\n\n        description: |\n\n            This system extension provides Intel microcode binaries.\n\n        compatibility:\n\n            talos:\n\n                version: '>= v1.0.0'\n\nExample: gVisor\n\nSee readme of the gVisor extension.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Local Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/local-platforms/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nLocal Platforms\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\n1: Docker\n2: QEMU\n3: VirtualBox\n1 - Docker\nCreating Talos Kubernetes cluster using Docker.\n\nIn this guide we will create a Kubernetes cluster in Docker, using a containerized version of Talos.\n\nRunning Talos in Docker is intended to be used in CI pipelines, and local testing when you need a quick and easy cluster. Furthermore, if you are running Talos in production, it provides an excellent way for developers to develop against the same version of Talos.\n\nRequirements\n\nThe follow are requirements for running Talos in Docker:\n\nDocker 18.03 or greater\na recent version of talosctl\nCaveats\n\nDue to the fact that Talos will be running in a container, certain APIs are not available. For example upgrade, reset, and similar APIs don’t apply in container mode. Further, when running on a Mac in docker, due to networking limitations, VIPs are not supported.\n\nCreate the Cluster\n\nCreating a local cluster is as simple as:\n\ntalosctl cluster create --wait\n\n\nOnce the above finishes successfully, your talosconfig(~/.talos/config) will be configured to point to the new cluster.\n\nNote: Startup times can take up to a minute or more before the cluster is available.\n\nFinally, we just need to specify which nodes you want to communicate with using talosctl. Talosctl can operate on one or all the nodes in the cluster – this makes cluster wide commands much easier.\n\ntalosctl config nodes 10.5.0.2 10.5.0.3\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nCleaning Up\n\nTo cleanup, run:\n\ntalosctl cluster destroy\n\nRunning Talos in Docker Manually\n\nTo run Talos in a container manually, run:\n\ndocker run --rm -it \\\n\n  --name tutorial \\\n\n  --hostname talos-cp \\\n\n  --read-only \\\n\n  --privileged \\\n\n  --security-opt seccomp=unconfined \\\n\n  --mount type=tmpfs,destination=/run \\\n\n  --mount type=tmpfs,destination=/system \\\n\n  --mount type=tmpfs,destination=/tmp \\\n\n  --mount type=volume,destination=/system/state \\\n\n  --mount type=volume,destination=/var \\\n\n  --mount type=volume,destination=/etc/cni \\\n\n  --mount type=volume,destination=/etc/kubernetes \\\n\n  --mount type=volume,destination=/usr/libexec/kubernetes \\\n\n  --mount type=volume,destination=/usr/etc/udev \\\n\n  --mount type=volume,destination=/opt \\\n\n  -e PLATFORM=container \\\n\n  ghcr.io/siderolabs/talos:v1.6.2\n\n2 - QEMU\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nIn this guide we will create a Kubernetes cluster using QEMU.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\nLinux\na kernel with\nKVM enabled (/dev/kvm must exist)\nCONFIG_NET_SCH_NETEM enabled\nCONFIG_NET_SCH_INGRESS enabled\nat least CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities\nQEMU\nbridge, static and firewall CNI plugins from the standard CNI plugins, and tc-redirect-tap CNI plugin from the awslabs tc-redirect-tap installed to /opt/cni/bin (installed automatically by talosctl)\niptables\n/var/run/netns directory should exist\nInstallation\nHow to get QEMU\n\nInstall QEMU with your operating system package manager. For example, on Ubuntu for x86:\n\napt install qemu-system-x86 qemu-kvm\n\nInstall talosctl\n\nDownload talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nInstall Talos kernel and initramfs\n\nQEMU provisioner depends on Talos kernel (vmlinuz) and initramfs (initramfs.xz). These files can be downloaded from the Talos release:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/vmlinuz-<arch> -L -o _out/vmlinuz-<arch>\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/initramfs-<arch>.xz -L -o _out/initramfs-<arch>.xz\n\n\nFor example version v1.6.2:\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/vmlinuz-amd64 -L -o _out/vmlinuz-amd64\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/initramfs-amd64.xz -L -o _out/initramfs-amd64.xz\n\nCreate the Cluster\n\nFor the first time, create root state directory as your user so that you can inspect the logs as non-root user:\n\nmkdir -p ~/.talos/clusters\n\n\nCreate the cluster:\n\nsudo --preserve-env=HOME talosctl cluster create --provisioner qemu\n\n\nBefore the first cluster is created, talosctl will download the CNI bundle for the VM provisioning and install it to ~/.talos/cni directory.\n\nOnce the above finishes successfully, your talosconfig (~/.talos/config) will be configured to point to the new cluster, and kubeconfig will be downloaded and merged into default kubectl config location (~/.kube/config).\n\nCluster provisioning process can be optimized with registry pull-through caches.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl -n 10.5.0.2 containers for a list of containers in the system namespace, or talosctl -n 10.5.0.2 containers -k for the k8s.io namespace. To view the logs of a container, use talosctl -n 10.5.0.2 logs <container> or talosctl -n 10.5.0.2 logs -k <container>.\n\nA bridge interface will be created, and assigned the default IP 10.5.0.1. Each node will be directly accessible on the subnet specified at cluster creation time. A loadbalancer runs on 10.5.0.1 by default, which handles loadbalancing for the Kubernetes APIs.\n\nYou can see a summary of the cluster state by running:\n\n$ talosctl cluster show --provisioner qemu\n\nPROVISIONER       qemu\n\nNAME              talos-default\n\nNETWORK NAME      talos-default\n\nNETWORK CIDR      10.5.0.0/24\n\nNETWORK GATEWAY   10.5.0.1\n\nNETWORK MTU       1500\n\n\n\nNODES:\n\n\n\nNAME                           TYPE           IP         CPU    RAM      DISK\n\ntalos-default-controlplane-1   ControlPlane   10.5.0.2   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-2   ControlPlane   10.5.0.3   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-3   ControlPlane   10.5.0.4   1.00   1.6 GB   4.3 GB\n\ntalos-default-worker-1         Worker         10.5.0.5   1.00   1.6 GB   4.3 GB\n\nCleaning Up\n\nTo cleanup, run:\n\nsudo --preserve-env=HOME talosctl cluster destroy --provisioner qemu\n\n\nNote: In that case that the host machine is rebooted before destroying the cluster, you may need to manually remove ~/.talos/clusters/talos-default.\n\nManual Clean Up\n\nThe talosctl cluster destroy command depends heavily on the clusters state directory. It contains all related information of the cluster. The PIDs and network associated with the cluster nodes.\n\nIf you happened to have deleted the state folder by mistake or you would like to cleanup the environment, here are the steps how to do it manually:\n\nRemove VM Launchers\n\nFind the process of talosctl qemu-launch:\n\nps -elf | grep 'talosctl qemu-launch'\n\n\nTo remove the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 157615 and 157617\n\nps -elf | grep '[t]alosctl qemu-launch'\n\n0 S root      157615    2835  0  80   0 - 184934 -     07:53 ?        00:00:00 talosctl qemu-launch\n\n0 S root      157617    2835  0  80   0 - 185062 -     07:53 ?        00:00:00 talosctl qemu-launch\n\nsudo kill -s SIGTERM 157615\n\nsudo kill -s SIGTERM 157617\n\nStopping VMs\n\nFind the process of qemu-system:\n\nps -elf | grep 'qemu-system'\n\n\nTo stop the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 158065 and 158216\n\nps -elf | grep qemu-system\n\n2 S root     1061663 1061168 26  80   0 - 1786238 -    14:05 ?        01:53:56 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/home/username/.talos/clusters/talos-default/bootstrap-master.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=1e:86:c6:b4:7c:c4 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=7ec0a73c-826e-4eeb-afd1-39ff9f9160ca -machine q35,accel=kvm\n\n2 S root     1061663 1061170 67  80   0 - 621014 -     21:23 ?        00:00:07 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/homeusername/.talos/clusters/talos-default/pxe-1.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=36:f3:2f:c3:9f:06 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=ce12a0d0-29c8-490f-b935-f6073ab916a6 -machine q35,accel=kvm\n\nsudo kill -s SIGTERM 1061663\n\nsudo kill -s SIGTERM 1061663\n\nRemove load balancer\n\nFind the process of talosctl loadbalancer-launch:\n\nps -elf | grep 'talosctl loadbalancer-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl loadbalancer-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl loadbalancer-launch --loadbalancer-addr 10.5.0.1 --loadbalancer-upstreams 10.5.0.2\n\nsudo kill -s SIGTERM 157609\n\nRemove DHCP server\n\nFind the process of talosctl dhcpd-launch:\n\nps -elf | grep 'talosctl dhcpd-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl dhcpd-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl dhcpd-launch --state-path /home/username/.talos/clusters/talos-default --addr 10.5.0.1 --interface talosbd9c32bc\n\nsudo kill -s SIGTERM 157609\n\nRemove network\n\nThis is more tricky part as if you have already deleted the state folder. If you didn’t then it is written in the state.yaml in the ~/.talos/clusters/<cluster-name> directory.\n\nsudo cat ~/.talos/clusters/<cluster-name>/state.yaml | grep bridgename\n\nbridgename: talos<uuid>\n\n\nIf you only had one cluster, then it will be the interface with name talos<uuid>\n\n46: talos<uuid>: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n\n    link/ether a6:72:f4:0a:d3:9c brd ff:ff:ff:ff:ff:ff\n\n    inet 10.5.0.1/24 brd 10.5.0.255 scope global talos17c13299\n\n       valid_lft forever preferred_lft forever\n\n    inet6 fe80::a472:f4ff:fe0a:d39c/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n\nTo remove this interface:\n\nsudo ip link del talos<uuid>\n\nRemove state directory\n\nTo remove the state directory execute:\n\nsudo rm -Rf /home/$USER/.talos/clusters/<cluster-name>\n\nTroubleshooting\nLogs\n\nInspect logs directory\n\nsudo cat ~/.talos/clusters/<cluster-name>/*.log\n\n\nLogs are saved under <cluster-name>-<role>-<node-id>.log\n\nFor example in case of k8s cluster name:\n\nls -la ~/.talos/clusters/k8s | grep log\n\n-rw-r--r--. 1 root root      69415 Apr 26 20:58 k8s-master-1.log\n\n-rw-r--r--. 1 root root      68345 Apr 26 20:58 k8s-worker-1.log\n\n-rw-r--r--. 1 root root      24621 Apr 26 20:59 lb.log\n\n\nInspect logs during the installation\n\ntail -f ~/.talos/clusters/<cluster-name>/*.log\n\n3 - VirtualBox\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\nIn this guide we will create a Kubernetes cluster using VirtualBox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get VirtualBox\n\nInstall VirtualBox with your operating system package manager or from the website. For example, on Ubuntu for x86:\n\napt install virtualbox\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nDownload the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nCreate VMs\n\nStart by creating a new VM by clicking the “New” button in the VirtualBox UI:\n\nSupply a name for this VM, and specify the Type and Version:\n\nEdit the memory to supply at least 2GB of RAM for the VM:\n\nProceed through the disk settings, keeping the defaults. You can increase the disk space if desired.\n\nOnce created, select the VM and hit “Settings”:\n\nIn the “System” section, supply at least 2 CPUs:\n\nIn the “Network” section, switch the network “Attached To” section to “Bridged Adapter”:\n\nFinally, in the “Storage” section, select the optical drive and, on the right, select the ISO by browsing your filesystem:\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”. Once the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-vbox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the VirtualBox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig $TALOSCONFIG config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig $TALOSCONFIG config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig $TALOSCONFIG bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig $TALOSCONFIG kubeconfig .\n\n\nYou can then use kubectl in this fashion:\n\nkubectl get nodes\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the VirtualBox UI.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Single Board Computers | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nSingle Board Computers\nInstallation of Talos Linux on single-board computers.\n1: Banana Pi M64\n2: Friendlyelec Nano PI R4S\n3: Jetson Nano\n4: Libre Computer Board ALL-H3-CC\n5: Pine64\n6: Pine64 Rock64\n7: Radxa ROCK PI 4\n8: Radxa ROCK PI 4C\n9: Raspberry Pi Series\n1 - Banana Pi M64\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-bananapi_m64-arm64.raw.xz\n\nxz -d metal-bananapi_m64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-bananapi_m64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n2 - Friendlyelec Nano PI R4S\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-nanopi_r4s-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-nanopi_r4s-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n3 - Jetson Nano\nInstalling Talos on Jetson Nano SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card/USB drive\ncrane CLI\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nFlashing the firmware to on-board SPI flash\n\nFlashing the firmware only needs to be done once.\n\nWe will use the R32.7.2 release for the Jetson Nano. Most of the instructions is similar to this doc except that we’d be using a upstream version of u-boot with patches from NVIDIA u-boot so that USB boot also works.\n\nBefore flashing we need the following:\n\nA USB-A to micro USB cable\nA jumper wire to enable recovery mode\nA HDMI monitor to view the logs if the USB serial adapter is not available\nA USB to Serial adapter with 3.3V TTL (optional)\nA 5V DC barrel jack\n\nIf you’re planning to use the serial console follow the documentation here\n\nFirst start by downloading the Jetson Nano L4T release.\n\ncurl -SLO https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/t210/jetson-210_linux_r32.7.2_aarch64.tbz2\n\n\nNext we will extract the L4T release and replace the u-boot binary with the patched version.\n\ntar xf jetson-210_linux_r32.6.1_aarch64.tbz2\n\ncd Linux_for_Tegra\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C bootloader/t210ref/p3450-0000/ jetson_nano/u-boot.bin\n\n\nNext we will flash the firmware to the Jetson Nano SPI flash. In order to do that we need to put the Jetson Nano into Force Recovery Mode (FRC). We will use the instructions from here\n\nEnsure that the Jetson Nano is powered off. There is no need for the SD card/USB storage/network cable to be connected\nConnect the micro USB cable to the micro USB port on the Jetson Nano, don’t plug the other end to the PC yet\nEnable Force Recovery Mode (FRC) by placing a jumper across the FRC pins on the Jetson Nano\nFor board revision A02, these are pins 3 and 4 of header J40\nFor board revision B01, these are pins 9 and 10 of header J50\nPlace another jumper across J48 to enable power from the DC jack and connect the Jetson Nano to the DC jack J25\nNow connect the other end of the micro USB cable to the PC and remove the jumper wire from the FRC pins\n\nNow the Jetson Nano is in Force Recovery Mode (FRC) and can be confirmed by running the following command\n\nlsusb | grep -i \"nvidia\"\n\n\nNow we can move on the flashing the firmware.\n\nsudo ./flash p3448-0000-max-spi external\n\n\nThis will flash the firmware to the Jetson Nano SPI flash and you’ll see a lot of output. If you’ve connected the serial console you’ll also see the progress there. Once the flashing is done you can disconnect the USB cable and power off the Jetson Nano.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-jetson_nano-arm64.raw.xz\n\nxz -d metal-jetson_nano-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card/USB storage:\n\nsudo dd if=metal-jetson_nano-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M status=progress\n\n\n| Replace /dev/mmcblk0 with the name of your SD card/USB storage.\n\nBootstrapping the Node\n\nInsert the SD card/USB storage to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n4 - Libre Computer Board ALL-H3-CC\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nxz -d metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-libretech_all_h3_cc_h5-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5 - Pine64\nInstalling Talos on a Pine64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-pine64-arm64.raw.xz\n\nxz -d metal-pine64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-pine64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n6 - Pine64 Rock64\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rock64-arm64.raw.xz\n\nxz -d metal-rock64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rock64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n7 - Radxa ROCK PI 4\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n8 - Radxa ROCK PI 4C\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4c-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4c/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4c-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n9 - Raspberry Pi Series\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\nTalos disk image for the Raspberry Pi generic should in theory work for the boards supported by u-boot rpi_arm64_defconfig. This has only been officialy tested on the Raspberry Pi 4 and community tested on one variant of the Compute Module 4 using Super 6C boards. If you have tested this on other Raspberry Pi boards, please let us know.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nUpdating the EEPROM\n\nUse Raspberry Pi Imager to write an EEPROM update image to a spare SD card. Select Misc utility images under the Operating System tab.\n\nRemove the SD card from your local machine and insert it into the Raspberry Pi. Power the Raspberry Pi on, and wait at least 10 seconds. If successful, the green LED light will blink rapidly (forever), otherwise an error pattern will be displayed. If an HDMI display is attached to the port closest to the power/USB-C port, the screen will display green for success or red if a failure occurs. Power off the Raspberry Pi and remove the SD card from it.\n\nNote: Updating the bootloader only needs to be done once.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rpi_generic-arm64.raw.xz\n\nxz -d metal-rpi_generic-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rpi_generic-arm64.raw of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nNote: if you have an HDMI display attached and it shows only a rainbow splash, please use the other HDMI port, the one closest to the power/USB-C port.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\nTroubleshooting\n\nThe following table can be used to troubleshoot booting issues:\n\nLong Flashes\tShort Flashes\tStatus\n0\t3\tGeneric failure to boot\n0\t4\tstart*.elf not found\n0\t7\tKernel image not found\n0\t8\tSDRAM failure\n0\t9\tInsufficient SDRAM\n0\t10\tIn HALT state\n2\t1\tPartition not FAT\n2\t2\tFailed to read from partition\n2\t3\tExtended partition not FAT\n2\t4\tFile signature/hash mismatch - Pi 4\n4\t4\tUnsupported board type\n4\t5\tFatal firmware error\n4\t6\tPower failure type A\n4\t7\tPower failure type B\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Installation | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nInstallation\nHow to install Talos Linux on various platforms\n1: Bare Metal Platforms\n1.1: Digital Rebar\n1.2: Equinix Metal\n1.3: ISO\n1.4: Matchbox\n1.5: Network Configuration\n1.6: PXE\n1.7: SecureBoot\n2: Virtualized Platforms\n2.1: Hyper-V\n2.2: KVM\n2.3: Proxmox\n2.4: Vagrant & Libvirt\n2.5: VMware\n2.6: Xen\n3: Cloud Platforms\n3.1: AWS\n3.2: Azure\n3.3: DigitalOcean\n3.4: Exoscale\n3.5: GCP\n3.6: Hetzner\n3.7: Nocloud\n3.8: Openstack\n3.9: Oracle\n3.10: Scaleway\n3.11: UpCloud\n3.12: Vultr\n4: Local Platforms\n4.1: Docker\n4.2: QEMU\n4.3: VirtualBox\n5: Single Board Computers\n5.1: Banana Pi M64\n5.2: Friendlyelec Nano PI R4S\n5.3: Jetson Nano\n5.4: Libre Computer Board ALL-H3-CC\n5.5: Pine64\n5.6: Pine64 Rock64\n5.7: Radxa ROCK PI 4\n5.8: Radxa ROCK PI 4C\n5.9: Raspberry Pi Series\n6: Boot Assets\n7: Omni SaaS\n1 - Bare Metal Platforms\nInstallation of Talos Linux on various bare-metal platforms.\n1.1 - Digital Rebar\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\nPrerequisites\n3 nodes (please see hardware requirements)\nLoadbalancer\nDigital Rebar Server\nTalosctl access (see talosctl setup)\nCreating a Cluster\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes. We assume an existing digital rebar deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe loadbalancer is used to distribute the load across multiple controlplane nodes. This isn’t covered in detail, because we assume some loadbalancing knowledge before hand. If you think this should be added to the docs, please create a issue.\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nDigital Rebar has a built-in fileserver, which means we can use this feature to expose the talos configuration files. We will place controlplane.yaml, and worker.yaml into Digital Rebar file server by using the drpcli tools.\n\nCopy the generated files from the step above into your Digital Rebar installation.\n\ndrpcli file upload <file>.yaml as <file>.yaml\n\n\nReplacing <file> with controlplane or worker.\n\nDownload the boot files\n\nDownload a recent version of boot.tar.gz from github.\n\nUpload to DRB:\n\n$ drpcli isos upload boot.tar.gz as talos.tar.gz\n\n{\n\n  \"Path\": \"talos.tar.gz\",\n\n  \"Size\": 96470072\n\n}\n\n\nWe have some Digital Rebar example files in the Git repo you can use to provision Digital Rebar with drpcli.\n\nTo apply these configs you need to create them, and then apply them as follow:\n\n$ drpcli bootenvs create talos\n\n{\n\n  \"Available\": true,\n\n  \"BootParams\": \"\",\n\n  \"Bundle\": \"\",\n\n  \"Description\": \"\",\n\n  \"Documentation\": \"\",\n\n  \"Endpoint\": \"\",\n\n  \"Errors\": [],\n\n  \"Initrds\": [],\n\n  \"Kernel\": \"\",\n\n  \"Meta\": {},\n\n  \"Name\": \"talos\",\n\n  \"OS\": {\n\n    \"Codename\": \"\",\n\n    \"Family\": \"\",\n\n    \"IsoFile\": \"\",\n\n    \"IsoSha256\": \"\",\n\n    \"IsoUrl\": \"\",\n\n    \"Name\": \"\",\n\n    \"SupportedArchitectures\": {},\n\n    \"Version\": \"\"\n\n  },\n\n  \"OnlyUnknown\": false,\n\n  \"OptionalParams\": [],\n\n  \"ReadOnly\": false,\n\n  \"RequiredParams\": [],\n\n  \"Templates\": [],\n\n  \"Validated\": true\n\n}\n\ndrpcli bootenvs update talos - < bootenv.yaml\n\n\nYou need to do this for all files in the example directory. If you don’t have access to the drpcli tools you can also use the webinterface.\n\nIt’s important to have a corresponding SHA256 hash matching the boot.tar.gz\n\nBootenv BootParams\n\nWe’re using some of Digital Rebar built in templating to make sure the machine gets the correct role assigned.\n\ntalos.platform=metal talos.config={{ .ProvisionerURL }}/files/{{.Param \\\"talos/role\\\"}}.yaml\"\n\nThis is why we also include a params.yaml in the example directory to make sure the role is set to one of the following:\n\ncontrolplane\nworker\n\nThe {{.Param \\\"talos/role\\\"}} then gets populated with one of the above roles.\n\nBoot the Machines\n\nIn the UI of Digital Rebar you need to select the machines you want to provision. Once selected, you need to assign to following:\n\nProfile\nWorkflow\n\nThis will provision the Stage and Bootenv with the talos values. Once this is done, you can boot the machine.\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.2 - Equinix Metal\nCreating Talos clusters with Equinix Metal.\n\nYou can create a Talos Linux cluster on Equinix Metal in a variety of ways, such as through the EM web UI, the metal command line too, or through PXE booting. Talos Linux is a supported OS install option on Equinix Metal, so it’s an easy process.\n\nRegardless of the method, the process is:\n\nCreate a DNS entry for your Kubernetes endpoint.\nGenerate the configurations using talosctl.\nProvision your machines on Equinix Metal.\nPush the configurations to your servers (if not done as part of the machine provisioning).\nconfigure your Kubernetes endpoint to point to the newly created control plane nodes\nbootstrap the cluster\nDefine the Kubernetes Endpoint\n\nThere are a variety of ways to create an HA endpoint for the Kubernetes cluster. Some of the ways are:\n\nDNS\nLoad Balancer\nBGP\n\nWhatever way is chosen, it should result in an IP address/DNS name that routes traffic to all the control plane nodes. We do not know the control plane node IP addresses at this stage, but we should define the endpoint DNS entry so that we can use it in creating the cluster configuration. After the nodes are provisioned, we can use their addresses to create the endpoint A records, or bind them to the load balancer, etc.\n\nCreate the Machine Configuration Files\nGenerating Configurations\n\nUsing the DNS name of the loadbalancer defined above, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-em-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe port used above should be 6443, unless your load balancer maps a different port to port 6443 on the control plane nodes.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode metal\n\ntalosctl validate --config worker.yaml --mode metal\n\n\nNote: Validation of the install disk could potentially fail as validation is performed on your local machine and the specified disk may not exist.\n\nPassing in the configuration as User Data\n\nYou can use the metadata service provide by Equinix Metal to pass in the machines configuration. It is required to add a shebang to the top of the configuration file.\n\nThe convention we use is #!talos.\n\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\n\nSimply select the location and type of machines in the Equinix Metal web interface. Select Talos as the Operating System, then select the number of servers to create, and name them (in lowercase only.) Under optional settings, you can optionally paste in the contents of controlplane.yaml that was generated, above (ensuring you add a first line of #!talos).\n\nYou can repeat this process to create machines of different types for control plane and worker nodes (although you would pass in worker.yaml for the worker nodes, as user data).\n\nIf you did not pass in the machine configuration as User Data, you need to provide it to each machine, with the following command:\n\ntalosctl apply-config --insecure --nodes <Node IP> --file ./controlplane.yaml\n\nCreating a Cluster via the Equinix Metal CLI\n\nThis guide assumes the user has a working API token,and the Equinix Metal CLI installed.\n\nBecause Talos Linux is a supported operating system, Talos Linux machines can be provisioned directly via the CLI, using the -O talos_v1 parameter (for Operating System).\n\nNote: Ensure you have prepended #!talos to the controlplane.yaml file.\n\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --operating-system \"talos_v1\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\ne.g. metal device create -p <projectID> -f da11 -O talos_v1 -P c3.small.x86 -H steve.test.11 --userdata-file ./controlplane.yaml\n\nRepeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nNetwork Booting via iPXE\n\nTalos Linux can be PXE-booted on Equinix Metal using Image Factory, using the equinixMetal platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/equinixMetal-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate the Control Plane Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\nNote: Repeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nCreate the Worker Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file worker.yaml\n\nUpdate the Kubernetes endpoint\n\nNow our control plane nodes have been created, and we know their IP addresses, we can associate them with the Kubernetes endpoint. Configure your load balancer to route traffic to these nodes, or add A records to your DNS entry for the endpoint, for each control plane node. e.g.\n\nhost endpoint.mydomain.com\n\nendpoint.mydomain.com has address 145.40.90.201\n\nendpoint.mydomain.com has address 147.75.109.71\n\nendpoint.mydomain.com has address 145.40.90.177\n\nBootstrap Etcd\n\nSet the endpoints and nodes for talosctl:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\n\nThis only needs to be issued to one control plane node.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.3 - ISO\nBooting Talos on bare-metal with ISO.\n\nTalos can be installed on bare-metal machine using an ISO image. ISO images for amd64 and arm64 architectures are available on the Talos releases page.\n\nTalos doesn’t install itself to disk when booted from an ISO until the machine configuration is applied.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from a Talos ISO. The boot order should prefer disk over ISO, or the ISO should be removed after the installation to make Talos boot from disk.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nThere are two flavors of ISO images available:\n\nmetal-<arch>.iso supports booting on BIOS and UEFI systems (for x86, UEFI only for arm64)\nmetal-<arch>-secureboot.iso supports booting on only UEFI systems in SecureBoot mode (via Image Factory)\n1.4 - Matchbox\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\nCreating a Cluster\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing load balancer, matchbox deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nIn bare-metal setups it is up to the user to provide the configuration files over HTTP(S). A special kernel parameter (talos.config) must be used to inform Talos about where it should retrieve its configuration file. To keep things simple we will place controlplane.yaml, and worker.yaml into Matchbox’s assets directory. This directory is automatically served by Matchbox.\n\nCreate the Matchbox Configuration Files\n\nThe profiles we will create will reference vmlinuz, and initramfs.xz. Download these files from the release of your choice, and place them in /var/lib/matchbox/assets.\n\nProfiles\nControl Plane Nodes\n{\n\n  \"id\": \"control-plane\",\n\n  \"name\": \"control-plane\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/controlplane.yaml\"\n\n    ]\n\n  }\n\n}\n\n\nNote: Be sure to change http://matchbox.talos.dev to the endpoint of your matchbox server.\n\nWorker Nodes\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/worker.yaml\"\n\n    ]\n\n  }\n\n}\n\nGroups\n\nNow, create the following groups, and ensure that the selectors are accurate for your specific setup.\n\n{\n\n  \"id\": \"control-plane-1\",\n\n  \"name\": \"control-plane-1\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-2\",\n\n  \"name\": \"control-plane-2\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-3\",\n\n  \"name\": \"control-plane-3\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"profile\": \"default\"\n\n}\n\nBoot the Machines\n\nNow that we have our configuration files in place, boot all the machines. Talos will come up on each machine, grab its configuration file, and bootstrap itself.\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n1.5 - Network Configuration\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nBy default, Talos will run DHCP client on all interfaces which have a link, and that might be enough for most of the cases. If some advanced network configuration is required, it can be done via the machine configuration file.\n\nBut sometimes it is required to apply network configuration even before the machine configuration can be fetched from the network.\n\nKernel Command Line\n\nTalos supports some kernel command line parameters to configure network before the machine configuration is fetched.\n\nNote: Kernel command line parameters are not persisted after Talos installation, so proper network configuration should be done via the machine configuration.\n\nAddress, default gateway and DNS servers can be configured via ip= kernel command line parameter:\n\nip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\n\nBonding can be configured via bond= kernel command line parameter:\n\nbond=bond0:eth0,eth1:balance-rr\n\n\nVLANs can be configured via vlan= kernel command line parameter:\n\nvlan=eth0.100:eth0\n\n\nSee kernel parameters reference for more details.\n\nPlatform Network Configuration\n\nSome platforms (e.g. AWS, Google Cloud, etc.) have their own network configuration mechanisms, which can be used to perform the initial network configuration. There is no such mechanism for bare-metal platforms, so Talos provides a way to use platform network config on the metal platform to submit the initial network configuration.\n\nThe platform network configuration is a YAML document which contains resource specifications for various network resources. For the metal platform, the interactive dashboard can be used to edit the platform network configuration, also the configuration can be created manually.\n\nThe current value of the platform network configuration can be retrieved using the MetaKeys resource (key 0xa):\n\ntalosctl get meta 0xa\n\n\nThe platform network configuration can be updated using the talosctl meta command for the running node:\n\ntalosctl meta write 0xa '{\"externalIPs\": [\"1.2.3.4\"]}'\n\ntalosctl meta delete 0xa\n\n\nThe initial platform network configuration for the metal platform can be also included into the generated Talos image:\n\ndocker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\ndocker run --rm -i --privileged ghcr.io/siderolabs/imager:v1.6.2 image --platform metal --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\n\nThe platform network configuration gets merged with other sources of network configuration, the details can be found in the network resources guide.\n\n1.6 - PXE\nBooting Talos over the network on bare-metal with PXE.\n\nTalos can be installed on bare-metal using PXE service. There are two more detailed guides for PXE booting using Matchbox and Digital Rebar.\n\nThis guide describes generic steps for PXE booting Talos on bare-metal.\n\nFirst, download the vmlinuz and initramfs assets from the Talos releases page. Set up the machines to PXE boot from the network (usually by setting the boot order in the BIOS). There might be options specific to the hardware being used, booting in BIOS or UEFI mode, using iPXE, etc.\n\nTalos requires the following kernel parameters to be set on the initial boot:\n\ntalos.platform=metal\nslab_nomerge\npti=on\n\nWhen booted from the network without machine configuration, Talos will start in maintenance mode.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from network. The boot order should prefer disk over network.\n\nTalos can automatically fetch the machine configuration from the network on the initial boot using talos.config kernel parameter. A metadata service (HTTP service) can be implemented to deliver customized configuration to each node for example by using the MAC address of the node:\n\ntalos.config=https://metadata.service/talos/config?mac=${mac}\n\n\nNote: The talos.config kernel parameter supports other substitution variables, see kernel parameters reference for the full list.\n\nPXE booting can be also performed via Image Factory.\n\n1.7 - SecureBoot\nBooting Talos in SecureBoot mode on UEFI platforms.\n\nTalos now supports booting on UEFI systems in SecureBoot mode. When combined with TPM-based disk encryption, this provides Trusted Boot experience.\n\nNote: SecureBoot is not supported on x86 platforms in BIOS mode.\n\nThe implementation is using systemd-boot as a boot menu implementation, while the Talos kernel, initramfs and cmdline arguments are combined into the Unified Kernel Image (UKI) format. UEFI firmware loads the systemd-boot bootloader, which then loads the UKI image. Both systemd-boot and Talos UKI image are signed with the key, which is enrolled into the UEFI firmware.\n\nAs Talos Linux is fully contained in the UKI image, the full operating system is verified and booted by the UEFI firmware.\n\nNote: There is no support at the moment to upgrade non-UKI (GRUB-based) Talos installation to use UKI/SecureBoot, so a fresh installation is required.\n\nSecureBoot with Sidero Labs Images\n\nSidero Labs provides Talos images signed with the Sidero Labs SecureBoot key via Image Factory.\n\nNote: The SecureBoot images are available for Talos releases starting from v1.5.0.\n\nThe easiest way to get started with SecureBoot is to download the ISO, and boot it on a UEFI-enabled system which has SecureBoot enabled in setup mode.\n\nThe ISO bootloader will enroll the keys in the UEFI firmware, and boot the Talos Linux in SecureBoot mode. The install should performed using SecureBoot installer (put it Talos machine configuration): factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2.\n\nNote: SecureBoot images can also be generated with custom keys.\n\nBooting Talos Linux in SecureBoot Mode\n\nIn this guide we will use the ISO image to boot Talos Linux in SecureBoot mode, followed by submitting machine configuration to the machine in maintenance mode. We will use one the ways to generate and submit machine configuration to the node, please refer to the Production Notes for the full guide.\n\nFirst, make sure SecureBoot is enabled in the UEFI firmware. For the first boot, the UEFI firmware should be in the setup mode, so that the keys can be enrolled into the UEFI firmware automatically. If the UEFI firmware does not support automatic enrollment, you may need to hit Esc to force the boot menu to appear, and select the Enroll Secure Boot keys: auto option.\n\nNote: There are other ways to enroll the keys into the UEFI firmware, but this is out of scope of this guide.\n\nOnce Talos is running in maintenance mode, verify that secure boot is enabled:\n\n$ talosctl -n <IP> get securitystate --insecure\n\nNODE   NAMESPACE   TYPE            ID              VERSION   SECUREBOOT\n\n       runtime     SecurityState   securitystate   1         true\n\n\nNow we will generate the machine configuration for the node supplying the installer-secureboot container image, and applying the patch to enable TPM-based disk encryption (requires TPM 2.0):\n\n# tpm-disk-encryption.yaml\n\nmachine:\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n\nGenerate machine configuration:\n\ntalosctl gen config <cluster-name> https://<endpoint>:6443 --install-image=factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2 --install-disk=/dev/sda --config-patch @tpm-disk-encryption.yaml\n\n\nApply machine configuration to the node:\n\ntalosctl -n <IP> apply-config --insecure -f controlplane.yaml\n\n\nTalos will perform the installation to the disk and reboot the node. Please make sure that the ISO image is not attached to the node anymore, otherwise the node will boot from the ISO image again.\n\nOnce the node is rebooted, verify that the node is running in secure boot mode:\n\ntalosctl -n <IP> --talosconfig=talosconfig get securitystate\n\nUpgrading Talos Linux\n\nAny change to the boot asset (kernel, initramfs, kernel command line) requires the UKI to be regenerated and the installer image to be rebuilt. Follow the steps above to generate new installer image updating the boot assets: use new Talos version, add a system extension, or modify the kernel command line. Once the new installer image is pushed to the registry, upgrade the node using the new installer image.\n\nIt is important to preserve the UKI signing key and the PCR signing key, otherwise the node will not be able to boot with the new UKI and unlock the encrypted partitions.\n\nDisk Encryption with TPM\n\nWhen encrypting the disk partition for the first time, Talos Linux generates a random disk encryption key and seals (encrypts) it with the TPM device. The TPM unlock policy is configured to trust the expected policy signed by the PCR signing key. This way TPM unlocking doesn’t depend on the exact PCR measurements, but rather on the expected policy signed by the PCR signing key and the state of SecureBoot (PCR 7 measurement, including secureboot status and the list of enrolled keys).\n\nWhen the UKI image is generated, the UKI is measured and expected measurements are combined into TPM unlock policy and signed with the PCR signing key. During the boot process, systemd-stub component of the UKI performs measurements of the UKI sections into the TPM device. Talos Linux during the boot appends to the PCR register the measurements of the boot phases, and once the boot reaches the point of mounting the encrypted disk partition, the expected signed policy from the UKI is matched against measured values to unlock the TPM, and TPM unseals the disk encryption key which is then used to unlock the disk partition.\n\nDuring the upgrade, as long as the new UKI is contains PCR policy signed with the same PCR signing key, and SecureBoot state has not changed the disk partition will be unlocked successfully.\n\nDisk encryption is also tied to the state of PCR register 7, so that it unlocks only if SecureBoot is enabled and the set of enrolled keys hasn’t changed.\n\nOther Boot Options\n\nUnified Kernel Image (UKI) is a UEFI-bootable image which can be booted directly from the UEFI firmware skipping the systemd-boot bootloader. In network boot mode, the UKI can be used directly as well, as it contains the full set of boot assets required to boot Talos Linux.\n\nWhen SecureBoot is enabled, the UKI image ignores any kernel command line arguments passed to it, but rather uses the kernel command line arguments embedded into the UKI image itself. If kernel command line arguments need to be changed, the UKI image needs to be rebuilt with the new kernel command line arguments.\n\nSecureBoot with Custom Keys\nGenerating the Keys\n\nTalos requires two set of keys to be used for the SecureBoot process:\n\nSecureBoot key is used to sign the boot assets and it is enrolled into the UEFI firmware.\nPCR Signing Key is used to sign the TPM policy, which is used to seal the disk encryption key.\n\nThe same key might be used for both, but it is recommended to use separate keys for each purpose.\n\nTalos provides a utility to generate the keys, but existing PKI infrastructure can be used as well:\n\n$ talosctl gen secureboot uki --common-name \"SecureBoot Key\"\n\nwriting _out/uki-signing-cert.pem\n\nwriting _out/uki-signing-cert.der\n\nwriting _out/uki-signing-key.pem\n\n\nThe generated certificate and private key are written to disk in PEM-encoded format (RSA 4096-bit key). The certificate is also written in DER format for the systems which expect the certificate in DER format.\n\nPCR signing key can be generated with:\n\n$ talosctl gen secureboot pcr\n\nwriting _out/pcr-signing-key.pem\n\n\nThe file containing the private key is written to disk in PEM-encoded format (RSA 2048-bit key).\n\nOptionally, UEFI automatic key enrollment database can be generated using the _out/uki-signing-* files as input:\n\n$ talosctl gen secureboot database\n\nwriting _out/db.auth\n\nwriting _out/KEK.auth\n\nwriting _out/PK.auth\n\n\nThese files can be used to enroll the keys into the UEFI firmware automatically when booting from a SecureBoot ISO while UEFI firmware is in the setup mode.\n\nGenerating the SecureBoot Assets\n\nOnce the keys are generated, they can be used to sign the Talos boot assets to generate required ISO images, PXE boot assets, disk images, installer containers, etc. In this guide we will generate a SecureBoot ISO image and an installer image.\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-iso\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.5.0-alpha.3-35-ge0f383598-dirty\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\nISO ready\n\noutput asset path: /out/metal-amd64-secureboot.iso\n\n\nNext, the installer image should be generated to install Talos to disk on a SecureBoot-enabled system:\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-installer\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: installer\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\ninstaller container image ready\n\noutput asset path: /out/installer-amd64-secureboot.tar\n\n\nThe generated container image should be pushed to some container registry which Talos can access during the installation, e.g.:\n\ncrane push _out/installer-amd64-secureboot.tar ghcr.io/<user>/installer-amd64-secureboot:v1.6.2\n\n\nThe generated ISO and installer images might be further customized with system extensions, extra kernel command line arguments, etc.\n\n2 - Virtualized Platforms\nInstallation of Talos Linux for virtualization platforms.\n2.1 - Hyper-V\nCreating a Talos Kubernetes cluster using Hyper-V.\nPre-requisities\nDownload the latest metal-amd64.iso ISO from github releases page\nCreate a New-TalosVM folder in any of your PS Module Path folders $env:PSModulePath -split ';' and save the New-TalosVM.psm1 there\nPlan Overview\n\nHere we will create a basic 3 node cluster with a single control-plane node and two worker nodes. The only difference between control plane and worker node is the amount of RAM and an additional storage VHD. This is personal preference and can be configured to your liking.\n\nWe are using a VMNamePrefix argument for a VM Name prefix and not the full hostname. This command will find any existing VM with that prefix and “+1” the highest suffix it finds. For example, if VMs talos-cp01 and talos-cp02 exist, this will create VMs starting from talos-cp03, depending on NumberOfVMs argument.\n\nSetup a Control Plane Node\n\nUse the following command to create a single control plane node:\n\nNew-TalosVM -VMNamePrefix talos-cp -CPUCount 2 -StartupMemory 4GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 1 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos'\n\n\nThis will create talos-cp01 VM and power it on.\n\nSetup Worker Nodes\n\nUse the following command to create 2 worker nodes:\n\nNew-TalosVM -VMNamePrefix talos-worker -CPUCount 4 -StartupMemory 8GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 2 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos' -StorageVHDSize 50GB\n\n\nThis will create two VMs: talos-worker01 and talos-wworker02 and attach an additional VHD of 50GB for storage (which in my case will be passed to Mayastor).\n\nPushing Config to the Nodes\n\nNow that our VMs are ready, find their IP addresses from console of VM. With that information, push config to the control plane node with:\n\n# set control plane IP variable\n\n$CONTROL_PLANE_IP='10.10.10.x'\n\n\n\n# Generate talos config\n\ntalosctl gen config talos-cluster https://$($CONTROL_PLANE_IP):6443 --output-dir .\n\n\n\n# Apply config to control plane node\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file .\\controlplane.yaml\n\nPushing Config to Worker Nodes\n\nSimilarly, for the workers:\n\ntalosctl apply-config --insecure --nodes 10.10.10.x --file .\\worker.yaml\n\n\nApply the config to both nodes.\n\nBootstrap Cluster\n\nNow that our nodes are ready, we are ready to bootstrap the Kubernetes cluster.\n\n# Use following command to set node and endpoint permanantly in config so you dont have to type it everytime\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\n\n\n# Bootstrap cluster\n\ntalosctl bootstrap\n\n\n\n# Generate kubeconfig\n\ntalosctl kubeconfig .\n\n\nThis will generate the kubeconfig file, you can use to connect to the cluster.\n\n2.2 - KVM\n\nTalos is known to work on KVM.\n\nWe don’t yet have a documented guide specific to KVM; however, you can have a look at our Vagrant & Libvirt guide which uses KVM for virtualization.\n\nIf you run into any issues, our community can probably help!\n\n2.3 - Proxmox\nCreating Talos Kubernetes cluster using Proxmox.\n\nIn this guide we will create a Kubernetes cluster using Proxmox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get Proxmox\n\nIt is assumed that you have already installed Proxmox onto the server you wish to create Talos VMs on. Visit the Proxmox downloads page if necessary.\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nIn order to install Talos in Proxmox, you will need the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nUpload ISO\n\nFrom the Proxmox UI, select the “local” storage and enter the “Content” section. Click the “Upload” button:\n\nSelect the ISO you downloaded previously, then hit “Upload”\n\nCreate VMs\n\nBefore starting, familiarise yourself with the system requirements for Talos and assign VM resources accordingly.\n\nCreate a new VM by clicking the “Create VM” button in the Proxmox UI:\n\nFill out a name for the new VM:\n\nIn the OS tab, select the ISO we uploaded earlier:\n\nKeep the defaults set in the “System” tab.\n\nKeep the defaults in the “Hard Disk” tab as well, only changing the size if desired.\n\nIn the “CPU” section, give at least 2 cores to the VM:\n\nNote: As of Talos v1.0 (which requires the x86-64-v2 microarchitecture), prior to Proxmox V8.0, booting with the default Processor Type kvm64 will not work. You can enable the required CPU features after creating the VM by adding the following line in the corresponding /etc/pve/qemu-server/<vmid>.conf file:\n\nargs: -cpu kvm64,+cx16,+lahf_lm,+popcnt,+sse3,+ssse3,+sse4.1,+sse4.2\n\n\nAlternatively, you can set the Processor Type to host if your Proxmox host supports these CPU features, this however prevents using live VM migration.\n\nVerify that the RAM is set to at least 2GB:\n\nKeep the default values for networking, verifying that the VM is set to come up on the bridge interface:\n\nFinish creating the VM by clicking through the “Confirm” tab and then “Finish”.\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nNote: Talos doesn’t support memory hot plugging, if creating the VM programmatically don’t enable memory hotplug on your Talos VM’s. Doing so will cause Talos to be unable to see all available memory and have insufficient memory to complete installation of the cluster.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”.\n\nWith DHCP server\n\nOnce the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nWithout DHCP server\n\nTo apply the machine configurations in maintenance mode, VM has to have IP on the network. So you can set it on boot time manually.\n\nPress e on the boot time. And set the IP parameters for the VM. Format is:\n\nip=<client-ip>:<srv-ip>:<gw-ip>:<netmask>:<host>:<device>:<autoconf>\n\n\nFor example $CONTROL_PLANE_IP will be 192.168.0.100 and gateway 192.168.0.1\n\nlinux /boot/vmlinuz init_on_alloc=1 slab_nomerge pti=on panic=0 consoleblank=0 printk.devkmsg=on earlyprintk=ttyS0 console=tty0 console=ttyS0 talos.platform=metal ip=192.168.0.100::192.168.0.1:255.255.255.0::eth0:off\n\n\nThen press Ctrl-x or F10\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nNote: The Talos config by default will install to /dev/sda. Depending on your setup the virtual disk may be mounted differently Eg: /dev/vda. You can check for disks running the following command:\n\ntalosctl disks --insecure --nodes $CONTROL_PLANE_IP\n\n\nUpdate controlplane.yaml and worker.yaml config files to point to the correct disk location.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the Proxmox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\ntalosctl bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig .\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the Proxmox UI.\n\n2.4 - Vagrant & Libvirt\nPre-requisities\nLinux OS\nVagrant installed\nvagrant-libvirt plugin installed\ntalosctl installed\nkubectl installed\nOverview\n\nWe will use Vagrant and its libvirt plugin to create a KVM-based cluster with 3 control plane nodes and 1 worker node.\n\nFor this, we will mount Talos ISO into the VMs using a virtual CD-ROM, and configure the VMs to attempt to boot from the disk first with the fallback to the CD-ROM.\n\nWe will also configure a virtual IP address on Talos to achieve high-availability on kube-apiserver.\n\nPreparing the environment\n\nFirst, we download the latest metal-amd64.iso ISO from GitHub releases into the /tmp directory.\n\nwget --timestamping https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -O /tmp/metal-amd64.iso\n\n\nCreate a Vagrantfile with the following contents:\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define \"control-plane-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-2\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-2.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-3\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-3.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"worker-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 1\n\n      domain.memory = 1024\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/worker-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\nend\n\nBring up the nodes\n\nCheck the status of vagrant VMs:\n\nvagrant status\n\n\nYou should see the VMs in “not created” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      not created (libvirt)\n\ncontrol-plane-node-2      not created (libvirt)\n\ncontrol-plane-node-3      not created (libvirt)\n\nworker-node-1             not created (libvirt)\n\n\nBring up the vagrant environment:\n\nvagrant up --provider=libvirt\n\n\nCheck the status again:\n\nvagrant status\n\n\nNow you should see the VMs in “running” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      running (libvirt)\n\ncontrol-plane-node-2      running (libvirt)\n\ncontrol-plane-node-3      running (libvirt)\n\nworker-node-1             running (libvirt)\n\n\nFind out the IP addresses assigned by the libvirt DHCP by running:\n\nvirsh list | grep vagrant | awk '{print $2}' | xargs -t -L1 virsh domifaddr\n\n\nOutput will look like the following:\n\nvirsh domifaddr vagrant_control-plane-node-2\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet0      52:54:00:f9:10:e5    ipv4         192.168.121.119/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet1      52:54:00:0f:ae:59    ipv4         192.168.121.203/24\n\n\n\nvirsh domifaddr vagrant_worker-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet2      52:54:00:6f:28:95    ipv4         192.168.121.69/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-3\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet3      52:54:00:03:45:10    ipv4         192.168.121.125/24\n\n\nOur control plane nodes have the IPs: 192.168.121.203, 192.168.121.119, 192.168.121.125 and the worker node has the IP 192.168.121.69.\n\nNow you should be able to interact with Talos nodes that are in maintenance mode:\n\ntalosctl -n 192.168.121.203 disks --insecure\n\n\nSample output:\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID   MODALIAS                    NAME   SIZE     BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      8.6 GB   /pci0000:00/0000:00:03.0/virtio0/\n\nInstalling Talos\n\nPick an endpoint IP in the vagrant-libvirt subnet but not used by any nodes, for example 192.168.121.100.\n\nGenerate a machine configuration:\n\ntalosctl gen config my-cluster https://192.168.121.100:6443 --install-disk /dev/vda\n\n\nEdit controlplane.yaml to add the virtual IP you picked to a network interface under .machine.network.interfaces, for example:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.121.100\n\n\nApply the configuration to the initial control plane node:\n\ntalosctl -n 192.168.121.203 apply-config --insecure --file controlplane.yaml\n\n\nYou can tail the logs of the node:\n\nsudo tail -f /tmp/control-plane-node-1.log\n\n\nSet up your shell to use the generated talosconfig and configure its endpoints (use the IPs of the control plane nodes):\n\nexport TALOSCONFIG=$(realpath ./talosconfig)\n\ntalosctl config endpoint 192.168.121.203 192.168.121.119 192.168.121.125\n\n\nBootstrap the Kubernetes cluster from the initial control plane node:\n\ntalosctl -n 192.168.121.203 bootstrap\n\n\nFinally, apply the machine configurations to the remaining nodes:\n\ntalosctl -n 192.168.121.119 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.125 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.69 apply-config --insecure --file worker.yaml\n\n\nAfter a while, you should see that all the members have joined:\n\ntalosctl -n 192.168.121.203 get members\n\n\nThe output will be like the following:\n\nNODE              NAMESPACE   TYPE     ID                      VERSION   HOSTNAME                MACHINE TYPE   OS               ADDRESSES\n\n192.168.121.203   cluster     Member   talos-192-168-121-119   1         talos-192-168-121-119   controlplane   Talos (v1.1.0)   [\"192.168.121.119\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-69    1         talos-192-168-121-69    worker         Talos (v1.1.0)   [\"192.168.121.69\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-203   6         talos-192-168-121-203   controlplane   Talos (v1.1.0)   [\"192.168.121.100\",\"192.168.121.203\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-125   1         talos-192-168-121-125   controlplane   Talos (v1.1.0)   [\"192.168.121.125\"]\n\nInteracting with Kubernetes cluster\n\nRetrieve the kubeconfig from the cluster:\n\ntalosctl -n 192.168.121.203 kubeconfig ./kubeconfig\n\n\nList the nodes in the cluster:\n\nkubectl --kubeconfig ./kubeconfig get node -owide\n\n\nYou will see an output similar to:\n\nNAME                    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-192-168-121-203   Ready    control-plane,master   3m10s   v1.24.2   192.168.121.203   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-69    Ready    <none>                 2m25s   v1.24.2   192.168.121.69    <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-119   Ready    control-plane,master   8m46s   v1.24.2   192.168.121.119   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-125   Ready    control-plane,master   3m11s   v1.24.2   192.168.121.125   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\n\nCongratulations, you have a highly-available Talos cluster running!\n\nCleanup\n\nYou can destroy the vagrant environment by running:\n\nvagrant destroy -f\n\n\nAnd remove the ISO image you downloaded:\n\nsudo rm -f /tmp/metal-amd64.iso\n\n2.5 - VMware\nCreating Talos Kubernetes cluster using VMware.\nCreating a Cluster via the govc CLI\n\nIn this guide we will create an HA Kubernetes cluster with 2 worker nodes. We will use the govc cli which can be downloaded here.\n\nPrereqs/Assumptions\n\nThis guide will use the virtual IP (“VIP”) functionality that is built into Talos in order to provide a stable, known IP for the Kubernetes control plane. This simply means the user should pick an IP on their “VM Network” to designate for this purpose and keep it handy for future steps.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the VIP chosen in the prereq steps, we will now generate the base configuration files for the Talos machines. This can be done with the talosctl gen config ... command. Take note that we will also use a JSON6902 patch when creating the configs so that the control plane nodes get some special information about the VIP we chose earlier, as well as a daemonset to install vmware tools on talos nodes.\n\nFirst, download cp.patch.yaml to your local machine and edit the VIP to match your chosen IP. You can do this by issuing: curl -fsSLO https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/cp.patch.yaml. It’s contents should look like the following:\n\n- op: add\n\n  path: /machine/network\n\n  value:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: <VIP>\n\n- op: replace\n\n  path: /cluster/extraManifests\n\n  value:\n\n    - \"https://raw.githubusercontent.com/mologie/talos-vmtoolsd/master/deploy/unstable.yaml\"\n\n\nWith the patch in hand, generate machine configs with:\n\n$ talosctl gen config vmware-test https://<VIP>:<port> --config-patch-control-plane @cp.patch.yaml\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking if needed. Optionally, you can specify additional patches by adding to the cp.patch.yaml file downloaded earlier, or create your own patch files.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nSet Environment Variables\n\ngovc makes use of the following environment variables\n\nexport GOVC_URL=<vCenter url>\n\nexport GOVC_USERNAME=<vCenter username>\n\nexport GOVC_PASSWORD=<vCenter password>\n\n\nNote: If your vCenter installation makes use of self signed certificates, you’ll want to export GOVC_INSECURE=true.\n\nThere are some additional variables that you may need to set:\n\nexport GOVC_DATACENTER=<vCenter datacenter>\n\nexport GOVC_RESOURCE_POOL=<vCenter resource pool>\n\nexport GOVC_DATASTORE=<vCenter datastore>\n\nexport GOVC_NETWORK=<vCenter network>\n\nChoose Install Approach\n\nAs part of this guide, we have a more automated install script that handles some of the complexity of importing OVAs and creating VMs. If you wish to use this script, we will detail that next. If you wish to carry out the manual approach, simply skip ahead to the “Manual Approach” section.\n\nScripted Install\n\nDownload the vmware.sh script to your local machine. You can do this by issuing curl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/vmware.sh\". This script has default variables for things like Talos version and cluster name that may be interesting to tweak before deploying.\n\nImport OVA\n\nTo create a content library and import the Talos OVA corresponding to the mentioned Talos version, simply issue:\n\n./vmware.sh upload_ova\n\nCreate Cluster\n\nWith the OVA uploaded to the content library, you can create a 5 node (by default) cluster with 3 control plane and 2 worker nodes:\n\n./vmware.sh create\n\n\nThis step will create a VM from the OVA, edit the settings based on the env variables used for VM size/specs, then power on the VMs.\n\nYou may now skip past the “Manual Approach” section down to “Bootstrap Cluster”.\n\nManual Approach\nImport the OVA into vCenter\n\nA talos.ova asset is published with each release. We will refer to the version of the release as $TALOS_VERSION below. It can be easily exported with export TALOS_VERSION=\"v0.3.0-alpha.10\" or similar.\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/$TALOS_VERSION/talos.ova\n\n\nCreate a content library (if needed) with:\n\ngovc library.create <library name>\n\n\nImport the OVA to the library with:\n\ngovc library.import -n talos-${TALOS_VERSION} <library name> /path/to/downloaded/talos.ova\n\nCreate the Bootstrap Node\n\nWe’ll clone the OVA to create the bootstrap node (our first control plane node).\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-1\n\n\nTalos makes use of the guestinfo facility of VMware to provide the machine/cluster configuration. This can be set using the govc vm.change command. To facilitate persistent storage using the vSphere cloud provider integration with Kubernetes, disk.enableUUID=1 is used.\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(cat controlplane.yaml | base64)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-1\n\nUpdate Hardware Resources for the Bootstrap Node\n-c is used to configure the number of cpus\n-m is used to configure the amount of memory (in MB)\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-1\n\n\nThe following can be used to adjust the EPHEMERAL disk size.\n\ngovc vm.disk.change -vm control-plane-1 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-1\n\nCreate the Remaining Control Plane Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-2\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-3\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-3\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-2\n\n\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-3\n\ngovc vm.disk.change -vm control-plane-2 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm control-plane-3 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-2\n\n\n\ngovc vm.power -on control-plane-3\n\nUpdate Settings for the Worker Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-1\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-1\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-2\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-1\n\n\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-2\n\ngovc vm.disk.change -vm worker-1 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm worker-2 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on worker-1\n\n\n\ngovc vm.power -on worker-2\n\nBootstrap Cluster\n\nIn the vSphere UI, open a console to one of the control plane nodes. You should see some output stating that etcd should be bootstrapped. This text should look like:\n\n\"etcd is waiting to join the cluster, if this node is the first node in the cluster, please run `talosctl bootstrap` against one of the following IPs:\n\n\nTake note of the IP mentioned here and issue:\n\ntalosctl --talosconfig talosconfig bootstrap -e <control plane IP> -n <control plane IP>\n\n\nKeep this IP handy for the following steps as well.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane IP>\n\ntalosctl --talosconfig talosconfig config node <control plane IP>\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nConfigure talos-vmtoolsd\n\nThe talos-vmtoolsd application was deployed as a daemonset as part of the cluster creation; however, we must now provide a talos credentials file for it to use.\n\nCreate a new talosconfig with:\n\ntalosctl --talosconfig talosconfig -n <control plane IP> config new vmtoolsd-secret.yaml --roles os:admin\n\n\nCreate a secret from the talosconfig:\n\nkubectl -n kube-system create secret generic talos-vmtoolsd-config \\\n\n  --from-file=talosconfig=./vmtoolsd-secret.yaml\n\n\nClean up the generated file from local system:\n\nrm vmtoolsd-secret.yaml\n\n\nOnce configured, you should now see these daemonset pods go into “Running” state and in vCenter, you will now see IPs and info from the Talos nodes present in the UI.\n\n2.6 - Xen\n\nTalos is known to work on Xen. We don’t yet have a documented guide specific to Xen; however, you can follow the General Getting Started Guide. If you run into any issues, our community can probably help!\n\n3 - Cloud Platforms\nInstallation of Talos Linux on many cloud platforms.\n3.1 - AWS\nCreating a cluster via the AWS CLI.\nCreating a Cluster via the AWS CLI\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing VPC, and some familiarity with AWS. If you need more information on AWS specifics, please see the official AWS documentation.\n\nSet the needed info\n\nChange to your desired region:\n\nREGION=\"us-west-2\"\n\naws ec2 describe-vpcs --region $REGION\n\n\n\nVPC=\"(the VpcId from the above command)\"\n\nCreate the Subnet\n\nUse a CIDR block that is present on the VPC specified above.\n\naws ec2 create-subnet \\\n\n    --region $REGION \\\n\n    --vpc-id $VPC \\\n\n    --cidr-block ${CIDR_BLOCK}\n\n\nNote the subnet ID that was returned, and assign it to a variable for ease of later use:\n\nSUBNET=\"(the subnet ID of the created subnet)\"\n\nOfficial AMI Images\n\nOfficial AMI image ID can be found in the cloud-images.json file attached to the Talos release:\n\nAMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.6.2/cloud-images.json | \\\n\n    jq -r '.[] | select(.region == \"'$REGION'\") | select (.arch == \"amd64\") | .id'`\n\necho $AMI\n\n\nReplace amd64 in the line above with the desired architecture. Note the AMI id that is returned is assigned to an environment variable: it will be used later when booting instances.\n\nIf using the official AMIs, you can skip to Creating the Security group\n\nCreate your own AMIs\n\nThe use of the official Talos AMIs are recommended, but if you wish to build your own AMIs, follow the procedure below.\n\nCreate the S3 Bucket\naws s3api create-bucket \\\n\n    --bucket $BUCKET \\\n\n    --create-bucket-configuration LocationConstraint=$REGION \\\n\n    --acl private\n\nCreate the vmimport Role\n\nIn order to create an AMI, ensure that the vmimport role exists as described in the official AWS documentation.\n\nNote that the role should be associated with the S3 bucket we created above.\n\nCreate the Image Snapshot\n\nFirst, download the AWS image from a Talos release:\n\ncurl -L https://github.com/siderolabs/talos/releases/download/v1.6.2/aws-amd64.raw.xz | xz -d > disk.raw\n\n\nCopy the RAW disk to S3 and import it as a snapshot:\n\naws s3 cp disk.raw s3://$BUCKET/talos-aws-tutorial.raw\n\naws ec2 import-snapshot \\\n\n    --region $REGION \\\n\n    --description \"Talos kubernetes tutorial\" \\\n\n    --disk-container \"Format=raw,UserBucket={S3Bucket=$BUCKET,S3Key=talos-aws-tutorial.raw}\"\n\n\nSave the SnapshotId, as we will need it once the import is done. To check on the status of the import, run:\n\naws ec2 describe-import-snapshot-tasks \\\n\n    --region $REGION \\\n\n    --import-task-ids\n\n\nOnce the SnapshotTaskDetail.Status indicates completed, we can register the image.\n\nRegister the Image\naws ec2 register-image \\\n\n    --region $REGION \\\n\n    --block-device-mappings \"DeviceName=/dev/xvda,VirtualName=talos,Ebs={DeleteOnTermination=true,SnapshotId=$SNAPSHOT,VolumeSize=4,VolumeType=gp2}\" \\\n\n    --root-device-name /dev/xvda \\\n\n    --virtualization-type hvm \\\n\n    --architecture x86_64 \\\n\n    --ena-support \\\n\n    --name talos-aws-tutorial-ami\n\n\nWe now have an AMI we can use to create our cluster. Save the AMI ID, as we will need it when we create EC2 instances.\n\nAMI=\"(AMI ID of the register image command)\"\n\nCreate a Security Group\naws ec2 create-security-group \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --description \"Security Group for EC2 instances to allow ports required by Talos\"\n\n\n\nSECURITY_GROUP=\"(security group id that is returned)\"\n\n\nUsing the security group from above, allow all internal traffic within the same security group:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol all \\\n\n    --port 0 \\\n\n    --source-group talos-aws-tutorial-sg\n\n\nand expose the Talos and Kubernetes APIs:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 6443 \\\n\n    --cidr 0.0.0.0/0\n\n\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 50000-50001 \\\n\n    --cidr 0.0.0.0/0\n\n\nIf you are using KubeSpan and will be adding workers outside of AWS, you need to allow inbound UDP for the Wireguard port:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol udp --port 51820 --cidr 0.0.0.0/0\n\nCreate a Load Balancer\naws elbv2 create-load-balancer \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-lb \\\n\n    --type network --subnets $SUBNET\n\n\nTake note of the DNS name and ARN. We will need these soon.\n\nLOAD_BALANCER_ARN=\"(arn of the load balancer)\"\n\naws elbv2 create-target-group \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-tg \\\n\n    --protocol TCP \\\n\n    --port 6443 \\\n\n    --target-type ip \\\n\n    --vpc-id $VPC\n\n\nAlso note the TargetGroupArn that is returned.\n\nTARGET_GROUP_ARN=\"(target group arn)\"\n\nCreate the Machine Configuration Files\n\nUsing the DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines.\n\nNote that the port used here is the externally accessible port configured on the load balancer - 443 - not the internal port of 6443:\n\n$ talosctl gen config talos-k8s-aws-tutorial https://<load balancer DNS>:<port> --with-examples=false --with-docs=false\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nNote that the generated configs are too long for AWS userdata field if the --with-examples and --with-docs flags are not passed.\n\nAt this point, you can modify the generated configs to your liking.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the EC2 Instances\n\nchange the instance type if desired. Note: There is a known issue that prevents Talos from running on T2 instance types. Please use T3 if you need burstable instance types.\n\nCreate the Control Plane Nodes\nCP_COUNT=1\n\nwhile [[ \"$CP_COUNT\" -lt 4 ]]; do\n\n  aws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 1 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://controlplane.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP \\\n\n    --associate-public-ip-address \\\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\"\n\n  ((CP_COUNT++))\n\ndone\n\n\nMake a note of the resulting PrivateIpAddress from the controlplane nodes for later use.\n\nCreate the Worker Nodes\naws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 3 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://worker.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\"\n\nConfigure the Load Balancer\n\nNow, using the load balancer target group’s ARN, and the PrivateIpAddress from the controlplane instances that you created :\n\naws elbv2 register-targets \\\n\n    --region $REGION \\\n\n    --target-group-arn $TARGET_GROUP_ARN \\\n\n    --targets Id=$CP_NODE_1_IP  Id=$CP_NODE_2_IP  Id=$CP_NODE_3_IP\n\n\nUsing the ARNs of the load balancer and target group from previous steps, create the listener:\n\naws elbv2 create-listener \\\n\n    --region $REGION \\\n\n    --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n    --protocol TCP \\\n\n    --port 443 \\\n\n    --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN\n\nBootstrap Etcd\n\nSet the endpoints (the control plane node to which talosctl commands are sent) and nodes (the nodes that the command operates on):\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 PUBLIC IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 PUBLIC IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nThe different control plane nodes should sendi/receive traffic via the load balancer, notice that one of the control plane has intiated the etcd cluster, and the others should join. You can now watch as your cluster bootstraps, by using\n\ntalosctl --talosconfig talosconfig  health\n\n\nYou can also watch the performance of a node, via:\n\ntalosctl  --talosconfig talosconfig dashboard\n\n\nAnd use standard kubectl commands.\n\n3.2 - Azure\nCreating a cluster via the CLI on Azure.\nCreating a Cluster via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node. We assume existing Blob Storage, and some familiarity with Azure. If you need more information on Azure specifics, please see the official Azure documentation.\n\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_ACCOUNT=\"StorageAccountName\"\n\n\n\n# Storage container to upload to\n\nexport STORAGE_CONTAINER=\"StorageContainerName\"\n\n\n\n# Resource group name\n\nexport GROUP=\"ResourceGroupName\"\n\n\n\n# Location\n\nexport LOCATION=\"centralus\"\n\n\n\n# Get storage account connection string based on info above\n\nexport CONNECTION=$(az storage account show-connection-string \\\n\n                    -n $STORAGE_ACCOUNT \\\n\n                    -g $GROUP \\\n\n                    -o tsv)\n\nChoose an Image\n\nThere are two methods of deployment in this tutorial.\n\nIf you would like to use the official Talos image uploaded to Azure Community Galleries by SideroLabs, you may skip ahead to setting up your network infrastructure.\n\nNetwork Infrastructure\n\nOtherwise, if you would like to upload your own image to Azure and use it to deploy Talos, continue to Creating an Image.\n\nCreate the Image\n\nFirst, download the Azure image from a Talos release. Once downloaded, untar with tar -xvf /path/to/azure-amd64.tar.gz\n\nUpload the VHD\n\nOnce you have pulled down the image, you can upload it to blob storage with:\n\naz storage blob upload \\\n\n  --connection-string $CONNECTION \\\n\n  --container-name $STORAGE_CONTAINER \\\n\n  -f /path/to/extracted/talos-azure.vhd \\\n\n  -n talos-azure.vhd\n\nRegister the Image\n\nNow that the image is present in our blob storage, we’ll register it.\n\naz image create \\\n\n  --name talos \\\n\n  --source https://$STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER/talos-azure.vhd \\\n\n  --os-type linux \\\n\n  -g $GROUP\n\nNetwork Infrastructure\nVirtual Networks and Security Groups\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a network security group and add rules to it.\n\n# Create vnet\n\naz network vnet create \\\n\n  --resource-group $GROUP \\\n\n  --location $LOCATION \\\n\n  --name talos-vnet \\\n\n  --subnet-name talos-subnet\n\n\n\n# Create network security group\n\naz network nsg create -g $GROUP -n talos-sg\n\n\n\n# Client -> apid\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n apid \\\n\n  --priority 1001 \\\n\n  --destination-port-ranges 50000 \\\n\n  --direction inbound\n\n\n\n# Trustd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n trustd \\\n\n  --priority 1002 \\\n\n  --destination-port-ranges 50001 \\\n\n  --direction inbound\n\n\n\n# etcd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n etcd \\\n\n  --priority 1003 \\\n\n  --destination-port-ranges 2379-2380 \\\n\n  --direction inbound\n\n\n\n# Kubernetes API Server\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n kube \\\n\n  --priority 1004 \\\n\n  --destination-port-ranges 6443 \\\n\n  --direction inbound\n\nLoad Balancer\n\nWe will create a public ip, load balancer, and a health check that we will use for our control plane.\n\n# Create public ip\n\naz network public-ip create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-public-ip \\\n\n  --allocation-method static\n\n\n\n# Create lb\n\naz network lb create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-lb \\\n\n  --public-ip-address talos-public-ip \\\n\n  --frontend-ip-name talos-fe \\\n\n  --backend-pool-name talos-be-pool\n\n\n\n# Create health check\n\naz network lb probe create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-lb-health \\\n\n  --protocol tcp \\\n\n  --port 6443\n\n\n\n# Create lb rule for 6443\n\naz network lb rule create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-6443 \\\n\n  --protocol tcp \\\n\n  --frontend-ip-name talos-fe \\\n\n  --frontend-port 6443 \\\n\n  --backend-pool-name talos-be-pool \\\n\n  --backend-port 6443 \\\n\n  --probe-name talos-lb-health\n\nNetwork Interfaces\n\nIn Azure, we have to pre-create the NICs for our control plane so that they can be associated with our load balancer.\n\nfor i in $( seq 0 1 2 ); do\n\n  # Create public IP for each nic\n\n  az network public-ip create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-public-ip-$i \\\n\n    --allocation-method static\n\n\n\n\n\n  # Create nic\n\n  az network nic create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-nic-$i \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --network-security-group talos-sg \\\n\n    --public-ip-address talos-controlplane-public-ip-$i\\\n\n    --lb-name talos-lb \\\n\n    --lb-address-pools talos-be-pool\n\ndone\n\n\n\n# NOTES:\n\n# Talos can detect PublicIPs automatically if PublicIP SKU is Basic.\n\n# Use `--sku Basic` to set SKU to Basic.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(az network public-ip show \\\n\n              --resource-group $GROUP \\\n\n              --name talos-public-ip \\\n\n              --query \"ipAddress\" \\\n\n              --output tsv)\n\n\n\ntalosctl gen config talos-k8s-azure-tutorial https://${LB_PUBLIC_IP}:6443\n\nCompute Creation\n\nWe are now ready to create our azure nodes. Azure allows you to pass Talos machine configuration to the virtual machine at bootstrap time via user-data or custom-data methods.\n\nTalos supports only custom-data method, machine configuration is available to the VM only on the first boot.\n\nUse the steps below depending on whether you have manually uploaded a Talos image or if you are using the Community Gallery image.\n\nManual Image Upload\nAzure Community Gallery Image\nManual Image Upload\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image talos \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image talos \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nAzure Community Gallery Image\n\nTalos is updated in Azure’s Community Galleries (Preview) on every release.\n\nTo use the Talos image for the current release create the following environment variables.\n\nEdit the variables below if you would like to use a different architecture or version.\n\n# The architecture you would like to use. Options are \"talos-x64\" or \"talos-arm64\"\n\nARCHITECTURE=\"talos-x64\"\n\n\n\n# This will use the latest version of Talos. The version must be \"latest\" or in the format Major(int).Minor(int).Patch(int), e.g. 1.5.0\n\nVERSION=\"latest\"\n\n\nCreate the Virtual Machines.\n\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(az network public-ip show \\\n\n                    --resource-group $GROUP \\\n\n                    --name talos-controlplane-public-ip-0 \\\n\n                    --query \"ipAddress\" \\\n\n                    --output tsv)\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3.3 - DigitalOcean\nCreating a cluster via the CLI on DigitalOcean.\nCreating a Talos Linux Cluster on Digital Ocean via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node, in the NYC region. We assume an existing Space, and some familiarity with DigitalOcean. If you need more information on DigitalOcean specifics, please see the official DigitalOcean documentation.\n\nCreate the Image\n\nDownload the DigitalOcean image digital-ocean-amd64.raw.gz from the latest Talos release.\n\nNote: the minimum version of Talos required to support Digital Ocean is v1.3.3.\n\nUsing an upload method of your choice (doctl does not have Spaces support), upload the image to a space. (It’s easy to drag the image file to the space using DigitalOcean’s web console.)\n\nNote: Make sure you upload the file as public.\n\nNow, create an image using the URL of the uploaded image:\n\nexport REGION=nyc3\n\n\n\ndoctl compute image create \\\n\n    --region $REGION \\\n\n    --image-description talos-digital-ocean-tutorial \\\n\n    --image-url https://$SPACENAME.$REGION.digitaloceanspaces.com/digital-ocean-amd64.raw.gz \\\n\n    Talos\n\n\nSave the image ID. We will need it when creating droplets.\n\nCreate a Load Balancer\ndoctl compute load-balancer create \\\n\n    --region $REGION \\\n\n    --name talos-digital-ocean-tutorial-lb \\\n\n    --tag-name talos-digital-ocean-tutorial-control-plane \\\n\n    --health-check protocol:tcp,port:6443,check_interval_seconds:10,response_timeout_seconds:5,healthy_threshold:5,unhealthy_threshold:3 \\\n\n    --forwarding-rules entry_protocol:tcp,entry_port:443,target_protocol:tcp,target_port:6443\n\n\nNote the returned ID of the load balancer.\n\nWe will need the IP of the load balancer. Using the ID of the load balancer, run:\n\ndoctl compute load-balancer get --format IP <load balancer ID>\n\n\nNote that it may take a few minutes before the load balancer is provisioned, so repeat this command until it returns with the IP address.\n\nCreate the Machine Configuration Files\n\nUsing the IP address (or DNS name, if you have created one) of the loadbalancer, generate the base configuration files for the Talos machines. Also note that the load balancer forwards port 443 to port 6443 on the associated nodes, so we should use 443 as the port in the config definition:\n\n$ talosctl gen config talos-k8s-digital-ocean-tutorial https://<load balancer IP or DNS>:443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCreate the Droplets\nCreate a dummy SSH key\n\nAlthough SSH is not used by Talos, DigitalOcean requires that an SSH key be associated with a droplet during creation. We will create a dummy key that can be used to satisfy this requirement.\n\ndoctl compute ssh-key create --public-key \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDbl0I1s/yOETIKjFr7mDLp8LmJn6OIZ68ILjVCkoN6lzKmvZEqEm1YYeWoI0xgb80hQ1fKkl0usW6MkSqwrijoUENhGFd6L16WFL53va4aeJjj2pxrjOr3uBFm/4ATvIfFTNVs+VUzFZ0eGzTgu1yXydX8lZMWnT4JpsMraHD3/qPP+pgyNuI51LjOCG0gVCzjl8NoGaQuKnl8KqbSCARIpETg1mMw+tuYgaKcbqYCMbxggaEKA0ixJ2MpFC/kwm3PcksTGqVBzp3+iE5AlRe1tnbr6GhgT839KLhOB03j7lFl1K9j1bMTOEj5Io8z7xo/XeF2ZQKHFWygAJiAhmKJ dummy@dummy.local\" dummy\n\n\nNote the ssh key ID that is returned - we will use it in creating the droplets.\n\nCreate the Control Plane Nodes\n\nRun the following commands to create three control plane nodes:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-1\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-2\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-3\n\n\nNote the droplet ID returned for the first control plane node.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --user-data-file worker.yaml \\\n\n    --ssh-keys <ssh key ID>  \\\n\n    talos-worker-1\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\ndoctl compute droplet get --format PublicIPv4 <droplet ID>\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nWe can also watch the cluster bootstrap via:\n\ntalosctl --talosconfig talosconfig health\n\n3.4 - Exoscale\nCreating a cluster via the CLI using exoscale.com\n\nTalos is known to work on exoscale.com; however, it is currently undocumented.\n\n3.5 - GCP\nCreating a cluster via the CLI on Google Cloud Platform.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in GCP with 1 worker node. We will assume an existing Cloud Storage bucket, and some familiarity with Google Cloud. If you need more information on Google Cloud specifics, please see the official Google documentation.\n\njq and talosctl also needs to be installed\n\nManual Setup\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_BUCKET=\"StorageBucketName\"\n\n# Region\n\nexport REGION=\"us-central1\"\n\nCreate the Image\n\nFirst, download the Google Cloud image from a Talos release. These images are called gcp-$ARCH.tar.gz.\n\nUpload the Image\n\nOnce you have downloaded the image, you can upload it to your storage bucket with:\n\ngsutil cp /path/to/gcp-amd64.tar.gz gs://$STORAGE_BUCKET\n\nRegister the image\n\nNow that the image is present in our bucket, we’ll register it.\n\ngcloud compute images create talos \\\n\n --source-uri=gs://$STORAGE_BUCKET/gcp-amd64.tar.gz \\\n\n --guest-os-features=VIRTIO_SCSI_MULTIQUEUE\n\nNetwork Infrastructure\nLoad Balancers and Firewalls\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a firewall, load balancer, and their required components.\n\n130.211.0.0/22 and 35.191.0.0/16 are the GCP Load Balancer IP ranges\n\n# Create Instance Group\n\ngcloud compute instance-groups unmanaged create talos-ig \\\n\n  --zone $REGION-b\n\n\n\n# Create port for IG\n\ngcloud compute instance-groups set-named-ports talos-ig \\\n\n    --named-ports tcp6443:6443 \\\n\n    --zone $REGION-b\n\n\n\n# Create health check\n\ngcloud compute health-checks create tcp talos-health-check --port 6443\n\n\n\n# Create backend\n\ngcloud compute backend-services create talos-be \\\n\n    --global \\\n\n    --protocol TCP \\\n\n    --health-checks talos-health-check \\\n\n    --timeout 5m \\\n\n    --port-name tcp6443\n\n\n\n# Add instance group to backend\n\ngcloud compute backend-services add-backend talos-be \\\n\n    --global \\\n\n    --instance-group talos-ig \\\n\n    --instance-group-zone $REGION-b\n\n\n\n# Create tcp proxy\n\ngcloud compute target-tcp-proxies create talos-tcp-proxy \\\n\n    --backend-service talos-be \\\n\n    --proxy-header NONE\n\n\n\n# Create LB IP\n\ngcloud compute addresses create talos-lb-ip --global\n\n\n\n# Forward 443 from LB IP to tcp proxy\n\ngcloud compute forwarding-rules create talos-fwd-rule \\\n\n    --global \\\n\n    --ports 443 \\\n\n    --address talos-lb-ip \\\n\n    --target-tcp-proxy talos-tcp-proxy\n\n\n\n# Create firewall rule for health checks\n\ngcloud compute firewall-rules create talos-controlplane-firewall \\\n\n     --source-ranges 130.211.0.0/22,35.191.0.0/16 \\\n\n     --target-tags talos-controlplane \\\n\n     --allow tcp:6443\n\n\n\n# Create firewall rule to allow talosctl access\n\ngcloud compute firewall-rules create talos-controlplane-talosctl \\\n\n  --source-ranges 0.0.0.0/0 \\\n\n  --target-tags talos-controlplane \\\n\n  --allow tcp:50000\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(gcloud compute forwarding-rules describe talos-fwd-rule \\\n\n               --global \\\n\n               --format json \\\n\n               | jq -r .IPAddress)\n\n\n\ntalosctl gen config talos-k8s-gcp-tutorial https://${LB_PUBLIC_IP}:443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our GCP nodes.\n\n# Create the control plane nodes.\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instances create talos-controlplane-$i \\\n\n    --image talos \\\n\n    --zone $REGION-b \\\n\n    --tags talos-controlplane \\\n\n    --boot-disk-size 20GB \\\n\n    --metadata-from-file=user-data=./controlplane.yaml\n\n    --tags talos-controlplane-$i\n\ndone\n\n\n\n# Add control plane nodes to instance group\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instance-groups unmanaged add-instances talos-ig \\\n\n      --zone $REGION-b \\\n\n      --instances talos-controlplane-$i\n\ndone\n\n\n\n# Create worker\n\ngcloud compute instances create talos-worker-0 \\\n\n  --image talos \\\n\n  --zone $REGION-b \\\n\n  --boot-disk-size 20GB \\\n\n  --metadata-from-file=user-data=./worker.yaml\n\n  --tags talos-worker-$i\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(gcloud compute instances describe talos-controlplane-0 \\\n\n                     --zone $REGION-b \\\n\n                     --format json \\\n\n                     | jq -r '.networkInterfaces[0].accessConfigs[0].natIP')\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nCleanup\n# cleanup VM's\n\ngcloud compute instances delete \\\n\n  talos-worker-0 \\\n\n  talos-controlplane-0 \\\n\n  talos-controlplane-1 \\\n\n  talos-controlplane-2\n\n\n\n# cleanup firewall rules\n\ngcloud compute firewall-rules delete \\\n\n  talos-controlplane-talosctl \\\n\n  talos-controlplane-firewall\n\n\n\n# cleanup forwarding rules\n\ngcloud compute forwarding-rules delete \\\n\n  talos-fwd-rule\n\n\n\n# cleanup addresses\n\ngcloud compute addresses delete \\\n\n  talos-lb-ip\n\n\n\n# cleanup proxies\n\ngcloud compute target-tcp-proxies delete \\\n\n  talos-tcp-proxy\n\n\n\n# cleanup backend services\n\ngcloud compute backend-services delete \\\n\n  talos-be\n\n\n\n# cleanup health checks\n\ngcloud compute health-checks delete \\\n\n  talos-health-check\n\n\n\n# cleanup unmanaged instance groups\n\ngcloud compute instance-groups unmanaged delete \\\n\n  talos-ig\n\n\n\n# cleanup Talos image\n\ngcloud compute images delete \\\n\n  talos\n\nUsing GCP Deployment manager\n\nUsing GCP deployment manager automatically creates a Google Storage bucket and uploads the Talos image to it. Once the deployment is complete the generated talosconfig and kubeconfig files are uploaded to the bucket.\n\nBy default this setup creates a three node control plane and a single worker in us-west1-b\n\nFirst we need to create a folder to store our deployment manifests and perform all subsequent operations from that folder.\n\nmkdir -p talos-gcp-deployment\n\ncd talos-gcp-deployment\n\nGetting the deployment manifests\n\nWe need to download two deployment manifests for the deployment from the Talos github repository.\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/config.yaml\"\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/talos-ha.jinja\"\n\n# if using ccm\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/gcp-ccm.yaml\"\n\nUpdating the config\n\nNow we need to update the local config.yaml file with any required changes such as changing the default zone, Talos version, machine sizes, nodes count etc.\n\nAn example config.yaml file is shown below:\n\nimports:\n\n  - path: talos-ha.jinja\n\n\n\nresources:\n\n  - name: talos-ha\n\n    type: talos-ha.jinja\n\n    properties:\n\n      zone: us-west1-b\n\n      talosVersion: v1.6.2\n\n      externalCloudProvider: false\n\n      controlPlaneNodeCount: 5\n\n      controlPlaneNodeType: n1-standard-1\n\n      workerNodeCount: 3\n\n      workerNodeType: n1-standard-1\n\noutputs:\n\n  - name: bucketName\n\n    value: $(ref.talos-ha.bucketName)\n\nEnabling external cloud provider\n\nNote: The externalCloudProvider property is set to false by default. The manifest used for deploying the ccm (cloud controller manager) is currently using the GCP ccm provided by openshift since there are no public images for the ccm yet.\n\nSince the routes controller is disabled while deploying the CCM, the CNI pods needs to be restarted after the CCM deployment is complete to remove the node.kubernetes.io/network-unavailable taint. See Nodes network-unavailable taint not removed after installing ccm for more information\n\nUse a custom built image for the ccm deployment if required.\n\nCreating the deployment\n\nNow we are ready to create the deployment. Confirm with y for any prompts. Run the following command to create the deployment:\n\n# use a unique name for the deployment, resources are prefixed with the deployment name\n\nexport DEPLOYMENT_NAME=\"<deployment name>\"\n\ngcloud deployment-manager deployments create \"${DEPLOYMENT_NAME}\" --config config.yaml\n\nRetrieving the outputs\n\nFirst we need to get the deployment outputs.\n\n# first get the outputs\n\nOUTPUTS=$(gcloud deployment-manager deployments describe \"${DEPLOYMENT_NAME}\" --format json | jq '.outputs[]')\n\n\n\nBUCKET_NAME=$(jq -r '. | select(.name == \"bucketName\").finalValue' <<< \"${OUTPUTS}\")\n\n# used when cloud controller is enabled\n\nSERVICE_ACCOUNT=$(jq -r '. | select(.name == \"serviceAccount\").finalValue' <<< \"${OUTPUTS}\")\n\nPROJECT=$(jq -r '. | select(.name == \"project\").finalValue' <<< \"${OUTPUTS}\")\n\n\nNote: If cloud controller manager is enabled, the below command needs to be run to allow the controller custom role to access cloud resources\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\nDownloading talos and kube config\n\nIn addition to the talosconfig and kubeconfig files, the storage bucket contains the controlplane.yaml and worker.yaml files used to join additional nodes to the cluster.\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/talosconfig\" .\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/kubeconfig\" .\n\nDeploying the cloud controller manager\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  apply \\\n\n  --filename gcp-ccm.yaml\n\n#  wait for the ccm to be up\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset cloud-controller-manager\n\n\nIf the cloud controller manager is enabled, we need to restart the CNI pods to remove the node.kubernetes.io/network-unavailable taint.\n\n# restart the CNI pods, in this case flannel\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout restart \\\n\n  daemonset kube-flannel\n\n# wait for the pods to be restarted\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset kube-flannel\n\nCheck cluster status\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  get nodes\n\nCleanup deployment\n\nWarning: This will delete the deployment and all resources associated with it.\n\nRun below if cloud controller manager is enabled\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\n\nNow we can finally remove the deployment\n\n# delete the objects in the bucket first\n\ngsutil -m rm -r \"gs://${BUCKET_NAME}\"\n\ngcloud deployment-manager deployments delete \"${DEPLOYMENT_NAME}\" --quiet\n\n3.6 - Hetzner\nCreating a cluster via the CLI (hcloud) on Hetzner.\nUpload image\n\nHetzner Cloud does not support uploading custom images. You can email their support to get a Talos ISO uploaded by following issues:3599 or you can prepare image snapshot by yourself.\n\nThere are two options to upload your own.\n\nRun an instance in rescue mode and replace the system OS with the Talos image\nUse Hashicorp packer to prepare an image\nRescue mode\n\nCreate a new Server in the Hetzner console. Enable the Hetzner Rescue System for this server and reboot. Upon a reboot, the server will boot a special minimal Linux distribution designed for repair and reinstall. Once running, login to the server using ssh to prepare the system disk by doing the following:\n\n# Check that you in Rescue mode\n\ndf\n\n\n\n### Result is like:\n\n# udev                   987432         0    987432   0% /dev\n\n# 213.133.99.101:/nfs 308577696 247015616  45817536  85% /root/.oldroot/nfs\n\n# overlay                995672      8340    987332   1% /\n\n# tmpfs                  995672         0    995672   0% /dev/shm\n\n# tmpfs                  398272       572    397700   1% /run\n\n# tmpfs                    5120         0      5120   0% /run/lock\n\n# tmpfs                  199132         0    199132   0% /run/user/0\n\n\n\n# Download the Talos image\n\ncd /tmp\n\nwget -O /tmp/talos.raw.xz https://github.com/siderolabs/talos/releases/download/v1.6.2/hcloud-amd64.raw.xz\n\n# Replace system\n\nxz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\n\n# shutdown the instance\n\nshutdown -h now\n\n\nTo make sure disk content is consistent, it is recommended to shut the server down before taking an image (snapshot). Once shutdown, simply create an image (snapshot) from the console. You can now use this snapshot to run Talos on the cloud.\n\nPacker\n\nInstall packer to the local machine.\n\nCreate a config file for packer to use:\n\n# hcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    hcloud = {\n\n      source  = \"github.com/hetznercloud/hcloud\"\n\n      version = \"~> 1\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nvariable \"arch\" {\n\n  type    = string\n\n  default = \"amd64\"\n\n}\n\n\n\nvariable \"server_type\" {\n\n  type    = string\n\n  default = \"cx11\"\n\n}\n\n\n\nvariable \"server_location\" {\n\n  type    = string\n\n  default = \"hel1\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/hcloud-${var.arch}.raw.xz\"\n\n}\n\n\n\nsource \"hcloud\" \"talos\" {\n\n  rescue       = \"linux64\"\n\n  image        = \"debian-11\"\n\n  location     = \"${var.server_location}\"\n\n  server_type  = \"${var.server_type}\"\n\n  ssh_username = \"root\"\n\n\n\n  snapshot_name   = \"talos system disk - ${var.arch} - ${var.talos_version}\"\n\n  snapshot_labels = {\n\n    type    = \"infra\",\n\n    os      = \"talos\",\n\n    version = \"${var.talos_version}\",\n\n    arch    = \"${var.arch}\",\n\n  }\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.hcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget\",\n\n      \"wget -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\",\n\n    ]\n\n  }\n\n}\n\n\nAdditionally you could create a file containing\n\narch            = \"arm64\"\n\nserver_type     = \"cax11\"\n\nserver_location = \"fsn1\"\n\n\nand build the snapshot for arm64.\n\nCreate a new image by issuing the commands shown below. Note that to create a new API token for your Project, switch into the Hetzner Cloud Console choose a Project, go to Access → Security, and create a new token.\n\n# First you need set API Token\n\nexport HCLOUD_TOKEN=${TOKEN}\n\n\n\n# Upload image\n\npacker init .\n\npacker build .\n\n# Save the image ID\n\nexport IMAGE_ID=<image-id-in-packer-output>\n\n\nAfter doing this, you can find the snapshot in the console interface.\n\nCreating a Cluster via the CLI\n\nThis section assumes you have the hcloud console utility on your local machine.\n\n# Set hcloud context and api key\n\nhcloud context create talos-tutorial\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nhcloud load-balancer create --name controlplane --network-zone eu-central --type lb11 --label 'type=controlplane'\n\n\n\n### Result is like:\n\n# LoadBalancer 484487 created\n\n# IPv4: 49.12.X.X\n\n# IPv6: 2a01:4f8:X:X::1\n\n\n\nhcloud load-balancer add-service controlplane \\\n\n    --listen-port 6443 --destination-port 6443 --protocol tcp\n\nhcloud load-balancer add-target controlplane \\\n\n    --label-selector 'type=controlplane'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-hcloud-tutorial https://<load balancer IP or DNS>:6443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\n\nWe can now create our servers. Note that you can find IMAGE_ID in the snapshot section of the console: https://console.hetzner.cloud/projects/$PROJECT_ID/servers/snapshots.\n\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport IMAGE_ID=<your-image-id>\n\n\n\nhcloud server create --name talos-control-plane-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-2 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location fsn1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-3 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location nbg1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nhcloud server create --name talos-worker-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=worker' \\\n\n    --user-data-from-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nhcloud server list | grep talos-control-plane\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <control-plane-1-IP>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3.7 - Nocloud\nCreating a cluster via the CLI using qemu.\n\nTalos supports nocloud data source implementation.\n\nThere are two ways to configure Talos server with nocloud platform:\n\nvia SMBIOS “serial number” option\nusing CDROM or USB-flash filesystem\n\nNote: This requires the nocloud image which can be found on the Github Releases page.\n\nSMBIOS Serial Number\n\nThis method requires the network connection to be up (e.g. via DHCP). Configuration is delivered from the HTTP server.\n\nds=nocloud-net;s=http://10.10.0.1/configs/;h=HOSTNAME\n\n\nAfter the network initialization is complete, Talos fetches:\n\nthe machine config from http://10.10.0.1/configs/user-data\nthe network config (if available) from http://10.10.0.1/configs/network-config\nSMBIOS: QEMU\n\nAdd the following flag to qemu command line when starting a VM:\n\nqemu-system-x86_64 \\\n\n  ...\\\n\n  -smbios type=1,serial=ds=nocloud-net;s=http://10.10.0.1/configs/\n\nSMBIOS: Proxmox\n\nSet the source machine config through the serial number on Proxmox GUI.\n\nThe Proxmox stores the VM config at /etc/pve/qemu-server/$ID.conf ($ID - VM ID number of virtual machine), you will see something like:\n\n...\nsmbios1: uuid=ceae4d10,serial=ZHM9bm9jbG91ZC1uZXQ7cz1odHRwOi8vMTAuMTAuMC4xL2NvbmZpZ3Mv,base64=1\n...\n\n\nWhere serial holds the base64-encoded string version of ds=nocloud-net;s=http://10.10.0.1/configs/.\n\nCDROM/USB\n\nTalos can also get machine config from local attached storage without any prior network connection being established.\n\nYou can provide configs to the server via files on a VFAT or ISO9660 filesystem. The filesystem volume label must be cidata or CIDATA.\n\nExample: QEMU\n\nCreate and prepare Talos machine config:\n\nexport CONTROL_PLANE_IP=192.168.1.10\n\n\n\ntalosctl gen config talos-nocloud https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nPrepare cloud-init configs:\n\nmkdir -p iso\n\nmv _out/controlplane.yaml iso/user-data\n\necho \"local-hostname: controlplane-1\" > iso/meta-data\n\ncat > iso/network-config << EOF\n\nversion: 1\n\nconfig:\n\n   - type: physical\n\n     name: eth0\n\n     mac_address: \"52:54:00:12:34:00\"\n\n     subnets:\n\n        - type: static\n\n          address: 192.168.1.10\n\n          netmask: 255.255.255.0\n\n          gateway: 192.168.1.254\n\nEOF\n\n\nCreate cloud-init iso image\n\ncd iso && genisoimage -output cidata.iso -V cidata -r -J user-data meta-data network-config\n\n\nStart the VM\n\nqemu-system-x86_64 \\\n\n    ...\n\n    -cdrom iso/cidata.iso \\\n\n    ...\n\nExample: Proxmox\n\nProxmox can create cloud-init disk for you. Edit the cloud-init config information in Proxmox as follows, substitute your own information as necessary:\n\nand then update cicustom param at /etc/pve/qemu-server/$ID.conf.\n\ncicustom: user=local:snippets/controlplane-1.yml\nipconfig0: ip=192.168.1.10/24,gw=192.168.10.254\nnameserver: 1.1.1.1\nsearchdomain: local\n\n\nNote: snippets/controlplane-1.yml is Talos machine config. It is usually located at /var/lib/vz/snippets/controlplane-1.yml. This file must be placed to this path manually, as Proxmox does not support snippet uploading via API/GUI.\n\nClick on Regenerate Image button after the above changes are made.\n\n3.8 - Openstack\nCreating a cluster via the CLI on Openstack.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in Openstack with 1 worker node. We will assume an existing some familiarity with Openstack. If you need more information on Openstack specifics, please see the official Openstack documentation.\n\nEnvironment Setup\n\nYou should have an existing openrc file. This file will provide environment variables necessary to talk to your Openstack cloud. See here for instructions on fetching this file.\n\nCreate the Image\n\nFirst, download the Openstack image from a Talos release. These images are called openstack-$ARCH.tar.gz. Untar this file with tar -xvf openstack-$ARCH.tar.gz. The resulting file will be called disk.raw.\n\nUpload the Image\n\nOnce you have the image, you can upload to Openstack with:\n\nopenstack image create --public --disk-format raw --file disk.raw talos\n\nNetwork Infrastructure\nLoad Balancer and Network Ports\n\nOnce the image is prepared, you will need to work through setting up the network. Issue the following to create a load balancer, the necessary network ports for each control plane node, and associations between the two.\n\nCreating loadbalancer:\n\n# Create load balancer, updating vip-subnet-id if necessary\n\nopenstack loadbalancer create --name talos-control-plane --vip-subnet-id public\n\n\n\n# Create listener\n\nopenstack loadbalancer listener create --name talos-control-plane-listener --protocol TCP --protocol-port 6443 talos-control-plane\n\n\n\n# Pool and health monitoring\n\nopenstack loadbalancer pool create --name talos-control-plane-pool --lb-algorithm ROUND_ROBIN --listener talos-control-plane-listener --protocol TCP\n\nopenstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP talos-control-plane-pool\n\n\nCreating ports:\n\n# Create ports for control plane nodes, updating network name if necessary\n\nopenstack port create --network shared talos-control-plane-1\n\nopenstack port create --network shared talos-control-plane-2\n\nopenstack port create --network shared talos-control-plane-3\n\n\n\n# Create floating IPs for the ports, so that you will have talosctl connectivity to each control plane\n\nopenstack floating ip create --port talos-control-plane-1 public\n\nopenstack floating ip create --port talos-control-plane-2 public\n\nopenstack floating ip create --port talos-control-plane-3 public\n\n\nNote: Take notice of the private and public IPs associated with each of these ports, as they will be used in the next step. Additionally, take node of the port ID, as it will be used in server creation.\n\nAssociate port’s private IPs to loadbalancer:\n\n# Create members for each port IP, updating subnet-id and address as necessary.\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-1 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-2 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-3 PORT> --protocol-port 6443 talos-control-plane-pool\n\nSecurity Groups\n\nThis example uses the default security group in Openstack. Ports have been opened to ensure that connectivity from both inside and outside the group is possible. You will want to allow, at a minimum, ports 6443 (Kubernetes API server) and 50000 (Talos API) from external sources. It is also recommended to allow communication over all ports from within the subnet.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(openstack loadbalancer show talos-control-plane -f json | jq -r .vip_address)\n\n\n\ntalosctl gen config talos-k8s-openstack-tutorial https://${LB_PUBLIC_IP}:6443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our Openstack nodes.\n\nCreate control plane:\n\n# Create control planes 2 and 3, substituting the same info.\n\nfor i in $( seq 1 3 ); do\n\n  openstack server create talos-control-plane-$i --flavor m1.small --nic port-id=talos-control-plane-$i --image talos --user-data /path/to/controlplane.yaml\n\ndone\n\n\nCreate worker:\n\n# Update network name as necessary.\n\nopenstack server create talos-worker-1 --flavor m1.small --network shared --image talos --user-data /path/to/worker.yaml\n\n\nNote: This step can be repeated to add more workers.\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will use one of the floating IPs we allocated earlier. It does not matter which one.\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3.9 - Oracle\nCreating a cluster via the CLI (oci) on OracleCloud.com.\nUpload image\n\nOracle Cloud at the moment does not have a Talos official image. So you can use Bring Your Own Image (BYOI) approach.\n\nOnce the image is uploaded, set the Boot volume type to Paravirtualized mode.\n\nOracleCloud has highly available NTP service, it can be enabled in Talos machine config with:\n\nmachine:\n\n  time:\n\n    servers:\n\n      - 169.254.169.254\n\nCreating a Cluster via the CLI\n\nLogin to the console. And open the Cloud Shell.\n\nCreate a network\nexport cidr_block=10.0.0.0/16\n\nexport subnet_block=10.0.0.0/24\n\nexport compartment_id=<substitute-value-of-compartment_id> # https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/latest/oci_cli_docs/cmdref/network/vcn/create.html#cmdoption-compartment-id\n\n\n\nexport vcn_id=$(oci network vcn create --cidr-block $cidr_block --display-name talos-example --compartment-id $compartment_id --query data.id --raw-output)\n\nexport rt_id=$(oci network subnet create --cidr-block $subnet_block --display-name kubernetes --compartment-id $compartment_id --vcn-id $vcn_id --query data.route-table-id --raw-output)\n\nexport ig_id=$(oci network internet-gateway create --compartment-id $compartment_id --is-enabled true --vcn-id $vcn_id --query data.id --raw-output)\n\n\n\noci network route-table update --rt-id $rt_id --route-rules \"[{\\\"cidrBlock\\\":\\\"0.0.0.0/0\\\",\\\"networkEntityId\\\":\\\"$ig_id\\\"}]\" --force\n\n\n\n# disable firewall\n\nexport sl_id=$(oci network vcn list --compartment-id $compartment_id --query 'data[0].\"default-security-list-id\"' --raw-output)\n\n\n\noci network security-list update --security-list-id $sl_id --egress-security-rules '[{\"destination\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --ingress-security-rules '[{\"source\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --force\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer create --compartment-id $compartment_id --display-name controlplane-lb --subnet-id $subnet_id --is-preserve-source-destination false --is-private false --query data.id --raw-output)\n\n\n\ncat <<EOF > talos-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 50000,\n\n  \"protocol\": \"TCP\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://talos-health-checker.json --name talos --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name talos --name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --protocol TCP\n\n\n\ncat <<EOF > controlplane-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 6443,\n\n  \"protocol\": \"HTTPS\",\n\n  \"returnCode\": 401,\n\n  \"urlPath\": \"/readyz\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://controlplane-health-checker.json --name controlplane --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name controlplane --name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --protocol TCP\n\n\n\n# Save the external IP\n\noci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].\"ip-addresses\"'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-oracle-tutorial https://<load balancer IP or DNS>:6443 --additional-sans <load balancer IP or DNS>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport shape='VM.Standard.A1.Flex'\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --shape $shape --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].id' --raw-output)\n\n\n\ncat <<EOF > shape.json\n\n{\n\n  \"memoryInGBs\": 4,\n\n  \"ocpus\": 1\n\n}\n\nEOF\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-1 --private-ip 10.0.0.11 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-2 --private-ip 10.0.0.12 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-3 --private-ip 10.0.0.13 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport shape='VM.Standard.E2.1.Micro'\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-1 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-2 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-3 --assign-public-ip true --user-data-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nexport instance_id=$(oci compute instance list --compartment-id $compartment_id --display-name controlplane-1 --query 'data[0].id' --raw-output)\n\n\n\noci compute instance list-vnics --instance-id $instance_id --query 'data[0].\"private-ip\"' --raw-output\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <load balancer IP or DNS>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3.10 - Scaleway\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nTalos is known to work on scaleway.com; however, it is currently undocumented.\n\n3.11 - UpCloud\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nIn this guide we will create an HA Kubernetes cluster 3 control plane nodes and 1 worker node. We assume some familiarity with UpCloud. If you need more information on UpCloud specifics, please see the official UpCloud documentation.\n\nCreate the Image\n\nThe best way to create an image for UpCloud, is to build one using Hashicorp packer, with the upcloud-amd64.raw.xz image found on the Talos Releases. Using the general ISO is also possible, but the UpCloud image has some UpCloud specific features implemented, such as the fetching of metadata and user data to configure the nodes.\n\nTo create the cluster, you need a few things locally installed:\n\nUpCloud CLI\nHashicorp Packer\n\nNOTE: Make sure your account allows API connections. To do so, log into UpCloud control panel and go to People -> Account -> Permissions -> Allow API connections checkbox. It is recommended to create a separate subaccount for your API access and only set the API permission.\n\nTo use the UpCloud CLI, you need to create a config in $HOME/.config/upctl.yaml\n\nusername: your_upcloud_username\n\npassword: your_upcloud_password\n\n\nTo use the UpCloud packer plugin, you need to also export these credentials to your environment variables, by e.g. putting the following in your .bashrc or .zshrc\n\nexport UPCLOUD_USERNAME=\"<username>\"\n\nexport UPCLOUD_PASSWORD=\"<password>\"\n\n\nNext create a config file for packer to use:\n\n# upcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    upcloud = {\n\n      version = \">=v1.0.0\"\n\n      source  = \"github.com/UpCloudLtd/upcloud\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/upcloud-amd64.raw.xz\"\n\n}\n\n\n\nvariable \"username\" {\n\n  type        = string\n\n  description = \"UpCloud API username\"\n\n  default     = \"${env(\"UPCLOUD_USERNAME\")}\"\n\n}\n\n\n\nvariable \"password\" {\n\n  type        = string\n\n  description = \"UpCloud API password\"\n\n  default     = \"${env(\"UPCLOUD_PASSWORD\")}\"\n\n  sensitive   = true\n\n}\n\n\n\nsource \"upcloud\" \"talos\" {\n\n  username        = \"${var.username}\"\n\n  password        = \"${var.password}\"\n\n  zone            = \"us-nyc1\"\n\n  storage_name    = \"Debian GNU/Linux 11 (Bullseye)\"\n\n  template_name   = \"Talos (${var.talos_version})\"\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.upcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget xz-utils\",\n\n      \"wget -q -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/vda\",\n\n    ]\n\n  }\n\n\n\n  provisioner \"shell-local\" {\n\n      inline = [\n\n      \"upctl server stop --type hard custom\",\n\n      ]\n\n  }\n\n}\n\n\nNow create a new image by issuing the commands shown below.\n\npacker init .\n\npacker build .\n\n\nAfter doing this, you can find the custom image in the console interface under storage.\n\nCreating a Cluster via the CLI\nCreate an Endpoint\n\nTo communicate with the Talos cluster you will need a single endpoint that is used to access the cluster. This can either be a loadbalancer that will sit in front of all your control plane nodes, a DNS name with one or more A or AAAA records pointing to the control plane nodes, or directly the IP of a control plane node.\n\nWhich option is best for you will depend on your needs. Endpoint selection has been further documented here.\n\nAfter you decide on which endpoint to use, note down the domain name or IP, as we will need it in the next step.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the endpoint created earlier, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-upcloud-tutorial https://<load balancer IP or DNS>:<port> --install-disk /dev/vda\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Depending on the Kubernetes version you want to run, you might need to select a different Talos version, as not all versions are compatible. You can find the support matrix here.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch or yamlpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nRun the following to create three total control plane nodes:\n\nfor ID in $(seq 3); do\n\n    upctl server create \\\n\n      --zone us-nyc1 \\\n\n      --title talos-us-nyc1-master-$ID \\\n\n      --hostname talos-us-nyc1-master-$ID \\\n\n      --plan 2xCPU-4GB \\\n\n      --os \"Talos (v1.6.2)\" \\\n\n      --user-data \"$(cat controlplane.yaml)\" \\\n\n      --enable-metada\n\ndone\n\n\nNote: modify the zone and OS depending on your preferences. The OS should match the template name generated with packer in the previous step.\n\nNote the IP address of the first control plane node, as we will need it later.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nupctl server create \\\n\n  --zone us-nyc1 \\\n\n  --title talos-us-nyc1-worker-1 \\\n\n  --hostname talos-us-nyc1-worker-1 \\\n\n  --plan 2xCPU-4GB \\\n\n  --os \"Talos (v1.6.2)\" \\\n\n  --user-data \"$(cat worker.yaml)\" \\\n\n  --enable-metada\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP, as noted earlier. We only add one node IP, as that is the entry into our cluster against which our commands will be run. All requests to other nodes are proxied through the endpoint, and therefore not all nodes need to be manually added to the config. You don’t want to run your commands against all nodes, as this can destroy your cluster if you are not careful (further documentation).\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig\n\n\nIt will take a few minutes before Kubernetes has been fully bootstrapped, and is accessible.\n\nYou can check if the nodes are registered in Talos by running\n\ntalosctl --talosconfig talosconfig get members\n\n\nTo check if your nodes are ready, run\n\nkubectl get nodes\n\n3.12 - Vultr\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\nCreating a Cluster using the Vultr CLI\n\nThis guide will demonstrate how to create a highly-available Kubernetes cluster with one worker using the Vultr cloud provider.\n\nVultr have a very well documented REST API, and an open-source CLI tool to interact with the API which will be used in this guide. Make sure to follow installation and authentication instructions for the vultr-cli tool.\n\nBoot Options\nUpload an ISO Image\n\nFirst step is to make the Talos ISO available to Vultr by uploading the latest release of the ISO to the Vultr ISO server.\n\nvultr-cli iso create --url https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\n\n\nMake a note of the ID in the output, it will be needed later when creating the instances.\n\nPXE Booting via Image Factory\n\nTalos Linux can be PXE-booted on Vultr using Image Factory, using the vultr platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/vultr-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate a Load Balancer\n\nA load balancer is needed to serve as the Kubernetes endpoint for the cluster.\n\nvultr-cli load-balancer create \\\n\n   --region $REGION \\\n\n   --label \"Talos Kubernetes Endpoint\" \\\n\n   --port 6443 \\\n\n   --protocol tcp \\\n\n   --check-interval 10 \\\n\n   --response-timeout 5 \\\n\n   --healthy-threshold 5 \\\n\n   --unhealthy-threshold 3 \\\n\n   --forwarding-rules frontend_protocol:tcp,frontend_port:443,backend_protocol:tcp,backend_port:6443\n\n\nMake a note of the ID of the load balancer from the output of the above command, it will be needed after the control plane instances are created.\n\nvultr-cli load-balancer get $LOAD_BALANCER_ID | grep ^IP\n\n\nMake a note of the IP address, it will be needed later when generating the configuration.\n\nCreate the Machine Configuration\nGenerate Base Configuration\n\nUsing the IP address (or DNS name if one was created) of the load balancer created above, generate the machine configuration files for the new cluster.\n\ntalosctl gen config talos-kubernetes-vultr https://$LOAD_BALANCER_ADDRESS\n\n\nOnce generated, the machine configuration can be modified as necessary for the new cluster, for instance updating disk installation, or adding SANs for the certificates.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode cloud\n\ntalosctl validate --config worker.yaml --mode cloud\n\nCreate the Nodes\nCreate the Control Plane Nodes\n\nFirst a control plane needs to be created, with the example below creating 3 instances in a loop. The instance type (noted by the --plan vc2-2c-4gb argument) in the example is for a minimum-spec control plane node, and should be updated to suit the cluster being created.\n\nfor id in $(seq 3); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-2c-4gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-cp${id} \\\n\n        --label \"Talos Kubernetes Control Plane\" \\\n\n        --tags talos,kubernetes,control-plane\n\ndone\n\n\nMake a note of the instance IDs, as they are needed to attach to the load balancer created earlier.\n\nvultr-cli load-balancer update $LOAD_BALANCER_ID --instances $CONTROL_PLANE_1_ID,$CONTROL_PLANE_2_ID,$CONTROL_PLANE_3_ID\n\n\nOnce the nodes are booted and waiting in maintenance mode, the machine configuration can be applied to each one in turn.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_1_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_2_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_3_ADDRESS --file controlplane.yaml\n\nCreate the Worker Nodes\n\nNow worker nodes can be created and configured in a similar way to the control plane nodes, the difference being mainly in the machine configuration file. Note that like with the control plane nodes, the instance type (here set by --plan vc2-1-1gb) should be changed for the actual cluster requirements.\n\nfor id in $(seq 1); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-1c-1gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-worker${id} \\\n\n        --label \"Talos Kubernetes Worker\" \\\n\n        --tags talos,kubernetes,worker\n\ndone\n\n\nOnce the worker is booted and in maintenance mode, the machine configuration can be applied in the following manner.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $WORKER_1_ADDRESS --file worker.yaml\n\nBootstrap etcd\n\nOnce all the cluster nodes are correctly configured, the cluster can be bootstrapped to become functional. It is important that the talosctl bootstrap command be executed only once and against only a single control plane node.\n\ntalosctl --talosconfig talosconfig boostrap --endpoints $CONTROL_PLANE_1_ADDRESS --nodes $CONTROL_PLANE_1_ADDRESS\n\nConfigure Endpoints and Nodes\n\nWhile the cluster goes through the bootstrapping process and beings to self-manage, the talosconfig can be updated with the endpoints and nodes.\n\ntalosctl --talosconfig talosconfig config endpoints $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS\n\ntalosctl --talosconfig talosconfig config nodes $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS WORKER_1_ADDRESS\n\nRetrieve the kubeconfig\n\nFinally, with the cluster fully running, the administrative kubeconfig can be retrieved from the Talos API to be saved locally.\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nNow the kubeconfig can be used by any of the usual Kubernetes tools to interact with the Talos-based Kubernetes cluster as normal.\n\n4 - Local Platforms\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\n4.1 - Docker\nCreating Talos Kubernetes cluster using Docker.\n\nIn this guide we will create a Kubernetes cluster in Docker, using a containerized version of Talos.\n\nRunning Talos in Docker is intended to be used in CI pipelines, and local testing when you need a quick and easy cluster. Furthermore, if you are running Talos in production, it provides an excellent way for developers to develop against the same version of Talos.\n\nRequirements\n\nThe follow are requirements for running Talos in Docker:\n\nDocker 18.03 or greater\na recent version of talosctl\nCaveats\n\nDue to the fact that Talos will be running in a container, certain APIs are not available. For example upgrade, reset, and similar APIs don’t apply in container mode. Further, when running on a Mac in docker, due to networking limitations, VIPs are not supported.\n\nCreate the Cluster\n\nCreating a local cluster is as simple as:\n\ntalosctl cluster create --wait\n\n\nOnce the above finishes successfully, your talosconfig(~/.talos/config) will be configured to point to the new cluster.\n\nNote: Startup times can take up to a minute or more before the cluster is available.\n\nFinally, we just need to specify which nodes you want to communicate with using talosctl. Talosctl can operate on one or all the nodes in the cluster – this makes cluster wide commands much easier.\n\ntalosctl config nodes 10.5.0.2 10.5.0.3\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nCleaning Up\n\nTo cleanup, run:\n\ntalosctl cluster destroy\n\nRunning Talos in Docker Manually\n\nTo run Talos in a container manually, run:\n\ndocker run --rm -it \\\n\n  --name tutorial \\\n\n  --hostname talos-cp \\\n\n  --read-only \\\n\n  --privileged \\\n\n  --security-opt seccomp=unconfined \\\n\n  --mount type=tmpfs,destination=/run \\\n\n  --mount type=tmpfs,destination=/system \\\n\n  --mount type=tmpfs,destination=/tmp \\\n\n  --mount type=volume,destination=/system/state \\\n\n  --mount type=volume,destination=/var \\\n\n  --mount type=volume,destination=/etc/cni \\\n\n  --mount type=volume,destination=/etc/kubernetes \\\n\n  --mount type=volume,destination=/usr/libexec/kubernetes \\\n\n  --mount type=volume,destination=/usr/etc/udev \\\n\n  --mount type=volume,destination=/opt \\\n\n  -e PLATFORM=container \\\n\n  ghcr.io/siderolabs/talos:v1.6.2\n\n4.2 - QEMU\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nIn this guide we will create a Kubernetes cluster using QEMU.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\nLinux\na kernel with\nKVM enabled (/dev/kvm must exist)\nCONFIG_NET_SCH_NETEM enabled\nCONFIG_NET_SCH_INGRESS enabled\nat least CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities\nQEMU\nbridge, static and firewall CNI plugins from the standard CNI plugins, and tc-redirect-tap CNI plugin from the awslabs tc-redirect-tap installed to /opt/cni/bin (installed automatically by talosctl)\niptables\n/var/run/netns directory should exist\nInstallation\nHow to get QEMU\n\nInstall QEMU with your operating system package manager. For example, on Ubuntu for x86:\n\napt install qemu-system-x86 qemu-kvm\n\nInstall talosctl\n\nDownload talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nInstall Talos kernel and initramfs\n\nQEMU provisioner depends on Talos kernel (vmlinuz) and initramfs (initramfs.xz). These files can be downloaded from the Talos release:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/vmlinuz-<arch> -L -o _out/vmlinuz-<arch>\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/initramfs-<arch>.xz -L -o _out/initramfs-<arch>.xz\n\n\nFor example version v1.6.2:\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/vmlinuz-amd64 -L -o _out/vmlinuz-amd64\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/initramfs-amd64.xz -L -o _out/initramfs-amd64.xz\n\nCreate the Cluster\n\nFor the first time, create root state directory as your user so that you can inspect the logs as non-root user:\n\nmkdir -p ~/.talos/clusters\n\n\nCreate the cluster:\n\nsudo --preserve-env=HOME talosctl cluster create --provisioner qemu\n\n\nBefore the first cluster is created, talosctl will download the CNI bundle for the VM provisioning and install it to ~/.talos/cni directory.\n\nOnce the above finishes successfully, your talosconfig (~/.talos/config) will be configured to point to the new cluster, and kubeconfig will be downloaded and merged into default kubectl config location (~/.kube/config).\n\nCluster provisioning process can be optimized with registry pull-through caches.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl -n 10.5.0.2 containers for a list of containers in the system namespace, or talosctl -n 10.5.0.2 containers -k for the k8s.io namespace. To view the logs of a container, use talosctl -n 10.5.0.2 logs <container> or talosctl -n 10.5.0.2 logs -k <container>.\n\nA bridge interface will be created, and assigned the default IP 10.5.0.1. Each node will be directly accessible on the subnet specified at cluster creation time. A loadbalancer runs on 10.5.0.1 by default, which handles loadbalancing for the Kubernetes APIs.\n\nYou can see a summary of the cluster state by running:\n\n$ talosctl cluster show --provisioner qemu\n\nPROVISIONER       qemu\n\nNAME              talos-default\n\nNETWORK NAME      talos-default\n\nNETWORK CIDR      10.5.0.0/24\n\nNETWORK GATEWAY   10.5.0.1\n\nNETWORK MTU       1500\n\n\n\nNODES:\n\n\n\nNAME                           TYPE           IP         CPU    RAM      DISK\n\ntalos-default-controlplane-1   ControlPlane   10.5.0.2   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-2   ControlPlane   10.5.0.3   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-3   ControlPlane   10.5.0.4   1.00   1.6 GB   4.3 GB\n\ntalos-default-worker-1         Worker         10.5.0.5   1.00   1.6 GB   4.3 GB\n\nCleaning Up\n\nTo cleanup, run:\n\nsudo --preserve-env=HOME talosctl cluster destroy --provisioner qemu\n\n\nNote: In that case that the host machine is rebooted before destroying the cluster, you may need to manually remove ~/.talos/clusters/talos-default.\n\nManual Clean Up\n\nThe talosctl cluster destroy command depends heavily on the clusters state directory. It contains all related information of the cluster. The PIDs and network associated with the cluster nodes.\n\nIf you happened to have deleted the state folder by mistake or you would like to cleanup the environment, here are the steps how to do it manually:\n\nRemove VM Launchers\n\nFind the process of talosctl qemu-launch:\n\nps -elf | grep 'talosctl qemu-launch'\n\n\nTo remove the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 157615 and 157617\n\nps -elf | grep '[t]alosctl qemu-launch'\n\n0 S root      157615    2835  0  80   0 - 184934 -     07:53 ?        00:00:00 talosctl qemu-launch\n\n0 S root      157617    2835  0  80   0 - 185062 -     07:53 ?        00:00:00 talosctl qemu-launch\n\nsudo kill -s SIGTERM 157615\n\nsudo kill -s SIGTERM 157617\n\nStopping VMs\n\nFind the process of qemu-system:\n\nps -elf | grep 'qemu-system'\n\n\nTo stop the VMs manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 158065 and 158216\n\nps -elf | grep qemu-system\n\n2 S root     1061663 1061168 26  80   0 - 1786238 -    14:05 ?        01:53:56 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/home/username/.talos/clusters/talos-default/bootstrap-master.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=1e:86:c6:b4:7c:c4 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=7ec0a73c-826e-4eeb-afd1-39ff9f9160ca -machine q35,accel=kvm\n\n2 S root     1061663 1061170 67  80   0 - 621014 -     21:23 ?        00:00:07 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/homeusername/.talos/clusters/talos-default/pxe-1.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=36:f3:2f:c3:9f:06 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=ce12a0d0-29c8-490f-b935-f6073ab916a6 -machine q35,accel=kvm\n\nsudo kill -s SIGTERM 1061663\n\nsudo kill -s SIGTERM 1061663\n\nRemove load balancer\n\nFind the process of talosctl loadbalancer-launch:\n\nps -elf | grep 'talosctl loadbalancer-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl loadbalancer-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl loadbalancer-launch --loadbalancer-addr 10.5.0.1 --loadbalancer-upstreams 10.5.0.2\n\nsudo kill -s SIGTERM 157609\n\nRemove DHCP server\n\nFind the process of talosctl dhcpd-launch:\n\nps -elf | grep 'talosctl dhcpd-launch'\n\n\nTo remove the LB manually, execute:\n\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nps -elf | grep '[t]alosctl dhcpd-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl dhcpd-launch --state-path /home/username/.talos/clusters/talos-default --addr 10.5.0.1 --interface talosbd9c32bc\n\nsudo kill -s SIGTERM 157609\n\nRemove network\n\nThis is more tricky part as if you have already deleted the state folder. If you didn’t then it is written in the state.yaml in the ~/.talos/clusters/<cluster-name> directory.\n\nsudo cat ~/.talos/clusters/<cluster-name>/state.yaml | grep bridgename\n\nbridgename: talos<uuid>\n\n\nIf you only had one cluster, then it will be the interface with name talos<uuid>\n\n46: talos<uuid>: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n\n    link/ether a6:72:f4:0a:d3:9c brd ff:ff:ff:ff:ff:ff\n\n    inet 10.5.0.1/24 brd 10.5.0.255 scope global talos17c13299\n\n       valid_lft forever preferred_lft forever\n\n    inet6 fe80::a472:f4ff:fe0a:d39c/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n\nTo remove this interface:\n\nsudo ip link del talos<uuid>\n\nRemove state directory\n\nTo remove the state directory execute:\n\nsudo rm -Rf /home/$USER/.talos/clusters/<cluster-name>\n\nTroubleshooting\nLogs\n\nInspect logs directory\n\nsudo cat ~/.talos/clusters/<cluster-name>/*.log\n\n\nLogs are saved under <cluster-name>-<role>-<node-id>.log\n\nFor example in case of k8s cluster name:\n\nls -la ~/.talos/clusters/k8s | grep log\n\n-rw-r--r--. 1 root root      69415 Apr 26 20:58 k8s-master-1.log\n\n-rw-r--r--. 1 root root      68345 Apr 26 20:58 k8s-worker-1.log\n\n-rw-r--r--. 1 root root      24621 Apr 26 20:59 lb.log\n\n\nInspect logs during the installation\n\ntail -f ~/.talos/clusters/<cluster-name>/*.log\n\n4.3 - VirtualBox\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\nIn this guide we will create a Kubernetes cluster using VirtualBox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get VirtualBox\n\nInstall VirtualBox with your operating system package manager or from the website. For example, on Ubuntu for x86:\n\napt install virtualbox\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nDownload the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nCreate VMs\n\nStart by creating a new VM by clicking the “New” button in the VirtualBox UI:\n\nSupply a name for this VM, and specify the Type and Version:\n\nEdit the memory to supply at least 2GB of RAM for the VM:\n\nProceed through the disk settings, keeping the defaults. You can increase the disk space if desired.\n\nOnce created, select the VM and hit “Settings”:\n\nIn the “System” section, supply at least 2 CPUs:\n\nIn the “Network” section, switch the network “Attached To” section to “Bridged Adapter”:\n\nFinally, in the “Storage” section, select the optical drive and, on the right, select the ISO by browsing your filesystem:\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”. Once the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-vbox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the VirtualBox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig $TALOSCONFIG config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig $TALOSCONFIG config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig $TALOSCONFIG bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig $TALOSCONFIG kubeconfig .\n\n\nYou can then use kubectl in this fashion:\n\nkubectl get nodes\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the VirtualBox UI.\n\n5 - Single Board Computers\nInstallation of Talos Linux on single-board computers.\n5.1 - Banana Pi M64\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-bananapi_m64-arm64.raw.xz\n\nxz -d metal-bananapi_m64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-bananapi_m64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.2 - Friendlyelec Nano PI R4S\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-nanopi_r4s-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-nanopi_r4s-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.3 - Jetson Nano\nInstalling Talos on Jetson Nano SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card/USB drive\ncrane CLI\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nFlashing the firmware to on-board SPI flash\n\nFlashing the firmware only needs to be done once.\n\nWe will use the R32.7.2 release for the Jetson Nano. Most of the instructions is similar to this doc except that we’d be using a upstream version of u-boot with patches from NVIDIA u-boot so that USB boot also works.\n\nBefore flashing we need the following:\n\nA USB-A to micro USB cable\nA jumper wire to enable recovery mode\nA HDMI monitor to view the logs if the USB serial adapter is not available\nA USB to Serial adapter with 3.3V TTL (optional)\nA 5V DC barrel jack\n\nIf you’re planning to use the serial console follow the documentation here\n\nFirst start by downloading the Jetson Nano L4T release.\n\ncurl -SLO https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/t210/jetson-210_linux_r32.7.2_aarch64.tbz2\n\n\nNext we will extract the L4T release and replace the u-boot binary with the patched version.\n\ntar xf jetson-210_linux_r32.6.1_aarch64.tbz2\n\ncd Linux_for_Tegra\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C bootloader/t210ref/p3450-0000/ jetson_nano/u-boot.bin\n\n\nNext we will flash the firmware to the Jetson Nano SPI flash. In order to do that we need to put the Jetson Nano into Force Recovery Mode (FRC). We will use the instructions from here\n\nEnsure that the Jetson Nano is powered off. There is no need for the SD card/USB storage/network cable to be connected\nConnect the micro USB cable to the micro USB port on the Jetson Nano, don’t plug the other end to the PC yet\nEnable Force Recovery Mode (FRC) by placing a jumper across the FRC pins on the Jetson Nano\nFor board revision A02, these are pins 3 and 4 of header J40\nFor board revision B01, these are pins 9 and 10 of header J50\nPlace another jumper across J48 to enable power from the DC jack and connect the Jetson Nano to the DC jack J25\nNow connect the other end of the micro USB cable to the PC and remove the jumper wire from the FRC pins\n\nNow the Jetson Nano is in Force Recovery Mode (FRC) and can be confirmed by running the following command\n\nlsusb | grep -i \"nvidia\"\n\n\nNow we can move on the flashing the firmware.\n\nsudo ./flash p3448-0000-max-spi external\n\n\nThis will flash the firmware to the Jetson Nano SPI flash and you’ll see a lot of output. If you’ve connected the serial console you’ll also see the progress there. Once the flashing is done you can disconnect the USB cable and power off the Jetson Nano.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-jetson_nano-arm64.raw.xz\n\nxz -d metal-jetson_nano-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card/USB storage:\n\nsudo dd if=metal-jetson_nano-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M status=progress\n\n\n| Replace /dev/mmcblk0 with the name of your SD card/USB storage.\n\nBootstrapping the Node\n\nInsert the SD card/USB storage to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.4 - Libre Computer Board ALL-H3-CC\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nxz -d metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-libretech_all_h3_cc_h5-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.5 - Pine64\nInstalling Talos on a Pine64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-pine64-arm64.raw.xz\n\nxz -d metal-pine64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-pine64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.6 - Pine64 Rock64\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rock64-arm64.raw.xz\n\nxz -d metal-rock64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rock64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.7 - Radxa ROCK PI 4\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.8 - Radxa ROCK PI 4C\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rockpi_4c-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4c/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4c-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\n5.9 - Raspberry Pi Series\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\nTalos disk image for the Raspberry Pi generic should in theory work for the boards supported by u-boot rpi_arm64_defconfig. This has only been officialy tested on the Raspberry Pi 4 and community tested on one variant of the Compute Module 4 using Super 6C boards. If you have tested this on other Raspberry Pi boards, please let us know.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nUpdating the EEPROM\n\nUse Raspberry Pi Imager to write an EEPROM update image to a spare SD card. Select Misc utility images under the Operating System tab.\n\nRemove the SD card from your local machine and insert it into the Raspberry Pi. Power the Raspberry Pi on, and wait at least 10 seconds. If successful, the green LED light will blink rapidly (forever), otherwise an error pattern will be displayed. If an HDMI display is attached to the port closest to the power/USB-C port, the screen will display green for success or red if a failure occurs. Power off the Raspberry Pi and remove the SD card from it.\n\nNote: Updating the bootloader only needs to be done once.\n\nDownload the Image\n\nDownload the image and decompress it:\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rpi_generic-arm64.raw.xz\n\nxz -d metal-rpi_generic-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card:\n\nsudo dd if=metal-rpi_generic-arm64.raw of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nNote: if you have an HDMI display attached and it shows only a rainbow splash, please use the other HDMI port, the one closest to the power/USB-C port.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig\n\nTroubleshooting\n\nThe following table can be used to troubleshoot booting issues:\n\nLong Flashes\tShort Flashes\tStatus\n0\t3\tGeneric failure to boot\n0\t4\tstart*.elf not found\n0\t7\tKernel image not found\n0\t8\tSDRAM failure\n0\t9\tInsufficient SDRAM\n0\t10\tIn HALT state\n2\t1\tPartition not FAT\n2\t2\tFailed to read from partition\n2\t3\tExtended partition not FAT\n2\t4\tFile signature/hash mismatch - Pi 4\n4\t4\tUnsupported board type\n4\t5\tFatal firmware error\n4\t6\tPower failure type A\n4\t7\tPower failure type B\n6 - Boot Assets\nCreating customized Talos boot assets, disk images, ISO and installer images.\n\nTalos Linux provides a set of pre-built images on the release page, but these images can be customized further for a specific use case:\n\nadding system extensions\nupdating kernel command line arguments\nusing custom META contents, e.g. for metal network configuration\ngenerating SecureBoot images signed with a custom key\n\nThere are two ways to generate Talos boot assets:\n\nusing Image Factory service (recommended)\nmanually using imager container image (advanced)\n\nImage Factory is easier to use, but it only produces images for official Talos Linux releases and official Talos Linux system extensions. The imager container can be used to generate images from main branch, with local changes, or with custom system extensions.\n\nImage Factory\n\nImage Factory is a service that generates Talos boot assets on-demand. Image Factory allows to generate boot assets for the official Talos Linux releases and official Talos Linux system extensions.\n\nThe main concept of the Image Factory is a schematic which defines the customization of the boot asset. Once the schematic is configured, Image Factory can be used to pull various Talos Linux images, ISOs, installer images, PXE booting bare-metal machines across different architectures, versions of Talos and platforms.\n\nSidero Labs maintains a public Image Factory instance at https://factory.talos.dev. Image Factory provides a simple UI to prepare schematics and retrieve asset links.\n\nExample: Bare-metal with Image Factory\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument.\n\nFirst, let’s create the schematic file bare-metal.yaml:\n\n# bare-metal.yaml\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n      - siderolabs/intel-ucode\n\n\nThe schematic doesn’t contain system extension versions, Image Factory will pick the correct version matching Talos Linux release.\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @bare-metal.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176\"}\n\n\nThe returned schematic ID b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176 we will use to generate the boot assets.\n\nThe schematic ID is based on the schematic contents, so uploading the same schematic will return the same ID.\n\nNow we have two options to boot our bare-metal machine:\n\nusing ISO image: https://factory.talos.dev/image/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64.iso (download it and burn to a CD/DVD or USB stick)\nPXE booting via iPXE script: https://factory.talos.dev/pxe/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64\n\nThe Image Factory URL contains both schematic ID and Talos version, and both can be changed to generate different boot assets.\n\nOnce the bare-metal machine is booted up for the first time, it will require Talos Linux installer image to be installed on the disk. The installer image will be produced by the Image Factory as well:\n\n# Talos machine configuration patch\n\nmachine:\n\n  install:\n\n    image: factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:v1.6.2\n\n\nOnce installed, the machine can be upgraded to a new version of Talos by referencing new installer image:\n\ntalosctl upgrade --image factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:<new_version>\n\n\nSame way upgrade process can be used to transition to a new set of system extensions: generate new schematic with the new set of system extensions, and upgrade the machine to the new schematic ID:\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nExample: AWS with Image Factory\n\nTalos Linux is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required. Let’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s create the schematic file aws.yaml:\n\n# aws.yaml\n\ncustomization:\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\n$ curl -X POST --data-binary @aws.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5\"}\n\n\nThe returned schematic ID d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5 we will use to generate the boot assets.\n\nNow we can download the AWS disk image from the Image Factory:\n\ncurl -LO https://factory.talos.dev/image/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5/v1.6.2/aws-amd64.raw.xz\n\n\nNow the aws-amd64.raw.xz file contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nOnce the AWS VM is created from the AMI, it can be upgraded to a different Talos version or a different schematic using talosctl upgrade:\n\n# upgrade to a new Talos version\n\ntalosctl upgrade --image factory.talos.dev/installer/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5:<new_version>\n\n# upgrade to a new schematic\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nImager\n\nA custom disk image, boot asset can be generated by using the Talos Linux imager container: ghcr.io/siderolabs/imager:v1.6.2. The imager container image can be checked by verifying its signature.\n\nThe generation process can be run with a simple docker run command:\n\ndocker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 <image-kind> [optional: customization]\n\n\nA quick guide to the flags used for docker run:\n\n--rm flag removes the container after the run (as it’s not going to be used anymore)\n-t attaches a terminal for colorized output, it can be removed if used in scripts\n-v $PWD/_out:/secureboot:ro mounts the SecureBoot keys into the container (can be skipped if not generating SecureBoot image)\n-v $PWD/_out:/out mounts the output directory (where the generated image will be placed) into the container\n-v /dev:/dev --privileged is required to generate disk images (loop devices are used), but not required for ISOs, installer container images\n\nThe <image-kind> argument to the imager defines the base profile to be used for the image generation. There are several built-in profiles:\n\niso builds a Talos ISO image (see ISO)\nsecureboot-iso builds a Talos ISO image with SecureBoot (see SecureBoot)\nmetal builds a generic disk image for bare-metal machines\nsecureboot-metal builds a generic disk image for bare-metal machines with SecureBoot\nsecureboot-installer builds an installer container image with SecureBoot (see SecureBoot)\naws, gcp, azure, etc. builds a disk image for a specific Talos platform\n\nThe base profile can be customized with the additional flags to the imager:\n\n--arch specifies the architecture of the image to be generated (default: host architecture)\n--meta allows to set initial META values\n--extra-kernel-arg allows to customize the kernel command line arguments. Default kernel arg can be removed by prefixing the argument with a -. For example -console removes all console=<value> arguments, whereas -console=tty0 removes the console=tty0 default argument.\n--system-extension-image allows to install a system extension into the image\nExtension Image Reference\n\nWhile Image Factory automatically resolves the extension name into a matching container image for a specific version of Talos, imager requires the full explicit container image reference. The imager also allows to install custom extensions which are not part of the official Talos Linux system extensions.\n\nTo get the official Talos Linux system extension container image reference matching a Talos release, use the following command:\n\ncrane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep EXTENSION-NAME\n\n\nNote: this command is using crane tool, but any other tool which allows to export the image contents can be used.\n\nFor each Talos release, the ghcr.io/siderolabs/extensions:VERSION image contains a pinned reference to each system extension container image.\n\nExample: Bare-metal with Imager\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument and replace the Talos default console arguments and add a custom console arg.\n\nFirst, let’s lookup extension images for Intel CPU microcode updates and gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep -E 'gvisor|intel-ucode'\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\nghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow we can generate the ISO image with the following command:\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d --extra-kernel-arg net.ifnames=0 --extra-kernel-arg=-console --extra-kernel-arg=console=ttyS1\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: false\n\nversion: v1.6.2\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  systemExtensions:\n\n    - imageRef: ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n    - imageRef: ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\ninitramfs ready\n\nkernel command line: talos.platform=metal console=ttyS1 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 net.ifnames=0\n\nISO ready\n\noutput asset path: /out/metal-amd64.iso\n\n\nNow the _out/metal-amd64.iso contains the customized Talos ISO image.\n\nIf the machine is going to be booted using PXE, we can instead generate kernel and initramfs images:\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind kernel\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind initramfs --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow the _out/kernel-amd64 and _out/initramfs-amd64 contain the customized Talos kernel and initramfs images.\n\nNote: the extra kernel args are not used now, as they are set via the PXE boot process, and can’t be embedded into the kernel or initramfs.\n\nAs the next step, we should generate a custom installer image which contains all required system extensions (kernel args can’t be specified with the installer image, but they are set in the machine configuration):\n\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 installer --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n...\n\noutput asset path: /out/metal-amd64-installer.tar\n\n\nThe installer container image should be pushed to the container registry:\n\ncrane push _out/metal-amd64-installer.tar ghcr.io/<username></username>/installer:v1.6.2\n\n\nNow we can use the customized installer image to install Talos on the bare-metal machine.\n\nWhen it’s time to upgrade a machine, a new installer image can be generated using the new version of imager, and updating the system extension images to the matching versions. The custom installer image can now be used to upgrade Talos machine.\n\nExample: AWS with Imager\n\nTalos is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required.\n\nLet’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s lookup extension images for the gvisor container runtime in the extensions repository:\n\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep gvisor\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n\nNext, let’s generate AWS disk image with that system extension:\n\n$ docker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 aws --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n...\n\noutput asset path: /out/aws-amd64.raw\n\ncompression done: /out/aws-amd64.raw.xz\n\n\nNow the _out/aws-amd64.raw.xz contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nIf the AWS machine is later going to be upgraded to a new version of Talos (or a new set of system extensions), generate a customized installer image following the steps above, and upgrade Talos to that installer image.\n\n7 - Omni SaaS\nOmni is a project created by the Talos team that has native support for Talos Linux.\n\nOmni allows you to start with bare metal, virtual machines or a cloud provider, and create clusters spanning all of your locations, with a few clicks.\n\nYou provide the machines – edge compute, bare metal, VMs, or in your cloud account. Boot from an Omni Talos Linux image. Click to allocate to a cluster. That’s it!\n\nVanilla Kubernetes, on your machines, under your control.\nElegant UI for management and operations\nSecurity taken care of – ties into your Enterprise ID provider\nHighly Available Kubernetes API end point built in\nFirewall friendly: manage Edge nodes securely\nFrom single-node clusters to the largest scale\nSupport for GPUs and most CSIs.\n\nThe Omni SaaS is available to run locally, to support air-gapped security and data sovereignty concerns.\n\nOmni handles the lifecycle of Talos Linux machines, provides unified access to the Talos and Kubernetes API tied to the identity provider of your choice, and provides a UI for cluster management and operations. Omni automates scaling the clusters up and down, and provides a unified view of the state of your clusters.\n\nSee more in the Omni documentation.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Virtualized Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nVirtualized Platforms\nInstallation of Talos Linux for virtualization platforms.\n1: Hyper-V\n2: KVM\n3: Proxmox\n4: Vagrant & Libvirt\n5: VMware\n6: Xen\n1 - Hyper-V\nCreating a Talos Kubernetes cluster using Hyper-V.\nPre-requisities\nDownload the latest metal-amd64.iso ISO from github releases page\nCreate a New-TalosVM folder in any of your PS Module Path folders $env:PSModulePath -split ';' and save the New-TalosVM.psm1 there\nPlan Overview\n\nHere we will create a basic 3 node cluster with a single control-plane node and two worker nodes. The only difference between control plane and worker node is the amount of RAM and an additional storage VHD. This is personal preference and can be configured to your liking.\n\nWe are using a VMNamePrefix argument for a VM Name prefix and not the full hostname. This command will find any existing VM with that prefix and “+1” the highest suffix it finds. For example, if VMs talos-cp01 and talos-cp02 exist, this will create VMs starting from talos-cp03, depending on NumberOfVMs argument.\n\nSetup a Control Plane Node\n\nUse the following command to create a single control plane node:\n\nNew-TalosVM -VMNamePrefix talos-cp -CPUCount 2 -StartupMemory 4GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 1 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos'\n\n\nThis will create talos-cp01 VM and power it on.\n\nSetup Worker Nodes\n\nUse the following command to create 2 worker nodes:\n\nNew-TalosVM -VMNamePrefix talos-worker -CPUCount 4 -StartupMemory 8GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 2 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos' -StorageVHDSize 50GB\n\n\nThis will create two VMs: talos-worker01 and talos-wworker02 and attach an additional VHD of 50GB for storage (which in my case will be passed to Mayastor).\n\nPushing Config to the Nodes\n\nNow that our VMs are ready, find their IP addresses from console of VM. With that information, push config to the control plane node with:\n\n# set control plane IP variable\n\n$CONTROL_PLANE_IP='10.10.10.x'\n\n\n\n# Generate talos config\n\ntalosctl gen config talos-cluster https://$($CONTROL_PLANE_IP):6443 --output-dir .\n\n\n\n# Apply config to control plane node\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file .\\controlplane.yaml\n\nPushing Config to Worker Nodes\n\nSimilarly, for the workers:\n\ntalosctl apply-config --insecure --nodes 10.10.10.x --file .\\worker.yaml\n\n\nApply the config to both nodes.\n\nBootstrap Cluster\n\nNow that our nodes are ready, we are ready to bootstrap the Kubernetes cluster.\n\n# Use following command to set node and endpoint permanantly in config so you dont have to type it everytime\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\n\n\n# Bootstrap cluster\n\ntalosctl bootstrap\n\n\n\n# Generate kubeconfig\n\ntalosctl kubeconfig .\n\n\nThis will generate the kubeconfig file, you can use to connect to the cluster.\n\n2 - KVM\n\nTalos is known to work on KVM.\n\nWe don’t yet have a documented guide specific to KVM; however, you can have a look at our Vagrant & Libvirt guide which uses KVM for virtualization.\n\nIf you run into any issues, our community can probably help!\n\n3 - Proxmox\nCreating Talos Kubernetes cluster using Proxmox.\n\nIn this guide we will create a Kubernetes cluster using Proxmox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get Proxmox\n\nIt is assumed that you have already installed Proxmox onto the server you wish to create Talos VMs on. Visit the Proxmox downloads page if necessary.\n\nInstall talosctl\n\nYou can download talosctl via\n\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nIn order to install Talos in Proxmox, you will need the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nUpload ISO\n\nFrom the Proxmox UI, select the “local” storage and enter the “Content” section. Click the “Upload” button:\n\nSelect the ISO you downloaded previously, then hit “Upload”\n\nCreate VMs\n\nBefore starting, familiarise yourself with the system requirements for Talos and assign VM resources accordingly.\n\nCreate a new VM by clicking the “Create VM” button in the Proxmox UI:\n\nFill out a name for the new VM:\n\nIn the OS tab, select the ISO we uploaded earlier:\n\nKeep the defaults set in the “System” tab.\n\nKeep the defaults in the “Hard Disk” tab as well, only changing the size if desired.\n\nIn the “CPU” section, give at least 2 cores to the VM:\n\nNote: As of Talos v1.0 (which requires the x86-64-v2 microarchitecture), prior to Proxmox V8.0, booting with the default Processor Type kvm64 will not work. You can enable the required CPU features after creating the VM by adding the following line in the corresponding /etc/pve/qemu-server/<vmid>.conf file:\n\nargs: -cpu kvm64,+cx16,+lahf_lm,+popcnt,+sse3,+ssse3,+sse4.1,+sse4.2\n\n\nAlternatively, you can set the Processor Type to host if your Proxmox host supports these CPU features, this however prevents using live VM migration.\n\nVerify that the RAM is set to at least 2GB:\n\nKeep the default values for networking, verifying that the VM is set to come up on the bridge interface:\n\nFinish creating the VM by clicking through the “Confirm” tab and then “Finish”.\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nNote: Talos doesn’t support memory hot plugging, if creating the VM programmatically don’t enable memory hotplug on your Talos VM’s. Doing so will cause Talos to be unable to see all available memory and have insufficient memory to complete installation of the cluster.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”.\n\nWith DHCP server\n\nOnce the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nWithout DHCP server\n\nTo apply the machine configurations in maintenance mode, VM has to have IP on the network. So you can set it on boot time manually.\n\nPress e on the boot time. And set the IP parameters for the VM. Format is:\n\nip=<client-ip>:<srv-ip>:<gw-ip>:<netmask>:<host>:<device>:<autoconf>\n\n\nFor example $CONTROL_PLANE_IP will be 192.168.0.100 and gateway 192.168.0.1\n\nlinux /boot/vmlinuz init_on_alloc=1 slab_nomerge pti=on panic=0 consoleblank=0 printk.devkmsg=on earlyprintk=ttyS0 console=tty0 console=ttyS0 talos.platform=metal ip=192.168.0.100::192.168.0.1:255.255.255.0::eth0:off\n\n\nThen press Ctrl-x or F10\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\ntalosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nNote: The Talos config by default will install to /dev/sda. Depending on your setup the virtual disk may be mounted differently Eg: /dev/vda. You can check for disks running the following command:\n\ntalosctl disks --insecure --nodes $CONTROL_PLANE_IP\n\n\nUpdate controlplane.yaml and worker.yaml config files to point to the correct disk location.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the Proxmox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\ntalosctl bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl kubeconfig .\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the Proxmox UI.\n\n4 - Vagrant & Libvirt\nPre-requisities\nLinux OS\nVagrant installed\nvagrant-libvirt plugin installed\ntalosctl installed\nkubectl installed\nOverview\n\nWe will use Vagrant and its libvirt plugin to create a KVM-based cluster with 3 control plane nodes and 1 worker node.\n\nFor this, we will mount Talos ISO into the VMs using a virtual CD-ROM, and configure the VMs to attempt to boot from the disk first with the fallback to the CD-ROM.\n\nWe will also configure a virtual IP address on Talos to achieve high-availability on kube-apiserver.\n\nPreparing the environment\n\nFirst, we download the latest metal-amd64.iso ISO from GitHub releases into the /tmp directory.\n\nwget --timestamping https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -O /tmp/metal-amd64.iso\n\n\nCreate a Vagrantfile with the following contents:\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define \"control-plane-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-2\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-2.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-3\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-3.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"worker-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 1\n\n      domain.memory = 1024\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/worker-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\nend\n\nBring up the nodes\n\nCheck the status of vagrant VMs:\n\nvagrant status\n\n\nYou should see the VMs in “not created” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      not created (libvirt)\n\ncontrol-plane-node-2      not created (libvirt)\n\ncontrol-plane-node-3      not created (libvirt)\n\nworker-node-1             not created (libvirt)\n\n\nBring up the vagrant environment:\n\nvagrant up --provider=libvirt\n\n\nCheck the status again:\n\nvagrant status\n\n\nNow you should see the VMs in “running” state:\n\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      running (libvirt)\n\ncontrol-plane-node-2      running (libvirt)\n\ncontrol-plane-node-3      running (libvirt)\n\nworker-node-1             running (libvirt)\n\n\nFind out the IP addresses assigned by the libvirt DHCP by running:\n\nvirsh list | grep vagrant | awk '{print $2}' | xargs -t -L1 virsh domifaddr\n\n\nOutput will look like the following:\n\nvirsh domifaddr vagrant_control-plane-node-2\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet0      52:54:00:f9:10:e5    ipv4         192.168.121.119/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet1      52:54:00:0f:ae:59    ipv4         192.168.121.203/24\n\n\n\nvirsh domifaddr vagrant_worker-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet2      52:54:00:6f:28:95    ipv4         192.168.121.69/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-3\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet3      52:54:00:03:45:10    ipv4         192.168.121.125/24\n\n\nOur control plane nodes have the IPs: 192.168.121.203, 192.168.121.119, 192.168.121.125 and the worker node has the IP 192.168.121.69.\n\nNow you should be able to interact with Talos nodes that are in maintenance mode:\n\ntalosctl -n 192.168.121.203 disks --insecure\n\n\nSample output:\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID   MODALIAS                    NAME   SIZE     BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      8.6 GB   /pci0000:00/0000:00:03.0/virtio0/\n\nInstalling Talos\n\nPick an endpoint IP in the vagrant-libvirt subnet but not used by any nodes, for example 192.168.121.100.\n\nGenerate a machine configuration:\n\ntalosctl gen config my-cluster https://192.168.121.100:6443 --install-disk /dev/vda\n\n\nEdit controlplane.yaml to add the virtual IP you picked to a network interface under .machine.network.interfaces, for example:\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.121.100\n\n\nApply the configuration to the initial control plane node:\n\ntalosctl -n 192.168.121.203 apply-config --insecure --file controlplane.yaml\n\n\nYou can tail the logs of the node:\n\nsudo tail -f /tmp/control-plane-node-1.log\n\n\nSet up your shell to use the generated talosconfig and configure its endpoints (use the IPs of the control plane nodes):\n\nexport TALOSCONFIG=$(realpath ./talosconfig)\n\ntalosctl config endpoint 192.168.121.203 192.168.121.119 192.168.121.125\n\n\nBootstrap the Kubernetes cluster from the initial control plane node:\n\ntalosctl -n 192.168.121.203 bootstrap\n\n\nFinally, apply the machine configurations to the remaining nodes:\n\ntalosctl -n 192.168.121.119 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.125 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.69 apply-config --insecure --file worker.yaml\n\n\nAfter a while, you should see that all the members have joined:\n\ntalosctl -n 192.168.121.203 get members\n\n\nThe output will be like the following:\n\nNODE              NAMESPACE   TYPE     ID                      VERSION   HOSTNAME                MACHINE TYPE   OS               ADDRESSES\n\n192.168.121.203   cluster     Member   talos-192-168-121-119   1         talos-192-168-121-119   controlplane   Talos (v1.1.0)   [\"192.168.121.119\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-69    1         talos-192-168-121-69    worker         Talos (v1.1.0)   [\"192.168.121.69\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-203   6         talos-192-168-121-203   controlplane   Talos (v1.1.0)   [\"192.168.121.100\",\"192.168.121.203\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-125   1         talos-192-168-121-125   controlplane   Talos (v1.1.0)   [\"192.168.121.125\"]\n\nInteracting with Kubernetes cluster\n\nRetrieve the kubeconfig from the cluster:\n\ntalosctl -n 192.168.121.203 kubeconfig ./kubeconfig\n\n\nList the nodes in the cluster:\n\nkubectl --kubeconfig ./kubeconfig get node -owide\n\n\nYou will see an output similar to:\n\nNAME                    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-192-168-121-203   Ready    control-plane,master   3m10s   v1.24.2   192.168.121.203   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-69    Ready    <none>                 2m25s   v1.24.2   192.168.121.69    <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-119   Ready    control-plane,master   8m46s   v1.24.2   192.168.121.119   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-125   Ready    control-plane,master   3m11s   v1.24.2   192.168.121.125   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\n\nCongratulations, you have a highly-available Talos cluster running!\n\nCleanup\n\nYou can destroy the vagrant environment by running:\n\nvagrant destroy -f\n\n\nAnd remove the ISO image you downloaded:\n\nsudo rm -f /tmp/metal-amd64.iso\n\n5 - VMware\nCreating Talos Kubernetes cluster using VMware.\nCreating a Cluster via the govc CLI\n\nIn this guide we will create an HA Kubernetes cluster with 2 worker nodes. We will use the govc cli which can be downloaded here.\n\nPrereqs/Assumptions\n\nThis guide will use the virtual IP (“VIP”) functionality that is built into Talos in order to provide a stable, known IP for the Kubernetes control plane. This simply means the user should pick an IP on their “VM Network” to designate for this purpose and keep it handy for future steps.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the VIP chosen in the prereq steps, we will now generate the base configuration files for the Talos machines. This can be done with the talosctl gen config ... command. Take note that we will also use a JSON6902 patch when creating the configs so that the control plane nodes get some special information about the VIP we chose earlier, as well as a daemonset to install vmware tools on talos nodes.\n\nFirst, download cp.patch.yaml to your local machine and edit the VIP to match your chosen IP. You can do this by issuing: curl -fsSLO https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/cp.patch.yaml. It’s contents should look like the following:\n\n- op: add\n\n  path: /machine/network\n\n  value:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: <VIP>\n\n- op: replace\n\n  path: /cluster/extraManifests\n\n  value:\n\n    - \"https://raw.githubusercontent.com/mologie/talos-vmtoolsd/master/deploy/unstable.yaml\"\n\n\nWith the patch in hand, generate machine configs with:\n\n$ talosctl gen config vmware-test https://<VIP>:<port> --config-patch-control-plane @cp.patch.yaml\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking if needed. Optionally, you can specify additional patches by adding to the cp.patch.yaml file downloaded earlier, or create your own patch files.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nSet Environment Variables\n\ngovc makes use of the following environment variables\n\nexport GOVC_URL=<vCenter url>\n\nexport GOVC_USERNAME=<vCenter username>\n\nexport GOVC_PASSWORD=<vCenter password>\n\n\nNote: If your vCenter installation makes use of self signed certificates, you’ll want to export GOVC_INSECURE=true.\n\nThere are some additional variables that you may need to set:\n\nexport GOVC_DATACENTER=<vCenter datacenter>\n\nexport GOVC_RESOURCE_POOL=<vCenter resource pool>\n\nexport GOVC_DATASTORE=<vCenter datastore>\n\nexport GOVC_NETWORK=<vCenter network>\n\nChoose Install Approach\n\nAs part of this guide, we have a more automated install script that handles some of the complexity of importing OVAs and creating VMs. If you wish to use this script, we will detail that next. If you wish to carry out the manual approach, simply skip ahead to the “Manual Approach” section.\n\nScripted Install\n\nDownload the vmware.sh script to your local machine. You can do this by issuing curl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/vmware.sh\". This script has default variables for things like Talos version and cluster name that may be interesting to tweak before deploying.\n\nImport OVA\n\nTo create a content library and import the Talos OVA corresponding to the mentioned Talos version, simply issue:\n\n./vmware.sh upload_ova\n\nCreate Cluster\n\nWith the OVA uploaded to the content library, you can create a 5 node (by default) cluster with 3 control plane and 2 worker nodes:\n\n./vmware.sh create\n\n\nThis step will create a VM from the OVA, edit the settings based on the env variables used for VM size/specs, then power on the VMs.\n\nYou may now skip past the “Manual Approach” section down to “Bootstrap Cluster”.\n\nManual Approach\nImport the OVA into vCenter\n\nA talos.ova asset is published with each release. We will refer to the version of the release as $TALOS_VERSION below. It can be easily exported with export TALOS_VERSION=\"v0.3.0-alpha.10\" or similar.\n\ncurl -LO https://github.com/siderolabs/talos/releases/download/$TALOS_VERSION/talos.ova\n\n\nCreate a content library (if needed) with:\n\ngovc library.create <library name>\n\n\nImport the OVA to the library with:\n\ngovc library.import -n talos-${TALOS_VERSION} <library name> /path/to/downloaded/talos.ova\n\nCreate the Bootstrap Node\n\nWe’ll clone the OVA to create the bootstrap node (our first control plane node).\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-1\n\n\nTalos makes use of the guestinfo facility of VMware to provide the machine/cluster configuration. This can be set using the govc vm.change command. To facilitate persistent storage using the vSphere cloud provider integration with Kubernetes, disk.enableUUID=1 is used.\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(cat controlplane.yaml | base64)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-1\n\nUpdate Hardware Resources for the Bootstrap Node\n-c is used to configure the number of cpus\n-m is used to configure the amount of memory (in MB)\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-1\n\n\nThe following can be used to adjust the EPHEMERAL disk size.\n\ngovc vm.disk.change -vm control-plane-1 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-1\n\nCreate the Remaining Control Plane Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-2\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-3\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-3\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-2\n\n\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-3\n\ngovc vm.disk.change -vm control-plane-2 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm control-plane-3 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on control-plane-2\n\n\n\ngovc vm.power -on control-plane-3\n\nUpdate Settings for the Worker Nodes\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-1\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-1\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-2\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-1\n\n\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-2\n\ngovc vm.disk.change -vm worker-1 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm worker-2 -disk.name disk-1000-0 -size 10G\n\ngovc vm.power -on worker-1\n\n\n\ngovc vm.power -on worker-2\n\nBootstrap Cluster\n\nIn the vSphere UI, open a console to one of the control plane nodes. You should see some output stating that etcd should be bootstrapped. This text should look like:\n\n\"etcd is waiting to join the cluster, if this node is the first node in the cluster, please run `talosctl bootstrap` against one of the following IPs:\n\n\nTake note of the IP mentioned here and issue:\n\ntalosctl --talosconfig talosconfig bootstrap -e <control plane IP> -n <control plane IP>\n\n\nKeep this IP handy for the following steps as well.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane IP>\n\ntalosctl --talosconfig talosconfig config node <control plane IP>\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nConfigure talos-vmtoolsd\n\nThe talos-vmtoolsd application was deployed as a daemonset as part of the cluster creation; however, we must now provide a talos credentials file for it to use.\n\nCreate a new talosconfig with:\n\ntalosctl --talosconfig talosconfig -n <control plane IP> config new vmtoolsd-secret.yaml --roles os:admin\n\n\nCreate a secret from the talosconfig:\n\nkubectl -n kube-system create secret generic talos-vmtoolsd-config \\\n\n  --from-file=talosconfig=./vmtoolsd-secret.yaml\n\n\nClean up the generated file from local system:\n\nrm vmtoolsd-secret.yaml\n\n\nOnce configured, you should now see these daemonset pods go into “Running” state and in vCenter, you will now see IPs and info from the Talos nodes present in the UI.\n\n6 - Xen\n\nTalos is known to work on Xen. We don’t yet have a documented guide specific to Xen; however, you can follow the General Getting Started Guide. If you run into any issues, our community can probably help!\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Cloud Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nCloud Platforms\nInstallation of Talos Linux on many cloud platforms.\n1: AWS\n2: Azure\n3: DigitalOcean\n4: Exoscale\n5: GCP\n6: Hetzner\n7: Nocloud\n8: Openstack\n9: Oracle\n10: Scaleway\n11: UpCloud\n12: Vultr\n1 - AWS\nCreating a cluster via the AWS CLI.\nCreating a Cluster via the AWS CLI\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing VPC, and some familiarity with AWS. If you need more information on AWS specifics, please see the official AWS documentation.\n\nSet the needed info\n\nChange to your desired region:\n\nREGION=\"us-west-2\"\n\naws ec2 describe-vpcs --region $REGION\n\n\n\nVPC=\"(the VpcId from the above command)\"\n\nCreate the Subnet\n\nUse a CIDR block that is present on the VPC specified above.\n\naws ec2 create-subnet \\\n\n    --region $REGION \\\n\n    --vpc-id $VPC \\\n\n    --cidr-block ${CIDR_BLOCK}\n\n\nNote the subnet ID that was returned, and assign it to a variable for ease of later use:\n\nSUBNET=\"(the subnet ID of the created subnet)\"\n\nOfficial AMI Images\n\nOfficial AMI image ID can be found in the cloud-images.json file attached to the Talos release:\n\nAMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.6.2/cloud-images.json | \\\n\n    jq -r '.[] | select(.region == \"'$REGION'\") | select (.arch == \"amd64\") | .id'`\n\necho $AMI\n\n\nReplace amd64 in the line above with the desired architecture. Note the AMI id that is returned is assigned to an environment variable: it will be used later when booting instances.\n\nIf using the official AMIs, you can skip to Creating the Security group\n\nCreate your own AMIs\n\nThe use of the official Talos AMIs are recommended, but if you wish to build your own AMIs, follow the procedure below.\n\nCreate the S3 Bucket\naws s3api create-bucket \\\n\n    --bucket $BUCKET \\\n\n    --create-bucket-configuration LocationConstraint=$REGION \\\n\n    --acl private\n\nCreate the vmimport Role\n\nIn order to create an AMI, ensure that the vmimport role exists as described in the official AWS documentation.\n\nNote that the role should be associated with the S3 bucket we created above.\n\nCreate the Image Snapshot\n\nFirst, download the AWS image from a Talos release:\n\ncurl -L https://github.com/siderolabs/talos/releases/download/v1.6.2/aws-amd64.raw.xz | xz -d > disk.raw\n\n\nCopy the RAW disk to S3 and import it as a snapshot:\n\naws s3 cp disk.raw s3://$BUCKET/talos-aws-tutorial.raw\n\naws ec2 import-snapshot \\\n\n    --region $REGION \\\n\n    --description \"Talos kubernetes tutorial\" \\\n\n    --disk-container \"Format=raw,UserBucket={S3Bucket=$BUCKET,S3Key=talos-aws-tutorial.raw}\"\n\n\nSave the SnapshotId, as we will need it once the import is done. To check on the status of the import, run:\n\naws ec2 describe-import-snapshot-tasks \\\n\n    --region $REGION \\\n\n    --import-task-ids\n\n\nOnce the SnapshotTaskDetail.Status indicates completed, we can register the image.\n\nRegister the Image\naws ec2 register-image \\\n\n    --region $REGION \\\n\n    --block-device-mappings \"DeviceName=/dev/xvda,VirtualName=talos,Ebs={DeleteOnTermination=true,SnapshotId=$SNAPSHOT,VolumeSize=4,VolumeType=gp2}\" \\\n\n    --root-device-name /dev/xvda \\\n\n    --virtualization-type hvm \\\n\n    --architecture x86_64 \\\n\n    --ena-support \\\n\n    --name talos-aws-tutorial-ami\n\n\nWe now have an AMI we can use to create our cluster. Save the AMI ID, as we will need it when we create EC2 instances.\n\nAMI=\"(AMI ID of the register image command)\"\n\nCreate a Security Group\naws ec2 create-security-group \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --description \"Security Group for EC2 instances to allow ports required by Talos\"\n\n\n\nSECURITY_GROUP=\"(security group id that is returned)\"\n\n\nUsing the security group from above, allow all internal traffic within the same security group:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol all \\\n\n    --port 0 \\\n\n    --source-group talos-aws-tutorial-sg\n\n\nand expose the Talos and Kubernetes APIs:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 6443 \\\n\n    --cidr 0.0.0.0/0\n\n\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 50000-50001 \\\n\n    --cidr 0.0.0.0/0\n\n\nIf you are using KubeSpan and will be adding workers outside of AWS, you need to allow inbound UDP for the Wireguard port:\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol udp --port 51820 --cidr 0.0.0.0/0\n\nCreate a Load Balancer\naws elbv2 create-load-balancer \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-lb \\\n\n    --type network --subnets $SUBNET\n\n\nTake note of the DNS name and ARN. We will need these soon.\n\nLOAD_BALANCER_ARN=\"(arn of the load balancer)\"\n\naws elbv2 create-target-group \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-tg \\\n\n    --protocol TCP \\\n\n    --port 6443 \\\n\n    --target-type ip \\\n\n    --vpc-id $VPC\n\n\nAlso note the TargetGroupArn that is returned.\n\nTARGET_GROUP_ARN=\"(target group arn)\"\n\nCreate the Machine Configuration Files\n\nUsing the DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines.\n\nNote that the port used here is the externally accessible port configured on the load balancer - 443 - not the internal port of 6443:\n\n$ talosctl gen config talos-k8s-aws-tutorial https://<load balancer DNS>:<port> --with-examples=false --with-docs=false\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nNote that the generated configs are too long for AWS userdata field if the --with-examples and --with-docs flags are not passed.\n\nAt this point, you can modify the generated configs to your liking.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the EC2 Instances\n\nchange the instance type if desired. Note: There is a known issue that prevents Talos from running on T2 instance types. Please use T3 if you need burstable instance types.\n\nCreate the Control Plane Nodes\nCP_COUNT=1\n\nwhile [[ \"$CP_COUNT\" -lt 4 ]]; do\n\n  aws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 1 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://controlplane.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP \\\n\n    --associate-public-ip-address \\\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\"\n\n  ((CP_COUNT++))\n\ndone\n\n\nMake a note of the resulting PrivateIpAddress from the controlplane nodes for later use.\n\nCreate the Worker Nodes\naws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 3 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://worker.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\"\n\nConfigure the Load Balancer\n\nNow, using the load balancer target group’s ARN, and the PrivateIpAddress from the controlplane instances that you created :\n\naws elbv2 register-targets \\\n\n    --region $REGION \\\n\n    --target-group-arn $TARGET_GROUP_ARN \\\n\n    --targets Id=$CP_NODE_1_IP  Id=$CP_NODE_2_IP  Id=$CP_NODE_3_IP\n\n\nUsing the ARNs of the load balancer and target group from previous steps, create the listener:\n\naws elbv2 create-listener \\\n\n    --region $REGION \\\n\n    --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n    --protocol TCP \\\n\n    --port 443 \\\n\n    --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN\n\nBootstrap Etcd\n\nSet the endpoints (the control plane node to which talosctl commands are sent) and nodes (the nodes that the command operates on):\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 PUBLIC IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 PUBLIC IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nThe different control plane nodes should sendi/receive traffic via the load balancer, notice that one of the control plane has intiated the etcd cluster, and the others should join. You can now watch as your cluster bootstraps, by using\n\ntalosctl --talosconfig talosconfig  health\n\n\nYou can also watch the performance of a node, via:\n\ntalosctl  --talosconfig talosconfig dashboard\n\n\nAnd use standard kubectl commands.\n\n2 - Azure\nCreating a cluster via the CLI on Azure.\nCreating a Cluster via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node. We assume existing Blob Storage, and some familiarity with Azure. If you need more information on Azure specifics, please see the official Azure documentation.\n\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_ACCOUNT=\"StorageAccountName\"\n\n\n\n# Storage container to upload to\n\nexport STORAGE_CONTAINER=\"StorageContainerName\"\n\n\n\n# Resource group name\n\nexport GROUP=\"ResourceGroupName\"\n\n\n\n# Location\n\nexport LOCATION=\"centralus\"\n\n\n\n# Get storage account connection string based on info above\n\nexport CONNECTION=$(az storage account show-connection-string \\\n\n                    -n $STORAGE_ACCOUNT \\\n\n                    -g $GROUP \\\n\n                    -o tsv)\n\nChoose an Image\n\nThere are two methods of deployment in this tutorial.\n\nIf you would like to use the official Talos image uploaded to Azure Community Galleries by SideroLabs, you may skip ahead to setting up your network infrastructure.\n\nNetwork Infrastructure\n\nOtherwise, if you would like to upload your own image to Azure and use it to deploy Talos, continue to Creating an Image.\n\nCreate the Image\n\nFirst, download the Azure image from a Talos release. Once downloaded, untar with tar -xvf /path/to/azure-amd64.tar.gz\n\nUpload the VHD\n\nOnce you have pulled down the image, you can upload it to blob storage with:\n\naz storage blob upload \\\n\n  --connection-string $CONNECTION \\\n\n  --container-name $STORAGE_CONTAINER \\\n\n  -f /path/to/extracted/talos-azure.vhd \\\n\n  -n talos-azure.vhd\n\nRegister the Image\n\nNow that the image is present in our blob storage, we’ll register it.\n\naz image create \\\n\n  --name talos \\\n\n  --source https://$STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER/talos-azure.vhd \\\n\n  --os-type linux \\\n\n  -g $GROUP\n\nNetwork Infrastructure\nVirtual Networks and Security Groups\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a network security group and add rules to it.\n\n# Create vnet\n\naz network vnet create \\\n\n  --resource-group $GROUP \\\n\n  --location $LOCATION \\\n\n  --name talos-vnet \\\n\n  --subnet-name talos-subnet\n\n\n\n# Create network security group\n\naz network nsg create -g $GROUP -n talos-sg\n\n\n\n# Client -> apid\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n apid \\\n\n  --priority 1001 \\\n\n  --destination-port-ranges 50000 \\\n\n  --direction inbound\n\n\n\n# Trustd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n trustd \\\n\n  --priority 1002 \\\n\n  --destination-port-ranges 50001 \\\n\n  --direction inbound\n\n\n\n# etcd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n etcd \\\n\n  --priority 1003 \\\n\n  --destination-port-ranges 2379-2380 \\\n\n  --direction inbound\n\n\n\n# Kubernetes API Server\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n kube \\\n\n  --priority 1004 \\\n\n  --destination-port-ranges 6443 \\\n\n  --direction inbound\n\nLoad Balancer\n\nWe will create a public ip, load balancer, and a health check that we will use for our control plane.\n\n# Create public ip\n\naz network public-ip create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-public-ip \\\n\n  --allocation-method static\n\n\n\n# Create lb\n\naz network lb create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-lb \\\n\n  --public-ip-address talos-public-ip \\\n\n  --frontend-ip-name talos-fe \\\n\n  --backend-pool-name talos-be-pool\n\n\n\n# Create health check\n\naz network lb probe create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-lb-health \\\n\n  --protocol tcp \\\n\n  --port 6443\n\n\n\n# Create lb rule for 6443\n\naz network lb rule create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-6443 \\\n\n  --protocol tcp \\\n\n  --frontend-ip-name talos-fe \\\n\n  --frontend-port 6443 \\\n\n  --backend-pool-name talos-be-pool \\\n\n  --backend-port 6443 \\\n\n  --probe-name talos-lb-health\n\nNetwork Interfaces\n\nIn Azure, we have to pre-create the NICs for our control plane so that they can be associated with our load balancer.\n\nfor i in $( seq 0 1 2 ); do\n\n  # Create public IP for each nic\n\n  az network public-ip create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-public-ip-$i \\\n\n    --allocation-method static\n\n\n\n\n\n  # Create nic\n\n  az network nic create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-nic-$i \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --network-security-group talos-sg \\\n\n    --public-ip-address talos-controlplane-public-ip-$i\\\n\n    --lb-name talos-lb \\\n\n    --lb-address-pools talos-be-pool\n\ndone\n\n\n\n# NOTES:\n\n# Talos can detect PublicIPs automatically if PublicIP SKU is Basic.\n\n# Use `--sku Basic` to set SKU to Basic.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(az network public-ip show \\\n\n              --resource-group $GROUP \\\n\n              --name talos-public-ip \\\n\n              --query \"ipAddress\" \\\n\n              --output tsv)\n\n\n\ntalosctl gen config talos-k8s-azure-tutorial https://${LB_PUBLIC_IP}:6443\n\nCompute Creation\n\nWe are now ready to create our azure nodes. Azure allows you to pass Talos machine configuration to the virtual machine at bootstrap time via user-data or custom-data methods.\n\nTalos supports only custom-data method, machine configuration is available to the VM only on the first boot.\n\nUse the steps below depending on whether you have manually uploaded a Talos image or if you are using the Community Gallery image.\n\nManual Image Upload\nAzure Community Gallery Image\nManual Image Upload\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image talos \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image talos \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nAzure Community Gallery Image\n\nTalos is updated in Azure’s Community Galleries (Preview) on every release.\n\nTo use the Talos image for the current release create the following environment variables.\n\nEdit the variables below if you would like to use a different architecture or version.\n\n# The architecture you would like to use. Options are \"talos-x64\" or \"talos-arm64\"\n\nARCHITECTURE=\"talos-x64\"\n\n\n\n# This will use the latest version of Talos. The version must be \"latest\" or in the format Major(int).Minor(int).Patch(int), e.g. 1.5.0\n\nVERSION=\"latest\"\n\n\nCreate the Virtual Machines.\n\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(az network public-ip show \\\n\n                    --resource-group $GROUP \\\n\n                    --name talos-controlplane-public-ip-0 \\\n\n                    --query \"ipAddress\" \\\n\n                    --output tsv)\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3 - DigitalOcean\nCreating a cluster via the CLI on DigitalOcean.\nCreating a Talos Linux Cluster on Digital Ocean via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node, in the NYC region. We assume an existing Space, and some familiarity with DigitalOcean. If you need more information on DigitalOcean specifics, please see the official DigitalOcean documentation.\n\nCreate the Image\n\nDownload the DigitalOcean image digital-ocean-amd64.raw.gz from the latest Talos release.\n\nNote: the minimum version of Talos required to support Digital Ocean is v1.3.3.\n\nUsing an upload method of your choice (doctl does not have Spaces support), upload the image to a space. (It’s easy to drag the image file to the space using DigitalOcean’s web console.)\n\nNote: Make sure you upload the file as public.\n\nNow, create an image using the URL of the uploaded image:\n\nexport REGION=nyc3\n\n\n\ndoctl compute image create \\\n\n    --region $REGION \\\n\n    --image-description talos-digital-ocean-tutorial \\\n\n    --image-url https://$SPACENAME.$REGION.digitaloceanspaces.com/digital-ocean-amd64.raw.gz \\\n\n    Talos\n\n\nSave the image ID. We will need it when creating droplets.\n\nCreate a Load Balancer\ndoctl compute load-balancer create \\\n\n    --region $REGION \\\n\n    --name talos-digital-ocean-tutorial-lb \\\n\n    --tag-name talos-digital-ocean-tutorial-control-plane \\\n\n    --health-check protocol:tcp,port:6443,check_interval_seconds:10,response_timeout_seconds:5,healthy_threshold:5,unhealthy_threshold:3 \\\n\n    --forwarding-rules entry_protocol:tcp,entry_port:443,target_protocol:tcp,target_port:6443\n\n\nNote the returned ID of the load balancer.\n\nWe will need the IP of the load balancer. Using the ID of the load balancer, run:\n\ndoctl compute load-balancer get --format IP <load balancer ID>\n\n\nNote that it may take a few minutes before the load balancer is provisioned, so repeat this command until it returns with the IP address.\n\nCreate the Machine Configuration Files\n\nUsing the IP address (or DNS name, if you have created one) of the loadbalancer, generate the base configuration files for the Talos machines. Also note that the load balancer forwards port 443 to port 6443 on the associated nodes, so we should use 443 as the port in the config definition:\n\n$ talosctl gen config talos-k8s-digital-ocean-tutorial https://<load balancer IP or DNS>:443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCreate the Droplets\nCreate a dummy SSH key\n\nAlthough SSH is not used by Talos, DigitalOcean requires that an SSH key be associated with a droplet during creation. We will create a dummy key that can be used to satisfy this requirement.\n\ndoctl compute ssh-key create --public-key \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDbl0I1s/yOETIKjFr7mDLp8LmJn6OIZ68ILjVCkoN6lzKmvZEqEm1YYeWoI0xgb80hQ1fKkl0usW6MkSqwrijoUENhGFd6L16WFL53va4aeJjj2pxrjOr3uBFm/4ATvIfFTNVs+VUzFZ0eGzTgu1yXydX8lZMWnT4JpsMraHD3/qPP+pgyNuI51LjOCG0gVCzjl8NoGaQuKnl8KqbSCARIpETg1mMw+tuYgaKcbqYCMbxggaEKA0ixJ2MpFC/kwm3PcksTGqVBzp3+iE5AlRe1tnbr6GhgT839KLhOB03j7lFl1K9j1bMTOEj5Io8z7xo/XeF2ZQKHFWygAJiAhmKJ dummy@dummy.local\" dummy\n\n\nNote the ssh key ID that is returned - we will use it in creating the droplets.\n\nCreate the Control Plane Nodes\n\nRun the following commands to create three control plane nodes:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-1\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-2\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-3\n\n\nNote the droplet ID returned for the first control plane node.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --user-data-file worker.yaml \\\n\n    --ssh-keys <ssh key ID>  \\\n\n    talos-worker-1\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\ndoctl compute droplet get --format PublicIPv4 <droplet ID>\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nWe can also watch the cluster bootstrap via:\n\ntalosctl --talosconfig talosconfig health\n\n4 - Exoscale\nCreating a cluster via the CLI using exoscale.com\n\nTalos is known to work on exoscale.com; however, it is currently undocumented.\n\n5 - GCP\nCreating a cluster via the CLI on Google Cloud Platform.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in GCP with 1 worker node. We will assume an existing Cloud Storage bucket, and some familiarity with Google Cloud. If you need more information on Google Cloud specifics, please see the official Google documentation.\n\njq and talosctl also needs to be installed\n\nManual Setup\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\n# Storage account to use\n\nexport STORAGE_BUCKET=\"StorageBucketName\"\n\n# Region\n\nexport REGION=\"us-central1\"\n\nCreate the Image\n\nFirst, download the Google Cloud image from a Talos release. These images are called gcp-$ARCH.tar.gz.\n\nUpload the Image\n\nOnce you have downloaded the image, you can upload it to your storage bucket with:\n\ngsutil cp /path/to/gcp-amd64.tar.gz gs://$STORAGE_BUCKET\n\nRegister the image\n\nNow that the image is present in our bucket, we’ll register it.\n\ngcloud compute images create talos \\\n\n --source-uri=gs://$STORAGE_BUCKET/gcp-amd64.tar.gz \\\n\n --guest-os-features=VIRTIO_SCSI_MULTIQUEUE\n\nNetwork Infrastructure\nLoad Balancers and Firewalls\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a firewall, load balancer, and their required components.\n\n130.211.0.0/22 and 35.191.0.0/16 are the GCP Load Balancer IP ranges\n\n# Create Instance Group\n\ngcloud compute instance-groups unmanaged create talos-ig \\\n\n  --zone $REGION-b\n\n\n\n# Create port for IG\n\ngcloud compute instance-groups set-named-ports talos-ig \\\n\n    --named-ports tcp6443:6443 \\\n\n    --zone $REGION-b\n\n\n\n# Create health check\n\ngcloud compute health-checks create tcp talos-health-check --port 6443\n\n\n\n# Create backend\n\ngcloud compute backend-services create talos-be \\\n\n    --global \\\n\n    --protocol TCP \\\n\n    --health-checks talos-health-check \\\n\n    --timeout 5m \\\n\n    --port-name tcp6443\n\n\n\n# Add instance group to backend\n\ngcloud compute backend-services add-backend talos-be \\\n\n    --global \\\n\n    --instance-group talos-ig \\\n\n    --instance-group-zone $REGION-b\n\n\n\n# Create tcp proxy\n\ngcloud compute target-tcp-proxies create talos-tcp-proxy \\\n\n    --backend-service talos-be \\\n\n    --proxy-header NONE\n\n\n\n# Create LB IP\n\ngcloud compute addresses create talos-lb-ip --global\n\n\n\n# Forward 443 from LB IP to tcp proxy\n\ngcloud compute forwarding-rules create talos-fwd-rule \\\n\n    --global \\\n\n    --ports 443 \\\n\n    --address talos-lb-ip \\\n\n    --target-tcp-proxy talos-tcp-proxy\n\n\n\n# Create firewall rule for health checks\n\ngcloud compute firewall-rules create talos-controlplane-firewall \\\n\n     --source-ranges 130.211.0.0/22,35.191.0.0/16 \\\n\n     --target-tags talos-controlplane \\\n\n     --allow tcp:6443\n\n\n\n# Create firewall rule to allow talosctl access\n\ngcloud compute firewall-rules create talos-controlplane-talosctl \\\n\n  --source-ranges 0.0.0.0/0 \\\n\n  --target-tags talos-controlplane \\\n\n  --allow tcp:50000\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(gcloud compute forwarding-rules describe talos-fwd-rule \\\n\n               --global \\\n\n               --format json \\\n\n               | jq -r .IPAddress)\n\n\n\ntalosctl gen config talos-k8s-gcp-tutorial https://${LB_PUBLIC_IP}:443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our GCP nodes.\n\n# Create the control plane nodes.\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instances create talos-controlplane-$i \\\n\n    --image talos \\\n\n    --zone $REGION-b \\\n\n    --tags talos-controlplane \\\n\n    --boot-disk-size 20GB \\\n\n    --metadata-from-file=user-data=./controlplane.yaml\n\n    --tags talos-controlplane-$i\n\ndone\n\n\n\n# Add control plane nodes to instance group\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instance-groups unmanaged add-instances talos-ig \\\n\n      --zone $REGION-b \\\n\n      --instances talos-controlplane-$i\n\ndone\n\n\n\n# Create worker\n\ngcloud compute instances create talos-worker-0 \\\n\n  --image talos \\\n\n  --zone $REGION-b \\\n\n  --boot-disk-size 20GB \\\n\n  --metadata-from-file=user-data=./worker.yaml\n\n  --tags talos-worker-$i\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCONTROL_PLANE_0_IP=$(gcloud compute instances describe talos-controlplane-0 \\\n\n                     --zone $REGION-b \\\n\n                     --format json \\\n\n                     | jq -r '.networkInterfaces[0].accessConfigs[0].natIP')\n\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nCleanup\n# cleanup VM's\n\ngcloud compute instances delete \\\n\n  talos-worker-0 \\\n\n  talos-controlplane-0 \\\n\n  talos-controlplane-1 \\\n\n  talos-controlplane-2\n\n\n\n# cleanup firewall rules\n\ngcloud compute firewall-rules delete \\\n\n  talos-controlplane-talosctl \\\n\n  talos-controlplane-firewall\n\n\n\n# cleanup forwarding rules\n\ngcloud compute forwarding-rules delete \\\n\n  talos-fwd-rule\n\n\n\n# cleanup addresses\n\ngcloud compute addresses delete \\\n\n  talos-lb-ip\n\n\n\n# cleanup proxies\n\ngcloud compute target-tcp-proxies delete \\\n\n  talos-tcp-proxy\n\n\n\n# cleanup backend services\n\ngcloud compute backend-services delete \\\n\n  talos-be\n\n\n\n# cleanup health checks\n\ngcloud compute health-checks delete \\\n\n  talos-health-check\n\n\n\n# cleanup unmanaged instance groups\n\ngcloud compute instance-groups unmanaged delete \\\n\n  talos-ig\n\n\n\n# cleanup Talos image\n\ngcloud compute images delete \\\n\n  talos\n\nUsing GCP Deployment manager\n\nUsing GCP deployment manager automatically creates a Google Storage bucket and uploads the Talos image to it. Once the deployment is complete the generated talosconfig and kubeconfig files are uploaded to the bucket.\n\nBy default this setup creates a three node control plane and a single worker in us-west1-b\n\nFirst we need to create a folder to store our deployment manifests and perform all subsequent operations from that folder.\n\nmkdir -p talos-gcp-deployment\n\ncd talos-gcp-deployment\n\nGetting the deployment manifests\n\nWe need to download two deployment manifests for the deployment from the Talos github repository.\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/config.yaml\"\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/talos-ha.jinja\"\n\n# if using ccm\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/gcp-ccm.yaml\"\n\nUpdating the config\n\nNow we need to update the local config.yaml file with any required changes such as changing the default zone, Talos version, machine sizes, nodes count etc.\n\nAn example config.yaml file is shown below:\n\nimports:\n\n  - path: talos-ha.jinja\n\n\n\nresources:\n\n  - name: talos-ha\n\n    type: talos-ha.jinja\n\n    properties:\n\n      zone: us-west1-b\n\n      talosVersion: v1.6.2\n\n      externalCloudProvider: false\n\n      controlPlaneNodeCount: 5\n\n      controlPlaneNodeType: n1-standard-1\n\n      workerNodeCount: 3\n\n      workerNodeType: n1-standard-1\n\noutputs:\n\n  - name: bucketName\n\n    value: $(ref.talos-ha.bucketName)\n\nEnabling external cloud provider\n\nNote: The externalCloudProvider property is set to false by default. The manifest used for deploying the ccm (cloud controller manager) is currently using the GCP ccm provided by openshift since there are no public images for the ccm yet.\n\nSince the routes controller is disabled while deploying the CCM, the CNI pods needs to be restarted after the CCM deployment is complete to remove the node.kubernetes.io/network-unavailable taint. See Nodes network-unavailable taint not removed after installing ccm for more information\n\nUse a custom built image for the ccm deployment if required.\n\nCreating the deployment\n\nNow we are ready to create the deployment. Confirm with y for any prompts. Run the following command to create the deployment:\n\n# use a unique name for the deployment, resources are prefixed with the deployment name\n\nexport DEPLOYMENT_NAME=\"<deployment name>\"\n\ngcloud deployment-manager deployments create \"${DEPLOYMENT_NAME}\" --config config.yaml\n\nRetrieving the outputs\n\nFirst we need to get the deployment outputs.\n\n# first get the outputs\n\nOUTPUTS=$(gcloud deployment-manager deployments describe \"${DEPLOYMENT_NAME}\" --format json | jq '.outputs[]')\n\n\n\nBUCKET_NAME=$(jq -r '. | select(.name == \"bucketName\").finalValue' <<< \"${OUTPUTS}\")\n\n# used when cloud controller is enabled\n\nSERVICE_ACCOUNT=$(jq -r '. | select(.name == \"serviceAccount\").finalValue' <<< \"${OUTPUTS}\")\n\nPROJECT=$(jq -r '. | select(.name == \"project\").finalValue' <<< \"${OUTPUTS}\")\n\n\nNote: If cloud controller manager is enabled, the below command needs to be run to allow the controller custom role to access cloud resources\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\nDownloading talos and kube config\n\nIn addition to the talosconfig and kubeconfig files, the storage bucket contains the controlplane.yaml and worker.yaml files used to join additional nodes to the cluster.\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/talosconfig\" .\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/kubeconfig\" .\n\nDeploying the cloud controller manager\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  apply \\\n\n  --filename gcp-ccm.yaml\n\n#  wait for the ccm to be up\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset cloud-controller-manager\n\n\nIf the cloud controller manager is enabled, we need to restart the CNI pods to remove the node.kubernetes.io/network-unavailable taint.\n\n# restart the CNI pods, in this case flannel\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout restart \\\n\n  daemonset kube-flannel\n\n# wait for the pods to be restarted\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset kube-flannel\n\nCheck cluster status\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  get nodes\n\nCleanup deployment\n\nWarning: This will delete the deployment and all resources associated with it.\n\nRun below if cloud controller manager is enabled\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\n\nNow we can finally remove the deployment\n\n# delete the objects in the bucket first\n\ngsutil -m rm -r \"gs://${BUCKET_NAME}\"\n\ngcloud deployment-manager deployments delete \"${DEPLOYMENT_NAME}\" --quiet\n\n6 - Hetzner\nCreating a cluster via the CLI (hcloud) on Hetzner.\nUpload image\n\nHetzner Cloud does not support uploading custom images. You can email their support to get a Talos ISO uploaded by following issues:3599 or you can prepare image snapshot by yourself.\n\nThere are two options to upload your own.\n\nRun an instance in rescue mode and replace the system OS with the Talos image\nUse Hashicorp packer to prepare an image\nRescue mode\n\nCreate a new Server in the Hetzner console. Enable the Hetzner Rescue System for this server and reboot. Upon a reboot, the server will boot a special minimal Linux distribution designed for repair and reinstall. Once running, login to the server using ssh to prepare the system disk by doing the following:\n\n# Check that you in Rescue mode\n\ndf\n\n\n\n### Result is like:\n\n# udev                   987432         0    987432   0% /dev\n\n# 213.133.99.101:/nfs 308577696 247015616  45817536  85% /root/.oldroot/nfs\n\n# overlay                995672      8340    987332   1% /\n\n# tmpfs                  995672         0    995672   0% /dev/shm\n\n# tmpfs                  398272       572    397700   1% /run\n\n# tmpfs                    5120         0      5120   0% /run/lock\n\n# tmpfs                  199132         0    199132   0% /run/user/0\n\n\n\n# Download the Talos image\n\ncd /tmp\n\nwget -O /tmp/talos.raw.xz https://github.com/siderolabs/talos/releases/download/v1.6.2/hcloud-amd64.raw.xz\n\n# Replace system\n\nxz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\n\n# shutdown the instance\n\nshutdown -h now\n\n\nTo make sure disk content is consistent, it is recommended to shut the server down before taking an image (snapshot). Once shutdown, simply create an image (snapshot) from the console. You can now use this snapshot to run Talos on the cloud.\n\nPacker\n\nInstall packer to the local machine.\n\nCreate a config file for packer to use:\n\n# hcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    hcloud = {\n\n      source  = \"github.com/hetznercloud/hcloud\"\n\n      version = \"~> 1\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nvariable \"arch\" {\n\n  type    = string\n\n  default = \"amd64\"\n\n}\n\n\n\nvariable \"server_type\" {\n\n  type    = string\n\n  default = \"cx11\"\n\n}\n\n\n\nvariable \"server_location\" {\n\n  type    = string\n\n  default = \"hel1\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/hcloud-${var.arch}.raw.xz\"\n\n}\n\n\n\nsource \"hcloud\" \"talos\" {\n\n  rescue       = \"linux64\"\n\n  image        = \"debian-11\"\n\n  location     = \"${var.server_location}\"\n\n  server_type  = \"${var.server_type}\"\n\n  ssh_username = \"root\"\n\n\n\n  snapshot_name   = \"talos system disk - ${var.arch} - ${var.talos_version}\"\n\n  snapshot_labels = {\n\n    type    = \"infra\",\n\n    os      = \"talos\",\n\n    version = \"${var.talos_version}\",\n\n    arch    = \"${var.arch}\",\n\n  }\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.hcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget\",\n\n      \"wget -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\",\n\n    ]\n\n  }\n\n}\n\n\nAdditionally you could create a file containing\n\narch            = \"arm64\"\n\nserver_type     = \"cax11\"\n\nserver_location = \"fsn1\"\n\n\nand build the snapshot for arm64.\n\nCreate a new image by issuing the commands shown below. Note that to create a new API token for your Project, switch into the Hetzner Cloud Console choose a Project, go to Access → Security, and create a new token.\n\n# First you need set API Token\n\nexport HCLOUD_TOKEN=${TOKEN}\n\n\n\n# Upload image\n\npacker init .\n\npacker build .\n\n# Save the image ID\n\nexport IMAGE_ID=<image-id-in-packer-output>\n\n\nAfter doing this, you can find the snapshot in the console interface.\n\nCreating a Cluster via the CLI\n\nThis section assumes you have the hcloud console utility on your local machine.\n\n# Set hcloud context and api key\n\nhcloud context create talos-tutorial\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nhcloud load-balancer create --name controlplane --network-zone eu-central --type lb11 --label 'type=controlplane'\n\n\n\n### Result is like:\n\n# LoadBalancer 484487 created\n\n# IPv4: 49.12.X.X\n\n# IPv6: 2a01:4f8:X:X::1\n\n\n\nhcloud load-balancer add-service controlplane \\\n\n    --listen-port 6443 --destination-port 6443 --protocol tcp\n\nhcloud load-balancer add-target controlplane \\\n\n    --label-selector 'type=controlplane'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-hcloud-tutorial https://<load balancer IP or DNS>:6443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\n\nWe can now create our servers. Note that you can find IMAGE_ID in the snapshot section of the console: https://console.hetzner.cloud/projects/$PROJECT_ID/servers/snapshots.\n\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport IMAGE_ID=<your-image-id>\n\n\n\nhcloud server create --name talos-control-plane-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-2 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location fsn1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-3 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location nbg1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nhcloud server create --name talos-worker-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=worker' \\\n\n    --user-data-from-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nhcloud server list | grep talos-control-plane\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <control-plane-1-IP>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n7 - Nocloud\nCreating a cluster via the CLI using qemu.\n\nTalos supports nocloud data source implementation.\n\nThere are two ways to configure Talos server with nocloud platform:\n\nvia SMBIOS “serial number” option\nusing CDROM or USB-flash filesystem\n\nNote: This requires the nocloud image which can be found on the Github Releases page.\n\nSMBIOS Serial Number\n\nThis method requires the network connection to be up (e.g. via DHCP). Configuration is delivered from the HTTP server.\n\nds=nocloud-net;s=http://10.10.0.1/configs/;h=HOSTNAME\n\n\nAfter the network initialization is complete, Talos fetches:\n\nthe machine config from http://10.10.0.1/configs/user-data\nthe network config (if available) from http://10.10.0.1/configs/network-config\nSMBIOS: QEMU\n\nAdd the following flag to qemu command line when starting a VM:\n\nqemu-system-x86_64 \\\n\n  ...\\\n\n  -smbios type=1,serial=ds=nocloud-net;s=http://10.10.0.1/configs/\n\nSMBIOS: Proxmox\n\nSet the source machine config through the serial number on Proxmox GUI.\n\nThe Proxmox stores the VM config at /etc/pve/qemu-server/$ID.conf ($ID - VM ID number of virtual machine), you will see something like:\n\n...\nsmbios1: uuid=ceae4d10,serial=ZHM9bm9jbG91ZC1uZXQ7cz1odHRwOi8vMTAuMTAuMC4xL2NvbmZpZ3Mv,base64=1\n...\n\n\nWhere serial holds the base64-encoded string version of ds=nocloud-net;s=http://10.10.0.1/configs/.\n\nCDROM/USB\n\nTalos can also get machine config from local attached storage without any prior network connection being established.\n\nYou can provide configs to the server via files on a VFAT or ISO9660 filesystem. The filesystem volume label must be cidata or CIDATA.\n\nExample: QEMU\n\nCreate and prepare Talos machine config:\n\nexport CONTROL_PLANE_IP=192.168.1.10\n\n\n\ntalosctl gen config talos-nocloud https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nPrepare cloud-init configs:\n\nmkdir -p iso\n\nmv _out/controlplane.yaml iso/user-data\n\necho \"local-hostname: controlplane-1\" > iso/meta-data\n\ncat > iso/network-config << EOF\n\nversion: 1\n\nconfig:\n\n   - type: physical\n\n     name: eth0\n\n     mac_address: \"52:54:00:12:34:00\"\n\n     subnets:\n\n        - type: static\n\n          address: 192.168.1.10\n\n          netmask: 255.255.255.0\n\n          gateway: 192.168.1.254\n\nEOF\n\n\nCreate cloud-init iso image\n\ncd iso && genisoimage -output cidata.iso -V cidata -r -J user-data meta-data network-config\n\n\nStart the VM\n\nqemu-system-x86_64 \\\n\n    ...\n\n    -cdrom iso/cidata.iso \\\n\n    ...\n\nExample: Proxmox\n\nProxmox can create cloud-init disk for you. Edit the cloud-init config information in Proxmox as follows, substitute your own information as necessary:\n\nand then update cicustom param at /etc/pve/qemu-server/$ID.conf.\n\ncicustom: user=local:snippets/controlplane-1.yml\nipconfig0: ip=192.168.1.10/24,gw=192.168.10.254\nnameserver: 1.1.1.1\nsearchdomain: local\n\n\nNote: snippets/controlplane-1.yml is Talos machine config. It is usually located at /var/lib/vz/snippets/controlplane-1.yml. This file must be placed to this path manually, as Proxmox does not support snippet uploading via API/GUI.\n\nClick on Regenerate Image button after the above changes are made.\n\n8 - Openstack\nCreating a cluster via the CLI on Openstack.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in Openstack with 1 worker node. We will assume an existing some familiarity with Openstack. If you need more information on Openstack specifics, please see the official Openstack documentation.\n\nEnvironment Setup\n\nYou should have an existing openrc file. This file will provide environment variables necessary to talk to your Openstack cloud. See here for instructions on fetching this file.\n\nCreate the Image\n\nFirst, download the Openstack image from a Talos release. These images are called openstack-$ARCH.tar.gz. Untar this file with tar -xvf openstack-$ARCH.tar.gz. The resulting file will be called disk.raw.\n\nUpload the Image\n\nOnce you have the image, you can upload to Openstack with:\n\nopenstack image create --public --disk-format raw --file disk.raw talos\n\nNetwork Infrastructure\nLoad Balancer and Network Ports\n\nOnce the image is prepared, you will need to work through setting up the network. Issue the following to create a load balancer, the necessary network ports for each control plane node, and associations between the two.\n\nCreating loadbalancer:\n\n# Create load balancer, updating vip-subnet-id if necessary\n\nopenstack loadbalancer create --name talos-control-plane --vip-subnet-id public\n\n\n\n# Create listener\n\nopenstack loadbalancer listener create --name talos-control-plane-listener --protocol TCP --protocol-port 6443 talos-control-plane\n\n\n\n# Pool and health monitoring\n\nopenstack loadbalancer pool create --name talos-control-plane-pool --lb-algorithm ROUND_ROBIN --listener talos-control-plane-listener --protocol TCP\n\nopenstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP talos-control-plane-pool\n\n\nCreating ports:\n\n# Create ports for control plane nodes, updating network name if necessary\n\nopenstack port create --network shared talos-control-plane-1\n\nopenstack port create --network shared talos-control-plane-2\n\nopenstack port create --network shared talos-control-plane-3\n\n\n\n# Create floating IPs for the ports, so that you will have talosctl connectivity to each control plane\n\nopenstack floating ip create --port talos-control-plane-1 public\n\nopenstack floating ip create --port talos-control-plane-2 public\n\nopenstack floating ip create --port talos-control-plane-3 public\n\n\nNote: Take notice of the private and public IPs associated with each of these ports, as they will be used in the next step. Additionally, take node of the port ID, as it will be used in server creation.\n\nAssociate port’s private IPs to loadbalancer:\n\n# Create members for each port IP, updating subnet-id and address as necessary.\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-1 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-2 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-3 PORT> --protocol-port 6443 talos-control-plane-pool\n\nSecurity Groups\n\nThis example uses the default security group in Openstack. Ports have been opened to ensure that connectivity from both inside and outside the group is possible. You will want to allow, at a minimum, ports 6443 (Kubernetes API server) and 50000 (Talos API) from external sources. It is also recommended to allow communication over all ports from within the subnet.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nLB_PUBLIC_IP=$(openstack loadbalancer show talos-control-plane -f json | jq -r .vip_address)\n\n\n\ntalosctl gen config talos-k8s-openstack-tutorial https://${LB_PUBLIC_IP}:6443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our Openstack nodes.\n\nCreate control plane:\n\n# Create control planes 2 and 3, substituting the same info.\n\nfor i in $( seq 1 3 ); do\n\n  openstack server create talos-control-plane-$i --flavor m1.small --nic port-id=talos-control-plane-$i --image talos --user-data /path/to/controlplane.yaml\n\ndone\n\n\nCreate worker:\n\n# Update network name as necessary.\n\nopenstack server create talos-worker-1 --flavor m1.small --network shared --image talos --user-data /path/to/worker.yaml\n\n\nNote: This step can be repeated to add more workers.\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will use one of the floating IPs we allocated earlier. It does not matter which one.\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n9 - Oracle\nCreating a cluster via the CLI (oci) on OracleCloud.com.\nUpload image\n\nOracle Cloud at the moment does not have a Talos official image. So you can use Bring Your Own Image (BYOI) approach.\n\nOnce the image is uploaded, set the Boot volume type to Paravirtualized mode.\n\nOracleCloud has highly available NTP service, it can be enabled in Talos machine config with:\n\nmachine:\n\n  time:\n\n    servers:\n\n      - 169.254.169.254\n\nCreating a Cluster via the CLI\n\nLogin to the console. And open the Cloud Shell.\n\nCreate a network\nexport cidr_block=10.0.0.0/16\n\nexport subnet_block=10.0.0.0/24\n\nexport compartment_id=<substitute-value-of-compartment_id> # https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/latest/oci_cli_docs/cmdref/network/vcn/create.html#cmdoption-compartment-id\n\n\n\nexport vcn_id=$(oci network vcn create --cidr-block $cidr_block --display-name talos-example --compartment-id $compartment_id --query data.id --raw-output)\n\nexport rt_id=$(oci network subnet create --cidr-block $subnet_block --display-name kubernetes --compartment-id $compartment_id --vcn-id $vcn_id --query data.route-table-id --raw-output)\n\nexport ig_id=$(oci network internet-gateway create --compartment-id $compartment_id --is-enabled true --vcn-id $vcn_id --query data.id --raw-output)\n\n\n\noci network route-table update --rt-id $rt_id --route-rules \"[{\\\"cidrBlock\\\":\\\"0.0.0.0/0\\\",\\\"networkEntityId\\\":\\\"$ig_id\\\"}]\" --force\n\n\n\n# disable firewall\n\nexport sl_id=$(oci network vcn list --compartment-id $compartment_id --query 'data[0].\"default-security-list-id\"' --raw-output)\n\n\n\noci network security-list update --security-list-id $sl_id --egress-security-rules '[{\"destination\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --ingress-security-rules '[{\"source\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --force\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer create --compartment-id $compartment_id --display-name controlplane-lb --subnet-id $subnet_id --is-preserve-source-destination false --is-private false --query data.id --raw-output)\n\n\n\ncat <<EOF > talos-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 50000,\n\n  \"protocol\": \"TCP\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://talos-health-checker.json --name talos --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name talos --name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --protocol TCP\n\n\n\ncat <<EOF > controlplane-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 6443,\n\n  \"protocol\": \"HTTPS\",\n\n  \"returnCode\": 401,\n\n  \"urlPath\": \"/readyz\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://controlplane-health-checker.json --name controlplane --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name controlplane --name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --protocol TCP\n\n\n\n# Save the external IP\n\noci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].\"ip-addresses\"'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\n$ talosctl gen config talos-k8s-oracle-tutorial https://<load balancer IP or DNS>:6443 --additional-sans <load balancer IP or DNS>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nexport shape='VM.Standard.A1.Flex'\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --shape $shape --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].id' --raw-output)\n\n\n\ncat <<EOF > shape.json\n\n{\n\n  \"memoryInGBs\": 4,\n\n  \"ocpus\": 1\n\n}\n\nEOF\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-1 --private-ip 10.0.0.11 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-2 --private-ip 10.0.0.12 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-3 --private-ip 10.0.0.13 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport shape='VM.Standard.E2.1.Micro'\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-1 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-2 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-3 --assign-public-ip true --user-data-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nexport instance_id=$(oci compute instance list --compartment-id $compartment_id --display-name controlplane-1 --query 'data[0].id' --raw-output)\n\n\n\noci compute instance list-vnics --instance-id $instance_id --query 'data[0].\"private-ip\"' --raw-output\n\n\nSet the endpoints and nodes for your talosconfig with:\n\ntalosctl --talosconfig talosconfig config endpoint <load balancer IP or DNS>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n10 - Scaleway\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nTalos is known to work on scaleway.com; however, it is currently undocumented.\n\n11 - UpCloud\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nIn this guide we will create an HA Kubernetes cluster 3 control plane nodes and 1 worker node. We assume some familiarity with UpCloud. If you need more information on UpCloud specifics, please see the official UpCloud documentation.\n\nCreate the Image\n\nThe best way to create an image for UpCloud, is to build one using Hashicorp packer, with the upcloud-amd64.raw.xz image found on the Talos Releases. Using the general ISO is also possible, but the UpCloud image has some UpCloud specific features implemented, such as the fetching of metadata and user data to configure the nodes.\n\nTo create the cluster, you need a few things locally installed:\n\nUpCloud CLI\nHashicorp Packer\n\nNOTE: Make sure your account allows API connections. To do so, log into UpCloud control panel and go to People -> Account -> Permissions -> Allow API connections checkbox. It is recommended to create a separate subaccount for your API access and only set the API permission.\n\nTo use the UpCloud CLI, you need to create a config in $HOME/.config/upctl.yaml\n\nusername: your_upcloud_username\n\npassword: your_upcloud_password\n\n\nTo use the UpCloud packer plugin, you need to also export these credentials to your environment variables, by e.g. putting the following in your .bashrc or .zshrc\n\nexport UPCLOUD_USERNAME=\"<username>\"\n\nexport UPCLOUD_PASSWORD=\"<password>\"\n\n\nNext create a config file for packer to use:\n\n# upcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    upcloud = {\n\n      version = \">=v1.0.0\"\n\n      source  = \"github.com/UpCloudLtd/upcloud\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/upcloud-amd64.raw.xz\"\n\n}\n\n\n\nvariable \"username\" {\n\n  type        = string\n\n  description = \"UpCloud API username\"\n\n  default     = \"${env(\"UPCLOUD_USERNAME\")}\"\n\n}\n\n\n\nvariable \"password\" {\n\n  type        = string\n\n  description = \"UpCloud API password\"\n\n  default     = \"${env(\"UPCLOUD_PASSWORD\")}\"\n\n  sensitive   = true\n\n}\n\n\n\nsource \"upcloud\" \"talos\" {\n\n  username        = \"${var.username}\"\n\n  password        = \"${var.password}\"\n\n  zone            = \"us-nyc1\"\n\n  storage_name    = \"Debian GNU/Linux 11 (Bullseye)\"\n\n  template_name   = \"Talos (${var.talos_version})\"\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.upcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget xz-utils\",\n\n      \"wget -q -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/vda\",\n\n    ]\n\n  }\n\n\n\n  provisioner \"shell-local\" {\n\n      inline = [\n\n      \"upctl server stop --type hard custom\",\n\n      ]\n\n  }\n\n}\n\n\nNow create a new image by issuing the commands shown below.\n\npacker init .\n\npacker build .\n\n\nAfter doing this, you can find the custom image in the console interface under storage.\n\nCreating a Cluster via the CLI\nCreate an Endpoint\n\nTo communicate with the Talos cluster you will need a single endpoint that is used to access the cluster. This can either be a loadbalancer that will sit in front of all your control plane nodes, a DNS name with one or more A or AAAA records pointing to the control plane nodes, or directly the IP of a control plane node.\n\nWhich option is best for you will depend on your needs. Endpoint selection has been further documented here.\n\nAfter you decide on which endpoint to use, note down the domain name or IP, as we will need it in the next step.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the endpoint created earlier, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-upcloud-tutorial https://<load balancer IP or DNS>:<port> --install-disk /dev/vda\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Depending on the Kubernetes version you want to run, you might need to select a different Talos version, as not all versions are compatible. You can find the support matrix here.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch or yamlpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nRun the following to create three total control plane nodes:\n\nfor ID in $(seq 3); do\n\n    upctl server create \\\n\n      --zone us-nyc1 \\\n\n      --title talos-us-nyc1-master-$ID \\\n\n      --hostname talos-us-nyc1-master-$ID \\\n\n      --plan 2xCPU-4GB \\\n\n      --os \"Talos (v1.6.2)\" \\\n\n      --user-data \"$(cat controlplane.yaml)\" \\\n\n      --enable-metada\n\ndone\n\n\nNote: modify the zone and OS depending on your preferences. The OS should match the template name generated with packer in the previous step.\n\nNote the IP address of the first control plane node, as we will need it later.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nupctl server create \\\n\n  --zone us-nyc1 \\\n\n  --title talos-us-nyc1-worker-1 \\\n\n  --hostname talos-us-nyc1-worker-1 \\\n\n  --plan 2xCPU-4GB \\\n\n  --os \"Talos (v1.6.2)\" \\\n\n  --user-data \"$(cat worker.yaml)\" \\\n\n  --enable-metada\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP, as noted earlier. We only add one node IP, as that is the entry into our cluster against which our commands will be run. All requests to other nodes are proxied through the endpoint, and therefore not all nodes need to be manually added to the config. You don’t want to run your commands against all nodes, as this can destroy your cluster if you are not careful (further documentation).\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig\n\n\nIt will take a few minutes before Kubernetes has been fully bootstrapped, and is accessible.\n\nYou can check if the nodes are registered in Talos by running\n\ntalosctl --talosconfig talosconfig get members\n\n\nTo check if your nodes are ready, run\n\nkubectl get nodes\n\n12 - Vultr\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\nCreating a Cluster using the Vultr CLI\n\nThis guide will demonstrate how to create a highly-available Kubernetes cluster with one worker using the Vultr cloud provider.\n\nVultr have a very well documented REST API, and an open-source CLI tool to interact with the API which will be used in this guide. Make sure to follow installation and authentication instructions for the vultr-cli tool.\n\nBoot Options\nUpload an ISO Image\n\nFirst step is to make the Talos ISO available to Vultr by uploading the latest release of the ISO to the Vultr ISO server.\n\nvultr-cli iso create --url https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\n\n\nMake a note of the ID in the output, it will be needed later when creating the instances.\n\nPXE Booting via Image Factory\n\nTalos Linux can be PXE-booted on Vultr using Image Factory, using the vultr platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/vultr-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate a Load Balancer\n\nA load balancer is needed to serve as the Kubernetes endpoint for the cluster.\n\nvultr-cli load-balancer create \\\n\n   --region $REGION \\\n\n   --label \"Talos Kubernetes Endpoint\" \\\n\n   --port 6443 \\\n\n   --protocol tcp \\\n\n   --check-interval 10 \\\n\n   --response-timeout 5 \\\n\n   --healthy-threshold 5 \\\n\n   --unhealthy-threshold 3 \\\n\n   --forwarding-rules frontend_protocol:tcp,frontend_port:443,backend_protocol:tcp,backend_port:6443\n\n\nMake a note of the ID of the load balancer from the output of the above command, it will be needed after the control plane instances are created.\n\nvultr-cli load-balancer get $LOAD_BALANCER_ID | grep ^IP\n\n\nMake a note of the IP address, it will be needed later when generating the configuration.\n\nCreate the Machine Configuration\nGenerate Base Configuration\n\nUsing the IP address (or DNS name if one was created) of the load balancer created above, generate the machine configuration files for the new cluster.\n\ntalosctl gen config talos-kubernetes-vultr https://$LOAD_BALANCER_ADDRESS\n\n\nOnce generated, the machine configuration can be modified as necessary for the new cluster, for instance updating disk installation, or adding SANs for the certificates.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode cloud\n\ntalosctl validate --config worker.yaml --mode cloud\n\nCreate the Nodes\nCreate the Control Plane Nodes\n\nFirst a control plane needs to be created, with the example below creating 3 instances in a loop. The instance type (noted by the --plan vc2-2c-4gb argument) in the example is for a minimum-spec control plane node, and should be updated to suit the cluster being created.\n\nfor id in $(seq 3); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-2c-4gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-cp${id} \\\n\n        --label \"Talos Kubernetes Control Plane\" \\\n\n        --tags talos,kubernetes,control-plane\n\ndone\n\n\nMake a note of the instance IDs, as they are needed to attach to the load balancer created earlier.\n\nvultr-cli load-balancer update $LOAD_BALANCER_ID --instances $CONTROL_PLANE_1_ID,$CONTROL_PLANE_2_ID,$CONTROL_PLANE_3_ID\n\n\nOnce the nodes are booted and waiting in maintenance mode, the machine configuration can be applied to each one in turn.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_1_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_2_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_3_ADDRESS --file controlplane.yaml\n\nCreate the Worker Nodes\n\nNow worker nodes can be created and configured in a similar way to the control plane nodes, the difference being mainly in the machine configuration file. Note that like with the control plane nodes, the instance type (here set by --plan vc2-1-1gb) should be changed for the actual cluster requirements.\n\nfor id in $(seq 1); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-1c-1gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-worker${id} \\\n\n        --label \"Talos Kubernetes Worker\" \\\n\n        --tags talos,kubernetes,worker\n\ndone\n\n\nOnce the worker is booted and in maintenance mode, the machine configuration can be applied in the following manner.\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $WORKER_1_ADDRESS --file worker.yaml\n\nBootstrap etcd\n\nOnce all the cluster nodes are correctly configured, the cluster can be bootstrapped to become functional. It is important that the talosctl bootstrap command be executed only once and against only a single control plane node.\n\ntalosctl --talosconfig talosconfig boostrap --endpoints $CONTROL_PLANE_1_ADDRESS --nodes $CONTROL_PLANE_1_ADDRESS\n\nConfigure Endpoints and Nodes\n\nWhile the cluster goes through the bootstrapping process and beings to self-manage, the talosconfig can be updated with the endpoints and nodes.\n\ntalosctl --talosconfig talosconfig config endpoints $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS\n\ntalosctl --talosconfig talosconfig config nodes $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS WORKER_1_ADDRESS\n\nRetrieve the kubeconfig\n\nFinally, with the cluster fully running, the administrative kubeconfig can be retrieved from the Talos API to be saved locally.\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nNow the kubeconfig can be used by any of the usual Kubernetes tools to interact with the Talos-based Kubernetes cluster as normal.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Bare Metal Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nBare Metal Platforms\nInstallation of Talos Linux on various bare-metal platforms.\n1: Digital Rebar\n2: Equinix Metal\n3: ISO\n4: Matchbox\n5: Network Configuration\n6: PXE\n7: SecureBoot\n1 - Digital Rebar\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\nPrerequisites\n3 nodes (please see hardware requirements)\nLoadbalancer\nDigital Rebar Server\nTalosctl access (see talosctl setup)\nCreating a Cluster\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes. We assume an existing digital rebar deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe loadbalancer is used to distribute the load across multiple controlplane nodes. This isn’t covered in detail, because we assume some loadbalancing knowledge before hand. If you think this should be added to the docs, please create a issue.\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nDigital Rebar has a built-in fileserver, which means we can use this feature to expose the talos configuration files. We will place controlplane.yaml, and worker.yaml into Digital Rebar file server by using the drpcli tools.\n\nCopy the generated files from the step above into your Digital Rebar installation.\n\ndrpcli file upload <file>.yaml as <file>.yaml\n\n\nReplacing <file> with controlplane or worker.\n\nDownload the boot files\n\nDownload a recent version of boot.tar.gz from github.\n\nUpload to DRB:\n\n$ drpcli isos upload boot.tar.gz as talos.tar.gz\n\n{\n\n  \"Path\": \"talos.tar.gz\",\n\n  \"Size\": 96470072\n\n}\n\n\nWe have some Digital Rebar example files in the Git repo you can use to provision Digital Rebar with drpcli.\n\nTo apply these configs you need to create them, and then apply them as follow:\n\n$ drpcli bootenvs create talos\n\n{\n\n  \"Available\": true,\n\n  \"BootParams\": \"\",\n\n  \"Bundle\": \"\",\n\n  \"Description\": \"\",\n\n  \"Documentation\": \"\",\n\n  \"Endpoint\": \"\",\n\n  \"Errors\": [],\n\n  \"Initrds\": [],\n\n  \"Kernel\": \"\",\n\n  \"Meta\": {},\n\n  \"Name\": \"talos\",\n\n  \"OS\": {\n\n    \"Codename\": \"\",\n\n    \"Family\": \"\",\n\n    \"IsoFile\": \"\",\n\n    \"IsoSha256\": \"\",\n\n    \"IsoUrl\": \"\",\n\n    \"Name\": \"\",\n\n    \"SupportedArchitectures\": {},\n\n    \"Version\": \"\"\n\n  },\n\n  \"OnlyUnknown\": false,\n\n  \"OptionalParams\": [],\n\n  \"ReadOnly\": false,\n\n  \"RequiredParams\": [],\n\n  \"Templates\": [],\n\n  \"Validated\": true\n\n}\n\ndrpcli bootenvs update talos - < bootenv.yaml\n\n\nYou need to do this for all files in the example directory. If you don’t have access to the drpcli tools you can also use the webinterface.\n\nIt’s important to have a corresponding SHA256 hash matching the boot.tar.gz\n\nBootenv BootParams\n\nWe’re using some of Digital Rebar built in templating to make sure the machine gets the correct role assigned.\n\ntalos.platform=metal talos.config={{ .ProvisionerURL }}/files/{{.Param \\\"talos/role\\\"}}.yaml\"\n\nThis is why we also include a params.yaml in the example directory to make sure the role is set to one of the following:\n\ncontrolplane\nworker\n\nThe {{.Param \\\"talos/role\\\"}} then gets populated with one of the above roles.\n\nBoot the Machines\n\nIn the UI of Digital Rebar you need to select the machines you want to provision. Once selected, you need to assign to following:\n\nProfile\nWorkflow\n\nThis will provision the Stage and Bootenv with the talos values. Once this is done, you can boot the machine.\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n2 - Equinix Metal\nCreating Talos clusters with Equinix Metal.\n\nYou can create a Talos Linux cluster on Equinix Metal in a variety of ways, such as through the EM web UI, the metal command line too, or through PXE booting. Talos Linux is a supported OS install option on Equinix Metal, so it’s an easy process.\n\nRegardless of the method, the process is:\n\nCreate a DNS entry for your Kubernetes endpoint.\nGenerate the configurations using talosctl.\nProvision your machines on Equinix Metal.\nPush the configurations to your servers (if not done as part of the machine provisioning).\nconfigure your Kubernetes endpoint to point to the newly created control plane nodes\nbootstrap the cluster\nDefine the Kubernetes Endpoint\n\nThere are a variety of ways to create an HA endpoint for the Kubernetes cluster. Some of the ways are:\n\nDNS\nLoad Balancer\nBGP\n\nWhatever way is chosen, it should result in an IP address/DNS name that routes traffic to all the control plane nodes. We do not know the control plane node IP addresses at this stage, but we should define the endpoint DNS entry so that we can use it in creating the cluster configuration. After the nodes are provisioned, we can use their addresses to create the endpoint A records, or bind them to the load balancer, etc.\n\nCreate the Machine Configuration Files\nGenerating Configurations\n\nUsing the DNS name of the loadbalancer defined above, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-em-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe port used above should be 6443, unless your load balancer maps a different port to port 6443 on the control plane nodes.\n\nValidate the Configuration Files\ntalosctl validate --config controlplane.yaml --mode metal\n\ntalosctl validate --config worker.yaml --mode metal\n\n\nNote: Validation of the install disk could potentially fail as validation is performed on your local machine and the specified disk may not exist.\n\nPassing in the configuration as User Data\n\nYou can use the metadata service provide by Equinix Metal to pass in the machines configuration. It is required to add a shebang to the top of the configuration file.\n\nThe convention we use is #!talos.\n\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\n\nSimply select the location and type of machines in the Equinix Metal web interface. Select Talos as the Operating System, then select the number of servers to create, and name them (in lowercase only.) Under optional settings, you can optionally paste in the contents of controlplane.yaml that was generated, above (ensuring you add a first line of #!talos).\n\nYou can repeat this process to create machines of different types for control plane and worker nodes (although you would pass in worker.yaml for the worker nodes, as user data).\n\nIf you did not pass in the machine configuration as User Data, you need to provide it to each machine, with the following command:\n\ntalosctl apply-config --insecure --nodes <Node IP> --file ./controlplane.yaml\n\nCreating a Cluster via the Equinix Metal CLI\n\nThis guide assumes the user has a working API token,and the Equinix Metal CLI installed.\n\nBecause Talos Linux is a supported operating system, Talos Linux machines can be provisioned directly via the CLI, using the -O talos_v1 parameter (for Operating System).\n\nNote: Ensure you have prepended #!talos to the controlplane.yaml file.\n\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --operating-system \"talos_v1\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\ne.g. metal device create -p <projectID> -f da11 -O talos_v1 -P c3.small.x86 -H steve.test.11 --userdata-file ./controlplane.yaml\n\nRepeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nNetwork Booting via iPXE\n\nTalos Linux can be PXE-booted on Equinix Metal using Image Factory, using the equinixMetal platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/equinixMetal-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate the Control Plane Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\nNote: Repeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nCreate the Worker Nodes\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file worker.yaml\n\nUpdate the Kubernetes endpoint\n\nNow our control plane nodes have been created, and we know their IP addresses, we can associate them with the Kubernetes endpoint. Configure your load balancer to route traffic to these nodes, or add A records to your DNS entry for the endpoint, for each control plane node. e.g.\n\nhost endpoint.mydomain.com\n\nendpoint.mydomain.com has address 145.40.90.201\n\nendpoint.mydomain.com has address 147.75.109.71\n\nendpoint.mydomain.com has address 145.40.90.177\n\nBootstrap Etcd\n\nSet the endpoints and nodes for talosctl:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\n\nThis only needs to be issued to one control plane node.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n3 - ISO\nBooting Talos on bare-metal with ISO.\n\nTalos can be installed on bare-metal machine using an ISO image. ISO images for amd64 and arm64 architectures are available on the Talos releases page.\n\nTalos doesn’t install itself to disk when booted from an ISO until the machine configuration is applied.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from a Talos ISO. The boot order should prefer disk over ISO, or the ISO should be removed after the installation to make Talos boot from disk.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nThere are two flavors of ISO images available:\n\nmetal-<arch>.iso supports booting on BIOS and UEFI systems (for x86, UEFI only for arm64)\nmetal-<arch>-secureboot.iso supports booting on only UEFI systems in SecureBoot mode (via Image Factory)\n4 - Matchbox\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\nCreating a Cluster\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing load balancer, matchbox deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nIn bare-metal setups it is up to the user to provide the configuration files over HTTP(S). A special kernel parameter (talos.config) must be used to inform Talos about where it should retrieve its configuration file. To keep things simple we will place controlplane.yaml, and worker.yaml into Matchbox’s assets directory. This directory is automatically served by Matchbox.\n\nCreate the Matchbox Configuration Files\n\nThe profiles we will create will reference vmlinuz, and initramfs.xz. Download these files from the release of your choice, and place them in /var/lib/matchbox/assets.\n\nProfiles\nControl Plane Nodes\n{\n\n  \"id\": \"control-plane\",\n\n  \"name\": \"control-plane\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/controlplane.yaml\"\n\n    ]\n\n  }\n\n}\n\n\nNote: Be sure to change http://matchbox.talos.dev to the endpoint of your matchbox server.\n\nWorker Nodes\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/worker.yaml\"\n\n    ]\n\n  }\n\n}\n\nGroups\n\nNow, create the following groups, and ensure that the selectors are accurate for your specific setup.\n\n{\n\n  \"id\": \"control-plane-1\",\n\n  \"name\": \"control-plane-1\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-2\",\n\n  \"name\": \"control-plane-2\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"control-plane-3\",\n\n  \"name\": \"control-plane-3\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"profile\": \"default\"\n\n}\n\nBoot the Machines\n\nNow that we have our configuration files in place, boot all the machines. Talos will come up on each machine, grab its configuration file, and bootstrap itself.\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\n5 - Network Configuration\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nBy default, Talos will run DHCP client on all interfaces which have a link, and that might be enough for most of the cases. If some advanced network configuration is required, it can be done via the machine configuration file.\n\nBut sometimes it is required to apply network configuration even before the machine configuration can be fetched from the network.\n\nKernel Command Line\n\nTalos supports some kernel command line parameters to configure network before the machine configuration is fetched.\n\nNote: Kernel command line parameters are not persisted after Talos installation, so proper network configuration should be done via the machine configuration.\n\nAddress, default gateway and DNS servers can be configured via ip= kernel command line parameter:\n\nip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\n\nBonding can be configured via bond= kernel command line parameter:\n\nbond=bond0:eth0,eth1:balance-rr\n\n\nVLANs can be configured via vlan= kernel command line parameter:\n\nvlan=eth0.100:eth0\n\n\nSee kernel parameters reference for more details.\n\nPlatform Network Configuration\n\nSome platforms (e.g. AWS, Google Cloud, etc.) have their own network configuration mechanisms, which can be used to perform the initial network configuration. There is no such mechanism for bare-metal platforms, so Talos provides a way to use platform network config on the metal platform to submit the initial network configuration.\n\nThe platform network configuration is a YAML document which contains resource specifications for various network resources. For the metal platform, the interactive dashboard can be used to edit the platform network configuration, also the configuration can be created manually.\n\nThe current value of the platform network configuration can be retrieved using the MetaKeys resource (key 0xa):\n\ntalosctl get meta 0xa\n\n\nThe platform network configuration can be updated using the talosctl meta command for the running node:\n\ntalosctl meta write 0xa '{\"externalIPs\": [\"1.2.3.4\"]}'\n\ntalosctl meta delete 0xa\n\n\nThe initial platform network configuration for the metal platform can be also included into the generated Talos image:\n\ndocker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\ndocker run --rm -i --privileged ghcr.io/siderolabs/imager:v1.6.2 image --platform metal --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\n\nThe platform network configuration gets merged with other sources of network configuration, the details can be found in the network resources guide.\n\n6 - PXE\nBooting Talos over the network on bare-metal with PXE.\n\nTalos can be installed on bare-metal using PXE service. There are two more detailed guides for PXE booting using Matchbox and Digital Rebar.\n\nThis guide describes generic steps for PXE booting Talos on bare-metal.\n\nFirst, download the vmlinuz and initramfs assets from the Talos releases page. Set up the machines to PXE boot from the network (usually by setting the boot order in the BIOS). There might be options specific to the hardware being used, booting in BIOS or UEFI mode, using iPXE, etc.\n\nTalos requires the following kernel parameters to be set on the initial boot:\n\ntalos.platform=metal\nslab_nomerge\npti=on\n\nWhen booted from the network without machine configuration, Talos will start in maintenance mode.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from network. The boot order should prefer disk over network.\n\nTalos can automatically fetch the machine configuration from the network on the initial boot using talos.config kernel parameter. A metadata service (HTTP service) can be implemented to deliver customized configuration to each node for example by using the MAC address of the node:\n\ntalos.config=https://metadata.service/talos/config?mac=${mac}\n\n\nNote: The talos.config kernel parameter supports other substitution variables, see kernel parameters reference for the full list.\n\nPXE booting can be also performed via Image Factory.\n\n7 - SecureBoot\nBooting Talos in SecureBoot mode on UEFI platforms.\n\nTalos now supports booting on UEFI systems in SecureBoot mode. When combined with TPM-based disk encryption, this provides Trusted Boot experience.\n\nNote: SecureBoot is not supported on x86 platforms in BIOS mode.\n\nThe implementation is using systemd-boot as a boot menu implementation, while the Talos kernel, initramfs and cmdline arguments are combined into the Unified Kernel Image (UKI) format. UEFI firmware loads the systemd-boot bootloader, which then loads the UKI image. Both systemd-boot and Talos UKI image are signed with the key, which is enrolled into the UEFI firmware.\n\nAs Talos Linux is fully contained in the UKI image, the full operating system is verified and booted by the UEFI firmware.\n\nNote: There is no support at the moment to upgrade non-UKI (GRUB-based) Talos installation to use UKI/SecureBoot, so a fresh installation is required.\n\nSecureBoot with Sidero Labs Images\n\nSidero Labs provides Talos images signed with the Sidero Labs SecureBoot key via Image Factory.\n\nNote: The SecureBoot images are available for Talos releases starting from v1.5.0.\n\nThe easiest way to get started with SecureBoot is to download the ISO, and boot it on a UEFI-enabled system which has SecureBoot enabled in setup mode.\n\nThe ISO bootloader will enroll the keys in the UEFI firmware, and boot the Talos Linux in SecureBoot mode. The install should performed using SecureBoot installer (put it Talos machine configuration): factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2.\n\nNote: SecureBoot images can also be generated with custom keys.\n\nBooting Talos Linux in SecureBoot Mode\n\nIn this guide we will use the ISO image to boot Talos Linux in SecureBoot mode, followed by submitting machine configuration to the machine in maintenance mode. We will use one the ways to generate and submit machine configuration to the node, please refer to the Production Notes for the full guide.\n\nFirst, make sure SecureBoot is enabled in the UEFI firmware. For the first boot, the UEFI firmware should be in the setup mode, so that the keys can be enrolled into the UEFI firmware automatically. If the UEFI firmware does not support automatic enrollment, you may need to hit Esc to force the boot menu to appear, and select the Enroll Secure Boot keys: auto option.\n\nNote: There are other ways to enroll the keys into the UEFI firmware, but this is out of scope of this guide.\n\nOnce Talos is running in maintenance mode, verify that secure boot is enabled:\n\n$ talosctl -n <IP> get securitystate --insecure\n\nNODE   NAMESPACE   TYPE            ID              VERSION   SECUREBOOT\n\n       runtime     SecurityState   securitystate   1         true\n\n\nNow we will generate the machine configuration for the node supplying the installer-secureboot container image, and applying the patch to enable TPM-based disk encryption (requires TPM 2.0):\n\n# tpm-disk-encryption.yaml\n\nmachine:\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n\nGenerate machine configuration:\n\ntalosctl gen config <cluster-name> https://<endpoint>:6443 --install-image=factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2 --install-disk=/dev/sda --config-patch @tpm-disk-encryption.yaml\n\n\nApply machine configuration to the node:\n\ntalosctl -n <IP> apply-config --insecure -f controlplane.yaml\n\n\nTalos will perform the installation to the disk and reboot the node. Please make sure that the ISO image is not attached to the node anymore, otherwise the node will boot from the ISO image again.\n\nOnce the node is rebooted, verify that the node is running in secure boot mode:\n\ntalosctl -n <IP> --talosconfig=talosconfig get securitystate\n\nUpgrading Talos Linux\n\nAny change to the boot asset (kernel, initramfs, kernel command line) requires the UKI to be regenerated and the installer image to be rebuilt. Follow the steps above to generate new installer image updating the boot assets: use new Talos version, add a system extension, or modify the kernel command line. Once the new installer image is pushed to the registry, upgrade the node using the new installer image.\n\nIt is important to preserve the UKI signing key and the PCR signing key, otherwise the node will not be able to boot with the new UKI and unlock the encrypted partitions.\n\nDisk Encryption with TPM\n\nWhen encrypting the disk partition for the first time, Talos Linux generates a random disk encryption key and seals (encrypts) it with the TPM device. The TPM unlock policy is configured to trust the expected policy signed by the PCR signing key. This way TPM unlocking doesn’t depend on the exact PCR measurements, but rather on the expected policy signed by the PCR signing key and the state of SecureBoot (PCR 7 measurement, including secureboot status and the list of enrolled keys).\n\nWhen the UKI image is generated, the UKI is measured and expected measurements are combined into TPM unlock policy and signed with the PCR signing key. During the boot process, systemd-stub component of the UKI performs measurements of the UKI sections into the TPM device. Talos Linux during the boot appends to the PCR register the measurements of the boot phases, and once the boot reaches the point of mounting the encrypted disk partition, the expected signed policy from the UKI is matched against measured values to unlock the TPM, and TPM unseals the disk encryption key which is then used to unlock the disk partition.\n\nDuring the upgrade, as long as the new UKI is contains PCR policy signed with the same PCR signing key, and SecureBoot state has not changed the disk partition will be unlocked successfully.\n\nDisk encryption is also tied to the state of PCR register 7, so that it unlocks only if SecureBoot is enabled and the set of enrolled keys hasn’t changed.\n\nOther Boot Options\n\nUnified Kernel Image (UKI) is a UEFI-bootable image which can be booted directly from the UEFI firmware skipping the systemd-boot bootloader. In network boot mode, the UKI can be used directly as well, as it contains the full set of boot assets required to boot Talos Linux.\n\nWhen SecureBoot is enabled, the UKI image ignores any kernel command line arguments passed to it, but rather uses the kernel command line arguments embedded into the UKI image itself. If kernel command line arguments need to be changed, the UKI image needs to be rebuilt with the new kernel command line arguments.\n\nSecureBoot with Custom Keys\nGenerating the Keys\n\nTalos requires two set of keys to be used for the SecureBoot process:\n\nSecureBoot key is used to sign the boot assets and it is enrolled into the UEFI firmware.\nPCR Signing Key is used to sign the TPM policy, which is used to seal the disk encryption key.\n\nThe same key might be used for both, but it is recommended to use separate keys for each purpose.\n\nTalos provides a utility to generate the keys, but existing PKI infrastructure can be used as well:\n\n$ talosctl gen secureboot uki --common-name \"SecureBoot Key\"\n\nwriting _out/uki-signing-cert.pem\n\nwriting _out/uki-signing-cert.der\n\nwriting _out/uki-signing-key.pem\n\n\nThe generated certificate and private key are written to disk in PEM-encoded format (RSA 4096-bit key). The certificate is also written in DER format for the systems which expect the certificate in DER format.\n\nPCR signing key can be generated with:\n\n$ talosctl gen secureboot pcr\n\nwriting _out/pcr-signing-key.pem\n\n\nThe file containing the private key is written to disk in PEM-encoded format (RSA 2048-bit key).\n\nOptionally, UEFI automatic key enrollment database can be generated using the _out/uki-signing-* files as input:\n\n$ talosctl gen secureboot database\n\nwriting _out/db.auth\n\nwriting _out/KEK.auth\n\nwriting _out/PK.auth\n\n\nThese files can be used to enroll the keys into the UEFI firmware automatically when booting from a SecureBoot ISO while UEFI firmware is in the setup mode.\n\nGenerating the SecureBoot Assets\n\nOnce the keys are generated, they can be used to sign the Talos boot assets to generate required ISO images, PXE boot assets, disk images, installer containers, etc. In this guide we will generate a SecureBoot ISO image and an installer image.\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-iso\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.5.0-alpha.3-35-ge0f383598-dirty\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\nISO ready\n\noutput asset path: /out/metal-amd64-secureboot.iso\n\n\nNext, the installer image should be generated to install Talos to disk on a SecureBoot-enabled system:\n\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-installer\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: installer\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\ninstaller container image ready\n\noutput asset path: /out/installer-amd64-secureboot.tar\n\n\nThe generated container image should be pushed to some container registry which Talos can access during the installation, e.g.:\n\ncrane push _out/installer-amd64-secureboot.tar ghcr.io/<user>/installer-amd64-secureboot:v1.6.2\n\n\nThe generated ISO and installer images might be further customized with system extensions, extra kernel command line arguments, etc.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Introduction | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/_print/",
    "html": "TALOS LINUX\nSidero Labs\nSidero Omni\nGitHub\nDocumentation\nReleases\nSearch\n⌘\nK\n\nThis is the multi-page printable view of this section. Click here to print.\n\nReturn to the regular view of this page.\n\nIntroduction\n1: What is Talos?\n2: Quickstart\n3: Getting Started\n4: Production Clusters\n5: System Requirements\n6: What's New in Talos 1.6.0\n7: Support Matrix\n8: Troubleshooting\n1 - What is Talos?\nA quick introduction in to what Talos is and why it should be used.\n\nTalos is a container optimized Linux distro; a reimagining of Linux for distributed systems such as Kubernetes. Designed to be as minimal as possible while still maintaining practicality. For these reasons, Talos has a number of features unique to it:\n\nit is immutable\nit is atomic\nit is ephemeral\nit is minimal\nit is secure by default\nit is managed via a single declarative configuration file and gRPC API\n\nTalos can be deployed on container, cloud, virtualized, and bare metal platforms.\n\nWhy Talos\n\nIn having less, Talos offers more. Security. Efficiency. Resiliency. Consistency.\n\nAll of these areas are improved simply by having less.\n\n2 - Quickstart\nA short guide on setting up a simple Talos Linux cluster locally with Docker.\nLocal Docker Cluster\n\nThe easiest way to try Talos is by using the CLI (talosctl) to create a cluster on a machine with docker installed.\n\nPrerequisites\ntalosctl\n\nDownload talosctl:\n\ncurl -sL https://talos.dev/install | sh\n\nkubectl\n\nDownload kubectl via one of methods outlined in the documentation.\n\nCreate the Cluster\n\nNow run the following:\n\ntalosctl cluster create\n\n\nYou can explore using Talos API commands:\n\ntalosctl dashboard --nodes 10.5.0.2\n\n\nVerify that you can reach Kubernetes:\n\n$ kubectl get nodes -o wide\n\nNAME                     STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-default-controlplane-1   Ready    master   115s   v1.29.0   10.5.0.2      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\ntalos-default-worker-1   Ready    <none>   115s   v1.29.0   10.5.0.3      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\nDestroy the Cluster\n\nWhen you are all done, remove the cluster:\n\ntalosctl cluster destroy\n\n3 - Getting Started\nA guide to setting up a Talos Linux cluster.\n\nThis document will walk you through installing a simple Talos Cluster with a single control plane node and one or more worker nodes, explaining some of the concepts.\n\nIf this is your first use of Talos Linux, we recommend the Quickstart first, to quickly create a local virtual cluster in containers on your workstation.\n\nFor a production cluster, extra steps are needed - see Production Notes.\n\nRegardless of where you run Talos, the steps to create a Kubernetes cluster are:\n\nboot machines off the Talos Linux image\ndefine the endpoint for the Kubernetes API and generate your machine configurations\nconfigure Talos Linux by applying machine configurations to the machines\nconfigure talosctl\nbootstrap Kubernetes\nPrerequisites\ntalosctl\n\ntalosctl is a CLI tool which interfaces with the Talos API. Talos Linux has no SSH access: talosctl is the tool you use to interact with the operating system on the machines.\n\nInstall talosctl before continuing:\n\ncurl -sL https://talos.dev/install | sh\n\n\nNote: If you boot systems off the ISO, Talos on the ISO image runs in RAM and acts as an installer. The version of talosctl that is used to create the machine configurations controls the version of Talos Linux that is installed on the machines - NOT the image that the machines are initially booted off. For example, booting a machine off the Talos 1.3.7 ISO, but creating the initial configuration with talosctl binary of version 1.4.1, will result in a machine running Talos Linux version 1.4.1.\n\nIt is advisable to use the same version of talosctl as the version of the boot media used.\n\nNetwork access\n\nThis guide assumes that the systems being installed have outgoing access to the internet, allowing them to pull installer and container images, query NTP, etc. If needed, see the documentation on registry proxies, local registries, and airgapped installation.\n\nAcquire the Talos Linux image and boot machines\n\nThe most general way to install Talos Linux is to use the ISO image.\n\nThe latest ISO image can be found on the Github Releases page:\n\nX86: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\nARM64: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-arm64.iso\n\nWhen booted from the ISO, Talos will run in RAM and will not install to disk until provided a configuration. Thus, it is safe to boot any machine from the ISO.\n\nAt this point, you should:\n\nboot one machine off the ISO to be the control plane node\nboot one or more machines off the same ISO to be the workers\nAlternative Booting\n\nFor network booting and self-built media, see Production Notes. There are installation methods specific to specific platforms, such as pre-built AMIs for AWS - check the specific Installation Guides.)\n\nDefine the Kubernetes Endpoint\n\nIn order to configure Kubernetes, Talos needs to know what the endpoint of the Kubernetes API Server will be.\n\nBecause we are only creating a single control plane node in this guide, we can use the control plane node directly as the Kubernetes API endpoint.\n\nIdentify the IP address or DNS name of the control plane node that was booted above, and convert it to a fully-qualified HTTPS URL endpoint address for the Kubernetes API Server which (by default) runs on port 6443. The endpoint should be formatted like:\n\nhttps://192.168.0.2:6443\nhttps://kube.mycluster.mydomain.com:6443\n\nNOTE: For a production cluster, you should have three control plane nodes, and have the endpoint allocate traffic to all three - see Production Notes.\n\nAccessing the Talos API\n\nAdministrative tasks are performed by calling the Talos API (usually with talosctl) on Talos Linux control plane nodes - thus, ensure your control plane node is directly reachable on TCP port 50000 from the workstation where you run the talosctl client. This may require changing firewall rules or cloud provider access-lists.\n\nFor production configurations, see Production Notes.\n\nConfigure Talos Linux\n\nWhen Talos boots without a configuration, such as when booting off the Talos ISO, it enters maintenance mode and waits for a configuration to be provided.\n\nA configuration can be passed in on boot via kernel parameters or metadata servers. See Production Notes.\n\nUnlike traditional Linux, Talos Linux is not configured by SSHing to the server and issuing commands. Instead, the entire state of the machine is defined by a machine config file which is passed to the server. This allows machines to be managed in a declarative way, and lends itself to GitOps and modern operations paradigms. The state of a machine is completely defined by, and can be reproduced from, the machine configuration file.\n\nTo generate the machine configurations for a cluster, run this command on the workstation where you installed talosctl:\n\ntalosctl gen config <cluster-name> <cluster-endpoint>\n\n\ncluster-name is an arbitrary name, used as a label in your local client configuration. It should be unique in the configuration on your local workstation.\n\ncluster-endpoint is the Kubernetes Endpoint you constructed from the control plane node’s IP address or DNS name above. It should be a complete URL, with https:// and port.\n\nFor example:\n\n$ talosctl gen config mycluster https://192.168.0.2:6443\n\ngenerating PKI and tokens\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n\nWhen you run this command, three files are created in your current directory:\n\ncontrolplane.yaml\nworker.yaml\ntalosconfig\n\nThe .yaml files are Machine Configs: they describe everything from what disk Talos should be installed on, to network settings. The controlplane.yaml file also describes how Talos should form a Kubernetes cluster.\n\nThe talosconfig file is your local client configuration file, used to connect to and authenticate access to the cluster.\n\nControlplane and Worker\n\nThe two types of Machine Configs correspond to the two roles of Talos nodes, control plane nodes (which run both the Talos and Kubernetes control planes) and worker nodes (which run the workloads).\n\nThe main difference between Controlplane Machine Config files and Worker Machine Config files is that the former contains information about how to form the Kubernetes cluster.\n\nModifying the Machine configs\n\nThe generated Machine Configs have defaults that work for most cases. They use DHCP for interface configuration, and install to /dev/sda.\n\nSometimes, you will need to modify the generated files to work with your systems. A common case is needing to change the installation disk. If you try to to apply the machine config to a node, and get an error like the below, you need to specify a different installation disk:\n\n$ talosctl apply-config --insecure -n 192.168.0.2 --file controlplane.yaml\n\nerror applying new configuration: rpc error: code = InvalidArgument desc = configuration validation failed: 1 error occurred:\n\n    * specified install disk does not exist: \"/dev/sda\"\n\n\nYou can verify which disks your nodes have by using the talosctl disks --insecure command.\n\nInsecure mode is needed at this point as the PKI infrastructure has not yet been set up.\n\nFor example, the talosctl disks command below shows that the system has a vda drive, not an sda:\n\n$ talosctl -n 192.168.0.2 disks --insecure\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID  MODALIAS                    NAME   SIZE    BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      69 GB   /pci0000:00/0000:00:06.0/virtio2/\n\n\nIn this case, you would modify the controlplane.yaml and worker.yaml files and edit the line:\n\ninstall:\n\n  disk: /dev/sda # The disk used for installations.\n\n\nto reflect vda instead of sda.\n\nFor information on customizing your machine configurations (such as to specify the version of Kubernetes), using machine configuration patches, or customizing configurations for individual machines (such as setting static IP addresses), see the Production Notes.\n\nUnderstand talosctl, endpoints and nodes\n\nIt is important to understand the concept of endpoints and nodes. In short: endpoints are where talosctl sends commands to, but the command operates on the specified nodes. The endpoint will forward the command to the nodes, if needed.\n\nEndpoints\n\nEndpoints are the IP addresses of control plane nodes, to which the talosctl client directly talks.\n\nEndpoints automatically proxy requests destined to another node in the cluster. This means that you only need access to the control plane nodes in order to manage the rest of the cluster.\n\nYou can pass in --endpoints <Control Plane IP Address> or -e <Control Plane IP Address> to the current talosctl command.\n\nIn this tutorial setup, the endpoint will always be the single control plane node.\n\nNodes\n\nNodes are the target(s) you wish to perform the operation on.\n\nWhen specifying nodes, the IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied through the endpoints.\n\nYou may provide -n or --nodes to any talosctl command to supply the node or (comma-separated) nodes on which you wish to perform the operation.\n\nFor example, to see the containers running on node 192.168.0.200, by routing the containers command through the control plane endpoint 192.168.0.2:\n\ntalosctl -e 192.168.0.2 -n 192.168.0.200 containers\n\n\nTo see the etcd logs on both nodes 192.168.0.10 and 192.168.0.11:\n\ntalosctl -e 192.168.0.2 -n 192.168.0.10,192.168.0.11 logs etcd\n\n\nFor a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nApply Configuration\n\nTo apply the Machine Configs, you need to know the machines’ IP addresses.\n\nTalos prints the IP addresses of the machines on the console during the boot process:\n\n[4.605369] [talos] task loadConfig (1/1): this machine is reachable at:\n[4.607358] [talos] task loadConfig (1/1):   192.168.0.2\n\n\nIf you do not have console access, the IP address may also be discoverable from your DHCP server.\n\nOnce you have the IP address, you can then apply the correct configuration. Apply the controlplane.yaml file to the control plane node, and the worker.yaml file to all the worker node(s).\n\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --file controlplane.yaml\n\n\nThe --insecure flag is necessary because the PKI infrastructure has not yet been made available to the node. Note: the connection will be encrypted, but not authenticated.\n\nWhen using the --insecure flag, it is not necessary to specify an endpoint.\n\nDefault talosconfig configuration file\n\nYou reference which configuration file to use by the --talosconfig parameter:\n\ntalosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 -e 192.168.0.2 version\n\n\nNote that talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. See Production Notes for more information.\n\nWhile getting started, a common mistake is referencing a configuration context for a different cluster, resulting in authentication or connection failures. Thus it is recommended to explicitly pass in the configuration file while becoming familiar with Talos Linux.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster with Talos is as simple as calling talosctl bootstrap on your control plane node:\n\ntalosctl bootstrap --nodes 192.168.0.2 --endpoints 192.168.0.2 \\\n\n  --talosconfig=./talosconfig\n\n\nThe bootstrap operation should only be called ONCE on a SINGLE control plane node. (If you have multiple control plane nodes, it doesn’t matter which one you issue the bootstrap command against.)\n\nAt this point, Talos will form an etcd cluster, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\n  talosctl kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\n  talosctl kubeconfig alternative-kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 health \\\n\n   --talosconfig=./talosconfig\n\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 dashboard \\\n\n   --talosconfig=./talosconfig\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n4 - Production Clusters\nRecommendations for setting up a Talos Linux cluster in production.\n\nThis document explains recommendations for running Talos Linux in production.\n\nAcquire the installation image\nAlternative Booting\n\nFor network booting and self-built media, you can use the published kernel and initramfs images:\n\nX86: vmlinuz-amd64 initramfs-amd64.xz\nARM64: vmlinuz-arm64 initramfs-arm64.xz\n\nNote that to use alternate booting, there are a number of required kernel parameters. Please see the kernel docs for more information.\n\nControl plane nodes\n\nFor a production, highly available Kubernetes cluster, it is recommended to use three control plane nodes. Using five nodes can provide greater fault tolerance, but imposes more replication overhead and can result in worse performance.\n\nBoot all three control plane nodes at this point. They will boot Talos Linux, and come up in maintenance mode, awaiting a configuration.\n\nDecide the Kubernetes Endpoint\n\nThe Kubernetes API Server endpoint, in order to be highly available, should be configured in a way that uses all available control plane nodes. There are three common ways to do this: using a load-balancer, using Talos Linux’s built in VIP functionality, or using multiple DNS records.\n\nDedicated Load-balancer\n\nIf you are using a cloud provider or have your own load-balancer (such as HAProxy, Nginx reverse proxy, or an F5 load-balancer), a dedicated load balancer is a natural choice. Create an appropriate frontend for the endpoint, listening on TCP port 6443, and point the backends at the addresses of each of the Talos control plane nodes. Your Kubernetes endpoint will be the IP address or DNS name of the load balancer front end, with the port appended (e.g. https://myK8s.mydomain.io:6443).\n\nNote: an HTTP load balancer can’t be used, as Kubernetes API server does TLS termination and mutual TLS authentication.\n\nLayer 2 VIP Shared IP\n\nTalos has integrated support for serving Kubernetes from a shared/virtual IP address. This requires Layer 2 connectivity between control plane nodes.\n\nChoose an unused IP address on the same subnet as the control plane nodes for the VIP. For instance, if your control plane node IPs are:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nyou could choose the IP 192.168.0.15 as your VIP IP address. (Make sure that 192.168.0.15 is not used by any other machine and is excluded from DHCP ranges.)\n\nOnce chosen, form the full HTTPS URL from this IP:\n\nhttps://192.168.0.15:6443\n\n\nIf you create a DNS record for this IP, note you will need to use the IP address itself, not the DNS name, to configure the shared IP (machine.network.interfaces[].vip.ip) in the Talos configuration.\n\nAfter the machine configurations are generated, you will want to edit the controlplane.yaml file to activate the VIP:\n\nmachine:\n\n    network:\n\n     interfaces:\n\n      - interface: enp2s0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.0.15\n\n\nFor more information about using a shared IP, see the related Guide\n\nDNS records\n\nAdd multiple A or AAAA records (one for each control plane node) to a DNS name.\n\nFor instance, you could add:\n\nkube.cluster1.mydomain.com  IN  A  192.168.0.10\nkube.cluster1.mydomain.com  IN  A  192.168.0.11\nkube.cluster1.mydomain.com  IN  A  192.168.0.12\n\n\nwhere the IP addresses are those of the control plane nodes.\n\nThen, your endpoint would be:\n\nhttps://kube.cluster1.mydomain.com:6443\n\nMultihoming\n\nIf your machines are multihomed, i.e., they have more than one IPv4 and/or IPv6 addresss other than loopback, then additional configuration is required. A point to note is that the machines may become multihomed via privileged workloads.\n\nMultihoming and etcd\n\nThe etcd cluster needs to establish a mesh of connections among the members. It is done using the so-called advertised address - each node learns the others’ addresses as they are advertised. It is crucial that these IP addresses are stable, i.e., that each node always advertises the same IP address. Moreover, it is beneficial to control them to establish the correct routes between the members and, e.g., avoid congested paths. In Talos, these addresses are controlled using the cluster.etcd.advertisedSubnets configuration key.\n\nMultihoming and kubelets\n\nStable IP addressing for kubelets (i.e., nodeIP) is not strictly necessary but highly recommended as it ensures that, e.g., kube-proxy and CNI routing take the desired routes. Analogously to etcd, for kubelets this is controlled via machine.kubelet.nodeIP.validSubnets.\n\nExample\n\nLet’s assume that we have a cluster with two networks:\n\npublic network\nprivate network 192.168.0.0/16\n\nWe want to use the private network for etcd and kubelet communication:\n\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.0.0/16\n\n#...\n\ncluster:\n\n  etcd:\n\n    advertisedSubnets: # listenSubnets defaults to advertisedSubnets if not set explicitly\n\n      - 192.168.0.0/16\n\n\nThis way we ensure that the etcd cluster will use the private network for communication and the kubelets will use the private network for communication with the control plane.\n\nLoad balancing the Talos API\n\nThe talosctl tool provides built-in client-side load-balancing across control plane nodes, so usually you do not need to configure a load balancer for the Talos API.\n\nHowever, if the control plane nodes are not directly reachable from the workstation where you run talosctl, then configure a load balancer to forward TCP port 50000 to the control plane nodes.\n\nNote: Because the Talos Linux API uses gRPC and mutual TLS, it cannot be proxied by a HTTP/S proxy, but only by a TCP load balancer.\n\nIf you create a load balancer to forward the Talos API calls, the load balancer IP or hostname will be used as the endpoint for talosctl.\n\nAdd the load balancer IP or hostname to the .machine.certSANs field of the machine configuration file.\n\nDo not use Talos Linux’s built in VIP function for accessing the Talos API. In the event of an error in etcd, the VIP will not function, and you will not be able to access the Talos API to recover.\n\nConfigure Talos\n\nIn many installation methods, a configuration can be passed in on boot.\n\nFor example, Talos can be booted with the talos.config kernel argument set to an HTTP(s) URL from which it should receive its configuration. Where a PXE server is available, this is much more efficient than manually configuring each node. If you do use this method, note that Talos requires a number of other kernel commandline parameters. See required kernel parameters.\n\nSimilarly, if creating EC2 kubernetes clusters, the configuration file can be passed in as --user-data to the aws ec2 run-instances command. See generally the Installation Guide for the platform being deployed.\n\nSeparating out secrets\n\nWhen generating the configuration files for a Talos Linux cluster, it is recommended to start with generating a secrets bundle which should be saved in a secure location. This bundle can be used to generate machine or client configurations at any time:\n\ntalosctl gen secrets -o secrets.yaml\n\n\nThe secrets.yaml can also be extracted from the existing controlplane machine configuration with talosctl gen secrets --from-controlplane-config controlplane.yaml -o secrets.yaml command.\n\nNow, we can generate the machine configuration for each node:\n\ntalosctl gen config --with-secrets secrets.yaml <cluster-name> <cluster-endpoint>\n\n\nHere, cluster-name is an arbitrary name for the cluster, used in your local client configuration as a label. It should be unique in the configuration on your local workstation.\n\nThe cluster-endpoint is the Kubernetes Endpoint you selected from above. This is the Kubernetes API URL, and it should be a complete URL, with https:// and port. (The default port is 6443, but you may have configured your load balancer to forward a different port.) For example:\n\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ngenerating PKI and tokens\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCustomizing Machine Configuration\n\nThe generated machine configuration provides sane defaults for most cases, but can be modified to fit specific needs.\n\nSome machine configuration options are available as flags for the talosctl gen config command, for example setting a specific Kubernetes version:\n\ntalosctl gen config --with-secrets secrets.yaml --kubernetes-version 1.25.4 my-cluster https://192.168.64.15:6443\n\n\nOther modifications are done with machine configuration patches. Machine configuration patches can be applied with talosctl gen config command:\n\ntalosctl gen config --with-secrets secrets.yaml --config-patch-control-plane @cni.patch my-cluster https://192.168.64.15:6443\n\n\nNote: @cni.patch means that the patch is read from a file named cni.patch.\n\nMachine Configs as Templates\n\nIndividual machines may need different settings: for instance, each may have a different static IP address.\n\nWhen different files are needed for machines of the same type, there are two supported flows:\n\nUse the talosctl gen config command to generate a template, and then patch the template for each machine with talosctl machineconfig patch.\nGenerate each machine configuration file separately with talosctl gen config while applying patches.\n\nFor example, given a machine configuration patch which sets the static machine hostname:\n\n# worker1.patch\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nEither of the following commands will generate a worker machine configuration file with the hostname set to worker1:\n\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n$ talosctl machineconfig patch worker.yaml --patch @worker1.patch --output worker1.yaml\n\ntalosctl gen config --with-secrets secrets.yaml --config-patch-worker @worker1.patch --output-types worker -o worker1.yaml my-cluster https://192.168.64.15:6443\n\nApply Configuration while validating the node identity\n\nIf you have console access you can extract the server certificate fingerprint and use it for an additional layer of validation:\n\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --cert-fingerprint xA9a1t2dMxB0NJ0qH1pDzilWbA3+DK/DjVbFaJBYheE= \\\n\n    --file cp0.yaml\n\n\nUsing the fingerprint allows you to be sure you are sending the configuration to the correct machine, but is completely optional. After the configuration is applied to a node, it will reboot. Repeat this process for each of the nodes in your cluster.\n\nFurther details about talosctl, endpoints and nodes\nEndpoints\n\nWhen passed multiple endpoints, talosctl will automatically load balance requests to, and fail over between, all endpoints.\n\nYou can pass in --endpoints <IP Address1>,<IP Address2> as a comma separated list of IP/DNS addresses to the current talosctl command. You can also set the endpoints in your talosconfig, by calling talosctl config endpoint <IP Address1> <IP Address2>. Note: these are space separated, not comma separated.\n\nAs an example, if the IP addresses of our control plane nodes are:\n\n192.168.0.2\n192.168.0.3\n192.168.0.4\n\nWe would set those in the talosconfig with:\n\n  talosctl --talosconfig=./talosconfig \\\n\n    config endpoint 192.168.0.2 192.168.0.3 192.168.0.4\n\nNodes\n\nThe node is the target you wish to perform the API call on.\n\nIt is possible to set a default set of nodes in the talosconfig file, but our recommendation is to explicitly pass in the node or nodes to be operated on with each talosctl command. For a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nDefault configuration file\n\nYou can reference which configuration file to use directly with the --talosconfig parameter:\n\n  talosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 version\n\n\nHowever, talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. This is done with the merge option.\n\n  talosctl config merge ./talosconfig\n\n\nThis will merge your new talosconfig into the default configuration file ($XDG_CONFIG_HOME/talos/config.yaml), creating it if necessary. Like Kubernetes, the talosconfig configuration files has multiple “contexts” which correspond to multiple clusters. The <cluster-name> you chose above will be used as the context name.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster by simply calling the bootstrap command against any of your control plane nodes (or the loadbalancer, if used for the Talos API endpoint).:\n\n  talosctl bootstrap --nodes 192.168.0.2\n\n\nThe bootstrap operation should only be called ONCE and only on a SINGLE control plane node!\n\nAt this point, Talos will form an etcd cluster, generate all of the core Kubernetes assets, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\n  talosctl kubeconfig\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\n  talosctl kubeconfig alternative-kubeconfig\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\n  talosctl -n <NODEIP> dashboard\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n5 - System Requirements\nHardware requirements for running Talos Linux.\nMinimum Requirements\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t2 GiB\t2\t10 GiB\nWorker\t1 GiB\t1\t10 GiB\nRecommended\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t4 GiB\t4\t100 GiB\nWorker\t2 GiB\t2\t100 GiB\n\nThese requirements are similar to that of Kubernetes.\n\nStorage\n\nTalos Linux itself only requires less than 100 MB of disk space, but the EPHEMERAL partition is used to store pulled images, container work directories, and so on. Thus a minimum is 10 GiB of disk space is required. 100 GiB is desired. Note, however, that because Talos Linux assumes complete control of the disk it is installed on, so that it can control the partition table for image based upgrades, you cannot partition the rest of the disk for use by workloads.\n\nThus it is recommended to install Talos Linux on a small, dedicated disk - using a Terabyte sized SSD for the Talos install disk would be wasteful. Sidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\n6 - What's New in Talos 1.6.0\nList of new and shiny features in Talos Linux.\n\nSee also upgrade notes for important changes.\n\nBreaking Changes\nLinux Firmware\n\nStarting with Talos 1.6, Linux firmware is not included in the default initramfs.\n\nUsers that need Linux firmware can pull them as an extension during install time using the Image Factory service. If the initial boot requires firmware, a custom ISO can be built with the firmware included using the Image Factory service or using the imager. This also ensures that the linux-firmware is not tied to a specific Talos version.\n\nThe list of firmware packages which were removed from the default initramfs and are now available as extensions:\n\nbnx2 and bnx2x firmware (Broadcom NetXtreme II)\nIntel ICE firmware (Intel(R) Ethernet Controller 800 Series)\nNetwork Device Selectors\n\nPreviously, network device selectors only matched the first link, now the configuration is applied to all matching links.\n\ntalosctl images command\n\nThe command images deprecated in Talos 1.5 was removed, please use talosctl images default instead.\n\n.persist Machine Configuration Option\n\nThe option .persist deprecated in Talos 1.5 was removed, the machine configuration is always persisted.\n\nNew Features\nKubernetes n-5 Version Support\n\nTalos Linux starting with version 1.6 supports the latest Kubernetes n-5 versions, for release 1.6.0 this means support for Kubernetes versions 1.24-1.29. This allows users to make it easier to upgrade to new Talos Linux versions without having to upgrade Kubernetes at the same time.\n\nSee Kubernetes release support for the list of supported versions by Kubernetes project.\n\nOAuth2 Machine Config Flow\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow.\n\nIngress Firewall\n\nTalos Linux now supports configuring the ingress firewall rules.\n\nImprovements\nComponent Updates\nLinux: 6.1.67\nKubernetes: 1.29.0\ncontainerd: 1.7.10\nrunc: 1.1.10\netcd: 3.5.11\nCoreDNS: 1.11.1\nFlannel: 0.23.0\n\nTalos is built with Go 1.21.5.\n\nExtension Services\n\nTalos now starts Extension Services early in the boot process, this allows guest agents packaged as extension services to be started in maintenance mode.\n\nFlannel Configuration\n\nTalos Linux now supports customizing default Flannel manifest with extra arguments for flanneld:\n\ncluster:\n\n  network:\n\n    cni:\n\n      flannel:\n\n        extraArgs:\n\n          - --iface-can-reach=192.168.1.1\n\nKernel Arguments\n\nTalos and Imager now supports dropping kernel arguments specified in .machine.install.extraKernelArgs or as --extra-kernel-arg to imager. Any kernel argument that starts with a - is dropped. Kernel arguments to be dropped can be specified either as -<key> which would remove all arguments that start with <key> or as -<key>=<value> which would remove the exact argument.\n\nFor example, console=ttyS0 can be dropped by specifying -console=ttyS0 as an extra argument.\n\nkube-scheduler Configuration\n\nTalos now supports specifying the kube-scheduler configuration in the Talos configuration file. It can be set under cluster.scheduler.config and kube-scheduler will be automatically configured to with the correct flags.\n\nKubernetes Node Taint Configuration\n\nSimilar to machine.nodeLabels Talos Linux now provides machine.nodeTaints machine configuration field to configure Kubernetes Node taints.\n\nKubelet Credential Provider Configuration\n\nTalos now supports specifying the kubelet credential provider configuration in the Talos configuration file. It can be set under machine.kubelet.credentialProviderConfig and kubelet will be automatically configured to with the correct flags. The credential binaries are expected to be present under /usr/local/lib/kubelet/credentialproviders. Talos System Extensions can be used to install the credential binaries.\n\nKubePrism\n\nKubePrism is enabled by default on port 7445.\n\nSysctl\n\nTalos now handles sysctl/sysfs key names in line with sysctl.conf(5):\n\nif the first separator is ‘/’, no conversion is done\nif the first separator is ‘.’, dots and slashes are remapped\n\nExample (both sysctls are equivalent):\n\nmachine:\n\n  sysctls:\n\n    net/ipv6/conf/eth0.100/disable_ipv6: \"1\"\n\n    net.ipv6.conf.eth0/100.disable_ipv6: \"1\"\n\nUser Disks\n\nTalos Linux now supports specifying user disks in .machine.disks machine configuration links via udev symlinks, e.g. /dev/disk/by-id/XXXX.\n\nPacket Capture\n\nTalos Linux provides more performant implementation server-side for the packet capture API (talosctl pcap CLI).\n\nMemory Usage and Performance\n\nTalos Linux core components now use less memory and start faster.\n\n7 - Support Matrix\nTable of supported Talos Linux versions and respective platforms.\nTalos Version\t1.6\t1.5\nRelease Date\t2023-12-15\t2023-08-17 (1.5.0)\nEnd of Community Support\t1.7.0 release (2024-04-15, TBD)\t1.6.0 release (2023-12-15)\nEnterprise Support\toffered by Sidero Labs Inc.\toffered by Sidero Labs Inc.\nKubernetes\t1.29, 1.28, 1.27, 1.26, 1.25, 1.24\t1.28, 1.27, 1.26\nArchitecture\tamd64, arm64\tamd64, arm64\nPlatforms\t\t\n- cloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\n- bare metal\tx86: BIOS, UEFI, SecureBoot; arm64: UEFI, SecureBoot; boot: ISO, PXE, disk image\tx86: BIOS, UEFI; arm64: UEFI; boot: ISO, PXE, disk image\n- virtualized\tVMware, Hyper-V, KVM, Proxmox, Xen\tVMware, Hyper-V, KVM, Proxmox, Xen\n- SBCs\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\n- local\tDocker, QEMU\tDocker, QEMU\nCluster API\t\t\nCAPI Bootstrap Provider Talos\t>= 0.6.3\t>= 0.6.1\nCAPI Control Plane Provider Talos\t>= 0.5.4\t>= 0.5.2\nSidero\t>= 0.6.2\t>= 0.6.0\nPlatform Tiers\nTier 1: Automated tests, high-priority fixes.\nTier 2: Tested from time to time, medium-priority bugfixes.\nTier 3: Not tested by core Talos team, community tested.\nTier 1\nMetal\nAWS\nGCP\nTier 2\nAzure\nDigital Ocean\nOpenStack\nVMWare\nTier 3\nExoscale\nHetzner\nnocloud\nOracle Cloud\nScaleway\nVultr\nUpcloud\n8 - Troubleshooting\nTroubleshoot control plane and other failures for Talos Linux clusters.\n\nIn this guide we assume that Talos is configured with default features enabled, such as Discovery Service and KubePrism. If these features are disabled, some of the troubleshooting steps may not apply or may need to be adjusted.\n\nThis guide is structured so that it can be followed step-by-step, skip sections which are not relevant to your issue.\n\nNetwork Configuration\n\nAs Talos Linux is an API-based operating system, it is important to have networking configured so that the API can be accessed. Some information can be gathered from the Interactive Dashboard which is available on the machine console.\n\nWhen running in the cloud the networking should be configured automatically. Whereas when running on bare-metal it may need more specific configuration, see networking metal configuration guide.\n\nTalos API\n\nThe Talos API runs on port 50000. Control plane nodes should always serve the Talos API, while worker nodes require access to the control plane nodes to issue TLS certificates for the workers.\n\nFirewall Issues\n\nMake sure that the firewall is not blocking port 50000, and communication on ports 50000/50001 inside the cluster.\n\nClient Configuration Issues\n\nMake sure to use correct talosconfig client configuration file matching your cluster. See getting started for more information.\n\nThe most common issue is that talosctl gen config writes talosconfig to the file in the current directory, while talosctl by default picks up the configuration from the default location (~/.talos/config). The path to the configuration file can be specified with --talosconfig flag to talosctl.\n\nConflict on Kubernetes and Host Subnets\n\nIf talosctl returns an error saying that certificate IPs are empty, it might be due to a conflict between Kubernetes and host subnets. The Talos API runs on the host network, but it automatically excludes Kubernetes pod & network subnets from the useable set of addresses.\n\nTalos default machine configuration specifies the following Kubernetes pod and subnet IPv4 CIDRs: 10.244.0.0/16 and 10.96.0.0/12. If the host network is configured with one of these subnets, change the machine configuration to use a different subnet.\n\nWrong Endpoints\n\nThe talosctl CLI connects to the Talos API via the specified endpoints, which should be a list of control plane machine addresses. The client will automatically retry on other endpoints if there are unavailable endpoints.\n\nWorker nodes should not be used as the endpoint, as they are not able to forward request to other nodes.\n\nThe VIP should never be used as Talos API endpoint.\n\nTCP Loadbalancer\n\nWhen using a TCP loadbalancer, make sure the loadbalancer endpoint is included in the .machine.certSANs list in the machine configuration.\n\nSystem Requirements\n\nIf minimum system requirements are not met, this might manifest itself in various ways, such as random failures when starting services, or failures to pull images from the container registry.\n\nRunning Health Checks\n\nTalos Linux provides a set of basic health checks with talosctl health command which can be used to check the health of the cluster.\n\nIn the default mode, talosctl health uses information from the discovery to get the information about cluster members. This can be overridden with command line flags --control-plane-nodes and --worker-nodes.\n\nGathering Logs\n\nWhile the logs and state of the system can be queried via the Talos API, it is often useful to gather the logs from all nodes in the cluster, and analyze them offline. The talosctl support command can be used to gather logs and other information from the nodes specified with --nodes flag (multiple nodes are supported).\n\nDiscovery and Cluster Membership\n\nTalos Linux uses Discovery Service to discover other nodes in the cluster.\n\nThe list of members on each machine should be consistent: talosctl -n <IP> get members.\n\nSome Members are Missing\n\nEnsure connectivity to the discovery service (default is discovery.talos.dev:443), and that the discovery registry is not disabled.\n\nDuplicate Members\n\nDon’t use same base secrets to generate machine configuration for multiple clusters, as some secrets are used to identify members of the same cluster. So if the same machine configuration (or secrets) are used to repeatedly create and destroy clusters, the discovery service will see the same nodes as members of different clusters.\n\nRemoved Members are Still Present\n\nTalos Linux removes itself from the discovery service when it is reset. If the machine was not reset, it might show up as a member of the cluster for the maximum TTL of the discovery service (30 minutes), and after that it will be automatically removed.\n\netcd Issues\n\netcd is the distributed key-value store used by Kubernetes to store its state. Talos Linux provides automation to manage etcd members running on control plane nodes. If etcd is not healthy, the Kubernetes API server will not be able to function correctly.\n\nIt is always recommended to run an odd number of etcd members, as with 3 or more members it provides fault tolerance for less than quorum member failures.\n\nCommon troubleshooting steps:\n\ncheck etcd service state with talosctl -n IP service etcd for each control plane node\ncheck etcd membership on each control plane node with talosctl -n IP etcd member list\ncheck etcd logs with talosctl -n IP logs etcd\ncheck etcd alarms with talosctl -n IP etcd alarm list\nAll etcd Services are Stuck in Pre State\n\nMake sure that a single member was bootstrapped.\n\nCheck that the machine is able to pull the etcd container image, check talosctl dmesg for messages starting with retrying: prefix.\n\nSome etcd Services are Stuck in Pre State\n\nMake sure traffic is not blocked on port 2380 between controlplane nodes.\n\nCheck that etcd quorum is not lost.\n\nCheck that all control plane nodes are reported in talosctl get members output.\n\netcd Reports and Alarm\n\nSee etcd maintenance guide.\n\netcd Quorum is Lost\n\nSee disaster recovery guide.\n\nOther Issues\n\netcd will only run on control plane nodes. If a node is designated as a worker node, you should not expect etcd to be running on it.\n\nWhen a node boots for the first time, the etcd data directory (/var/lib/etcd) is empty, and it will only be populated when etcd is launched.\n\nIf the etcd service is crashing and restarting, check its logs with talosctl -n <IP> logs etcd. The most common reasons for crashes are:\n\nwrong arguments passed via extraArgs in the configuration;\nbooting Talos on non-empty disk with an existing Talos installation, /var/lib/etcd contains data from the old cluster.\nkubelet and Kubernetes Node Issues\n\nThe kubelet service should be running on all Talos nodes, and it is responsible for running Kubernetes pods, static pods (including control plane components), and registering the node with the Kubernetes API server.\n\nIf the kubelet doesn’t run on a control plane node, it will block the control plane components from starting.\n\nThe node will not be registered in Kubernetes until the Kubernetes API server is up and initial Kubernetes manifests are applied.\n\nkubelet is not running\n\nCheck that kubelet image is available (talosctl image ls --namespace system).\n\nCheck kubelet logs with talosctl -n IP logs kubelet for startup errors:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure kubelet extra arguments and extra configuration supplied with Talos machine configuration is valid\nTalos Complains about Node Not Found\n\nkubelet hasn’t yet registered the node with the Kubernetes API server, this is expected during initial cluster bootstrap, the error will go away. If the message persists, check Kubernetes API health.\n\nThe Kubernetes controller manager (kube-controller-manager) is responsible for monitoring the certificate signing requests (CSRs) and issuing certificates for each of them. The kubelet is responsible for generating and submitting the CSRs for its associated node.\n\nThe state of any CSRs can be checked with kubectl get csr:\n\n$ kubectl get csr\n\nNAME        AGE   SIGNERNAME                                    REQUESTOR                 CONDITION\n\ncsr-jcn9j   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-p6b9q   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-sw6rm   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-vlghg   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\nkubectl get nodes Reports Wrong Internal IP\n\nConfigure the correct internal IP address with .machine.kubelet.nodeIP\n\nkubectl get nodes Reports Wrong External IP\n\nTalos Linux doesn’t manage the external IP, it is managed with the Kubernetes Cloud Controller Manager.\n\nkubectl get nodes Reports Wrong Node Name\n\nBy default, the Kubernetes node name is derived from the hostname. Update the hostname using the machine configuration, cloud configuration, or via DHCP server.\n\nNode Is Not Ready\n\nA Node in Kubernetes is marked as Ready only once its CNI is up. It takes a minute or two for the CNI images to be pulled and for the CNI to start. If the node is stuck in this state for too long, check CNI pods and logs with kubectl. Usually, CNI-related resources are created in kube-system namespace.\n\nFor example, for the default Talos Flannel CNI:\n\n$ kubectl -n kube-system get pods\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\n...\n\nkube-flannel-25drx                               1/1     Running   0          23m\n\nkube-flannel-8lmb6                               1/1     Running   0          23m\n\nkube-flannel-gl7nx                               1/1     Running   0          23m\n\nkube-flannel-jknt9                               1/1     Running   0          23m\n\n...\n\nDuplicate/Stale Nodes\n\nTalos Linux doesn’t remove Kubernetes nodes automatically, so if a node is removed from the cluster, it will still be present in Kubernetes. Remove the node from Kubernetes with kubectl delete node <node-name>.\n\nTalos Complains about Certificate Errors on kubelet API\n\nThis error might appear during initial cluster bootstrap, and it will go away once the Kubernetes API server is up and the node is registered.\n\nBy default configuration, kubelet issues a self-signed server certificate, but when rotate-server-certificates feature is enabled, kubelet issues its certificate using kube-apiserver. Make sure the kubelet CSR is approved by the Kubernetes API server.\n\nIn either case, this error is not critical, as it only affects reporting of the pod status to Talos Linux.\n\nKubernetes Control Plane\n\nThe Kubernetes control plane consists of the following components:\n\nkube-apiserver - the Kubernetes API server\nkube-controller-manager - the Kubernetes controller manager\nkube-scheduler - the Kubernetes scheduler\n\nOptionally, kube-proxy runs as a DaemonSet to provide pod-to-service communication.\n\ncoredns provides name resolution for the cluster.\n\nCNI is not part of the control plane, but it is required for Kubernetes pods using pod networking.\n\nTroubleshooting should always start with kube-apiserver, and then proceed to other components.\n\nTalos Linux configures kube-apiserver to talk to the etcd running on the same node, so etcd must be healthy before kube-apiserver can start. The kube-controller-manager and kube-scheduler are configured to talk to the kube-apiserver on the same node, so they will not start until kube-apiserver is healthy.\n\nControl Plane Static Pods\n\nTalos should generate the static pod definitions for the Kubernetes control plane as resources:\n\n$ talosctl -n <IP> get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.2   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.2   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.2   k8s         StaticPod   kube-scheduler            1\n\n\nTalos should report that the static pod definitions are rendered for the kubelet:\n\n$ talosctl -n <IP> dmesg | grep 'rendered new'\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.550527204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-apiserver\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.552186204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-controller-manager\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.554607204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-scheduler\"}\n\n\nIf the static pod definitions are not rendered, check etcd and kubelet service health (see above) and the controller runtime logs (talosctl logs controller-runtime).\n\nControl Plane Pod Status\n\nInitially the kube-apiserver component will not be running, and it takes some time before it becomes fully up during bootstrap (image should be pulled from the Internet, etc.)\n\nThe status of the control plane components on each of the control plane nodes can be checked with talosctl containers -k:\n\n$ talosctl -n <IP> containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                            IMAGE                                               PID    STATUS\n\n172.20.0.2   k8s.io      kube-system/kube-apiserver-talos-default-controlplane-1                                       registry.k8s.io/pause:3.2                                2539   SANDBOX_READY\n\n172.20.0.2   k8s.io      └─ kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271        registry.k8s.io/kube-apiserver:v1.29.0 2572   CONTAINER_RUNNING\n\n\nThe logs of the control plane components can be checked with talosctl logs --kubernetes (or with -k as a shorthand):\n\ntalosctl -n <IP> logs -k kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271\n\n\nIf the control plane component reports error on startup, check that:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure extra arguments and extra configuration supplied with Talos machine configuration is valid\nKubernetes Bootstrap Manifests\n\nAs part of the bootstrap process, Talos injects bootstrap manifests into Kubernetes API server. There are two kinds of these manifests: system manifests built-in into Talos and extra manifests downloaded (custom CNI, extra manifests in the machine config):\n\n$ talosctl -n <IP> get manifests\n\nNODE         NAMESPACE      TYPE       ID                               VERSION\n\n172.20.0.2   controlplane   Manifest   00-kubelet-bootstrapping-token   1\n\n172.20.0.2   controlplane   Manifest   01-csr-approver-role-binding     1\n\n172.20.0.2   controlplane   Manifest   01-csr-node-bootstrap            1\n\n172.20.0.2   controlplane   Manifest   01-csr-renewal-role-binding      1\n\n172.20.0.2   controlplane   Manifest   02-kube-system-sa-role-binding   1\n\n172.20.0.2   controlplane   Manifest   03-default-pod-security-policy   1\n\n172.20.0.2   controlplane   Manifest   05-https://docs.projectcalico.org/manifests/calico.yaml   1\n\n172.20.0.2   controlplane   Manifest   10-kube-proxy                    1\n\n172.20.0.2   controlplane   Manifest   11-core-dns                      1\n\n172.20.0.2   controlplane   Manifest   11-core-dns-svc                  1\n\n172.20.0.2   controlplane   Manifest   11-kube-config-in-cluster        1\n\n\nDetails of each manifest can be queried by adding -o yaml:\n\n$ talosctl -n <IP> get manifests 01-csr-approver-role-binding --namespace=controlplane -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: Manifests.kubernetes.talos.dev\n\n    id: 01-csr-approver-role-binding\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    - apiVersion: rbac.authorization.k8s.io/v1\n\n      kind: ClusterRoleBinding\n\n      metadata:\n\n        name: system-bootstrap-approve-node-client-csr\n\n      roleRef:\n\n        apiGroup: rbac.authorization.k8s.io\n\n        kind: ClusterRole\n\n        name: system:certificates.k8s.io:certificatesigningrequests:nodeclient\n\n      subjects:\n\n        - apiGroup: rbac.authorization.k8s.io\n\n          kind: Group\n\n          name: system:bootstrappers\n\nOther Control Plane Components\n\nOnce the Kubernetes API server is up, other control plane components issues can be troubleshooted with kubectl:\n\nkubectl get nodes -o wide\n\nkubectl get pods -o wide --all-namespaces\n\nkubectl describe pod -n NAMESPACE POD\n\nkubectl logs -n NAMESPACE POD\n\nKubernetes API\n\nThe Kubernetes API client configuration (kubeconfig) can be retrieved using Talos API with talosctl -n <IP> kubeconfig command. Talos Linux mostly doesn’t depend on the Kubernetes API endpoint for the cluster, but Kubernetes API endpoint should be configured correctly for external access to the cluster.\n\nKubernetes Control Plane Endpoint\n\nThe Kubernetes control plane endpoint is the single canonical URL by which the Kubernetes API is accessed. Especially with high-availability (HA) control planes, this endpoint may point to a load balancer or a DNS name which may have multiple A and AAAA records.\n\nLike Talos’ own API, the Kubernetes API uses mutual TLS, client certs, and a common Certificate Authority (CA). Unlike general-purpose websites, there is no need for an upstream CA, so tools such as cert-manager, Let’s Encrypt, or products such as validated TLS certificates are not required. Encryption, however, is, and hence the URL scheme will always be https://.\n\nBy default, the Kubernetes API server in Talos runs on port 6443. As such, the control plane endpoint URLs for Talos will almost always be of the form https://endpoint:6443. (The port, since it is not the https default of 443 is required.) The endpoint above may be a DNS name or IP address, but it should be directed to the set of all controlplane nodes, as opposed to a single one.\n\nAs mentioned above, this can be achieved by a number of strategies, including:\n\nan external load balancer\nDNS records\nTalos-builtin shared IP (VIP)\nBGP peering of a shared IP (such as with kube-vip)\n\nUsing a DNS name here is a good idea, since it allows any other option, while offering a layer of abstraction. It allows the underlying IP addresses to change without impacting the canonical URL.\n\nUnlike most services in Kubernetes, the API server runs with host networking, meaning that it shares the network namespace with the host. This means you can use the IP address(es) of the host to refer to the Kubernetes API server.\n\nFor availability of the API, it is important that any load balancer be aware of the health of the backend API servers, to minimize disruptions during common node operations like reboots and upgrades.\n\nMiscellaneous\nChecking Controller Runtime Logs\n\nTalos runs a set of controllers which operate on resources to build and support machine operations.\n\nSome debugging information can be queried from the controller logs with talosctl logs controller-runtime:\n\ntalosctl -n <IP> logs controller-runtime\n\n\nControllers continuously run a reconcile loop, so at any time, they may be starting, failing, or restarting. This is expected behavior.\n\nIf there are no new messages in the controller-runtime log, it means that the controllers have successfully finished reconciling, and that the current system state is the desired system state.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "talosctl | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/talosctl/",
    "html": "Client Configuration\nEndpoints and Nodes\nKubeconfig\nCommands\nDocumentation\nLearn More\ntalosctl\ntalosctl\nThe design and use of the Talos Linux control application.\n\nThe talosctl tool acts as a reference implementation for the Talos API, but it also handles a lot of conveniences for the use of Talos and its clusters.\n\nVideo Walkthrough\n\nTo see some live examples of talosctl usage, view the following video:\n\nClient Configuration\n\nTalosctl configuration is located in $XDG_CONFIG_HOME/talos/config.yaml if $XDG_CONFIG_HOME is defined. Otherwise it is in $HOME/.talos/config. The location can always be overridden by the TALOSCONFIG environment variable or the --talosconfig parameter.\n\nLike kubectl, talosctl uses the concept of configuration contexts, so any number of Talos clusters can be managed with a single configuration file. It also comes with some intelligent tooling to manage the merging of new contexts into the config. The default operation is a non-destructive merge, where if a context of the same name already exists in the file, the context to be added is renamed by appending an index number. You can easily overwrite instead, as well. See the talosctl config help for more information.\n\nEndpoints and Nodes\n\nendpoints are the communication endpoints to which the client directly talks. These can be load balancers, DNS hostnames, a list of IPs, etc. If multiple endpoints are specified, the client will automatically load balance and fail over between them. It is recommended that these point to the set of control plane nodes, either directly or through a load balancer.\n\nEach endpoint will automatically proxy requests destined to another node through it, so it is not necessary to change the endpoint configuration just because you wish to talk to a different node within the cluster.\n\nEndpoints do, however, need to be members of the same Talos cluster as the target node, because these proxied connections reply on certificate-based authentication.\n\nThe node is the target node on which you wish to perform the API call. While you can configure the target node (or even set of target nodes) inside the ’talosctl’ configuration file, it is recommended not to do so, but to explicitly declare the target node(s) using the -n or --nodes command-line parameter.\n\nWhen specifying nodes, their IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied first through the endpoints.\n\nKubeconfig\n\nThe configuration for accessing a Talos Kubernetes cluster is obtained with talosctl. By default, talosctl will safely merge the cluster into the default kubeconfig. Like talosctl itself, in the event of a naming conflict, the new context name will be index-appended before insertion. The --force option can be used to overwrite instead.\n\nYou can also specify an alternate path by supplying it as a positional parameter.\n\nThus, like Talos clusters themselves, talosctl makes it easy to manage any number of kubernetes clusters from the same workstation.\n\nCommands\n\nPlease see the CLI reference for the entire list of commands which are available from talosctl.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Knowledge Base | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/knowledge-base/",
    "html": "Disabling GracefulNodeShutdown on a node\nGenerating Talos Linux ISO image with custom kernel arguments\nLogging Kubernetes audit logs with loki\nSetting CPU scaling governer\nDisable admissionControl on control plane nodes\nDocumentation\nLearn More\nKnowledge Base\nKnowledge Base\nRecipes for common configuration tasks with Talos Linux.\nDisabling GracefulNodeShutdown on a node\n\nTalos Linux enables Graceful Node Shutdown Kubernetes feature by default.\n\nIf this feature should be disabled, modify the kubelet part of the machine configuration with:\n\nCopy\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      feature-gates: GracefulNodeShutdown=false\n\n    extraConfig:\n\n      shutdownGracePeriod: 0s\n\n      shutdownGracePeriodCriticalPods: 0s\n\nGenerating Talos Linux ISO image with custom kernel arguments\n\nPass additional kernel arguments using --extra-kernel-arg flag:\n\nCopy\n$ docker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --extra-kernel-arg console=ttyS1 --extra-kernel-arg console=tty0 | tar xz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/vmlinuz to /mnt/boot/vmlinuz\n\n2022/05/25 13:18:47 copying /usr/install/amd64/initramfs.xz to /mnt/boot/initramfs.xz\n\n2022/05/25 13:18:47 creating grub.cfg\n\n2022/05/25 13:18:47 creating ISO\n\n\nISO will be output to the file talos-<arch>.iso in the current directory.\n\nLogging Kubernetes audit logs with loki\n\nIf using loki-stack helm chart to gather logs from the Kubernetes cluster, you can use the helm values to configure loki-stack to log Kubernetes API server audit logs:\n\nCopy\npromtail:\n\n  extraArgs:\n\n    - -config.expand-env\n\n  # this is required so that the promtail process can read the kube-apiserver audit logs written as `nobody` user\n\n  containerSecurityContext:\n\n    capabilities:\n\n      add:\n\n        - DAC_READ_SEARCH\n\n  extraVolumes:\n\n    - name: audit-logs\n\n      hostPath:\n\n        path: /var/log/audit/kube\n\n  extraVolumeMounts:\n\n    - name: audit-logs\n\n      mountPath: /var/log/audit/kube\n\n      readOnly: true\n\n  config:\n\n    snippets:\n\n      extraScrapeConfigs: |\n\n        - job_name: auditlogs\n\n          static_configs:\n\n            - targets:\n\n                - localhost\n\n              labels:\n\n                job: auditlogs\n\n                host: ${HOSTNAME}\n\n                __path__: /var/log/audit/kube/*.log        \n\nSetting CPU scaling governer\n\nWhile its possible to set CPU scaling governer via .machine.sysfs it’s sometimes cumbersome to set it for all CPU’s individually. A more elegant approach would be set it via a kernel commandline parameter. This also means that the options are applied way early in the boot process.\n\nThis can be set in the machineconfig via the snippet below:\n\nCopy\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - cpufreq.default_governor=performance\n\n\nNote: Talos needs to be upgraded for the extraKernelArgs to take effect.\n\nDisable admissionControl on control plane nodes\n\nTalos Linux enables admission control in the API Server by default.\n\nAlthough it is not recommended from a security point of view, admission control can be removed by patching your control plane machine configuration:\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch-control-plane '[{\"op\": \"remove\", \"path\": \"/cluster/apiServer/admissionControl\"}]'\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "FAQs | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/faqs/",
    "html": "How is Talos different from other container optimized Linux distros?\nWhy no shell or SSH?\nWhy the name “Talos”?\nWhy does Talos rely on a separate configuration from Kubernetes?\nHow does Talos handle certificates?\nHow can I set the timezone of my Talos Linux clusters?\nHow do I see Talos kernel configuration?\nUsing Talos API\nUsing GitHub\nDocumentation\nLearn More\nFAQs\nFAQs\nFrequently Asked Questions about Talos Linux.\nHow is Talos different from other container optimized Linux distros?\n\nTalos integrates tightly with Kubernetes, and is not meant to be a general-purpose operating system. The most important difference is that Talos is fully controlled by an API via a gRPC interface, instead of an ordinary shell. We don’t ship SSH, and there is no console access. Removing components such as these has allowed us to dramatically reduce the footprint of Talos, and in turn, improve a number of other areas like security, predictability, reliability, and consistency across platforms. It’s a big change from how operating systems have been managed in the past, but we believe that API-driven OSes are the future.\n\nWhy no shell or SSH?\n\nSince Talos is fully API-driven, all maintenance and debugging operations are possible via the OS API. We would like for Talos users to start thinking about what a “machine” is in the context of a Kubernetes cluster. That is, that a Kubernetes cluster can be thought of as one massive machine, and the nodes are merely additional, undifferentiated resources. We don’t want humans to focus on the nodes, but rather on the machine that is the Kubernetes cluster. Should an issue arise at the node level, talosctl should provide the necessary tooling to assist in the identification, debugging, and remediation of the issue. However, the API is based on the Principle of Least Privilege, and exposes only a limited set of methods. We envision Talos being a great place for the application of control theory in order to provide a self-healing platform.\n\nWhy the name “Talos”?\n\nTalos was an automaton created by the Greek God of the forge to protect the island of Crete. He would patrol the coast and enforce laws throughout the land. We felt it was a fitting name for a security focused operating system designed to run Kubernetes.\n\nWhy does Talos rely on a separate configuration from Kubernetes?\n\nThe talosconfig file contains client credentials to access the Talos Linux API. Sometimes Kubernetes might be down for a number of reasons (etcd issues, misconfiguration, etc.), while Talos API access will always be available. The Talos API is a way to access the operating system and fix issues, e.g. fixing access to Kubernetes. When Talos Linux is running fine, using the Kubernetes APIs (via kubeconfig) is all you should need to deploy and manage Kubernetes workloads.\n\nHow does Talos handle certificates?\n\nDuring the machine config generation process, Talos generates a set of certificate authorities (CAs) that remains valid for 10 years. Talos is responsible for managing certificates for etcd, Talos API (apid), node certificates (kubelet), and other components. It also handles the automatic rotation of server-side certificates.\n\nHowever, client certificates such as talosconfig and kubeconfig are the user’s responsibility, and by default, they have a validity period of 1 year.\n\nTo renew the talosconfig certificate, the follow this process. To renew kubeconfig, use talosctl kubeconfig command, and the time-to-live (TTL) is defined in the configuration.\n\nHow can I set the timezone of my Talos Linux clusters?\n\nTalos doesn’t support timezones, and will always run in UTC. This ensures consistency of log timestamps for all Talos Linux clusters, simplifying debugging. Your containers can run with any timezone configuration you desire, but the timezone of Talos Linux is not configurable.\n\nHow do I see Talos kernel configuration?\nUsing Talos API\n\nCurrent kernel config can be read with talosctl -n <NODE> read /proc/config.gz.\n\nFor example:\n\nCopy\ntalosctl -n NODE read /proc/config.gz | zgrep E1000\n\nUsing GitHub\n\nFor amd64, see https://github.com/siderolabs/pkgs/blob/main/kernel/build/config-amd64. Use appropriate branch to see the kernel config matching your Talos release.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Process Capabilities | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/process-capabilities/",
    "html": "Documentation\nLearn More\nProcess Capabilities\nProcess Capabilities\nUnderstand the Linux process capabilities restrictions with Talos Linux.\n\nLinux defines a set of process capabilities that can be used to fine-tune the process permissions.\n\nTalos Linux for security reasons restricts any process from gaining the following capabilities:\n\nCAP_SYS_MODULE (loading kernel modules)\nCAP_SYS_BOOT (rebooting the system)\n\nThis means that any process including privileged Kubernetes pods will not be able to get these capabilities.\n\nIf you see the following error on starting a pod, make sure it doesn’t have any of the capabilities listed above in the spec:\n\nCopy\nError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply caps: operation not permitted: unknown\n\n\nNote: even with CAP_SYS_MODULE capability, Linux kernel module loading is restricted by requiring a valid signature. Talos Linux creates a throw away signing key during kernel build, so it’s not possible to build/sign a kernel module for Talos Linux outside of the build process.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "KubeSpan | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/kubespan/",
    "html": "WireGuard Peer Discovery\nNAT, Multiple Routes, Multiple IPs\nPacket Routing\nDesign Decisions\nRouting\nRouting Table\nNFTables\nRules\nFirewall Mark\nDocumentation\nLearn More\nKubeSpan\nKubeSpan\nUnderstand more about KubeSpan for Talos Linux.\nWireGuard Peer Discovery\n\nThe key pieces of information needed for WireGuard generally are:\n\nthe public key of the host you wish to connect to\nan IP address and port of the host you wish to connect to\n\nThe latter is really only required of one side of the pair. Once traffic is received, that information is learned and updated by WireGuard automatically.\n\nKubernetes, though, also needs to know which traffic goes to which WireGuard peer. Because this information may be dynamic, we need a way to keep this information up to date.\n\nIf we already have a connection to Kubernetes, it’s fairly easy: we can just keep that information in Kubernetes. Otherwise, we have to have some way to discover it.\n\nTalos Linux implements a multi-tiered approach to gathering this information. Each tier can operate independently, but the amalgamation of the mechanisms produces a more robust set of connection criteria.\n\nThese mechanisms are:\n\nan external service\na Kubernetes-based system\n\nSee discovery service to learn more about the external service.\n\nThe Kubernetes-based system utilizes annotations on Kubernetes Nodes which describe each node’s public key and local addresses.\n\nOn top of this, KubeSpan can optionally route Pod subnets. This is usually taken care of by the CNI, but there are many situations where the CNI fails to be able to do this itself, across networks.\n\nNAT, Multiple Routes, Multiple IPs\n\nOne of the difficulties in communicating across networks is that there is often not a single address and port which can identify a connection for each node on the system. For instance, a node sitting on the same network might see its peer as 192.168.2.10, but a node across the internet may see it as 2001:db8:1ef1::10.\n\nWe need to be able to handle any number of addresses and ports, and we also need to have a mechanism to try them. WireGuard only allows us to select one at a time.\n\nKubeSpan implements a controller which continuously discovers and rotates these IP:port pairs until a connection is established. It then starts trying again if that connection ever fails.\n\nPacket Routing\n\nAfter we have established a WireGuard connection, we have to make sure that the right packets get sent to the WireGuard interface.\n\nWireGuard supplies a convenient facility for tagging packets which come from it, which is great. But in our case, we need to be able to allow traffic which both does not come from WireGuard and also is not destined for another Kubernetes node to flow through the normal mechanisms.\n\nUnlike many corporate or privacy-oriented VPNs, we need to allow general internet traffic to flow normally.\n\nAlso, as our cluster grows, this set of IP addresses can become quite large and quite dynamic. This would be very cumbersome and slow in iptables. Luckily, the kernel supplies a convenient mechanism by which to define this arbitrarily large set of IP addresses: IP sets.\n\nTalos collects all of the IPs and subnets which are considered “in-cluster” and maintains these in the kernel as an IP set.\n\nNow that we have the IP set defined, we need to tell the kernel how to use it.\n\nThe traditional way of doing this would be to use iptables. However, there is a big problem with IPTables. It is a common namespace in which any number of other pieces of software may dump things. We have no surety that what we add will not be wiped out by something else (from Kubernetes itself, to the CNI, to some workload application), be rendered unusable by higher-priority rules, or just generally cause trouble and conflicts.\n\nInstead, we use a three-pronged system which is both more foundational and less centralised.\n\nNFTables offers a separately namespaced, decentralised way of marking packets for later processing based on IP sets. Instead of a common set of well-known tables, NFTables uses hooks into the kernel’s netfilter system, which are less vulnerable to being usurped, bypassed, or a source of interference than IPTables, but which are rendered down by the kernel to the same underlying XTables system.\n\nOur NFTables system is where we store the IP sets. Any packet which enters the system, either by forward from inside Kubernetes or by generation from the host itself, is compared against a hash table of this IP set. If it is matched, it is marked for later processing by our next stage. This is a high-performance system which exists fully in the kernel and which ultimately becomes an eBPF program, so it scales well to hundreds of nodes.\n\nThe next stage is the kernel router’s route rules. These are defined as a common ordered list of operations for the whole operating system, but they are intended to be tightly constrained and are rarely used by applications in any case. The rules we add are very simple: if a packet is marked by our NFTables system, send it to an alternate routing table.\n\nThis leads us to our third and final stage of packet routing. We have a custom routing table with two rules:\n\nsend all IPv4 traffic to the WireGuard interface\nsend all IPv6 traffic to the WireGuard interface\n\nSo in summary, we:\n\nmark packets destined for Kubernetes applications or Kubernetes nodes\nsend marked packets to a special routing table\nsend anything which is sent to that routing table through the WireGuard interface\n\nThis gives us an isolated, resilient, tolerant, and non-invasive way to route Kubernetes traffic safely, automatically, and transparently through WireGuard across almost any set of network topologies.\n\nDesign Decisions\nRouting\n\nRouting for Wireguard is a touch complicated when the set of possible peer endpoints includes at least one member of the set of destinations. That is, packets from Wireguard to a peer endpoint should not be sent to Wireguard, lest a loop be created.\n\nIn order to handle this situation, Wireguard provides the ability to mark packets which it generates, so their routing can be handled separately.\n\nIn our case, though, we actually want the inverse of this: we want to route Wireguard packets however the normal networking routes and rules say they should be routed, while packets destined for the other side of Wireguard Peers should be forced into Wireguard interfaces.\n\nWhile IP Rules allow you to invert matches, they do not support matching based on IP sets. That means, to use simple rules, we would have to add a rule for each destination, which could reach into hundreds or thousands of rules to manage. This is not really much of a performance issue, but it is a management issue, since it is expected that we would not be the only manager of rules in the system, and rules offer no facility to tag for ownership.\n\nIP Sets are supported by IPTables, and we could integrate there. However, IPTables exists in a global namespace, which makes it fragile having multiple parties manipulating it. The newer NFTables replacement for IPTables, though, allows users to independently hook into various points of XTables, keeping all such rules and sets independent. This means that regardless of what CNIs or other user-side routing rules may do, our KubeSpan setup will not be messed up.\n\nTherefore, we utilise NFTables (which natively supports IP sets and owner grouping) instead, to mark matching traffic which should be sent to the Wireguard interface. This way, we can keep all our KubeSpan set logic in one place, allowing us to simply use a single ip rule match: for our fwmark, and sending those matched packets to a separate routing table with one rule: default to the wireguard interface.\n\nSo we have three components:\n\nA routing table for Wireguard-destined packets\nAn NFTables table which defines the set of destinations packets to which will be marked with our firewall mark.\nHook into PreRouting (type Filter)\nHook into Outgoing (type Route)\nOne IP Rule which sends packets marked with our firewall mark to our Wireguard routing table.\nRouting Table\n\nThe routing table (number 180 by default) is simple, containing a single route for each family: send everything through the Wireguard interface.\n\nNFTables\n\nThe logic inside NFTables is fairly simple. First, everything is compiled into a single table: talos_kubespan.\n\nNext, two chains are set up: one for the prerouting hook (kubespan_prerouting) and the other for the outgoing hook (kubespan_outgoing).\n\nWe define two sets of target IP prefixes: one for IPv6 (kubespan_targets_ipv6) and the other for IPv4 (kubespan_targets_ipv4).\n\nLast, we add rules to each chain which basically specify:\n\nIf the packet is marked as from Wireguard, just accept it and terminate the chain.\nIf the packet matches an IP in either of the target IP sets, mark that packet with the to Wireguard mark.\nRules\n\nThere are two route rules defined: one to match IPv6 packets and the other to match IPv4 packets.\n\nThese rules say the same thing for each: if the packet is marked that it should go to Wireguard, send it to the Wireguard routing table.\n\nFirewall Mark\n\nKubeSpan is using only two bits of the firewall mark with the mask 0x00000060.\n\nNote: if other software on the node is using the bits 0x60 of the firewall mark, this might cause conflicts and break KubeSpan.\n\nAt the moment of the writing, it was confirmed that Calico CNI is using bits 0xffff0000 and Cilium CNI is using bits 0xf00, so KubeSpan is compatible with both. Flannel CNI uses 0x4000 mask, so it is also compatible.\n\nIn the routing rules table, we match on the mark 0x40 with the mask 0x60:\n\nCopy\n32500: from all fwmark 0x40/0x60 lookup 180\n\n\nIn the NFTables table, we match with the same mask 0x60 and we set the mask by only modifying bits from the 0x60 mask:\n\nCopy\nmeta mark & 0x00000060 == 0x00000020 accept\n\nip daddr @kubespan_targets_ipv4 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\nip6 daddr @kubespan_targets_ipv6 meta mark set meta mark & 0xffffffdf | 0x00000040 accept\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Networking Resources | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/networking-resources/",
    "html": "Resources\nObserving State\nInspecting Configuration\nConfiguration Merging\nNetwork Operators\nOther Network Resources\nNetwork Controllers\nConfiguration Sources\nDefaults\nCmdline\nPlatform\nOperator\nMachine Configuration\nNetwork Configuration Debugging\nDocumentation\nLearn More\nNetworking Resources\nNetworking Resources\nDelve deeper into networking of Talos Linux.\n\nTalos network configuration subsystem is powered by COSI. Talos translates network configuration from multiple sources: machine configuration, cloud metadata, network automatic configuration (e.g. DHCP) into COSI resources.\n\nNetwork configuration and network state can be inspected using talosctl get command.\n\nNetwork machine configuration can be modified using talosctl edit mc command (also variants talosctl patch mc, talosctl apply-config) without a reboot. As API access requires network connection, --mode=try can be used to test the configuration with automatic rollback to avoid losing network access to the node.\n\nResources\n\nThere are six basic network configuration items in Talos:\n\nAddress (IP address assigned to the interface/link);\nRoute (route to a destination);\nLink (network interface/link configuration);\nResolver (list of DNS servers);\nHostname (node hostname and domainname);\nTimeServer (list of NTP servers).\n\nEach network configuration item has two counterparts:\n\n*Status (e.g. LinkStatus) describes the current state of the system (Linux kernel state);\n*Spec (e.g. LinkSpec) defines the desired configuration.\nResource\tStatus\tSpec\nAddress\tAddressStatus\tAddressSpec\nRoute\tRouteStatus\tRouteSpec\nLink\tLinkStatus\tLinkSpec\nResolver\tResolverStatus\tResolverSpec\nHostname\tHostnameStatus\tHostnameSpec\nTimeServer\tTimeServerStatus\tTimeServerSpec\n\nStatus resources have aliases with the Status suffix removed, so for example AddressStatus is also available as Address.\n\nTalos networking controllers reconcile the state so that *Status equals the desired *Spec.\n\nObserving State\n\nThe current network configuration state can be observed by querying *Status resources via talosctl:\n\nCopy\n$ talosctl get addresses\n\nNODE         NAMESPACE   TYPE            ID                                       VERSION   ADDRESS                        LINK\n\n172.20.0.2   network     AddressStatus   eth0/172.20.0.2/24                       1         172.20.0.2/24                  eth0\n\n172.20.0.2   network     AddressStatus   eth0/fe80::9804:17ff:fe9d:3058/64        2         fe80::9804:17ff:fe9d:3058/64   eth0\n\n172.20.0.2   network     AddressStatus   flannel.1/10.244.4.0/32                  1         10.244.4.0/32                  flannel.1\n\n172.20.0.2   network     AddressStatus   flannel.1/fe80::10b5:44ff:fe62:6fb8/64   2         fe80::10b5:44ff:fe62:6fb8/64   flannel.1\n\n172.20.0.2   network     AddressStatus   lo/127.0.0.1/8                           1         127.0.0.1/8                    lo\n\n172.20.0.2   network     AddressStatus   lo/::1/128                               1         ::1/128                        lo\n\n\nIn the output there are addresses set up by Talos (e.g. eth0/172.20.0.2/24) and addresses set up by other facilities (e.g. flannel.1/10.244.4.0/32 set up by CNI).\n\nTalos networking controllers watch the kernel state and update resources accordingly.\n\nAdditional details about the address can be accessed via the YAML output:\n\nCopy\n# talosctl get address eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressStatuses.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 1\n\n    owner: network.AddressStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    address: 172.20.0.2/24\n\n    local: 172.20.0.2\n\n    broadcast: 172.20.0.255\n\n    linkIndex: 4\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n\nResources can be watched for changes with the --watch flag to see how configuration changes over time.\n\nOther networking status resources can be inspected with talosctl get routes, talosctl get links, etc. For example:\n\nCopy\n$ talosctl get resolvers\n\nNODE         NAMESPACE   TYPE             ID          VERSION   RESOLVERS\n\n172.20.0.2   network     ResolverStatus   resolvers   2         [\"8.8.8.8\",\"1.1.1.1\"]\n\nCopy\n# talosctl get links -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: LinkStatuses.net.talos.dev\n\n    id: eth0\n\n    version: 2\n\n    owner: network.LinkStatusController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    index: 4\n\n    type: ether\n\n    linkIndex: 0\n\n    flags: UP,BROADCAST,RUNNING,MULTICAST,LOWER_UP\n\n    hardwareAddr: 4e:95:8e:8f:e4:47\n\n    broadcastAddr: ff:ff:ff:ff:ff:ff\n\n    mtu: 1500\n\n    queueDisc: pfifo_fast\n\n    operationalState: up\n\n    kind: \"\"\n\n    slaveKind: \"\"\n\n    driver: virtio_net\n\n    linkState: true\n\n    speedMbit: 4294967295\n\n    port: Other\n\n    duplex: Unknown\n\nInspecting Configuration\n\nThe desired networking configuration is combined from multiple sources and presented as *Spec resources:\n\nCopy\n$ talosctl get addressspecs\n\nNODE         NAMESPACE   TYPE          ID                   VERSION\n\n172.20.0.2   network     AddressSpec   eth0/172.20.0.2/24   2\n\n172.20.0.2   network     AddressSpec   lo/127.0.0.1/8       2\n\n172.20.0.2   network     AddressSpec   lo/::1/128           2\n\n\nThese AddressSpecs are applied to the Linux kernel to reach the desired state. If, for example, an AddressSpec is removed, the address is removed from the Linux network interface as well.\n\n*Spec resources can’t be manipulated directly, they are generated automatically by Talos from multiple configuration sources (see a section below for details).\n\nIf a *Spec resource is queried in YAML format, some additional information is available:\n\nCopy\n# talosctl get addressspecs eth0/172.20.0.2/24 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: AddressSpecs.net.talos.dev\n\n    id: eth0/172.20.0.2/24\n\n    version: 2\n\n    owner: network.AddressMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.AddressSpecController\n\nspec:\n\n    address: 172.20.0.2/24\n\n    linkName: eth0\n\n    family: inet4\n\n    scope: global\n\n    flags: permanent\n\n    layer: operator\n\n\nAn important field is the layer field, which describes a configuration layer this spec is coming from: in this case, it’s generated by a network operator (see below) and is set by the DHCPv4 operator.\n\nConfiguration Merging\n\nSpec resources described in the previous section show the final merged configuration state, while initial specs are put to a different unmerged namespace network-config. Spec resources in the network-config namespace are merged with conflict resolution to produce the final merged representation in the network namespace.\n\nLet’s take HostnameSpec as an example. The final merged representation is:\n\nCopy\n# talosctl get hostnamespec -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: hostname\n\n    version: 2\n\n    owner: network.HostnameMergeController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\n    finalizers:\n\n        - network.HostnameSpecController\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that the final configuration for the hostname is talos-default-controlplane-1. And this is the hostname that was actually applied. This can be verified by querying a HostnameStatus resource:\n\nCopy\n$ talosctl get hostnamestatus\n\nNODE         NAMESPACE   TYPE             ID         VERSION   HOSTNAME                 DOMAINNAME\n\n172.20.0.2   network     HostnameStatus   hostname   1         talos-default-controlplane-1\n\n\nInitial configuration for the hostname in the network-config namespace is:\n\nCopy\n# talosctl get hostnamespec -o yaml --namespace network-config\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: default/hostname\n\n    version: 2\n\n    owner: network.HostnameConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-172-20-0-2\n\n    domainname: \"\"\n\n    layer: default\n\n---\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network-config\n\n    type: HostnameSpecs.net.talos.dev\n\n    id: dhcp4/eth0/hostname\n\n    version: 1\n\n    owner: network.OperatorSpecController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    hostname: talos-default-controlplane-1\n\n    domainname: \"\"\n\n    layer: operator\n\n\nWe can see that there are two specs for the hostname:\n\none from the default configuration layer which defines the hostname as talos-172-20-0-2 (default driven by the default node address);\nanother one from the layer operator that defines the hostname as talos-default-controlplane-1 (DHCP).\n\nTalos merges these two specs into a final HostnameSpec based on the configuration layer and merge rules. Here is the order of precedence from low to high:\n\ndefault (defaults provided by Talos);\ncmdline (from the kernel command line);\nplatform (driven by the cloud provider);\noperator (various dynamic configuration options: DHCP, Virtual IP, etc);\nconfiguration (derived from the machine configuration).\n\nSo in our example the operator layer HostnameSpec overrides the default layer producing the final hostname talos-default-controlplane-1.\n\nThe merge process applies to all six core networking specs. For each spec, the layer controls the merge behavior If multiple configuration specs appear at the same layer, they can be merged together if possible, otherwise merge result is stable but not defined (e.g. if DHCP on multiple interfaces provides two different hostnames for the node).\n\nLinkSpecs are merged across layers, so for example, machine configuration for the interface MTU overrides an MTU set by the DHCP server.\n\nNetwork Operators\n\nNetwork operators provide dynamic network configuration which can change over time as the node is running:\n\nDHCPv4\nDHCPv6\nVirtual IP\n\nNetwork operators produce specs for addresses, routes, links, etc., which are then merged and applied according to the rules described above.\n\nOperators are configured with OperatorSpec resources which describe when operators should run and additional configuration for the operator:\n\nCopy\n# talosctl get operatorspecs -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: network\n\n    type: OperatorSpecs.net.talos.dev\n\n    id: dhcp4/eth0\n\n    version: 1\n\n    owner: network.OperatorConfigController\n\n    phase: running\n\n    created: 2021-06-29T20:23:18Z\n\n    updated: 2021-06-29T20:23:18Z\n\nspec:\n\n    operator: dhcp4\n\n    linkName: eth0\n\n    requireUp: true\n\n    dhcp4:\n\n        routeMetric: 1024\n\n\nOperatorSpec resources are generated by Talos based on machine configuration mostly. DHCP4 operator is created automatically for all physical network links which are not configured explicitly via the kernel command line or the machine configuration. This also means that on the first boot, without a machine configuration, a DHCP request is made on all physical network interfaces by default.\n\nSpecs generated by operators are prefixed with the operator ID (dhcp4/eth0 in the example above) in the unmerged network-config namespace:\n\nCopy\n$ talosctl -n 172.20.0.2 get addressspecs --namespace network-config\n\nNODE         NAMESPACE        TYPE          ID                              VERSION\n\n172.20.0.2   network-config   AddressSpec   dhcp4/eth0/eth0/172.20.0.2/24   1\n\nOther Network Resources\n\nThere are some additional resources describing the network subsystem state.\n\nThe NodeAddress resource presents node addresses excluding link-local and loopback addresses:\n\nCopy\n$ talosctl get nodeaddresses\n\nNODE          NAMESPACE   TYPE          ID             VERSION   ADDRESSES\n\n10.100.2.23   network     NodeAddress   accumulative   6         [\"10.100.2.23\",\"147.75.98.173\",\"147.75.195.143\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   current        5         [\"10.100.2.23\",\"147.75.98.173\",\"192.168.95.64\",\"2604:1380:1:ca00::17\"]\n\n10.100.2.23   network     NodeAddress   default        1         [\"10.100.2.23\"]\n\ndefault is the node default address;\ncurrent is the set of addresses a node currently has;\naccumulative is the set of addresses a node had over time (it might include virtual IPs which are not owned by the node at the moment).\n\nNodeAddress resources are used to pick up the default address for etcd peer URL, to populate SANs field in the generated certificates, etc.\n\nAnother important resource is Nodename which provides Node name in Kubernetes:\n\nCopy\n$ talosctl get nodename\n\nNODE          NAMESPACE      TYPE       ID         VERSION   NODENAME\n\n10.100.2.23   controlplane   Nodename   nodename   1         infra-green-cp-mmf7v\n\n\nDepending on the machine configuration nodename might be just a hostname or the FQDN of the node.\n\nNetworkStatus aggregates the current state of the network configuration:\n\nCopy\n# talosctl get networkstatus -o yaml\n\nnode: 10.100.2.23\n\nmetadata:\n\n    namespace: network\n\n    type: NetworkStatuses.net.talos.dev\n\n    id: status\n\n    version: 5\n\n    owner: network.StatusController\n\n    phase: running\n\n    created: 2021-06-24T18:56:00Z\n\n    updated: 2021-06-24T18:56:02Z\n\nspec:\n\n    addressReady: true\n\n    connectivityReady: true\n\n    hostnameReady: true\n\n    etcFilesReady: true\n\nNetwork Controllers\n\nFor each of the six basic resource types, there are several controllers:\n\n*StatusController populates *Status resources observing the Linux kernel state.\n*ConfigController produces the initial unmerged *Spec resources in the network-config namespace based on defaults, kernel command line, and machine configuration.\n*MergeController merges *Spec resources into the final representation in the network namespace.\n*SpecController applies merged *Spec resources to the kernel state.\n\nFor the network operators:\n\nOperatorConfigController produces OperatorSpec resources based on machine configuration and deafauls.\nOperatorSpecController runs network operators watching OperatorSpec resources and producing various *Spec resources in the network-config namespace.\nConfiguration Sources\n\nThere are several configuration sources for the network configuration, which are described in this section.\n\nDefaults\nlo interface is assigned addresses 127.0.0.1/8 and ::1/128;\nhostname is set to the talos-<IP> where IP is the default node address;\nresolvers are set to 8.8.8.8, 1.1.1.1;\ntime servers are set to pool.ntp.org;\nDHCP4 operator is run on any physical interface which is not configured explicitly.\nCmdline\n\nThe kernel command line is parsed for the following options:\n\nip= option is parsed for node IP, default gateway, hostname, DNS servers, NTP servers;\nbond= option is parsed for bonding interfaces and their options;\ntalos.hostname= option is used to set node hostname;\ntalos.network.interface.ignore= can be used to make Talos skip network interface configuration completely.\nPlatform\n\nPlatform configuration delivers cloud environment-specific options (e.g. the hostname).\n\nPlatform configuration is specific to the environment metadata: for example, on Equinix Metal, Talos automatically configures public and private IPs, routing, link bonding, hostname.\n\nPlatform configuration is cached across reboots in /system/state/platform-network.yaml.\n\nOperator\n\nNetwork operators provide configuration for all basic resource types.\n\nMachine Configuration\n\nThe machine configuration is parsed for link configuration, addresses, routes, hostname, resolvers and time servers. Any changes to .machine.network configuration can be applied in immediate mode.\n\nNetwork Configuration Debugging\n\nMost of the network controller operations and failures are logged to the kernel console, additional logs with debug level are available with talosctl logs controller-runtime command. If the network configuration can’t be established and the API is not available, debug level logs can be sent to the console with debug: true option in the machine configuration.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network Connectivity | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/talos-network-connectivity/",
    "html": "Configuring Network Connectivity\nControl plane node(s)\nWorker node(s)\nDocumentation\nLearn More\nNetwork Connectivity\nNetwork Connectivity\nDescription of the Networking Connectivity needed by Talos Linux\nConfiguring Network Connectivity\n\nThe simplest way to deploy Talos is by ensuring that all the remote components of the system (talosctl, the control plane nodes, and worker nodes) all have layer 2 connectivity. This is not always possible, however, so this page lays out the minimal network access that is required to configure and operate a talos cluster.\n\nNote: These are the ports required for Talos specifically, and should be configured in addition to the ports required by kuberenetes. See the kubernetes docs for information on the ports used by kubernetes itself.\n\nControl plane node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\ttalosctl, control plane nodes\nTCP\tInbound\t50001*\ttrustd\tWorker nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\nWorker node(s)\nProtocol\tDirection\tPort Range\tPurpose\tUsed By\nTCP\tInbound\t50000*\tapid\tControl plane nodes\n\nPorts marked with a * are not currently configurable, but that may change in the future. Follow along here.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Controllers and Resources | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/controllers-resources/",
    "html": "Resources\nControllers\nQuerying Resources\nOutput\nWatching Changes\nExamples\nInspecting Controller Dependencies\nDocumentation\nLearn More\nControllers and Resources\nControllers and Resources\nDiscover how Talos Linux uses the concepts on Controllers and Resources.\n\nTalos implements concepts of resources and controllers to facilitate internal operations of the operating system. Talos resources and controllers are very similar to Kubernetes resources and controllers, but there are some differences. The content of this document is not required to operate Talos, but it is useful for troubleshooting.\n\nStarting with Talos 0.9, most of the Kubernetes control plane bootstrapping and operations is implemented via controllers and resources which allows Talos to be reactive to configuration changes, environment changes (e.g. time sync).\n\nResources\n\nA resource captures a piece of system state. Each resource belongs to a “Type” which defines resource contents. Resource state can be split in two parts:\n\nmetadata: fixed set of fields describing resource - namespace, type, ID, etc.\nspec: contents of the resource (depends on resource type).\n\nResource is uniquely identified by (namespace, type, id). Namespaces provide a way to avoid conflicts on duplicate resource IDs.\n\nAt the moment of this writing, all resources are local to the node and stored in memory. So on every reboot resource state is rebuilt from scratch (the only exception is MachineConfig resource which reflects current machine config).\n\nControllers\n\nControllers run as independent lightweight threads in Talos. The goal of the controller is to reconcile the state based on inputs and eventually update outputs.\n\nA controller can have any number of resource types (and namespaces) as inputs. In other words, it watches specified resources for changes and reconciles when these changes occur. A controller might also have additional inputs: running reconcile on schedule, watching etcd keys, etc.\n\nA controller has a single output: a set of resources of fixed type in a fixed namespace. Only one controller can manage resource type in the namespace, so conflicts are avoided.\n\nQuerying Resources\n\nTalos CLI tool talosctl provides read-only access to the resource API which includes getting specific resource, listing resources and watching for changes.\n\nTalos stores resources describing resource types and namespaces in meta namespace:\n\nCopy\n$ talosctl get resourcedefinitions\n\nNODE         NAMESPACE   TYPE                 ID                                               VERSION\n\n172.20.0.2   meta        ResourceDefinition   bootstrapstatuses.v1alpha1.talos.dev             1\n\n172.20.0.2   meta        ResourceDefinition   etcdsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   kubernetescontrolplaneconfigs.config.talos.dev   1\n\n172.20.0.2   meta        ResourceDefinition   kubernetessecrets.secrets.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   machineconfigs.config.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   machinetypes.config.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   manifests.kubernetes.talos.dev                   1\n\n172.20.0.2   meta        ResourceDefinition   manifeststatuses.kubernetes.talos.dev            1\n\n172.20.0.2   meta        ResourceDefinition   namespaces.meta.cosi.dev                         1\n\n172.20.0.2   meta        ResourceDefinition   resourcedefinitions.meta.cosi.dev                1\n\n172.20.0.2   meta        ResourceDefinition   rootsecrets.secrets.talos.dev                    1\n\n172.20.0.2   meta        ResourceDefinition   secretstatuses.kubernetes.talos.dev              1\n\n172.20.0.2   meta        ResourceDefinition   services.v1alpha1.talos.dev                      1\n\n172.20.0.2   meta        ResourceDefinition   staticpods.kubernetes.talos.dev                  1\n\n172.20.0.2   meta        ResourceDefinition   staticpodstatuses.kubernetes.talos.dev           1\n\n172.20.0.2   meta        ResourceDefinition   timestatuses.v1alpha1.talos.dev                  1\n\nCopy\n$ talosctl get namespaces\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\n172.20.0.2   meta        Namespace   controlplane   1\n\n172.20.0.2   meta        Namespace   meta           1\n\n172.20.0.2   meta        Namespace   runtime        1\n\n172.20.0.2   meta        Namespace   secrets        1\n\n\nMost of the time namespace flag (--namespace) can be omitted, as ResourceDefinition contains default namespace which is used if no namespace is given:\n\nCopy\n$ talosctl get resourcedefinitions resourcedefinitions.meta.cosi.dev -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: meta\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    id: resourcedefinitions.meta.cosi.dev\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    type: ResourceDefinitions.meta.cosi.dev\n\n    displayType: ResourceDefinition\n\n    aliases:\n\n        - resourcedefinitions\n\n        - resourcedefinition\n\n        - resourcedefinitions.meta\n\n        - resourcedefinitions.meta.cosi\n\n        - rd\n\n        - rds\n\n    printColumns: []\n\n    defaultNamespace: meta\n\n\nResource definition also contains type aliases which can be used interchangeably with canonical resource name:\n\nCopy\n$ talosctl get ns config\n\nNODE         NAMESPACE   TYPE        ID             VERSION\n\n172.20.0.2   meta        Namespace   config         1\n\nOutput\n\nCommand talosctl get supports following output modes:\n\ntable (default) prints resource list as a table\nyaml prints pretty formatted resources with details, including full metadata spec. This format carries most details from the backend resource (e.g. comments in MachineConfig resource)\njson prints same information as yaml, some additional details (e.g. comments) might be lost. This format is useful for automated processing with tools like jq.\nWatching Changes\n\nIf flag --watch is appended to the talosctl get command, the command switches to watch mode. If list of resources was requested, talosctl prints initial contents of the list and then appends resource information for every change:\n\nCopy\n$ talosctl get svc -w\n\nNODE         *   NAMESPACE   TYPE      ID     VERSION   RUNNING   HEALTHY\n\n172.20.0.2   +   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   trustd   2   true   true\n\n172.20.0.2   +   runtime   Service   udevd   2   true   true\n\n172.20.0.2   -   runtime   Service   timed   2   true   true\n\n172.20.0.2   +   runtime   Service   timed   1   true   false\n\n172.20.0.2       runtime   Service   timed   2   true   true\n\n\nColumn * specifies event type:\n\n+ is created\n- is deleted\nis updated\n\nIn YAML/JSON output, field event is added to the resource representation to describe the event type.\n\nExamples\n\nGetting machine config:\n\nCopy\n$ talosctl get machineconfig -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: MachineConfigs.config.talos.dev\n\n    id: v1alpha1\n\n    version: 2\n\n    phase: running\n\nspec:\n\n    version: v1alpha1 # Indicates the schema used to decode the contents.\n\n    debug: false # Enable verbose logging to the console.\n\n    persist: true # Indicates whether to pull the machine config upon every boot.\n\n    # Provides machine specific configuration options.\n\n...\n\n\nGetting control plane static pod statuses:\n\nCopy\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE      TYPE              ID                                                           VERSION   READY\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-1            3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-1   3         True\n\n172.20.0.2   controlplane   StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-1            4         True\n\n\nGetting static pod definition for kube-apiserver:\n\nCopy\n$ talosctl get sp kube-apiserver -n 172.20.0.2 -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: StaticPods.kubernetes.talos.dev\n\n    id: kube-apiserver\n\n    version: 3\n\n    phase: running\n\n    finalizers:\n\n        - k8s.StaticPodStatus(\"kube-apiserver\")\n\nspec:\n\n    apiVersion: v1\n\n    kind: Pod\n\n    metadata:\n\n        annotations:\n\n            talos.dev/config-version: \"1\"\n\n            talos.dev/secrets-version: \"2\"\n\n...\n\nInspecting Controller Dependencies\n\nTalos can report current dependencies between controllers and resources for debugging purposes:\n\nCopy\n$ talosctl inspect dependencies\n\ndigraph  {\n\n\n\n  n1[label=\"config.K8sControlPlaneController\",shape=\"box\"];\n\n  n3[label=\"config.MachineTypeController\",shape=\"box\"];\n\n  n2[fillcolor=\"azure2\",label=\"config:KubernetesControlPlaneConfigs.config.talos.dev\",shape=\"note\",style=\"filled\"];\n\n...\n\n\nThis outputs graph in graphviz format which can be rendered to PNG with command:\n\nCopy\ntalosctl inspect dependencies | dot -T png > deps.png\n\n\nGraph can be enhanced by replacing resource types with actual resource instances:\n\nCopy\ntalosctl inspect dependencies --with-resources | dot -T png > deps.png\n\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Control Plane | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/control-plane/",
    "html": "What is a control plane node?\nThe Control Plane and Etcd\nRecommendations for your control plane\nBootstrapping the Control Plane\nHigh-level Overview\nCluster Bootstrapping\nScaling Up the Control Plane\nScaling Down the Control Plane\nUpgrading Talos on Control Plane Nodes\nDocumentation\nLearn More\nControl Plane\nControl Plane\nUnderstand the Kubernetes Control Plane.\n\nThis guide provides information about the Kubernetes control plane, and details on how Talos runs and bootstraps the Kubernetes control plane.\n\nWhat is a control plane node?\n\nA control plane node is a node which:\n\nruns etcd, the Kubernetes database\nruns the Kubernetes control plane\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nserves as an administrative proxy to the worker nodes\n\nThese nodes are critical to the operation of your cluster. Without control plane nodes, Kubernetes will not respond to changes in the system, and certain central services may not be available.\n\nTalos nodes which have .machine.type of controlplane are control plane nodes. (check via talosctl get member)\n\nControl plane nodes are tainted by default to prevent workloads from being scheduled onto them. This is both to protect the control plane from workloads consuming resources and starving the control plane processes, and also to reduce the risk of a vulnerability exposes the control plane’s credentials to a workload.\n\nThe Control Plane and Etcd\n\nA critical design concept of Kubernetes (and Talos) is the etcd database.\n\nProperly managed (which Talos Linux does), etcd should never have split brain or noticeable down time. In order to do this, etcd maintains the concept of “membership” and of “quorum”. To perform any operation, read or write, the database requires quorum. That is, a majority of members must agree on the current leader, and absenteeism (members that are down, or not reachable) counts as a negative. For example, if there are three members, at least two out of the three must agree on the current leader. If two disagree or fail to answer, the etcd database will lock itself until quorum is achieved in order to protect the integrity of the data.\n\nThis design means that having two controlplane nodes is worse than having only one, because if either goes down, your database will lock (and the chance of one of two nodes going down is greater than the chance of just a single node going down). Similarly, a 4 node etcd cluster is worse than a 3 node etcd cluster - a 4 node cluster requires 3 nodes to be up to achieve quorum (in order to have a majority), while the 3 node cluster requires 2 nodes: i.e. both can support a single node failure and keep running - but the chance of a node failing in a 4 node cluster is higher than that in a 3 node cluster.\n\nAnother note about etcd: due to the need to replicate data amongst members, performance of etcd decreases as the cluster scales. A 5 node cluster can commit about 5% less writes per second than a 3 node cluster running on the same hardware.\n\nRecommendations for your control plane\nRun your clusters with three or five control plane nodes. Three is enough for most use cases. Five will give you better availability (in that it can tolerate two node failures simultaneously), but cost you more both in the number of nodes required, and also as each node may require more hardware resources to offset the performance degradation seen in larger clusters.\nImplement good monitoring and put processes in place to deal with a failed node in a timely manner (and test them!)\nEven with robust monitoring and procedures for replacing failed nodes in place, backup etcd and your control plane node configuration to guard against unforeseen disasters.\nMonitor the performance of your etcd clusters. If etcd performance is slow, vertically scale the nodes, not the number of nodes.\nIf a control plane node fails, remove it first, then add the replacement node. (This ensures that the failed node does not “vote” when adding in the new node, minimizing the chances of a quorum violation.)\nIf replacing a node that has not failed, add the new one, then remove the old.\nBootstrapping the Control Plane\n\nEvery new cluster must be bootstrapped only once, which is achieved by telling a single control plane node to initiate the bootstrap.\n\nBootstrapping itself does not do anything with Kubernetes. Bootstrapping only tells etcd to form a cluster, so don’t judge the success of a bootstrap by the failure of Kubernetes to start. Kubernetes relies on etcd, so bootstrapping is required, but it is not sufficient for Kubernetes to start. If your Kubernetes cluster fails to form for other reasons (say, a bad configuration option or unavailable container repository), if the bootstrap API call returns successfully, you do NOT need to bootstrap again: just fix the config or let Kubernetes retry.\n\nHigh-level Overview\n\nTalos cluster bootstrap flow:\n\nThe etcd service is started on control plane nodes. Instances of etcd on control plane nodes build the etcd cluster.\nThe kubelet service is started.\nControl plane components are started as static pods via the kubelet, and the kube-apiserver component connects to the local (running on the same node) etcd instance.\nThe kubelet issues client certificate using the bootstrap token using the control plane endpoint (via kube-apiserver and kube-controller-manager).\nThe kubelet registers the node in the API server.\nKubernetes control plane schedules pods on the nodes.\nCluster Bootstrapping\n\nAll nodes start the kubelet service. The kubelet tries to contact the control plane endpoint, but as it is not up yet, it keeps retrying.\n\nOne of the control plane nodes is chosen as the bootstrap node, and promoted using the bootstrap API (talosctl bootstrap). The bootstrap node initiates the etcd bootstrap process by initializing etcd as the first member of the cluster.\n\nOnce etcd is bootstrapped, the bootstrap node has no special role and acts the same way as other control plane nodes.\n\nServices etcd on non-bootstrap nodes try to get Endpoints resource via control plane endpoint, but that request fails as control plane endpoint is not up yet.\n\nAs soon as etcd is up on the bootstrap node, static pod definitions for the Kubernetes control plane components (kube-apiserver, kube-controller-manager, kube-scheduler) are rendered to disk. The kubelet service on the bootstrap node picks up the static pod definitions and starts the Kubernetes control plane components. As soon as kube-apiserver is launched, the control plane endpoint comes up.\n\nThe bootstrap node acquires an etcd mutex and injects the bootstrap manifests into the API server. The set of the bootstrap manifests specify the Kubernetes join token and kubelet CSR auto-approval. The kubelet service on all the nodes is now able to issue client certificates for themselves and register nodes in the API server.\n\nOther bootstrap manifests specify additional resources critical for Kubernetes operations (i.e. CNI, PSP, etc.)\n\nThe etcd service on non-bootstrap nodes is now able to discover other members of the etcd cluster via the Kubernetes Endpoints resource. The etcd cluster is now formed and consists of all control plane nodes.\n\nAll control plane nodes render static pod manifests for the control plane components. Each node now runs a full set of components to make the control plane HA.\n\nThe kubelet service on worker nodes is now able to issue the client certificate and register itself with the API server.\n\nScaling Up the Control Plane\n\nWhen new nodes are added to the control plane, the process is the same as the bootstrap process above: the etcd service discovers existing members of the control plane via the control plane endpoint, joins the etcd cluster, and the control plane components are scheduled on the node.\n\nScaling Down the Control Plane\n\nScaling down the control plane involves removing a node from the cluster. The most critical part is making sure that the node which is being removed leaves the etcd cluster. The recommended way to do this is to use:\n\ntalosctl -n IP.of.node.to.remove reset\nkubectl delete node\n\nWhen using talosctl reset command, the targeted control plane node leaves the etcd cluster as part of the reset sequence, and its disks are erased.\n\nUpgrading Talos on Control Plane Nodes\n\nWhen a control plane node is upgraded, Talos leaves etcd, wipes the system disk, installs a new version of itself, and reboots. The upgraded node then joins the etcd cluster on reboot. So upgrading a control plane node is equivalent to scaling down the control plane node followed by scaling up with a new version of Talos.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Image Factory | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/image-factory/",
    "html": "Schematics\nModels\nFrontends\nUI\nFind Schematic ID from Talos Installation\nRestrictions\nUnder the Hood\nSecurity\nRunning your own Image Factory\nDocumentation\nLearn More\nImage Factory\nImage Factory\nImage Factory generates customized Talos Linux images based on configured schematics.\n\nThe Image Factory provides a way to download Talos Linux artifacts. Artifacts can be generated with customizations defined by a “schematic”. A schematic can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”.\n\nThe following assets are provided:\n\nISO\nkernel, initramfs, and kernel command line\nUKI\ndisk images in various formats (e.g. AWS, GCP, VMware, etc.)\ninstaller container images\n\nThe supported frontends are:\n\nHTTP\nPXE\nContainer Registry\n\nThe official instance of Image Factory is available at https://factory.talos.dev.\n\nSee Boot Assets for an example of how to use the Image Factory to boot and upgrade Talos on different platforms. Full API documentation for the Image Factory is available at GitHub.\n\nSchematics\n\nSchematics are YAML files that define customizations to be applied to a Talos Linux image. Schematics can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”, which is a Talos Linux image with the customizations applied.\n\nSchematics are content-addressable, that is, the content of the schematic is used to generate a unique ID. The schematic should be uploaded to the Image Factory first, and then the ID can be used to reference the schematic in a model.\n\nSchematics can be generated using the Image Factory UI, or using the Image Factory API:\n\nCopy\ncustomization:\n\n  extraKernelArgs: # optional\n\n    - vga=791\n\n  meta: # optional, allows to set initial Talos META\n\n    - key: 0xa\n\n      value: \"{}\"\n\n  systemExtensions: # optional\n\n    officialExtensions: # optional\n\n      - siderolabs/gvisor\n\n      - siderolabs/amd-ucode\n\n\nThe “vanilla” schematic is:\n\nCopy\ncustomization:\n\n\nand has an ID of 376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba.\n\nThe schematic can be applied by uploading it to the Image Factory:\n\nCopy\ncurl -X POST --data-binary @schematic.yaml https://factory.talos.dev/schematics\n\n\nAs the schematic is content-addressable, the same schematic can be uploaded multiple times, and the Image Factory will return the same ID.\n\nModels\n\nModels are Talos Linux images with customizations applied. The inputs to generate a model are:\n\nschematic ID\nTalos Linux version\nmodel type (e.g. ISO, UKI, etc.)\narchitecture (e.g. amd64, arm64)\nvarious model type specific options (e.g. disk image format, disk image size, etc.)\nFrontends\n\nImage Factory provides several frontends to retrieve models:\n\nHTTP frontend to download models (e.g. download an ISO or a disk image)\nPXE frontend to boot bare-metal machines (PXE script references kernel/initramfs from HTTP frontend)\nRegistry frontend to fetch customized installer images (for initial Talos Linux installation and upgrades)\n\nThe links to different models are available in the Image Factory UI, and a full list of possible models is documented at GitHub.\n\nIn this guide we will provide a list of examples:\n\namd64 ISO (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64.iso\narm64 AWS image (for Talos v1.6.2, “vanilla” schematic) https://factory.talos.dev/image/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/aws-arm64.raw.xz\namd64 PXE boot script (for Talos v1.6.2, “vanilla” schematic) https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/metal-amd64\nTalos installer image (for Talos v1.6.2, “vanilla” schematic, architecture is detected automatically): factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2\n\nThe installer image can be used to install Talos Linux on a bare-metal machine, or to upgrade an existing Talos Linux installation. As the Talos version and schematic ID can be changed, via an upgrade process, the installer image can be used to upgrade to any version of Talos Linux, or replace a set of installed system extensions.\n\nUI\n\nThe Image Factory UI is available at https://factory.talos.dev. The UI provides a way to list supported Talos Linux versions, list of system extensions available for each release, and a way to generate schematic based on the selected system extensions.\n\nThe UI operations are equivalent to API operations.\n\nFind Schematic ID from Talos Installation\n\nImage Factory always appends “virtual” system extension with the version matching schematic ID used to generate the model. So, for any running Talos Linux instance the schematic ID can be found by looking at the list of system extensions:\n\nCopy\n$ talosctl get extensions\n\nNAMESPACE   TYPE              ID   VERSION   NAME       VERSION\n\nruntime     ExtensionStatus   0    1         schematic  376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba\n\nRestrictions\n\nSome models don’t include every customization of the schematic:\n\ninstaller and initramfs images only support system extensions (kernel args and META are ignored)\nkernel assets don’t depend on the schematic\n\nOther models have full support for all customizations:\n\nany disk image format\nISO, PXE boot script\n\nWhen installing Talos Linux using ISO/PXE boot, Talos will be installed on the disk using the installer image, so the installer image in the machine configuration should be using the same schematic as the ISO/PXE boot image.\n\nSome system extensions are not available for all Talos Linux versions, so an attempt to generate a model with an unsupported system extension will fail. List of supported Talos versions and supported system extensions for each version is available in the Image Factory UI and API.\n\nUnder the Hood\n\nImage Factory is based on the Talos imager container which provides both the Talos base boot assets, and the ability to generate custom assets based on a configuration. Image Factory manages a set of imager container images to acquire base Talos Linux boot assets (kernel, initramfs), a set of Talos Linux system extension images, and a set of schematics. When a model is requested, Image Factory uses the imager container to generate the requested assets based on the schematic and the Talos Linux version.\n\nSecurity\n\nImage Factory verifies signatures of all source container images fetched:\n\nimager container images (base boot assets)\nextensions system extensions catalogs\ninstaller contianer images (base installer layer)\nTalos Linux system extension images\n\nInternally, Image Factory caches generated boot assets and signs all cached images using a private key. Image Factory verifies the signature of the cached images before serving them to clients.\n\nImage Factory signs generated installer images, and verifies the signature of the installer images before serving them to clients.\n\nImage Factory does not provide a way to list all schematics, as schematics may contain sensitive information (e.g. private kernel boot arguments). As the schematic ID is content-addressable, it is not possible to guess the ID of a schematic without knowing the content of the schematic.\n\nRunning your own Image Factory\n\nImage Factory can be deployed on-premises to provide in-house asset generation.\n\nImage Factory requires following components:\n\nan OCI registry to store schematics (private)\nan OCI registry to store cached assets (private)\nan OCI registry to store installer images (should allow public read-only access)\na container image signing key: ECDSA P-256 private key in PEM format\n\nImage Factory is configured using command line flags, use --help to see a list of available flags. Image Factory should be configured to use proper authentication to push to the OCI registries:\n\nby mounting proper credentials via ~/.docker/config.json\nby supplying GITHUB_TOKEN (for ghcr.io)\n\nImage Factory performs HTTP redirects to the public registry endpoint for installer images, so the public endpoint should be available to Talos Linux machines to pull the installer images.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Components | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/components/",
    "html": "Components\napid\ncontainerd\nmachined\nkernel\ntrustd\nudevd\nDocumentation\nLearn More\nComponents\nComponents\nUnderstand the system components that make up Talos Linux.\n\nIn this section, we discuss the various components that underpin Talos.\n\nComponents\n\nTalos Linux and Kubernetes are tightly integrated.\n\nIn the following, the focus is on the Talos Linux specific components.\n\nComponent\tDescription\napid\tWhen interacting with Talos, the gRPC API endpoint you interact with directly is provided by apid. apid acts as the gateway for all component interactions and forwards the requests to machined.\ncontainerd\tAn industry-standard container runtime with an emphasis on simplicity, robustness, and portability. To learn more, see the containerd website.\nmachined\tTalos replacement for the traditional Linux init-process. Specially designed to run Kubernetes and does not allow starting arbitrary user services.\nkernel\tThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project.\ntrustd\tTo run and operate a Kubernetes cluster, a certain level of trust is required. Based on the concept of a ‘Root of Trust’, trustd is a simple daemon responsible for establishing trust within the system.\nudevd\tImplementation of eudev into machined. eudev is Gentoo’s fork of udev, systemd’s device file manager for the Linux kernel. It manages device nodes in /dev and handles all user space actions when adding or removing devices. To learn more, see the Gentoo Wiki.\napid\n\nWhen interacting with Talos, the gRPC api endpoint you will interact with directly is apid. Apid acts as the gateway for all component interactions. Apid provides a mechanism to route requests to the appropriate destination when running on a control plane node.\n\nWe’ll use some examples below to illustrate what apid is doing.\n\nWhen a user wants to interact with a Talos component via talosctl, there are two flags that control the interaction with apid. The -e | --endpoints flag specifies which Talos node ( via apid ) should handle the connection. Typically this is a public-facing server. The -n | --nodes flag specifies which Talos node(s) should respond to the request. If --nodes is omitted, the first endpoint will be used.\n\nNote: Typically, there will be an endpoint already defined in the Talos config file. Optionally, nodes can be included here as well.\n\nFor example, if a user wants to interact with machined, a command like talosctl -e cluster.talos.dev memory may be used.\n\nCopy\n$ talosctl -e cluster.talos.dev memory\n\nNODE                TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\ncluster.talos.dev   7938    1768   2390   145      53        3724    6571\n\n\nIn this case, talosctl is interacting with apid running on cluster.talos.dev and forwarding the request to the machined api.\n\nIf we wanted to extend our example to retrieve memory from another node in our cluster, we could use the command talosctl -e cluster.talos.dev -n node02 memory.\n\nCopy\n$ talosctl -e cluster.talos.dev -n node02 memory\n\nNODE    TOTAL   USED   FREE   SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode02  7938    1768   2390   145      53        3724    6571\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to apid running on node02, which forwards the request to the machined api.\n\nWe can further extend our example to retrieve memory for all nodes in our cluster by appending additional -n node flags or using a comma separated list of nodes ( -n node01,node02,node03 ):\n\nCopy\n$ talosctl -e cluster.talos.dev -n node01 -n node02 -n node03 memory\n\nNODE     TOTAL    USED    FREE     SHARED   BUFFERS   CACHE   AVAILABLE\n\nnode01   7938     871     4071     137      49        2945    7042\n\nnode02   257844   14408   190796   18138    49        52589   227492\n\nnode03   257844   1830    255186   125      49        777     254556\n\n\nThe apid instance on cluster.talos.dev receives the request and forwards it to node01, node02, and node03, which then forwards the request to their local machined api.\n\ncontainerd\n\nContainerd provides the container runtime to launch workloads on Talos and Kubernetes.\n\nTalos services are namespaced under the system namespace in containerd, whereas the Kubernetes services are namespaced under the k8s.io namespace.\n\nmachined\n\nA common theme throughout the design of Talos is minimalism. We believe strongly in the UNIX philosophy that each program should do one job well. The init included in Talos is one example of this, and we are calling it “machined”.\n\nWe wanted to create a focused init that had one job - run Kubernetes. To that extent, machined is relatively static in that it does not allow for arbitrary user-defined services. Only the services necessary to run Kubernetes and manage the node are available. This includes:\n\ncontainerd\netcd\nkubelet\nnetworkd\ntrustd\nudevd\n\nThe machined process handles all machine configuration, API handling, resource and controller management.\n\nkernel\n\nThe Linux kernel included with Talos is configured according to the recommendations outlined in the Kernel Self Protection Project (KSSP).\n\ntrustd\n\nSecurity is one of the highest priorities within Talos. To run a Kubernetes cluster, a certain level of trust is required to operate a cluster. For example, orchestrating the bootstrap of a highly available control plane requires sensitive PKI data distribution.\n\nTo that end, we created trustd. Based on a Root of Trust concept, trustd is a simple daemon responsible for establishing trust within the system. Once trust is established, various methods become available to the trustee. For example, it can accept a write request from another node to place a file on disk.\n\nAdditional methods and capabilities will be added to the trustd component to support new functionality in the rest of the Talos environment.\n\nudevd\n\nUdevd handles the kernel device notifications and sets up the necessary links in /dev.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Architecture | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/architecture/",
    "html": "Documentation\nLearn More\nArchitecture\nArchitecture\nLearn the system architecture of Talos Linux itself.\n\nTalos is designed to be atomic in deployment and modular in composition.\n\nIt is atomic in that the entirety of Talos is distributed as a single, self-contained image, which is versioned, signed, and immutable.\n\nIt is modular in that it is composed of many separate components which have clearly defined gRPC interfaces which facilitate internal flexibility and external operational guarantees.\n\nAll of the main Talos components communicate with each other by gRPC, through a socket on the local machine. This imposes a clear separation of concerns and ensures that changes over time which affect the interoperation of components are a part of the public git record. The benefit is that each component may be iterated and changed as its needs dictate, so long as the external API is controlled. This is a key component in reducing coupling and maintaining modularity.\n\nFile system partitions\n\nTalos uses these partitions with the following labels:\n\nEFI - stores EFI boot data.\nBIOS - used for GRUB’s second stage boot.\nBOOT - used for the boot loader, stores initramfs and kernel data.\nMETA - stores metadata about the talos node, such as node id’s.\nSTATE - stores machine configuration, node identity data for cluster discovery and KubeSpan info\nEPHEMERAL - stores ephemeral state information, mounted at /var\nThe File System\n\nOne of the unique design decisions in Talos is the layout of the root file system. There are three “layers” to the Talos root file system. At its core the rootfs is a read-only squashfs. The squashfs is then mounted as a loop device into memory. This provides Talos with an immutable base.\n\nThe next layer is a set of tmpfs file systems for runtime specific needs. Aside from the standard pseudo file systems such as /dev, /proc, /run, /sys and /tmp, a special /system is created for internal needs. One reason for this is that we need special files such as /etc/hosts, and /etc/resolv.conf to be writable (remember that the rootfs is read-only). For example, at boot Talos will write /system/etc/hosts and then bind mount it over /etc/hosts. This means that instead of making all of /etc writable, Talos only makes very specific files writable under /etc.\n\nAll files under /system are completely recreated on each boot. For files and directories that need to persist across boots, Talos creates overlayfs file systems. The /etc/kubernetes is a good example of this. Directories like this are overlayfs backed by an XFS file system mounted at /var.\n\nThe /var directory is owned by Kubernetes with the exception of the above overlayfs file systems. This directory is writable and used by etcd (in the case of control plane nodes), the kubelet, and the CRI (containerd). Its content survives machine reboots, but it is wiped and lost on machine upgrades and resets, unless the --preserve option of talosctl upgrade or the --system-labels-to-wipe option of talosctl reset is used.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Philosophy | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/philosophy/",
    "html": "Distributed\nImmutable\nMinimal\nEphemeral\nSecure\nDeclarative\nNot based on X distro\nAn Operating System designed for Kubernetes\nDocumentation\nLearn More\nPhilosophy\nPhilosophy\nLearn about the philosophy behind the need for Talos Linux.\nDistributed\n\nTalos is intended to be operated in a distributed manner: it is built for a high-availability dataplane first. Its etcd cluster is built in an ad-hoc manner, with each appointed node joining on its own directive (with proper security validations enforced, of course). Like Kubernetes, workloads are intended to be distributed across any number of compute nodes.\n\nThere should be no single points of failure, and the level of required coordination is as low as each platform allows.\n\nImmutable\n\nTalos takes immutability very seriously. Talos itself, even when installed on a disk, always runs from a SquashFS image, meaning that even if a directory is mounted to be writable, the image itself is never modified. All images are signed and delivered as single, versioned files. We can always run integrity checks on our image to verify that it has not been modified.\n\nWhile Talos does allow a few, highly-controlled write points to the filesystem, we strive to make them as non-unique and non-critical as possible. We call the writable partition the “ephemeral” partition precisely because we want to make sure none of us ever uses it for unique, non-replicated, non-recreatable data. Thus, if all else fails, we can always wipe the disk and get back up and running.\n\nMinimal\n\nWe are always trying to reduce Talos’ footprint. Because nearly the entire OS is built from scratch in Go, we are in a good position. We have no shell. We have no SSH. We have none of the GNU utilities, not even a rollup tool such as busybox. Everything in Talos is there because it is necessary, and nothing is included which isn’t.\n\nAs a result, the OS right now produces a SquashFS image size of less than 80 MB.\n\nEphemeral\n\nEverything Talos writes to its disk is either replicated or reconstructable. Since the controlplane is highly available, the loss of any node will cause neither service disruption nor loss of data. No writes are even allowed to the vast majority of the filesystem. We even call the writable partition “ephemeral” to keep this idea always in focus.\n\nSecure\n\nTalos has always been designed with security in mind. With its immutability, its minimalism, its signing, and its componenture, we are able to simply bypass huge classes of vulnerabilities. Moreover, because of the way we have designed Talos, we are able to take advantage of a number of additional settings, such as the recommendations of the Kernel Self Protection Project (kspp) and completely disabling dynamic modules.\n\nThere are no passwords in Talos. All networked communication is encrypted and key-authenticated. The Talos certificates are short-lived and automatically-rotating. Kubernetes is always constructed with its own separate PKI structure which is enforced.\n\nDeclarative\n\nEverything which can be configured in Talos is done through a single YAML manifest. There is no scripting and no procedural steps. Everything is defined by the one declarative YAML file. This configuration includes that of both Talos itself and the Kubernetes which it forms.\n\nThis is achievable because Talos is tightly focused to do one thing: run Kubernetes, in the easiest, most secure, most reliable way it can.\n\nNot based on X distro\n\nTalos Linux isn’t based on any other distribution. We think of ourselves as being the second-generation of container-optimised operating systems, where things like CoreOS, Flatcar, and Rancher represent the first generation (but the technology is not derived from any of those.)\n\nTalos Linux is actually a ground-up rewrite of the userspace, from PID 1. We run the Linux kernel, but everything downstream of that is our own custom code, written in Go, rigorously-tested, and published as an immutable, integrated image. The Linux kernel launches what we call machined, for instance, not systemd. There is no systemd on our system. There are no GNU utilities, no shell, no SSH, no packages, nothing you could associate with any other distribution.\n\nAn Operating System designed for Kubernetes\n\nTechnically, Talos Linux installs to a computer like any other operating system. Unlike other operating systems, Talos is not meant to run alone, on a single machine. A design goal of Talos Linux is eliminating the management of individual nodes as much as possible. In order to do that, Talos Linux operates as a cluster of machines, with lots of checking and coordination between them, at all levels.\n\nThere is only a cluster. Talos is meant to do one thing: maintain a Kubernetes cluster, and it does this very, very well.\n\nThe entirety of the configuration of any machine is specified by a single configuration file, which can often be the same configuration file used across many machines. Much like a biological system, if some component misbehaves, just cut it out and let a replacement grow. Rebuilds of Talos are remarkably fast, whether they be new machines, upgrades, or reinstalls. Never get hung up on an individual machine.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Kernel | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/kernel/",
    "html": "Commandline Parameters\nAvailable Talos-specific parameters\nip\nbond\nvlan\nnet.ifnames=0\npanic\ntalos.config\nmetal-iso\ntalos.config.auth.*\ntalos.platform\ntalos.board\ntalos.hostname\ntalos.shutdown\ntalos.network.interface.ignore\ntalos.experimental.wipe\ntalos.unified_cgroup_hierarchy\ntalos.dashboard.disabled\ntalos.environment\nDocumentation\nReference\nKernel\nKernel\nLinux kernel reference.\nCommandline Parameters\n\nTalos supports a number of kernel commandline parameters. Some are required for it to operate. Others are optional and useful in certain circumstances.\n\nSeveral of these are enforced by the Kernel Self Protection Project KSPP.\n\nRequired parameters:\n\ntalos.platform: can be one of aws, azure, container, digitalocean, equinixMetal, gcp, hcloud, metal, nocloud, openstack, oracle, scaleway, upcloud, vmware or vultr\nslab_nomerge: required by KSPP\npti=on: required by KSPP\n\nRecommended parameters:\n\ninit_on_alloc=1: advised by KSPP, enabled by default in kernel config\ninit_on_free=1: advised by KSPP, enabled by default in kernel config\nAvailable Talos-specific parameters\nip\n\nInitial configuration of the interface, routes, DNS, NTP servers (multiple ip= kernel parameters are accepted).\n\nFull documentation is available in the Linux kernel docs.\n\nip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>\n\nTalos will use the configuration supplied via the kernel parameter as the initial network configuration. This parameter is useful in the environments where DHCP doesn’t provide IP addresses or when default DNS and NTP servers should be overridden before loading machine configuration. Partial configuration can be applied as well, e.g. ip=:::::::<dns0-ip>:<dns1-ip>:<ntp0-ip> sets only the DNS and NTP servers.\n\nIPv6 addresses can be specified by enclosing them in the square brackets, e.g. ip=[2001:db8::a]:[2001:db8::b]:[fe80::1]::controlplane1:eth1::[2001:4860:4860::6464]:[2001:4860:4860::64]:[2001:4860:4806::].\n\n<netmask> can use either an IP address notation (IPv4: 255.255.255.0, IPv6: [ffff:ffff:ffff:ffff::0]), or simply a number of one bits in the netmask (24).\n\n<device> can be traditional interface naming scheme eth0, eth1 or enx<MAC>, example: enx78e7d1ea46da\n\nDHCP can be enabled by setting <autoconf> to dhcp, example: ip=:::::eth0.3:dhcp. Alternative syntax is ip=eth0.3:dhcp.\n\nbond\n\nBond interface configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nbond=<bondname>:<bondslaves>:<options>:<mtu>\n\nTalos will use the bond= kernel parameter if supplied to set the initial bond configuration. This parameter is useful in environments where the switch ports are suspended if the machine doesn’t setup a LACP bond.\n\nIf only the bond name is supplied, the bond will be created with eth0 and eth1 as slaves and bond mode set as balance-rr\n\nAll these below configurations are equivalent:\n\nbond=bond0\nbond=bond0:\nbond=bond0::\nbond=bond0:::\nbond=bond0:eth0,eth1\nbond=bond0:eth0,eth1:balance-rr\n\nAn example of a bond configuration with all options specified:\n\nbond=bond1:eth3,eth4:mode=802.3ad,xmit_hash_policy=layer2+3:1450\n\nThis will create a bond interface named bond1 with eth3 and eth4 as slaves and set the bond mode to 802.3ad, the transmit hash policy to layer2+3 and bond interface MTU to 1450.\n\nvlan\n\nThe interface vlan configuration.\n\nFull documentation is available in the Dracut kernel docs.\n\nTalos will use the vlan= kernel parameter if supplied to set the initial vlan configuration. This parameter is useful in environments where the switch ports are VLAN tagged with no native VLAN.\n\nOnly one vlan can be configured at this stage.\n\nAn example of a vlan configuration including static ip configuration:\n\nvlan=eth0.100:eth0 ip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\nThis will create a vlan interface named eth0.100 with eth0 as the underlying interface and set the vlan id to 100 with static IP 172.20.0.2/24 and 172.20.0.1 as default gateway.\n\nnet.ifnames=0\n\nDisable the predictable network interface names by specifying net.ifnames=0 on the kernel command line.\n\npanic\n\nThe amount of time to wait after a panic before a reboot is issued.\n\nTalos will always reboot if it encounters an unrecoverable error. However, when collecting debug information, it may reboot too quickly for humans to read the logs. This option allows the user to delay the reboot to give time to collect debug information from the console screen.\n\nA value of 0 disables automatic rebooting entirely.\n\ntalos.config\n\nThe URL at which the machine configuration data may be found (only for metal platform, with the kernel parameter talos.platform=metal).\n\nThis parameter supports variable substitution inside URL query values for the following case-insensitive placeholders:\n\n${uuid} the SMBIOS UUID\n${serial} the SMBIOS Serial Number\n${mac} the MAC address of the first network interface attaining link state up\n${hostname} the hostname of the machine\n\nThe following example\n\nhttp://example.com/metadata?h=${hostname}&m=${mac}&s=${serial}&u=${uuid}\n\nmay translate to\n\nhttp://example.com/metadata?h=myTestHostname&m=52%3A2f%3Afd%3Adf%3Afc%3Ac0&s=0OCZJ19N65&u=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nFor backwards compatibility we insert the system UUID into the query parameter uuid if its value is empty. As in http://example.com/metadata?uuid= => http://example.com/metadata?uuid=40dcbd19-3b10-444e-bfff-aaee44a51fda\n\nmetal-iso\n\nWhen the kernel parameter talos.config=metal-iso is set, Talos will attempt to load the machine configuration from any block device with a filesystem label of metal-iso. Talos will look for a file named config.yaml in the root of the filesystem.\n\nFor example, such ISO filesystem can be created with:\n\nCopy\nmkdir iso/\n\ncp config.yaml iso/\n\nmkisofs -joliet -rock -volid 'metal-iso' -output config.iso iso/\n\ntalos.config.auth.*\n\nKernel parameters prefixed with talos.config.auth. are used to configure OAuth2 authentication for the machine configuration.\n\ntalos.platform\n\nThe platform name on which Talos will run.\n\nValid options are:\n\naws\nazure\ncontainer\ndigitalocean\nequinixMetal\ngcp\nhcloud\nmetal\nnocloud\nopenstack\noracle\nscaleway\nupcloud\nvmware\nvultr\ntalos.board\n\nThe board name, if Talos is being used on an ARM64 SBC.\n\nSupported boards are:\n\nbananapi_m64: Banana Pi M64\nlibretech_all_h3_cc_h5: Libre Computer ALL-H3-CC\nrock64: Pine64 Rock64\n…\ntalos.hostname\n\nThe hostname to be used. The hostname is generally specified in the machine config. However, in some cases, the DHCP server needs to know the hostname before the machine configuration has been acquired.\n\nUnless specifically required, the machine configuration should be used instead.\n\ntalos.shutdown\n\nThe type of shutdown to use when Talos is told to shutdown.\n\nValid options are:\n\nhalt\npoweroff\ntalos.network.interface.ignore\n\nA network interface which should be ignored and not configured by Talos.\n\nBefore a configuration is applied (early on each boot), Talos attempts to configure each network interface by DHCP. If there are many network interfaces on the machine which have link but no DHCP server, this can add significant boot delays.\n\nThis option may be specified multiple times for multiple network interfaces.\n\ntalos.experimental.wipe\n\nResets the disk before starting up the system.\n\nValid options are:\n\nsystem resets system disk.\nsystem:EPHEMERAL,STATE resets ephemeral and state partitions. Doing this reverts Talos into maintenance mode.\ntalos.unified_cgroup_hierarchy\n\nTalos defaults to always using the unified cgroup hierarchy (cgroupsv2), but cgroupsv1 can be forced with talos.unified_cgroup_hierarchy=0.\n\nNote: cgroupsv1 is deprecated and it should be used only for compatibility with workloads which don’t support cgroupsv2 yet.\n\ntalos.dashboard.disabled\n\nBy default, Talos redirects kernel logs to virtual console /dev/tty1 and starts the dashboard on /dev/tty2, then switches to the dashboard tty.\n\nIf you set talos.dashboard.disabled=1, this behavior will be disabled. Kernel logs will be sent to the currently active console and the dashboard will not be started.\n\nIt is set to be 1 by default on SBCs.\n\ntalos.environment\n\nEach value of the argument sets a default environment variable. The expected format is key=value.\n\nExample:\n\nCopy\ntalos.environment=http_proxy=http://proxy.example.com:8080 talos.environment=https_proxy=http://proxy.example.com:8080\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Learn More | Talos Linux",
    "url": "https://www.talos.dev/v1.6/learn-more/",
    "html": "Documentation\nLearn More\nLearn More\nPhilosophy\n\nLearn about the philosophy behind the need for Talos Linux.\n\nArchitecture\n\nLearn the system architecture of Talos Linux itself.\n\nComponents\n\nUnderstand the system components that make up Talos Linux.\n\nControl Plane\n\nUnderstand the Kubernetes Control Plane.\n\nImage Factory\n\nImage Factory generates customized Talos Linux images based on configured schematics.\n\nControllers and Resources\n\nDiscover how Talos Linux uses the concepts on Controllers and Resources.\n\nNetworking Resources\n\nDelve deeper into networking of Talos Linux.\n\nNetwork Connectivity\n\nDescription of the Networking Connectivity needed by Talos Linux\n\nKubeSpan\n\nUnderstand more about KubeSpan for Talos Linux.\n\nProcess Capabilities\n\nUnderstand the Linux process capabilities restrictions with Talos Linux.\n\ntalosctl\n\nThe design and use of the Talos Linux control application.\n\nFAQs\n\nFrequently Asked Questions about Talos Linux.\n\nKnowledge Base\n\nRecipes for common configuration tasks with Talos Linux.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Config | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/v1alpha1/config/",
    "html": "machine\ncontrolPlane\ncontrollerManager\nscheduler\nkubelet\nextraMounts[]\nuidMappings[]\ngidMappings[]\nnodeIP\nnetwork\ninterfaces[]\ndeviceSelector\nroutes[]\nbond\ndeviceSelectors[]\nbridge\nstp\nvlans[]\nroutes[]\nvip\nequinixMetal\nhcloud\ndhcpOptions\ndhcpOptions\nwireguard\npeers[]\nvip\nequinixMetal\nhcloud\nextraHostEntries[]\nkubespan\nfilters\ndisks[]\npartitions[]\ninstall\ndiskSelector\nextensions[]\nfiles[]\ntime\nregistries\nmirrors.*\nconfig.*\ntls\nauth\nsystemDiskEncryption\nstate\nkeys[]\nstatic\nnodeID\nkms\ntpm\nephemeral\nkeys[]\nstatic\nnodeID\nkms\ntpm\nfeatures\nkubernetesTalosAPIAccess\nkubePrism\nudev\nlogging\ndestinations[]\nendpoint\nkernel\nmodules[]\nseccompProfiles[]\ncluster\ncontrolPlane\nendpoint\nnetwork\ncni\nflannel\napiServer\nextraVolumes[]\nadmissionControl[]\nresources\ncontrollerManager\nextraVolumes[]\nresources\nproxy\nscheduler\nextraVolumes[]\nresources\ndiscovery\nregistries\nkubernetes\nservice\netcd\ncoreDNS\nexternalCloudProvider\ninlineManifests[]\nadminKubeconfig\nDocumentation\nReference\nConfiguration\nv1alpha1\nConfig\nConfig\nConfig defines the v1alpha1.Config Talos machine configuration document.\nCopy\nversion: v1alpha1\n\nmachine: # ...\n\ncluster: # ...\nField\tType\tDescription\tValue(s)\nversion\tstring\tIndicates the schema used to decode the contents.\tv1alpha1\n\ndebug\tbool\t\nEnable verbose logging to the console.\n\ttrue\nyes\nfalse\nno\n\nmachine\tMachineConfig\tProvides machine specific configuration options.\t\ncluster\tClusterConfig\tProvides cluster specific configuration options.\t\nmachine\n\nMachineConfig represents the machine-specific config values.\n\nCopy\nmachine:\n\n    type: controlplane\n\n    # InstallConfig represents the installation options for preparing a node.\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ntype\tstring\t\nDefines the role of the machine within the cluster.\n\tcontrolplane\nworker\n\ntoken\tstring\t\nThe token is used by a machine to join the PKI of the cluster.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe root certificate authority of the PKI.\nShow example(s)\n\t\ncertSANs\t[]string\t\nExtra certificate subject alternative names for the machine’s certificate.\nShow example(s)\n\t\ncontrolPlane\tMachineControlPlaneConfig\tProvides machine specific control plane configuration options.\nShow example(s)\n\t\nkubelet\tKubeletConfig\tUsed to provide additional options to the kubelet.\nShow example(s)\n\t\npods\t[]Unstructured\t\nUsed to provide static pod definitions to be run by the kubelet directly bypassing the kube-apiserver.\nShow example(s)\n\t\nnetwork\tNetworkConfig\tProvides machine specific network configuration options.\nShow example(s)\n\t\ndisks\t[]MachineDisk\t\nUsed to partition, format and mount additional disks.\nShow example(s)\n\t\ninstall\tInstallConfig\t\nUsed to provide instructions for installations.\nShow example(s)\n\t\nfiles\t[]MachineFile\t\nAllows the addition of user specified files.\nShow example(s)\n\t\nenv\tEnv\t\nThe env field allows for the addition of environment variables.\nShow example(s)\n\tGRPC_GO_LOG_VERBOSITY_LEVEL\nGRPC_GO_LOG_SEVERITY_LEVEL\nhttp_proxy\nhttps_proxy\nno_proxy\n\ntime\tTimeConfig\tUsed to configure the machine’s time settings.\nShow example(s)\n\t\nsysctls\tmap[string]string\tUsed to configure the machine’s sysctls.\nShow example(s)\n\t\nsysfs\tmap[string]string\tUsed to configure the machine’s sysfs.\nShow example(s)\n\t\nregistries\tRegistriesConfig\t\nUsed to configure the machine’s container image registry mirrors.\nShow example(s)\n\t\nsystemDiskEncryption\tSystemDiskEncryptionConfig\t\nMachine system disk encryption configuration.\nShow example(s)\n\t\nfeatures\tFeaturesConfig\tFeatures describe individual Talos features that can be switched on or off.\nShow example(s)\n\t\nudev\tUdevConfig\tConfigures the udev system.\nShow example(s)\n\t\nlogging\tLoggingConfig\tConfigures the logging system.\nShow example(s)\n\t\nkernel\tKernelConfig\tConfigures the kernel.\nShow example(s)\n\t\nseccompProfiles\t[]MachineSeccompProfile\tConfigures the seccomp profiles for the machine.\nShow example(s)\n\t\nnodeLabels\tmap[string]string\tConfigures the node labels for the machine.\nShow example(s)\n\t\nnodeTaints\tmap[string]string\tConfigures the node taints for the machine. Effect is optional.\nShow example(s)\n\t\ncontrolPlane\n\nMachineControlPlaneConfig machine specific configuration options.\n\nCopy\nmachine:\n\n    controlPlane:\n\n        # Controller manager machine specific configuration options.\n\n        controllerManager:\n\n            disabled: false # Disable kube-controller-manager on the node.\n\n        # Scheduler machine specific configuration options.\n\n        scheduler:\n\n            disabled: true # Disable kube-scheduler on the node.\nField\tType\tDescription\tValue(s)\ncontrollerManager\tMachineControllerManagerConfig\tController manager machine specific configuration options.\t\nscheduler\tMachineSchedulerConfig\tScheduler machine specific configuration options.\t\ncontrollerManager\n\nMachineControllerManagerConfig represents the machine specific ControllerManager config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-controller-manager on the node.\t\nscheduler\n\nMachineSchedulerConfig represents the machine specific Scheduler config values.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-scheduler on the node.\t\nkubelet\n\nKubeletConfig represents the kubelet config values.\n\nCopy\nmachine:\n\n    kubelet:\n\n        image: ghcr.io/siderolabs/kubelet:v1.29.0 # The `image` field is an optional reference to an alternative kubelet image.\n\n        # The `extraArgs` field is used to provide additional flags to the kubelet.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n\n\n        # # The `ClusterDNS` field is an optional reference to an alternative kubelet clusterDNS ip list.\n\n        # clusterDNS:\n\n        #     - 10.96.0.10\n\n        #     - 169.254.2.53\n\n\n\n        # # The `extraMounts` field is used to add additional mounts to the kubelet container.\n\n        # extraMounts:\n\n        #     - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n        #       type: bind # Type specifies the mount kind.\n\n        #       source: /var/lib/example # Source specifies the source path of the mount.\n\n        #       # Options are fstab style mount options.\n\n        #       options:\n\n        #         - bind\n\n        #         - rshared\n\n        #         - rw\n\n\n\n        # # The `extraConfig` field is used to provide kubelet configuration overrides.\n\n        # extraConfig:\n\n        #     serverTLSBootstrap: true\n\n\n\n        # # The `KubeletCredentialProviderConfig` field is used to provide kubelet credential configuration.\n\n        # credentialProviderConfig:\n\n        #     apiVersion: kubelet.config.k8s.io/v1\n\n        #     kind: CredentialProviderConfig\n\n        #     providers:\n\n        #         - apiVersion: credentialprovider.kubelet.k8s.io/v1\n\n        #           defaultCacheDuration: 12h\n\n        #           matchImages:\n\n        #             - '*.dkr.ecr.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.*.amazonaws.com.cn'\n\n        #             - '*.dkr.ecr-fips.*.amazonaws.com'\n\n        #             - '*.dkr.ecr.us-iso-east-1.c2s.ic.gov'\n\n        #             - '*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov'\n\n        #           name: ecr-credential-provider\n\n\n\n        # # The `nodeIP` field is used to configure `--node-ip` flag for the kubelet.\n\n        # nodeIP:\n\n        #     # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n        #     validSubnets:\n\n        #         - 10.0.0.0/8\n\n        #         - '!10.0.0.3/32'\n\n        #         - fdc7::/16\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe image field is an optional reference to an alternative kubelet image.\nShow example(s)\n\t\nclusterDNS\t[]string\tThe ClusterDNS field is an optional reference to an alternative kubelet clusterDNS ip list.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tThe extraArgs field is used to provide additional flags to the kubelet.\nShow example(s)\n\t\nextraMounts\t[]ExtraMount\t\nThe extraMounts field is used to add additional mounts to the kubelet container.\nShow example(s)\n\t\nextraConfig\tUnstructured\t\nThe extraConfig field is used to provide kubelet configuration overrides.\nShow example(s)\n\t\ncredentialProviderConfig\tUnstructured\tThe KubeletCredentialProviderConfig field is used to provide kubelet credential configuration.\nShow example(s)\n\t\ndefaultRuntimeSeccompProfileEnabled\tbool\tEnable container runtime default Seccomp profile.\ttrue\nyes\nfalse\nno\n\nregisterWithFQDN\tbool\t\nThe registerWithFQDN field is used to force kubelet to use the node FQDN for registration.\n\ttrue\nyes\nfalse\nno\n\nnodeIP\tKubeletNodeIPConfig\t\nThe nodeIP field is used to configure --node-ip flag for the kubelet.\nShow example(s)\n\t\nskipNodeRegistration\tbool\t\nThe skipNodeRegistration is used to run the kubelet without registering with the apiserver.\n\ttrue\nyes\nfalse\nno\n\ndisableManifestsDirectory\tbool\t\nThe disableManifestsDirectory field configures the kubelet to get static pod manifests from the /etc/kubernetes/manifests directory.\n\ttrue\nyes\nfalse\nno\n\nextraMounts[]\n\nExtraMount wraps OCI Mount specification.\n\nCopy\nmachine:\n\n    kubelet:\n\n        extraMounts:\n\n            - destination: /var/lib/example # Destination is the absolute path where the mount will be placed in the container.\n\n              type: bind # Type specifies the mount kind.\n\n              source: /var/lib/example # Source specifies the source path of the mount.\n\n              # Options are fstab style mount options.\n\n              options:\n\n                - bind\n\n                - rshared\n\n                - rw\nField\tType\tDescription\tValue(s)\ndestination\tstring\tDestination is the absolute path where the mount will be placed in the container.\t\ntype\tstring\tType specifies the mount kind.\t\nsource\tstring\tSource specifies the source path of the mount.\t\noptions\t[]string\tOptions are fstab style mount options.\t\nuidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\ngidMappings\t[]LinuxIDMapping\t\nUID/GID mappings used for changing file owners w/o calling chown, fs should support it.\n\t\nuidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\ngidMappings[]\n\nLinuxIDMapping represents the Linux ID mapping.\n\nField\tType\tDescription\tValue(s)\ncontainerID\tuint32\tContainerID is the starting UID/GID in the container.\t\nhostID\tuint32\tHostID is the starting UID/GID on the host to be mapped to ‘ContainerID’.\t\nsize\tuint32\tSize is the number of IDs to be mapped.\t\nnodeIP\n\nKubeletNodeIPConfig represents the kubelet node IP configuration.\n\nCopy\nmachine:\n\n    kubelet:\n\n        nodeIP:\n\n            # The `validSubnets` field configures the networks to pick kubelet node IP from.\n\n            validSubnets:\n\n                - 10.0.0.0/8\n\n                - '!10.0.0.3/32'\n\n                - fdc7::/16\nField\tType\tDescription\tValue(s)\nvalidSubnets\t[]string\t\nThe validSubnets field configures the networks to pick kubelet node IP from.\n\t\nnetwork\n\nNetworkConfig represents the machine’s networking config values.\n\nCopy\nmachine:\n\n    network:\n\n        hostname: worker-1 # Used to statically set the hostname for the machine.\n\n        # `interfaces` is used to define the network interface configuration.\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\n\n        # Used to statically set the nameservers for the machine.\n\n        nameservers:\n\n            - 9.8.7.6\n\n            - 8.7.6.5\n\n\n\n        # # Allows for extra entries to be added to the `/etc/hosts` file\n\n        # extraHostEntries:\n\n        #     - ip: 192.168.1.100 # The IP of the host.\n\n        #       # The host alias.\n\n        #       aliases:\n\n        #         - example\n\n        #         - example.domain.tld\n\n\n\n        # # Configures KubeSpan feature.\n\n        # kubespan:\n\n        #     enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nhostname\tstring\tUsed to statically set the hostname for the machine.\t\ninterfaces\t[]Device\t\ninterfaces is used to define the network interface configuration.\nShow example(s)\n\t\nnameservers\t[]string\t\nUsed to statically set the nameservers for the machine.\nShow example(s)\n\t\nextraHostEntries\t[]ExtraHost\tAllows for extra entries to be added to the /etc/hosts file\nShow example(s)\n\t\nkubespan\tNetworkKubeSpan\tConfigures KubeSpan feature.\nShow example(s)\n\t\ndisableSearchDomain\tbool\t\nDisable generating a default search domain in /etc/resolv.conf\n\ttrue\nyes\nfalse\nno\n\ninterfaces[]\n\nDevice represents a network interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - interface: enp0s1 # The interface name.\n\n              # Assigns static IP addresses to the interface.\n\n              addresses:\n\n                - 192.168.2.0/24\n\n              # A list of routes associated with the interface.\n\n              routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 192.168.2.1 # The route's gateway (if empty, creates link scope route).\n\n                  metric: 1024 # The optional metric for the route.\n\n              mtu: 1500 # The interface's MTU.\n\n\n\n              # # Picks a network device using the selector.\n\n\n\n              # # select a device with bus prefix 00:*.\n\n              # deviceSelector:\n\n              #     busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              # # select a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #     driver: virtio # Kernel driver, supports matching by wildcard.\n\n              # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n              # deviceSelector:\n\n              #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #       driver: virtio # Kernel driver, supports matching by wildcard.\n\n\n\n              # # Bond specific options.\n\n              # bond:\n\n              #     # The interfaces that make up the bond.\n\n              #     interfaces:\n\n              #         - enp2s0\n\n              #         - enp2s1\n\n              #     # Picks a network device using the selector.\n\n              #     deviceSelectors:\n\n              #         - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n              #         - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n              #           driver: virtio # Kernel driver, supports matching by wildcard.\n\n              #     mode: 802.3ad # A bond option.\n\n              #     lacpRate: fast # A bond option.\n\n\n\n              # # Bridge specific options.\n\n              # bridge:\n\n              #     # The interfaces that make up the bridge.\n\n              #     interfaces:\n\n              #         - enxda4042ca9a51\n\n              #         - enxae2a6774c259\n\n              #     # A bridge option.\n\n              #     stp:\n\n              #         enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\n\n\n\n              # # Indicates if DHCP should be used to configure the interface.\n\n              # dhcp: true\n\n\n\n              # # DHCP specific options.\n\n              # dhcpOptions:\n\n              #     routeMetric: 1024 # The priority of all routes received via DHCP.\n\n\n\n              # # Wireguard specific configuration.\n\n\n\n              # # wireguard server example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     listenPort: 51111 # Specifies a device's listening port.\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n              # # wireguard peer example\n\n              # wireguard:\n\n              #     privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n              #     # Specifies a list of peer configurations to apply to a device.\n\n              #     peers:\n\n              #         - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n              #           endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n              #           persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n              #           # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n              #           allowedIPs:\n\n              #             - 192.168.1.0/24\n\n\n\n              # # Virtual (shared) IP address configuration.\n\n\n\n              # # layer2 vip example\n\n              # vip:\n\n              #     ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\ninterface\tstring\t\nThe interface name.\nShow example(s)\n\t\ndeviceSelector\tNetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\naddresses\t[]string\t\nAssigns static IP addresses to the interface.\nShow example(s)\n\t\nroutes\t[]Route\t\nA list of routes associated with the interface.\nShow example(s)\n\t\nbond\tBond\tBond specific options.\nShow example(s)\n\t\nbridge\tBridge\tBridge specific options.\nShow example(s)\n\t\nvlans\t[]Vlan\tVLAN specific options.\t\nmtu\tint\t\nThe interface’s MTU.\n\t\ndhcp\tbool\t\nIndicates if DHCP should be used to configure the interface.\nShow example(s)\n\t\nignore\tbool\tIndicates if the interface should be ignored (skips configuration).\t\ndummy\tbool\t\nIndicates if the interface is a dummy interface.\n\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\nShow example(s)\n\t\nwireguard\tDeviceWireguardConfig\t\nWireguard specific configuration.\nShow example(s)\n\t\nvip\tDeviceVIPConfig\tVirtual (shared) IP address configuration.\nShow example(s)\n\t\ndeviceSelector\n\nNetworkDeviceSelector struct describes network device selector.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                driver: virtio # Kernel driver, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - deviceSelector:\n\n                - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                  driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nroutes[]\n\nRoute represents a network route.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - routes:\n\n                - network: 0.0.0.0/0 # The route's network (destination).\n\n                  gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                - network: 10.2.0.0/16 # The route's network (destination).\n\n                  gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nbond\n\nBond contains the various options for configuring a bonded interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                # The interfaces that make up the bond.\n\n                interfaces:\n\n                    - enp2s0\n\n                    - enp2s1\n\n                mode: 802.3ad # A bond option.\n\n                lacpRate: fast # A bond option.\n\n\n\n                # # Picks a network device using the selector.\n\n\n\n                # # select a device with bus prefix 00:*, a device with mac address matching `*:f0:ab` and `virtio` kernel driver.\n\n                # deviceSelectors:\n\n                #     - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                #     - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                #       driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bond.\t\ndeviceSelectors\t[]NetworkDeviceSelector\t\nPicks a network device using the selector.\nShow example(s)\n\t\narpIPTarget\t[]string\t\nA bond option.\n\t\nmode\tstring\t\nA bond option.\n\t\nxmitHashPolicy\tstring\t\nA bond option.\n\t\nlacpRate\tstring\t\nA bond option.\n\t\nadActorSystem\tstring\t\nA bond option.\n\t\narpValidate\tstring\t\nA bond option.\n\t\narpAllTargets\tstring\t\nA bond option.\n\t\nprimary\tstring\t\nA bond option.\n\t\nprimaryReselect\tstring\t\nA bond option.\n\t\nfailOverMac\tstring\t\nA bond option.\n\t\nadSelect\tstring\t\nA bond option.\n\t\nmiimon\tuint32\t\nA bond option.\n\t\nupdelay\tuint32\t\nA bond option.\n\t\ndowndelay\tuint32\t\nA bond option.\n\t\narpInterval\tuint32\t\nA bond option.\n\t\nresendIgmp\tuint32\t\nA bond option.\n\t\nminLinks\tuint32\t\nA bond option.\n\t\nlpInterval\tuint32\t\nA bond option.\n\t\npacketsPerSlave\tuint32\t\nA bond option.\n\t\nnumPeerNotif\tuint8\t\nA bond option.\n\t\ntlbDynamicLb\tuint8\t\nA bond option.\n\t\nallSlavesActive\tuint8\t\nA bond option.\n\t\nuseCarrier\tbool\t\nA bond option.\n\t\nadActorSysPrio\tuint16\t\nA bond option.\n\t\nadUserPortKey\tuint16\t\nA bond option.\n\t\npeerNotifyDelay\tuint32\t\nA bond option.\n\t\ndeviceSelectors[]\n\nNetworkDeviceSelector struct describes network device selector.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                    driver: virtio # Kernel driver, supports matching by wildcard.\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bond:\n\n                deviceSelectors:\n\n                    - busPath: 00:* # PCI, USB bus prefix, supports matching by wildcard.\n\n                    - hardwareAddr: '*:f0:ab' # Device hardware address, supports matching by wildcard.\n\n                      driver: virtio # Kernel driver, supports matching by wildcard.\nField\tType\tDescription\tValue(s)\nbusPath\tstring\tPCI, USB bus prefix, supports matching by wildcard.\t\nhardwareAddr\tstring\tDevice hardware address, supports matching by wildcard.\t\npciID\tstring\tPCI ID (vendor ID, product ID), supports matching by wildcard.\t\ndriver\tstring\tKernel driver, supports matching by wildcard.\t\nbridge\n\nBridge contains the various options for configuring a bridge interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - bridge:\n\n                # The interfaces that make up the bridge.\n\n                interfaces:\n\n                    - enxda4042ca9a51\n\n                    - enxae2a6774c259\n\n                # A bridge option.\n\n                stp:\n\n                    enabled: true # Whether Spanning Tree Protocol (STP) is enabled.\nField\tType\tDescription\tValue(s)\ninterfaces\t[]string\tThe interfaces that make up the bridge.\t\nstp\tSTP\t\nA bridge option.\n\t\nstp\n\nSTP contains the various options for configuring the STP properties of a bridge interface.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tWhether Spanning Tree Protocol (STP) is enabled.\t\nvlans[]\n\nVlan represents vlan settings for a device.\n\nField\tType\tDescription\tValue(s)\naddresses\t[]string\tThe addresses in CIDR notation or as plain IPs to use.\t\nroutes\t[]Route\tA list of routes associated with the VLAN.\t\ndhcp\tbool\tIndicates if DHCP should be used.\t\nvlanId\tuint16\tThe VLAN’s ID.\t\nmtu\tuint32\tThe VLAN’s MTU.\t\nvip\tDeviceVIPConfig\tThe VLAN’s virtual IP address configuration.\t\ndhcpOptions\tDHCPOptions\t\nDHCP specific options.\n\t\nroutes[]\n\nRoute represents a network route.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - routes:\n\n                    - network: 0.0.0.0/0 # The route's network (destination).\n\n                      gateway: 10.5.0.1 # The route's gateway (if empty, creates link scope route).\n\n                    - network: 10.2.0.0/16 # The route's network (destination).\n\n                      gateway: 10.2.0.1 # The route's gateway (if empty, creates link scope route).\nField\tType\tDescription\tValue(s)\nnetwork\tstring\tThe route’s network (destination).\t\ngateway\tstring\tThe route’s gateway (if empty, creates link scope route).\t\nsource\tstring\tThe route’s source address (optional).\t\nmetric\tuint32\tThe optional metric for the route.\t\nmtu\tuint32\tThe optional MTU for the route.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - vip:\n\n                    ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vlans:\n\n                - dhcpOptions:\n\n                    routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\ndhcpOptions\n\nDHCPOptions contains options for configuring the DHCP settings for a given interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - dhcpOptions:\n\n                routeMetric: 1024 # The priority of all routes received via DHCP.\nField\tType\tDescription\tValue(s)\nrouteMetric\tuint32\tThe priority of all routes received via DHCP.\t\nipv4\tbool\tEnables DHCPv4 protocol for the interface (default is enabled).\t\nipv6\tbool\tEnables DHCPv6 protocol for the interface (default is disabled).\t\nduidv6\tstring\tSet client DUID (hex string).\t\nwireguard\n\nDeviceWireguardConfig contains settings for configuring Wireguard network interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                listenPort: 51111 # Specifies a device's listening port.\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.3 # Specifies the endpoint of this peer entry.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - wireguard:\n\n                privateKey: ABCDEF... # Specifies a private key configuration (base64 encoded).\n\n                # Specifies a list of peer configurations to apply to a device.\n\n                peers:\n\n                    - publicKey: ABCDEF... # Specifies the public key of this peer.\n\n                      endpoint: 192.168.1.2:51822 # Specifies the endpoint of this peer entry.\n\n                      persistentKeepaliveInterval: 10s # Specifies the persistent keepalive interval for this peer.\n\n                      # AllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\n\n                      allowedIPs:\n\n                        - 192.168.1.0/24\nField\tType\tDescription\tValue(s)\nprivateKey\tstring\t\nSpecifies a private key configuration (base64 encoded).\n\t\nlistenPort\tint\tSpecifies a device’s listening port.\t\nfirewallMark\tint\tSpecifies a device’s firewall mark.\t\npeers\t[]DeviceWireguardPeer\tSpecifies a list of peer configurations to apply to a device.\t\npeers[]\n\nDeviceWireguardPeer a WireGuard device peer configuration.\n\nField\tType\tDescription\tValue(s)\npublicKey\tstring\t\nSpecifies the public key of this peer.\n\t\nendpoint\tstring\tSpecifies the endpoint of this peer entry.\t\npersistentKeepaliveInterval\tDuration\t\nSpecifies the persistent keepalive interval for this peer.\n\t\nallowedIPs\t[]string\tAllowedIPs specifies a list of allowed IP addresses in CIDR notation for this peer.\t\nvip\n\nDeviceVIPConfig contains settings for configuring a Virtual Shared IP on an interface.\n\nCopy\nmachine:\n\n    network:\n\n        interfaces:\n\n            - vip:\n\n                ip: 172.16.199.55 # Specifies the IP address to be used.\nField\tType\tDescription\tValue(s)\nip\tstring\tSpecifies the IP address to be used.\t\nequinixMetal\tVIPEquinixMetalConfig\tSpecifies the Equinix Metal API settings to assign VIP to the node.\t\nhcloud\tVIPHCloudConfig\tSpecifies the Hetzner Cloud API settings to assign VIP to the node.\t\nequinixMetal\n\nVIPEquinixMetalConfig contains settings for Equinix Metal VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Equinix Metal API Token.\t\nhcloud\n\nVIPHCloudConfig contains settings for Hetzner Cloud VIP management.\n\nField\tType\tDescription\tValue(s)\napiToken\tstring\tSpecifies the Hetzner Cloud API Token.\t\nextraHostEntries[]\n\nExtraHost represents a host entry in /etc/hosts.\n\nCopy\nmachine:\n\n    network:\n\n        extraHostEntries:\n\n            - ip: 192.168.1.100 # The IP of the host.\n\n              # The host alias.\n\n              aliases:\n\n                - example\n\n                - example.domain.tld\nField\tType\tDescription\tValue(s)\nip\tstring\tThe IP of the host.\t\naliases\t[]string\tThe host alias.\t\nkubespan\n\nNetworkKubeSpan struct describes KubeSpan configuration.\n\nCopy\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the KubeSpan feature.\n\t\nadvertiseKubernetesNetworks\tbool\t\nControl whether Kubernetes pod CIDRs are announced over KubeSpan from the node.\n\t\nallowDownPeerBypass\tbool\t\nSkip sending traffic via KubeSpan if the peer connection state is not up.\n\t\nharvestExtraEndpoints\tbool\t\nKubeSpan can collect and publish extra endpoints for each member of the cluster\n\t\nmtu\tuint32\t\nKubeSpan link MTU size.\n\t\nfilters\tKubeSpanFilters\t\nKubeSpan advanced filtering of network addresses .\n\t\nfilters\n\nKubeSpanFilters struct describes KubeSpan advanced network addresses filtering.\n\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nFilter node addresses which will be advertised as KubeSpan endpoints for peer-to-peer Wireguard connections.\nShow example(s)\n\t\ndisks[]\n\nMachineDisk represents the options available for partitioning, formatting, and mounting extra disks.\n\nCopy\nmachine:\n\n    disks:\n\n        - device: /dev/sdb # The name of the disk to use.\n\n          # A list of partitions to create on the disk.\n\n          partitions:\n\n            - mountpoint: /var/mnt/extra # Where to mount the partition.\n\n\n\n              # # The size of partition: either bytes or human readable representation. If `size:` is omitted, the partition is sized to occupy the full disk.\n\n\n\n              # # Human readable representation.\n\n              # size: 100 MB\n\n              # # Precise value in bytes.\n\n              # size: 1073741824\nField\tType\tDescription\tValue(s)\ndevice\tstring\tThe name of the disk to use.\t\npartitions\t[]DiskPartition\tA list of partitions to create on the disk.\t\npartitions[]\n\nDiskPartition represents the options for a disk partition.\n\nField\tType\tDescription\tValue(s)\nsize\tDiskSize\tThe size of partition: either bytes or human readable representation. If size: is omitted, the partition is sized to occupy the full disk.\nShow example(s)\n\t\nmountpoint\tstring\tWhere to mount the partition.\t\ninstall\n\nInstallConfig represents the installation options for preparing a node.\n\nCopy\nmachine:\n\n    install:\n\n        disk: /dev/sda # The disk used for installations.\n\n        # Allows for supplying extra kernel args via the bootloader.\n\n        extraKernelArgs:\n\n            - console=ttyS1\n\n            - panic=10\n\n        image: ghcr.io/siderolabs/installer:latest # Allows for supplying the image used to perform the installation.\n\n        wipe: false # Indicates if the installation disk should be wiped at installation time.\n\n\n\n        # # Look up disk using disk attributes like model, size, serial and others.\n\n        # diskSelector:\n\n        #     size: 4GB # Disk size.\n\n        #     model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n        #     busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0 # Disk bus path.\n\n\n\n        # # Allows for supplying additional system extension images to install on top of base Talos image.\n\n        # extensions:\n\n        #     - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\ndisk\tstring\tThe disk used for installations.\nShow example(s)\n\t\ndiskSelector\tInstallDiskSelector\t\nLook up disk using disk attributes like model, size, serial and others.\nShow example(s)\n\t\nextraKernelArgs\t[]string\t\nAllows for supplying extra kernel args via the bootloader.\nShow example(s)\n\t\nimage\tstring\t\nAllows for supplying the image used to perform the installation.\nShow example(s)\n\t\nextensions\t[]InstallExtensionConfig\tAllows for supplying additional system extension images to install on top of base Talos image.\nShow example(s)\n\t\nwipe\tbool\t\nIndicates if the installation disk should be wiped at installation time.\n\ttrue\nyes\nfalse\nno\n\nlegacyBIOSSupport\tbool\t\nIndicates if MBR partition should be marked as bootable (active).\n\t\ndiskSelector\n\nInstallDiskSelector represents a disk query parameters for the install disk lookup.\n\nCopy\nmachine:\n\n    install:\n\n        diskSelector:\n\n            size: '>= 1TB' # Disk size.\n\n            model: WDC* # Disk model `/sys/block/<dev>/device/model`.\n\n\n\n            # # Disk bus path.\n\n            # busPath: /pci0000:00/0000:00:17.0/ata1/host0/target0:0:0/0:0:0:0\n\n            # busPath: /pci0000:00/*\nField\tType\tDescription\tValue(s)\nsize\tInstallDiskSizeMatcher\tDisk size.\nShow example(s)\n\t\nname\tstring\tDisk name /sys/block/<dev>/device/name.\t\nmodel\tstring\tDisk model /sys/block/<dev>/device/model.\t\nserial\tstring\tDisk serial number /sys/block/<dev>/serial.\t\nmodalias\tstring\tDisk modalias /sys/block/<dev>/device/modalias.\t\nuuid\tstring\tDisk UUID /sys/block/<dev>/uuid.\t\nwwid\tstring\tDisk WWID /sys/block/<dev>/wwid.\t\ntype\tInstallDiskType\tDisk Type.\tssd\nhdd\nnvme\nsd\n\nbusPath\tstring\tDisk bus path.\nShow example(s)\n\t\nextensions[]\n\nInstallExtensionConfig represents a configuration for a system extension.\n\nCopy\nmachine:\n\n    install:\n\n        extensions:\n\n            - image: ghcr.io/siderolabs/gvisor:20220117.0-v1.0.0 # System extension image.\nField\tType\tDescription\tValue(s)\nimage\tstring\tSystem extension image.\t\nfiles[]\n\nMachineFile represents a file to write to disk.\n\nCopy\nmachine:\n\n    files:\n\n        - content: '...' # The contents of the file.\n\n          permissions: 0o666 # The file's permissions in octal.\n\n          path: /tmp/file.txt # The path of the file.\n\n          op: append # The operation to use\nField\tType\tDescription\tValue(s)\ncontent\tstring\tThe contents of the file.\t\npermissions\tFileMode\tThe file’s permissions in octal.\t\npath\tstring\tThe path of the file.\t\nop\tstring\tThe operation to use\tcreate\nappend\noverwrite\n\ntime\n\nTimeConfig represents the options for configuring time on a machine.\n\nCopy\nmachine:\n\n    time:\n\n        disabled: false # Indicates if the time service is disabled for the machine.\n\n        # Specifies time (NTP) servers to use for setting the system time.\n\n        servers:\n\n            - time.cloudflare.com\n\n        bootTimeout: 2m0s # Specifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\t\nIndicates if the time service is disabled for the machine.\n\t\nservers\t[]string\t\nSpecifies time (NTP) servers to use for setting the system time.\n\t\nbootTimeout\tDuration\t\nSpecifies the timeout when the node time is considered to be in sync unlocking the boot sequence.\n\t\nregistries\n\nRegistriesConfig represents the image pull options.\n\nCopy\nmachine:\n\n    registries:\n\n        # Specifies mirror configuration for each registry host namespace.\n\n        mirrors:\n\n            docker.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.local\n\n        # Specifies TLS & auth configuration for HTTPS image registries.\n\n        config:\n\n            registry.local:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n                # The auth configuration for this registry.\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nmirrors\tmap[string]RegistryMirrorConfig\t\nSpecifies mirror configuration for each registry host namespace.\nShow example(s)\n\t\nconfig\tmap[string]RegistryConfig\t\nSpecifies TLS & auth configuration for HTTPS image registries.\nShow example(s)\n\t\nmirrors.*\n\nRegistryMirrorConfig represents mirror configuration for a registry.\n\nCopy\nmachine:\n\n    registries:\n\n        mirrors:\n\n            ghcr.io:\n\n                # List of endpoints (URLs) for registry mirrors to use.\n\n                endpoints:\n\n                    - https://registry.insecure\n\n                    - https://ghcr.io/v2/\nField\tType\tDescription\tValue(s)\nendpoints\t[]string\t\nList of endpoints (URLs) for registry mirrors to use.\n\t\noverridePath\tbool\t\nUse the exact path specified for the endpoint (don’t append /v2/).\n\t\nconfig.*\n\nRegistryConfig specifies auth & TLS config per registry.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            registry.insecure:\n\n                # The TLS configuration for the registry.\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n\n\n                # # The auth configuration for this registry.\n\n                # auth:\n\n                #     username: username # Optional registry authentication.\n\n                #     password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\ntls\tRegistryTLSConfig\tThe TLS configuration for the registry.\nShow example(s)\n\t\nauth\tRegistryAuthConfig\t\nThe auth configuration for this registry.\nShow example(s)\n\t\ntls\n\nRegistryTLSConfig specifies TLS config for HTTPS registries.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    # Enable mutual TLS authentication with the registry.\n\n                    clientIdentity:\n\n                        crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                        key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                tls:\n\n                    insecureSkipVerify: true # Skip TLS server certificate verification (not recommended).\n\n\n\n                    # # Enable mutual TLS authentication with the registry.\n\n                    # clientIdentity:\n\n                    #     crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n                    #     key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\nField\tType\tDescription\tValue(s)\nclientIdentity\tPEMEncodedCertificateAndKey\t\nEnable mutual TLS authentication with the registry.\nShow example(s)\n\t\nca\tBase64Bytes\t\nCA registry certificate to add the list of trusted certificates.\n\t\ninsecureSkipVerify\tbool\tSkip TLS server certificate verification (not recommended).\t\nauth\n\nRegistryAuthConfig specifies authentication configuration for a registry.\n\nCopy\nmachine:\n\n    registries:\n\n        config:\n\n            example.com:\n\n                auth:\n\n                    username: username # Optional registry authentication.\n\n                    password: password # Optional registry authentication.\nField\tType\tDescription\tValue(s)\nusername\tstring\t\nOptional registry authentication.\n\t\npassword\tstring\t\nOptional registry authentication.\n\t\nauth\tstring\t\nOptional registry authentication.\n\t\nidentityToken\tstring\t\nOptional registry authentication.\n\t\nsystemDiskEncryption\n\nSystemDiskEncryptionConfig specifies system disk partitions encryption settings.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        # Ephemeral partition encryption.\n\n        ephemeral:\n\n            provider: luks2 # Encryption provider to use for the encryption.\n\n            # Defines the encryption keys generation and storage method.\n\n            keys:\n\n                - # Deterministically generated key from the node UUID and PartitionLabel.\n\n                  nodeID: {}\n\n                  slot: 0 # Key slot number for LUKS2 encryption.\n\n\n\n                  # # KMS managed encryption key.\n\n                  # kms:\n\n                  #     endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\n\n\n\n            # # Cipher kind to use for the encryption. Depends on the encryption provider.\n\n            # cipher: aes-xts-plain64\n\n\n\n            # # Defines the encryption sector size.\n\n            # blockSize: 4096\n\n\n\n            # # Additional --perf parameters for the LUKS2 encryption.\n\n            # options:\n\n            #     - no_read_workqueue\n\n            #     - no_write_workqueue\nField\tType\tDescription\tValue(s)\nstate\tEncryptionConfig\tState partition encryption.\t\nephemeral\tEncryptionConfig\tEphemeral partition encryption.\t\nstate\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        state:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nephemeral\n\nEncryptionConfig represents partition encryption settings.\n\nField\tType\tDescription\tValue(s)\nprovider\tstring\tEncryption provider to use for the encryption.\nShow example(s)\n\t\nkeys\t[]EncryptionKey\tDefines the encryption keys generation and storage method.\t\ncipher\tstring\tCipher kind to use for the encryption. Depends on the encryption provider.\nShow example(s)\n\taes-xts-plain64\nxchacha12,aes-adiantum-plain64\nxchacha20,aes-adiantum-plain64\n\nkeySize\tuint\tDefines the encryption key length.\t\nblockSize\tuint64\tDefines the encryption sector size.\nShow example(s)\n\t\noptions\t[]string\tAdditional –perf parameters for the LUKS2 encryption.\nShow example(s)\n\tno_read_workqueue\nno_write_workqueue\nsame_cpu_crypt\n\nkeys[]\n\nEncryptionKey represents configuration for disk encryption key.\n\nField\tType\tDescription\tValue(s)\nstatic\tEncryptionKeyStatic\tKey which value is stored in the configuration file.\t\nnodeID\tEncryptionKeyNodeID\tDeterministically generated key from the node UUID and PartitionLabel.\t\nkms\tEncryptionKeyKMS\tKMS managed encryption key.\nShow example(s)\n\t\nslot\tint\tKey slot number for LUKS2 encryption.\t\ntpm\tEncryptionKeyTPM\tEnable TPM based disk encryption.\t\nstatic\n\nEncryptionKeyStatic represents throw away key type.\n\nField\tType\tDescription\tValue(s)\npassphrase\tstring\tDefines the static passphrase value.\t\nnodeID\n\nEncryptionKeyNodeID represents deterministically generated key from the node UUID and PartitionLabel.\n\nkms\n\nEncryptionKeyKMS represents a key that is generated and then sealed/unsealed by the KMS server.\n\nCopy\nmachine:\n\n    systemDiskEncryption:\n\n        ephemeral:\n\n            keys:\n\n                - kms:\n\n                    endpoint: https://192.168.88.21:4443 # KMS endpoint to Seal/Unseal the key.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tKMS endpoint to Seal/Unseal the key.\t\ntpm\n\nEncryptionKeyTPM represents a key that is generated and then sealed/unsealed by the TPM.\n\nfeatures\n\nFeaturesConfig describes individual Talos features that can be switched on or off.\n\nCopy\nmachine:\n\n    features:\n\n        rbac: true # Enable role-based access control (RBAC).\n\n\n\n        # # Configure Talos API access from Kubernetes pods.\n\n        # kubernetesTalosAPIAccess:\n\n        #     enabled: true # Enable Talos API access from Kubernetes pods.\n\n        #     # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n        #     allowedRoles:\n\n        #         - os:reader\n\n        #     # The list of Kubernetes namespaces Talos API access is available from.\n\n        #     allowedKubernetesNamespaces:\n\n        #         - kube-system\nField\tType\tDescription\tValue(s)\nrbac\tbool\tEnable role-based access control (RBAC).\t\nstableHostname\tbool\tEnable stable default hostname.\t\nkubernetesTalosAPIAccess\tKubernetesTalosAPIAccessConfig\t\nConfigure Talos API access from Kubernetes pods.\nShow example(s)\n\t\napidCheckExtKeyUsage\tbool\tEnable checks for extended key usage of client certificates in apid.\t\ndiskQuotaSupport\tbool\t\nEnable XFS project quota support for EPHEMERAL partition and user disks.\n\t\nkubePrism\tKubePrism\t\nKubePrism - local proxy/load balancer on defined port that will distribute\n\t\nkubernetesTalosAPIAccess\n\nKubernetesTalosAPIAccessConfig describes the configuration for the Talos API access from Kubernetes pods.\n\nCopy\nmachine:\n\n    features:\n\n        kubernetesTalosAPIAccess:\n\n            enabled: true # Enable Talos API access from Kubernetes pods.\n\n            # The list of Talos API roles which can be granted for access from Kubernetes pods.\n\n            allowedRoles:\n\n                - os:reader\n\n            # The list of Kubernetes namespaces Talos API access is available from.\n\n            allowedKubernetesNamespaces:\n\n                - kube-system\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable Talos API access from Kubernetes pods.\t\nallowedRoles\t[]string\t\nThe list of Talos API roles which can be granted for access from Kubernetes pods.\n\t\nallowedKubernetesNamespaces\t[]string\tThe list of Kubernetes namespaces Talos API access is available from.\t\nkubePrism\n\nKubePrism describes the configuration for the KubePrism load balancer.\n\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable KubePrism support - will start local load balacing proxy.\t\nport\tint\tKubePrism port.\t\nudev\n\nUdevConfig describes how the udev system should be configured.\n\nCopy\nmachine:\n\n    udev:\n\n        # List of udev rules to apply to the udev system\n\n        rules:\n\n            - SUBSYSTEM==\"drm\", KERNEL==\"renderD*\", GROUP=\"44\", MODE=\"0660\"\nField\tType\tDescription\tValue(s)\nrules\t[]string\tList of udev rules to apply to the udev system\t\nlogging\n\nLoggingConfig struct configures Talos logging.\n\nCopy\nmachine:\n\n    logging:\n\n        # Logging destination.\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345 # Where to send logs. Supported protocols are \"tcp\" and \"udp\".\n\n              format: json_lines # Logs format.\nField\tType\tDescription\tValue(s)\ndestinations\t[]LoggingDestination\tLogging destination.\t\ndestinations[]\n\nLoggingDestination struct configures Talos logging destination.\n\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\tWhere to send logs. Supported protocols are “tcp” and “udp”.\nShow example(s)\n\t\nformat\tstring\tLogs format.\tjson_lines\n\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://1.2.3.4:6443\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: https://cluster1.internal:6443\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: udp://127.0.0.1:12345\nCopy\nmachine:\n\n    logging:\n\n        destinations:\n\n            - endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nkernel\n\nKernelConfig struct configures Talos Linux kernel.\n\nCopy\nmachine:\n\n    kernel:\n\n        # Kernel modules to load.\n\n        modules:\n\n            - name: brtfs # Module name.\nField\tType\tDescription\tValue(s)\nmodules\t[]KernelModuleConfig\tKernel modules to load.\t\nmodules[]\n\nKernelModuleConfig struct configures Linux kernel modules to load.\n\nField\tType\tDescription\tValue(s)\nname\tstring\tModule name.\t\nparameters\t[]string\tModule parameters, changes applied after reboot.\t\nseccompProfiles[]\n\nMachineSeccompProfile defines seccomp profiles for the machine.\n\nCopy\nmachine:\n\n    seccompProfiles:\n\n        - name: audit.json # The `name` field is used to provide the file name of the seccomp profile.\n\n          # The `value` field is used to provide the seccomp profile.\n\n          value:\n\n            defaultAction: SCMP_ACT_LOG\nField\tType\tDescription\tValue(s)\nname\tstring\tThe name field is used to provide the file name of the seccomp profile.\t\nvalue\tUnstructured\tThe value field is used to provide the seccomp profile.\t\ncluster\n\nClusterConfig represents the cluster-wide config values.\n\nCopy\ncluster:\n\n    # ControlPlaneConfig represents the control plane configuration options.\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\n\n    clusterName: talos.local\n\n    # ClusterNetworkConfig represents kube networking configuration options.\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\nid\tstring\tGlobally unique identifier for this cluster (base64 encoded random 32 bytes).\t\nsecret\tstring\t\nShared secret of cluster (base64 encoded random 32 bytes).\n\t\ncontrolPlane\tControlPlaneConfig\tProvides control plane specific configuration options.\nShow example(s)\n\t\nclusterName\tstring\tConfigures the cluster’s name.\t\nnetwork\tClusterNetworkConfig\tProvides cluster specific network configuration options.\nShow example(s)\n\t\ntoken\tstring\tThe bootstrap token used to join the cluster.\nShow example(s)\n\t\naescbcEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nsecretboxEncryptionSecret\tstring\t\nA key used for the encryption of secret data at rest.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\tThe base64 encoded root certificate authority used by Kubernetes.\nShow example(s)\n\t\naggregatorCA\tPEMEncodedCertificateAndKey\t\nThe base64 encoded aggregator certificate authority used by Kubernetes for front-proxy certificate generation.\nShow example(s)\n\t\nserviceAccount\tPEMEncodedKey\tThe base64 encoded private key for service account token generation.\nShow example(s)\n\t\napiServer\tAPIServerConfig\tAPI server specific configuration options.\nShow example(s)\n\t\ncontrollerManager\tControllerManagerConfig\tController manager server specific configuration options.\nShow example(s)\n\t\nproxy\tProxyConfig\tKube-proxy server-specific configuration options\nShow example(s)\n\t\nscheduler\tSchedulerConfig\tScheduler server specific configuration options.\nShow example(s)\n\t\ndiscovery\tClusterDiscoveryConfig\tConfigures cluster member discovery.\nShow example(s)\n\t\netcd\tEtcdConfig\tEtcd specific configuration options.\nShow example(s)\n\t\ncoreDNS\tCoreDNS\tCore DNS specific configuration options.\nShow example(s)\n\t\nexternalCloudProvider\tExternalCloudProviderConfig\tExternal cloud provider configuration.\nShow example(s)\n\t\nextraManifests\t[]string\t\nA list of urls that point to additional manifests.\nShow example(s)\n\t\nextraManifestHeaders\tmap[string]string\tA map of key value pairs that will be added while fetching the extraManifests.\nShow example(s)\n\t\ninlineManifests\t[]ClusterInlineManifest\t\nA list of inline Kubernetes manifests.\nShow example(s)\n\t\nadminKubeconfig\tAdminKubeconfigConfig\t\nSettings for admin kubeconfig generation.\nShow example(s)\n\t\nallowSchedulingOnControlPlanes\tbool\tAllows running workload on control-plane nodes.\nShow example(s)\n\ttrue\nyes\nfalse\nno\n\ncontrolPlane\n\nControlPlaneConfig represents the control plane configuration options.\n\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\n\n        localAPIServerPort: 443 # The port that the API server listens on internally.\nField\tType\tDescription\tValue(s)\nendpoint\tEndpoint\t\nEndpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.\nShow example(s)\n\t\nlocalAPIServerPort\tint\t\nThe port that the API server listens on internally.\n\t\nendpoint\n\nEndpoint represents the endpoint URL parsed out of the machine config.\n\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://1.2.3.4:6443\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: https://cluster1.internal:6443\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: udp://127.0.0.1:12345\nCopy\ncluster:\n\n    controlPlane:\n\n        endpoint: tcp://1.2.3.4:12345\nField\tType\tDescription\tValue(s)\nnetwork\n\nClusterNetworkConfig represents kube networking configuration options.\n\nCopy\ncluster:\n\n    network:\n\n        # The CNI used.\n\n        cni:\n\n            name: flannel # Name of CNI to use.\n\n        dnsDomain: cluster.local # The domain used by Kubernetes DNS.\n\n        # The pod subnet CIDR.\n\n        podSubnets:\n\n            - 10.244.0.0/16\n\n        # The service subnet CIDR.\n\n        serviceSubnets:\n\n            - 10.96.0.0/12\nField\tType\tDescription\tValue(s)\ncni\tCNIConfig\t\nThe CNI used.\nShow example(s)\n\t\ndnsDomain\tstring\t\nThe domain used by Kubernetes DNS.\nShow example(s)\n\t\npodSubnets\t[]string\tThe pod subnet CIDR.\nShow example(s)\n\t\nserviceSubnets\t[]string\tThe service subnet CIDR.\nShow example(s)\n\t\ncni\n\nCNIConfig represents the CNI configuration options.\n\nCopy\ncluster:\n\n    network:\n\n        cni:\n\n            name: custom # Name of CNI to use.\n\n            # URLs containing manifests to apply for the CNI.\n\n            urls:\n\n                - https://docs.projectcalico.org/archive/v3.20/manifests/canal.yaml\nField\tType\tDescription\tValue(s)\nname\tstring\tName of CNI to use.\tflannel\ncustom\nnone\n\nurls\t[]string\t\nURLs containing manifests to apply for the CNI.\n\t\nflannel\tFlannelCNIConfig\t\ndescription:\n\tFlannel configuration options.\n\nflannel\n\nFlannelCNIConfig represents the Flannel CNI configuration options.\n\nField\tType\tDescription\tValue(s)\nextraArgs\t[]string\tExtra arguments for ‘flanneld’.\nShow example(s)\n\t\napiServer\n\nAPIServerConfig represents the kube apiserver configuration options.\n\nCopy\ncluster:\n\n    apiServer:\n\n        image: registry.k8s.io/kube-apiserver:v1.29.0 # The container image used in the API server manifest.\n\n        # Extra arguments to supply to the API server.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\n\n            http2-max-streams-per-connection: \"32\"\n\n        # Extra certificate subject alternative names for the API server's certificate.\n\n        certSANs:\n\n            - 1.2.3.4\n\n            - 4.5.6.7\n\n\n\n        # # Configure the API server admission plugins.\n\n        # admissionControl:\n\n        #     - name: PodSecurity # Name is the name of the admission controller.\n\n        #       # Configuration is an embedded configuration object to be used as the plugin's\n\n        #       configuration:\n\n        #         apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n        #         defaults:\n\n        #             audit: restricted\n\n        #             audit-version: latest\n\n        #             enforce: baseline\n\n        #             enforce-version: latest\n\n        #             warn: restricted\n\n        #             warn-version: latest\n\n        #         exemptions:\n\n        #             namespaces:\n\n        #                 - kube-system\n\n        #             runtimeClasses: []\n\n        #             usernames: []\n\n        #         kind: PodSecurityConfiguration\n\n\n\n        # # Configure the API server audit policy.\n\n        # auditPolicy:\n\n        #     apiVersion: audit.k8s.io/v1\n\n        #     kind: Policy\n\n        #     rules:\n\n        #         - level: Metadata\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the API server manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the API server.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the API server static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\ncertSANs\t[]string\tExtra certificate subject alternative names for the API server’s certificate.\t\ndisablePodSecurityPolicy\tbool\tDisable PodSecurityPolicy in the API server and default manifests.\t\nadmissionControl\t[]AdmissionPluginConfig\tConfigure the API server admission plugins.\nShow example(s)\n\t\nauditPolicy\tUnstructured\tConfigure the API server audit policy.\nShow example(s)\n\t\nresources\tResourcesConfig\tConfigure the API server resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nadmissionControl[]\n\nAdmissionPluginConfig represents the API server admission plugin configuration.\n\nCopy\ncluster:\n\n    apiServer:\n\n        admissionControl:\n\n            - name: PodSecurity # Name is the name of the admission controller.\n\n              # Configuration is an embedded configuration object to be used as the plugin's\n\n              configuration:\n\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n                defaults:\n\n                    audit: restricted\n\n                    audit-version: latest\n\n                    enforce: baseline\n\n                    enforce-version: latest\n\n                    warn: restricted\n\n                    warn-version: latest\n\n                exemptions:\n\n                    namespaces:\n\n                        - kube-system\n\n                    runtimeClasses: []\n\n                    usernames: []\n\n                kind: PodSecurityConfiguration\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName is the name of the admission controller.\n\t\nconfiguration\tUnstructured\t\nConfiguration is an embedded configuration object to be used as the plugin’s\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ncontrollerManager\n\nControllerManagerConfig represents the kube controller manager configuration options.\n\nCopy\ncluster:\n\n    controllerManager:\n\n        image: registry.k8s.io/kube-controller-manager:v1.29.0 # The container image used in the controller manager manifest.\n\n        # Extra arguments to supply to the controller manager.\n\n        extraArgs:\n\n            feature-gates: ServerSideApply=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the controller manager manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the controller manager.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the controller manager static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the controller manager resources.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\nproxy\n\nProxyConfig represents the kube proxy configuration options.\n\nCopy\ncluster:\n\n    proxy:\n\n        image: registry.k8s.io/kube-proxy:v1.29.0 # The container image used in the kube-proxy manifest.\n\n        mode: ipvs # proxy mode of kube-proxy.\n\n        # Extra arguments to supply to kube-proxy.\n\n        extraArgs:\n\n            proxy-mode: iptables\n\n\n\n        # # Disable kube-proxy deployment on cluster bootstrap.\n\n        # disabled: false\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable kube-proxy deployment on cluster bootstrap.\nShow example(s)\n\t\nimage\tstring\tThe container image used in the kube-proxy manifest.\nShow example(s)\n\t\nmode\tstring\t\nproxy mode of kube-proxy.\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to kube-proxy.\t\nscheduler\n\nSchedulerConfig represents the kube scheduler configuration options.\n\nCopy\ncluster:\n\n    scheduler:\n\n        image: registry.k8s.io/kube-scheduler:v1.29.0 # The container image used in the scheduler manifest.\n\n        # Extra arguments to supply to the scheduler.\n\n        extraArgs:\n\n            feature-gates: AllBeta=true\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used in the scheduler manifest.\nShow example(s)\n\t\nextraArgs\tmap[string]string\tExtra arguments to supply to the scheduler.\t\nextraVolumes\t[]VolumeMountConfig\tExtra volumes to mount to the scheduler static pod.\t\nenv\tEnv\tThe env field allows for the addition of environment variables for the control plane component.\t\nresources\tResourcesConfig\tConfigure the scheduler resources.\t\nconfig\tUnstructured\tSpecify custom kube-scheduler configuration.\t\nextraVolumes[]\n\nVolumeMountConfig struct describes extra volume mount for the static pods.\n\nField\tType\tDescription\tValue(s)\nhostPath\tstring\tPath on the host.\nShow example(s)\n\t\nmountPath\tstring\tPath in the container.\nShow example(s)\n\t\nreadonly\tbool\tMount the volume read only.\nShow example(s)\n\t\nresources\n\nResourcesConfig represents the pod resources.\n\nField\tType\tDescription\tValue(s)\nrequests\tUnstructured\tRequests configures the reserved cpu/memory resources.\nShow example(s)\n\t\nlimits\tUnstructured\tLimits configures the maximum cpu/memory resources a container can use.\nShow example(s)\n\t\ndiscovery\n\nClusterDiscoveryConfig struct configures cluster membership discovery.\n\nCopy\ncluster:\n\n    discovery:\n\n        enabled: true # Enable the cluster membership discovery feature.\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            # Kubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\n            kubernetes: {}\n\n            # Service registry is using an external service to push and pull information about cluster members.\n\n            service:\n\n                endpoint: https://discovery.talos.dev/ # External service endpoint.\nField\tType\tDescription\tValue(s)\nenabled\tbool\t\nEnable the cluster membership discovery feature.\n\t\nregistries\tDiscoveryRegistriesConfig\tConfigure registries used for cluster member discovery.\t\nregistries\n\nDiscoveryRegistriesConfig struct configures cluster membership discovery.\n\nField\tType\tDescription\tValue(s)\nkubernetes\tRegistryKubernetesConfig\t\nKubernetes registry uses Kubernetes API server to discover cluster members and stores additional information\n\t\nservice\tRegistryServiceConfig\tService registry is using an external service to push and pull information about cluster members.\t\nkubernetes\n\nRegistryKubernetesConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable Kubernetes discovery registry.\t\nservice\n\nRegistryServiceConfig struct configures Kubernetes discovery registry.\n\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable external service discovery registry.\t\nendpoint\tstring\tExternal service endpoint.\nShow example(s)\n\t\netcd\n\nEtcdConfig represents the etcd configuration options.\n\nCopy\ncluster:\n\n    etcd:\n\n        image: gcr.io/etcd-development/etcd:v3.5.11 # The container image used to create the etcd service.\n\n        # The `ca` is the root certificate authority of the PKI.\n\n        ca:\n\n            crt: LS0tIEVYQU1QTEUgQ0VSVElGSUNBVEUgLS0t\n\n            key: LS0tIEVYQU1QTEUgS0VZIC0tLQ==\n\n        # Extra arguments to supply to etcd.\n\n        extraArgs:\n\n            election-timeout: \"5000\"\n\n\n\n        # # The `advertisedSubnets` field configures the networks to pick etcd advertised IP from.\n\n        # advertisedSubnets:\n\n        #     - 10.0.0.0/8\nField\tType\tDescription\tValue(s)\nimage\tstring\tThe container image used to create the etcd service.\nShow example(s)\n\t\nca\tPEMEncodedCertificateAndKey\t\nThe ca is the root certificate authority of the PKI.\nShow example(s)\n\t\nextraArgs\tmap[string]string\t\nExtra arguments to supply to etcd.\n\t\nadvertisedSubnets\t[]string\t\nThe advertisedSubnets field configures the networks to pick etcd advertised IP from.\nShow example(s)\n\t\nlistenSubnets\t[]string\t\nThe listenSubnets field configures the networks for the etcd to listen for peer and client connections.\n\t\ncoreDNS\n\nCoreDNS represents the CoreDNS config values.\n\nCopy\ncluster:\n\n    coreDNS:\n\n        image: registry.k8s.io/coredns/coredns:v1.11.1 # The `image` field is an override to the default coredns image.\nField\tType\tDescription\tValue(s)\ndisabled\tbool\tDisable coredns deployment on cluster bootstrap.\t\nimage\tstring\tThe image field is an override to the default coredns image.\t\nexternalCloudProvider\n\nExternalCloudProviderConfig contains external cloud provider configuration.\n\nCopy\ncluster:\n\n    externalCloudProvider:\n\n        enabled: true # Enable external cloud provider.\n\n        # A list of urls that point to additional manifests for an external cloud provider.\n\n        manifests:\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/rbac.yaml\n\n            - https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.20.0-alpha.0/manifests/aws-cloud-controller-manager-daemonset.yaml\nField\tType\tDescription\tValue(s)\nenabled\tbool\tEnable external cloud provider.\ttrue\nyes\nfalse\nno\n\nmanifests\t[]string\t\nA list of urls that point to additional manifests for an external cloud provider.\nShow example(s)\n\t\ninlineManifests[]\n\nClusterInlineManifest struct describes inline bootstrap manifests for the user.\n\nCopy\ncluster:\n\n    inlineManifests:\n\n        - name: namespace-ci # Name of the manifest.\n\n          contents: |- # Manifest contents as a string.\n\n            apiVersion: v1\n\n            kind: Namespace\n\n            metadata:\n\n            \tname: ci\nField\tType\tDescription\tValue(s)\nname\tstring\t\nName of the manifest.\nShow example(s)\n\t\ncontents\tstring\tManifest contents as a string.\nShow example(s)\n\t\nadminKubeconfig\n\nAdminKubeconfigConfig contains admin kubeconfig settings.\n\nCopy\ncluster:\n\n    adminKubeconfig:\n\n        certLifetime: 1h0m0s # Admin kubeconfig certificate lifetime (default is 1 year).\nField\tType\tDescription\tValue(s)\ncertLifetime\tDuration\t\nAdmin kubeconfig certificate lifetime (default is 1 year).\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "v1alpha1 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/v1alpha1/",
    "html": "Documentation\nReference\nConfiguration\nv1alpha1\nv1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\nConfig\n\nConfig defines the v1alpha1.Config Talos machine configuration document.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "SideroLinkConfig | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/siderolink/siderolinkconfig/",
    "html": "Documentation\nReference\nConfiguration\nsiderolink\nSideroLinkConfig\nSideroLinkConfig\nSideroLinkConfig is a SideroLink connection machine configuration document.\nCopy\napiVersion: v1alpha1\n\nkind: SideroLinkConfig\n\napiUrl: https://siderolink.api/join?token=secret # SideroLink API URL to connect to.\nField\tType\tDescription\tValue(s)\napiUrl\tURL\tSideroLink API URL to connect to.\nShow example(s)\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "siderolink | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/siderolink/",
    "html": "Documentation\nReference\nConfiguration\nsiderolink\nsiderolink\nPackage siderolink provides SideroLink machine configuration documents.\nSideroLinkConfig\n\nSideroLinkConfig is a SideroLink connection machine configuration document.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "KmsgLogConfig | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/runtime/kmsglogconfig/",
    "html": "Documentation\nReference\nConfiguration\nruntime\nKmsgLogConfig\nKmsgLogConfig\nKmsgLogConfig is a event sink config document.\nCopy\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log # Name of the config document.\n\nurl: tcp://192.168.3.7:3478/ # The URL encodes the log destination.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nurl\tURL\t\nThe URL encodes the log destination.\nShow example(s)\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "EventSinkConfig | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/runtime/eventsinkconfig/",
    "html": "Documentation\nReference\nConfiguration\nruntime\nEventSinkConfig\nEventSinkConfig\nEventSinkConfig is a event sink config document.\nCopy\napiVersion: v1alpha1\n\nkind: EventSinkConfig\n\nendpoint: 192.168.10.3:3247 # The endpoint for the event sink as 'host:port'.\nField\tType\tDescription\tValue(s)\nendpoint\tstring\tThe endpoint for the event sink as ‘host:port’.\nShow example(s)\n\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "runtime | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/runtime/",
    "html": "Documentation\nReference\nConfiguration\nruntime\nruntime\nPackage runtime provides runtime machine configuration documents.\nEventSinkConfig\n\nEventSinkConfig is a event sink config document.\n\nKmsgLogConfig\n\nKmsgLogConfig is a event sink config document.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "NetworkRuleConfig | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/network/networkruleconfig/",
    "html": "Documentation\nReference\nConfiguration\nnetwork\nNetworkRuleConfig\nNetworkRuleConfig\nNetworkRuleConfig is a network firewall rule config document.\nCopy\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: ingress-apid # Name of the config document.\n\n# Port selector defines which ports and protocols on the host are affected by the rule.\n\nportSelector:\n\n    # Ports defines a list of port ranges or single ports.\n\n    ports:\n\n        - 50000\n\n    protocol: tcp # Protocol defines traffic protocol (e.g. TCP or UDP).\n\n# Ingress defines which source subnets are allowed to access the host ports/protocols defined by the `portSelector`.\n\ningress:\n\n    - subnet: 192.168.0.0/16 # Subnet defines a source subnet.\nField\tType\tDescription\tValue(s)\nname\tstring\tName of the config document.\t\nportSelector\tRulePortSelector\tPort selector defines which ports and protocols on the host are affected by the rule.\t\ningress\t[]IngressRule\tIngress defines which source subnets are allowed to access the host ports/protocols defined by the portSelector.\t\nportSelector\n\nRulePortSelector is a port selector for the network rule.\n\nField\tType\tDescription\tValue(s)\nports\tPortRanges\t\nPorts defines a list of port ranges or single ports.\nShow example(s)\n\t\nprotocol\tProtocol\tProtocol defines traffic protocol (e.g. TCP or UDP).\ttcp\nudp\nicmp\nicmpv6\n\ningress[]\n\nIngressRule is a ingress rule.\n\nField\tType\tDescription\tValue(s)\nsubnet\tPrefix\tSubnet defines a source subnet.\nShow example(s)\n\t\nexcept\tPrefix\tExcept defines a source subnet to exclude from the rule, it gets excluded from the subnet.\t\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "NetworkDefaultActionConfig | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/network/networkdefaultactionconfig/",
    "html": "Documentation\nReference\nConfiguration\nnetwork\nNetworkDefaultActionConfig\nNetworkDefaultActionConfig\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\nCopy\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: accept # Default action for all not explicitly configured ingress traffic: accept or block.\nField\tType\tDescription\tValue(s)\ningress\tDefaultAction\tDefault action for all not explicitly configured ingress traffic: accept or block.\taccept\nblock\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/network/",
    "html": "Documentation\nReference\nConfiguration\nnetwork\nnetwork\nPackage network provides network machine configuration documents.\nNetworkDefaultActionConfig\n\nNetworkDefaultActionConfig is a ingress firewall default action configuration document.\n\nNetworkRuleConfig\n\nNetworkRuleConfig is a network firewall rule config document.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/configuration/",
    "html": "Documentation\nReference\nConfiguration\nConfiguration\nTalos Linux machine configuration reference.\n\nTalos Linux machine is fully configured via a single YAML file called machine configuration.\n\nThe file might contain one or more configuration documents separated by --- (three dashes) lines. At the moment, majority of the configuration options are within the v1alpha1 document, so this is the only mandatory document in the configuration file.\n\nConfiguration documents might be named (contain a name: field) or unnamed. Unnamed documents can be supplied to the machine configuration file only once, while named documents can be supplied multiple times with unique names.\n\nThe v1alpha1 document has its own (legacy) structure, while every other document has the following set of fields:\n\nCopy\napiVersion: v1alpha1 # version of the document\n\nkind: NetworkRuleConfig # type of document\n\nname: rule1 # only for named documents\n\n\nThis section contains the configuration reference, to learn more about Talos Linux machine configuration management, please see:\n\nquick guide to configuration generation\nconfiguration management in production\nconfiguration patches\nediting live machine configuration\nnetwork\n\nPackage network provides network machine configuration documents.\n\nruntime\n\nPackage runtime provides runtime machine configuration documents.\n\nsiderolink\n\nPackage siderolink provides SideroLink machine configuration documents.\n\nv1alpha1\n\nPackage v1alpha1 contains definition of the v1alpha1 configuration document.\n\nEven though the machine configuration in Talos Linux is multi-document, at the moment this configuration document contains most of the configuration options.\n\nIt is expected that new configuration options will be added as new documents, and existing ones migrated to their own documents.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "API | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/api/",
    "html": "Table of Contents\ncommon/common.proto\nData\nDataResponse\nEmpty\nEmptyResponse\nError\nMetadata\nNetIP\nNetIPPort\nNetIPPrefix\nPEMEncodedCertificateAndKey\nPEMEncodedKey\nURL\nCode\nContainerDriver\nContainerdNamespace\nFile-level Extensions\nresource/definitions/cluster/cluster.proto\nAffiliateSpec\nConfigSpec\nControlPlane\nIdentitySpec\nInfoSpec\nKubeSpanAffiliateSpec\nMemberSpec\nresource/definitions/cri/cri.proto\nSeccompProfileSpec\nresource/definitions/enums/enums.proto\nKubespanPeerState\nMachineType\nNethelpersADSelect\nNethelpersARPAllTargets\nNethelpersARPValidate\nNethelpersAddressFlag\nNethelpersBondMode\nNethelpersBondXmitHashPolicy\nNethelpersConntrackState\nNethelpersDuplex\nNethelpersFailOverMAC\nNethelpersFamily\nNethelpersLACPRate\nNethelpersLinkType\nNethelpersMatchOperator\nNethelpersNfTablesChainHook\nNethelpersNfTablesChainPriority\nNethelpersNfTablesVerdict\nNethelpersOperationalState\nNethelpersPort\nNethelpersPrimaryReselect\nNethelpersProtocol\nNethelpersRouteFlag\nNethelpersRouteProtocol\nNethelpersRouteType\nNethelpersRoutingTable\nNethelpersScope\nNethelpersVLANProtocol\nNetworkConfigLayer\nNetworkOperator\nRuntimeMachineStage\nresource/definitions/etcd/etcd.proto\nConfigSpec\nConfigSpec.ExtraArgsEntry\nMemberSpec\nPKIStatusSpec\nSpecSpec\nSpecSpec.ExtraArgsEntry\nresource/definitions/extensions/extensions.proto\nCompatibility\nConstraint\nLayer\nMetadata\nresource/definitions/files/files.proto\nEtcFileSpecSpec\nEtcFileStatusSpec\nresource/definitions/hardware/hardware.proto\nMemoryModuleSpec\nProcessorSpec\nSystemInformationSpec\nresource/definitions/k8s/k8s.proto\nAPIServerConfigSpec\nAPIServerConfigSpec.EnvironmentVariablesEntry\nAPIServerConfigSpec.ExtraArgsEntry\nAdmissionControlConfigSpec\nAdmissionPluginSpec\nAuditPolicyConfigSpec\nBootstrapManifestsConfigSpec\nConfigStatusSpec\nControllerManagerConfigSpec\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nControllerManagerConfigSpec.ExtraArgsEntry\nEndpointSpec\nExtraManifest\nExtraManifest.ExtraHeadersEntry\nExtraManifestsConfigSpec\nExtraVolume\nKubePrismConfigSpec\nKubePrismEndpoint\nKubePrismEndpointsSpec\nKubePrismStatusesSpec\nKubeletConfigSpec\nKubeletConfigSpec.ExtraArgsEntry\nKubeletSpecSpec\nManifestSpec\nManifestStatusSpec\nNodeIPConfigSpec\nNodeIPSpec\nNodeLabelSpecSpec\nNodeStatusSpec\nNodeStatusSpec.AnnotationsEntry\nNodeStatusSpec.LabelsEntry\nNodeTaintSpecSpec\nNodenameSpec\nResources\nResources.LimitsEntry\nResources.RequestsEntry\nSchedulerConfigSpec\nSchedulerConfigSpec.EnvironmentVariablesEntry\nSchedulerConfigSpec.ExtraArgsEntry\nSecretsStatusSpec\nSingleManifest\nStaticPodServerStatusSpec\nStaticPodSpec\nStaticPodStatusSpec\nresource/definitions/kubeaccess/kubeaccess.proto\nConfigSpec\nresource/definitions/kubespan/kubespan.proto\nConfigSpec\nEndpointSpec\nIdentitySpec\nPeerSpecSpec\nPeerStatusSpec\nresource/definitions/network/network.proto\nAddressSpecSpec\nAddressStatusSpec\nBondMasterSpec\nBondSlave\nBridgeMasterSpec\nBridgeSlave\nDHCP4OperatorSpec\nDHCP6OperatorSpec\nHardwareAddrSpec\nHostnameSpecSpec\nHostnameStatusSpec\nLinkRefreshSpec\nLinkSpecSpec\nLinkStatusSpec\nNfTablesAddressMatch\nNfTablesChainSpec\nNfTablesClampMSS\nNfTablesConntrackStateMatch\nNfTablesIfNameMatch\nNfTablesLayer4Match\nNfTablesLimitMatch\nNfTablesMark\nNfTablesPortMatch\nNfTablesRule\nNodeAddressFilterSpec\nNodeAddressSpec\nOperatorSpecSpec\nPortRange\nProbeSpecSpec\nProbeStatusSpec\nResolverSpecSpec\nResolverStatusSpec\nRouteSpecSpec\nRouteStatusSpec\nSTPSpec\nStatusSpec\nTCPProbeSpec\nTimeServerSpecSpec\nTimeServerStatusSpec\nVIPEquinixMetalSpec\nVIPHCloudSpec\nVIPOperatorSpec\nVLANSpec\nWireguardPeer\nWireguardSpec\nresource/definitions/perf/perf.proto\nCPUSpec\nCPUStat\nMemorySpec\nresource/definitions/proto/proto.proto\nLinuxIDMapping\nMount\nresource/definitions/runtime/runtime.proto\nDevicesStatusSpec\nEventSinkConfigSpec\nKernelModuleSpecSpec\nKernelParamSpecSpec\nKernelParamStatusSpec\nKmsgLogConfigSpec\nMachineStatusSpec\nMachineStatusStatus\nMaintenanceServiceConfigSpec\nMetaKeySpec\nMetaLoadedSpec\nMountStatusSpec\nPlatformMetadataSpec\nSecurityStateSpec\nUniqueMachineTokenSpec\nUnmetCondition\nresource/definitions/secrets/secrets.proto\nAPICertsSpec\nCertSANSpec\nEtcdCertsSpec\nEtcdRootSpec\nKubeletSpec\nKubernetesCertsSpec\nKubernetesDynamicCertsSpec\nKubernetesRootSpec\nMaintenanceRootSpec\nMaintenanceServiceCertsSpec\nOSRootSpec\nTrustdCertsSpec\nresource/definitions/siderolink/siderolink.proto\nConfigSpec\nresource/definitions/time/time.proto\nAdjtimeStatusSpec\nStatusSpec\nresource/definitions/v1alpha1/v1alpha1.proto\nServiceSpec\ninspect/inspect.proto\nControllerDependencyEdge\nControllerRuntimeDependenciesResponse\nControllerRuntimeDependency\nDependencyEdgeType\nInspectService\nmachine/machine.proto\nAddressEvent\nApplyConfiguration\nApplyConfigurationRequest\nApplyConfigurationResponse\nBPFInstruction\nBootstrap\nBootstrapRequest\nBootstrapResponse\nCNIConfig\nCPUInfo\nCPUInfoResponse\nCPUStat\nCPUsInfo\nClusterConfig\nClusterNetworkConfig\nConfigLoadErrorEvent\nConfigValidationErrorEvent\nConnectRecord\nConnectRecord.Process\nContainer\nContainerInfo\nContainersRequest\nContainersResponse\nControlPlaneConfig\nCopyRequest\nDHCPOptionsConfig\nDiskStat\nDiskStats\nDiskStatsResponse\nDiskUsageInfo\nDiskUsageRequest\nDmesgRequest\nEtcdAlarm\nEtcdAlarmDisarm\nEtcdAlarmDisarmResponse\nEtcdAlarmListResponse\nEtcdDefragment\nEtcdDefragmentResponse\nEtcdForfeitLeadership\nEtcdForfeitLeadershipRequest\nEtcdForfeitLeadershipResponse\nEtcdLeaveCluster\nEtcdLeaveClusterRequest\nEtcdLeaveClusterResponse\nEtcdMember\nEtcdMemberAlarm\nEtcdMemberListRequest\nEtcdMemberListResponse\nEtcdMemberStatus\nEtcdMembers\nEtcdRecover\nEtcdRecoverResponse\nEtcdRemoveMember\nEtcdRemoveMemberByID\nEtcdRemoveMemberByIDRequest\nEtcdRemoveMemberByIDResponse\nEtcdRemoveMemberRequest\nEtcdRemoveMemberResponse\nEtcdSnapshotRequest\nEtcdStatus\nEtcdStatusResponse\nEvent\nEventsRequest\nFeaturesInfo\nFileInfo\nGenerateClientConfiguration\nGenerateClientConfigurationRequest\nGenerateClientConfigurationResponse\nGenerateConfiguration\nGenerateConfigurationRequest\nGenerateConfigurationResponse\nHostname\nHostnameResponse\nImageListRequest\nImageListResponse\nImagePull\nImagePullRequest\nImagePullResponse\nInstallConfig\nListRequest\nLoadAvg\nLoadAvgResponse\nLogsRequest\nMachineConfig\nMachineStatusEvent\nMachineStatusEvent.MachineStatus\nMachineStatusEvent.MachineStatus.UnmetCondition\nMemInfo\nMemory\nMemoryResponse\nMetaDelete\nMetaDeleteRequest\nMetaDeleteResponse\nMetaWrite\nMetaWriteRequest\nMetaWriteResponse\nMountStat\nMounts\nMountsResponse\nNetDev\nNetstat\nNetstatRequest\nNetstatRequest.Feature\nNetstatRequest.L4proto\nNetstatRequest.NetNS\nNetstatResponse\nNetworkConfig\nNetworkDeviceConfig\nNetworkDeviceStats\nNetworkDeviceStatsResponse\nPacketCaptureRequest\nPhaseEvent\nPlatformInfo\nProcess\nProcessInfo\nProcessesResponse\nReadRequest\nReboot\nRebootRequest\nRebootResponse\nReset\nResetPartitionSpec\nResetRequest\nResetResponse\nRestart\nRestartEvent\nRestartRequest\nRestartResponse\nRollback\nRollbackRequest\nRollbackResponse\nRouteConfig\nSequenceEvent\nServiceEvent\nServiceEvents\nServiceHealth\nServiceInfo\nServiceList\nServiceListResponse\nServiceRestart\nServiceRestartRequest\nServiceRestartResponse\nServiceStart\nServiceStartRequest\nServiceStartResponse\nServiceStateEvent\nServiceStop\nServiceStopRequest\nServiceStopResponse\nShutdown\nShutdownRequest\nShutdownResponse\nSoftIRQStat\nStat\nStats\nStatsRequest\nStatsResponse\nSystemStat\nSystemStatResponse\nTaskEvent\nUpgrade\nUpgradeRequest\nUpgradeResponse\nVersion\nVersionInfo\nVersionResponse\nApplyConfigurationRequest.Mode\nConnectRecord.State\nConnectRecord.TimerActive\nEtcdMemberAlarm.AlarmType\nListRequest.Type\nMachineConfig.MachineType\nMachineStatusEvent.MachineStage\nNetstatRequest.Filter\nPhaseEvent.Action\nRebootRequest.Mode\nResetRequest.WipeMode\nSequenceEvent.Action\nServiceStateEvent.Action\nTaskEvent.Action\nUpgradeRequest.RebootMode\nMachineService\nsecurity/security.proto\nCertificateRequest\nCertificateResponse\nSecurityService\nstorage/storage.proto\nDisk\nDisks\nDisksResponse\nDisk.DiskType\nStorageService\ntime/time.proto\nTime\nTimeRequest\nTimeResponse\nTimeService\nScalar Value Types\nDocumentation\nReference\nAPI\nAPI\nTalos gRPC API reference.\nTable of Contents\n\ncommon/common.proto\n\nData\n\nDataResponse\n\nEmpty\n\nEmptyResponse\n\nError\n\nMetadata\n\nNetIP\n\nNetIPPort\n\nNetIPPrefix\n\nPEMEncodedCertificateAndKey\n\nPEMEncodedKey\n\nURL\n\nCode\n\nContainerDriver\n\nContainerdNamespace\n\nFile-level Extensions\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\nConfigSpec\nControlPlane\nIdentitySpec\nInfoSpec\nKubeSpanAffiliateSpec\nMemberSpec\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\nMachineType\nNethelpersADSelect\nNethelpersARPAllTargets\nNethelpersARPValidate\nNethelpersAddressFlag\nNethelpersBondMode\nNethelpersBondXmitHashPolicy\nNethelpersConntrackState\nNethelpersDuplex\nNethelpersFailOverMAC\nNethelpersFamily\nNethelpersLACPRate\nNethelpersLinkType\nNethelpersMatchOperator\nNethelpersNfTablesChainHook\nNethelpersNfTablesChainPriority\nNethelpersNfTablesVerdict\nNethelpersOperationalState\nNethelpersPort\nNethelpersPrimaryReselect\nNethelpersProtocol\nNethelpersRouteFlag\nNethelpersRouteProtocol\nNethelpersRouteType\nNethelpersRoutingTable\nNethelpersScope\nNethelpersVLANProtocol\nNetworkConfigLayer\nNetworkOperator\nRuntimeMachineStage\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\nConfigSpec.ExtraArgsEntry\nMemberSpec\nPKIStatusSpec\nSpecSpec\nSpecSpec.ExtraArgsEntry\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\nConstraint\nLayer\nMetadata\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\nEtcFileStatusSpec\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\nProcessorSpec\nSystemInformationSpec\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\nAPIServerConfigSpec.EnvironmentVariablesEntry\nAPIServerConfigSpec.ExtraArgsEntry\nAdmissionControlConfigSpec\nAdmissionPluginSpec\nAuditPolicyConfigSpec\nBootstrapManifestsConfigSpec\nConfigStatusSpec\nControllerManagerConfigSpec\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nControllerManagerConfigSpec.ExtraArgsEntry\nEndpointSpec\nExtraManifest\nExtraManifest.ExtraHeadersEntry\nExtraManifestsConfigSpec\nExtraVolume\nKubePrismConfigSpec\nKubePrismEndpoint\nKubePrismEndpointsSpec\nKubePrismStatusesSpec\nKubeletConfigSpec\nKubeletConfigSpec.ExtraArgsEntry\nKubeletSpecSpec\nManifestSpec\nManifestStatusSpec\nNodeIPConfigSpec\nNodeIPSpec\nNodeLabelSpecSpec\nNodeStatusSpec\nNodeStatusSpec.AnnotationsEntry\nNodeStatusSpec.LabelsEntry\nNodeTaintSpecSpec\nNodenameSpec\nResources\nResources.LimitsEntry\nResources.RequestsEntry\nSchedulerConfigSpec\nSchedulerConfigSpec.EnvironmentVariablesEntry\nSchedulerConfigSpec.ExtraArgsEntry\nSecretsStatusSpec\nSingleManifest\nStaticPodServerStatusSpec\nStaticPodSpec\nStaticPodStatusSpec\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\nEndpointSpec\nIdentitySpec\nPeerSpecSpec\nPeerStatusSpec\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\nAddressStatusSpec\nBondMasterSpec\nBondSlave\nBridgeMasterSpec\nBridgeSlave\nDHCP4OperatorSpec\nDHCP6OperatorSpec\nHardwareAddrSpec\nHostnameSpecSpec\nHostnameStatusSpec\nLinkRefreshSpec\nLinkSpecSpec\nLinkStatusSpec\nNfTablesAddressMatch\nNfTablesChainSpec\nNfTablesClampMSS\nNfTablesConntrackStateMatch\nNfTablesIfNameMatch\nNfTablesLayer4Match\nNfTablesLimitMatch\nNfTablesMark\nNfTablesPortMatch\nNfTablesRule\nNodeAddressFilterSpec\nNodeAddressSpec\nOperatorSpecSpec\nPortRange\nProbeSpecSpec\nProbeStatusSpec\nResolverSpecSpec\nResolverStatusSpec\nRouteSpecSpec\nRouteStatusSpec\nSTPSpec\nStatusSpec\nTCPProbeSpec\nTimeServerSpecSpec\nTimeServerStatusSpec\nVIPEquinixMetalSpec\nVIPHCloudSpec\nVIPOperatorSpec\nVLANSpec\nWireguardPeer\nWireguardSpec\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\nCPUStat\nMemorySpec\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\nMount\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\nEventSinkConfigSpec\nKernelModuleSpecSpec\nKernelParamSpecSpec\nKernelParamStatusSpec\nKmsgLogConfigSpec\nMachineStatusSpec\nMachineStatusStatus\nMaintenanceServiceConfigSpec\nMetaKeySpec\nMetaLoadedSpec\nMountStatusSpec\nPlatformMetadataSpec\nSecurityStateSpec\nUniqueMachineTokenSpec\nUnmetCondition\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\nCertSANSpec\nEtcdCertsSpec\nEtcdRootSpec\nKubeletSpec\nKubernetesCertsSpec\nKubernetesDynamicCertsSpec\nKubernetesRootSpec\nMaintenanceRootSpec\nMaintenanceServiceCertsSpec\nOSRootSpec\nTrustdCertsSpec\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\nStatusSpec\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\ninspect/inspect.proto\n\nControllerDependencyEdge\n\nControllerRuntimeDependenciesResponse\n\nControllerRuntimeDependency\n\nDependencyEdgeType\n\nInspectService\n\nmachine/machine.proto\n\nAddressEvent\n\nApplyConfiguration\n\nApplyConfigurationRequest\n\nApplyConfigurationResponse\n\nBPFInstruction\n\nBootstrap\n\nBootstrapRequest\n\nBootstrapResponse\n\nCNIConfig\n\nCPUInfo\n\nCPUInfoResponse\n\nCPUStat\n\nCPUsInfo\n\nClusterConfig\n\nClusterNetworkConfig\n\nConfigLoadErrorEvent\n\nConfigValidationErrorEvent\n\nConnectRecord\n\nConnectRecord.Process\n\nContainer\n\nContainerInfo\n\nContainersRequest\n\nContainersResponse\n\nControlPlaneConfig\n\nCopyRequest\n\nDHCPOptionsConfig\n\nDiskStat\n\nDiskStats\n\nDiskStatsResponse\n\nDiskUsageInfo\n\nDiskUsageRequest\n\nDmesgRequest\n\nEtcdAlarm\n\nEtcdAlarmDisarm\n\nEtcdAlarmDisarmResponse\n\nEtcdAlarmListResponse\n\nEtcdDefragment\n\nEtcdDefragmentResponse\n\nEtcdForfeitLeadership\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\n\nEtcdLeaveCluster\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\n\nEtcdMember\n\nEtcdMemberAlarm\n\nEtcdMemberListRequest\n\nEtcdMemberListResponse\n\nEtcdMemberStatus\n\nEtcdMembers\n\nEtcdRecover\n\nEtcdRecoverResponse\n\nEtcdRemoveMember\n\nEtcdRemoveMemberByID\n\nEtcdRemoveMemberByIDRequest\n\nEtcdRemoveMemberByIDResponse\n\nEtcdRemoveMemberRequest\n\nEtcdRemoveMemberResponse\n\nEtcdSnapshotRequest\n\nEtcdStatus\n\nEtcdStatusResponse\n\nEvent\n\nEventsRequest\n\nFeaturesInfo\n\nFileInfo\n\nGenerateClientConfiguration\n\nGenerateClientConfigurationRequest\n\nGenerateClientConfigurationResponse\n\nGenerateConfiguration\n\nGenerateConfigurationRequest\n\nGenerateConfigurationResponse\n\nHostname\n\nHostnameResponse\n\nImageListRequest\n\nImageListResponse\n\nImagePull\n\nImagePullRequest\n\nImagePullResponse\n\nInstallConfig\n\nListRequest\n\nLoadAvg\n\nLoadAvgResponse\n\nLogsRequest\n\nMachineConfig\n\nMachineStatusEvent\n\nMachineStatusEvent.MachineStatus\n\nMachineStatusEvent.MachineStatus.UnmetCondition\n\nMemInfo\n\nMemory\n\nMemoryResponse\n\nMetaDelete\n\nMetaDeleteRequest\n\nMetaDeleteResponse\n\nMetaWrite\n\nMetaWriteRequest\n\nMetaWriteResponse\n\nMountStat\n\nMounts\n\nMountsResponse\n\nNetDev\n\nNetstat\n\nNetstatRequest\n\nNetstatRequest.Feature\n\nNetstatRequest.L4proto\n\nNetstatRequest.NetNS\n\nNetstatResponse\n\nNetworkConfig\n\nNetworkDeviceConfig\n\nNetworkDeviceStats\n\nNetworkDeviceStatsResponse\n\nPacketCaptureRequest\n\nPhaseEvent\n\nPlatformInfo\n\nProcess\n\nProcessInfo\n\nProcessesResponse\n\nReadRequest\n\nReboot\n\nRebootRequest\n\nRebootResponse\n\nReset\n\nResetPartitionSpec\n\nResetRequest\n\nResetResponse\n\nRestart\n\nRestartEvent\n\nRestartRequest\n\nRestartResponse\n\nRollback\n\nRollbackRequest\n\nRollbackResponse\n\nRouteConfig\n\nSequenceEvent\n\nServiceEvent\n\nServiceEvents\n\nServiceHealth\n\nServiceInfo\n\nServiceList\n\nServiceListResponse\n\nServiceRestart\n\nServiceRestartRequest\n\nServiceRestartResponse\n\nServiceStart\n\nServiceStartRequest\n\nServiceStartResponse\n\nServiceStateEvent\n\nServiceStop\n\nServiceStopRequest\n\nServiceStopResponse\n\nShutdown\n\nShutdownRequest\n\nShutdownResponse\n\nSoftIRQStat\n\nStat\n\nStats\n\nStatsRequest\n\nStatsResponse\n\nSystemStat\n\nSystemStatResponse\n\nTaskEvent\n\nUpgrade\n\nUpgradeRequest\n\nUpgradeResponse\n\nVersion\n\nVersionInfo\n\nVersionResponse\n\nApplyConfigurationRequest.Mode\n\nConnectRecord.State\n\nConnectRecord.TimerActive\n\nEtcdMemberAlarm.AlarmType\n\nListRequest.Type\n\nMachineConfig.MachineType\n\nMachineStatusEvent.MachineStage\n\nNetstatRequest.Filter\n\nPhaseEvent.Action\n\nRebootRequest.Mode\n\nResetRequest.WipeMode\n\nSequenceEvent.Action\n\nServiceStateEvent.Action\n\nTaskEvent.Action\n\nUpgradeRequest.RebootMode\n\nMachineService\n\nsecurity/security.proto\n\nCertificateRequest\n\nCertificateResponse\n\nSecurityService\n\nstorage/storage.proto\n\nDisk\n\nDisks\n\nDisksResponse\n\nDisk.DiskType\n\nStorageService\n\ntime/time.proto\n\nTime\n\nTimeRequest\n\nTimeResponse\n\nTimeService\n\nScalar Value Types\n\nTop\n\ncommon/common.proto\n\nData\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\nbytes\tbytes\t\t\n\nDataResponse\nField\tType\tLabel\tDescription\nmessages\tData\trepeated\t\n\nEmpty\nField\tType\tLabel\tDescription\nmetadata\tMetadata\t\t\n\nEmptyResponse\nField\tType\tLabel\tDescription\nmessages\tEmpty\trepeated\t\n\nError\nField\tType\tLabel\tDescription\ncode\tCode\t\t\nmessage\tstring\t\t\ndetails\tgoogle.protobuf.Any\trepeated\t\n\nMetadata\n\nCommon metadata message nested in all reply message types\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\thostname of the server response comes from (injected by proxy)\nerror\tstring\t\terror is set if request failed to the upstream (rest of response is undefined)\nstatus\tgoogle.rpc.Status\t\terror as gRPC Status\n\nNetIP\nField\tType\tLabel\tDescription\nip\tbytes\t\t\n\nNetIPPort\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nport\tint32\t\t\n\nNetIPPrefix\nField\tType\tLabel\tDescription\nip\tbytes\t\t\nprefix_length\tint32\t\t\n\nPEMEncodedCertificateAndKey\nField\tType\tLabel\tDescription\ncrt\tbytes\t\t\nkey\tbytes\t\t\n\nPEMEncodedKey\nField\tType\tLabel\tDescription\nkey\tbytes\t\t\n\nURL\nField\tType\tLabel\tDescription\nfull_path\tstring\t\t\n\nCode\nName\tNumber\tDescription\nFATAL\t0\t\nLOCKED\t1\t\nCANCELED\t2\t\n\nContainerDriver\nName\tNumber\tDescription\nCONTAINERD\t0\t\nCRI\t1\t\n\nContainerdNamespace\nName\tNumber\tDescription\nNS_UNKNOWN\t0\t\nNS_SYSTEM\t1\t\nNS_CRI\t2\t\n\nFile-level Extensions\nExtension\tType\tBase\tNumber\tDescription\nremove_deprecated_enum\tstring\t.google.protobuf.EnumOptions\t93117\tIndicates the Talos version when this deprecated enum will be removed from API.\nremove_deprecated_enum_value\tstring\t.google.protobuf.EnumValueOptions\t93117\tIndicates the Talos version when this deprecated enum value will be removed from API.\nremove_deprecated_field\tstring\t.google.protobuf.FieldOptions\t93117\tIndicates the Talos version when this deprecated filed will be removed from API.\nremove_deprecated_message\tstring\t.google.protobuf.MessageOptions\t93117\tIndicates the Talos version when this deprecated message will be removed from API.\nremove_deprecated_method\tstring\t.google.protobuf.MethodOptions\t93117\tIndicates the Talos version when this deprecated method will be removed from API.\nremove_deprecated_service\tstring\t.google.protobuf.ServiceOptions\t93117\tIndicates the Talos version when this deprecated service will be removed from API.\n\nTop\n\nresource/definitions/cluster/cluster.proto\n\nAffiliateSpec\n\nAffiliateSpec describes Affiliate state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nnodename\tstring\t\t\noperating_system\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\nkube_span\tKubeSpanAffiliateSpec\t\t\ncontrol_plane\tControlPlane\t\t\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration.\n\nField\tType\tLabel\tDescription\ndiscovery_enabled\tbool\t\t\nregistry_kubernetes_enabled\tbool\t\t\nregistry_service_enabled\tbool\t\t\nservice_endpoint\tstring\t\t\nservice_endpoint_insecure\tbool\t\t\nservice_encryption_key\tbytes\t\t\nservice_cluster_id\tstring\t\t\n\nControlPlane\n\nControlPlane describes ControlPlane data if any.\n\nField\tType\tLabel\tDescription\napi_server_port\tint64\t\t\n\nIdentitySpec\n\nIdentitySpec describes status of rendered secrets.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\n\nInfoSpec\n\nInfoSpec describes cluster information.\n\nField\tType\tLabel\tDescription\ncluster_id\tstring\t\t\ncluster_name\tstring\t\t\n\nKubeSpanAffiliateSpec\n\nKubeSpanAffiliateSpec describes additional information specific for the KubeSpan.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\naddress\tcommon.NetIP\t\t\nadditional_addresses\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\n\nMemberSpec\n\nMemberSpec describes Member state.\n\nField\tType\tLabel\tDescription\nnode_id\tstring\t\t\naddresses\tcommon.NetIP\trepeated\t\nhostname\tstring\t\t\nmachine_type\ttalos.resource.definitions.enums.MachineType\t\t\noperating_system\tstring\t\t\ncontrol_plane\tControlPlane\t\t\n\nTop\n\nresource/definitions/cri/cri.proto\n\nSeccompProfileSpec\n\nSeccompProfileSpec represents the SeccompProfile.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nvalue\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/enums/enums.proto\n\nKubespanPeerState\n\nKubespanPeerState is KubeSpan peer current state.\n\nName\tNumber\tDescription\nPEER_STATE_UNKNOWN\t0\t\nPEER_STATE_UP\t1\t\nPEER_STATE_DOWN\t2\t\n\nMachineType\n\nMachineType represents a machine type.\n\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\tTypeUnknown represents undefined node type, when there is no machine configuration yet.\nTYPE_INIT\t1\tTypeInit type designates the first control plane node to come up. You can think of it like a bootstrap node. This node will perform the initial steps to bootstrap the cluster – generation of TLS assets, starting of the control plane, etc.\nTYPE_CONTROL_PLANE\t2\tTypeControlPlane designates the node as a control plane member. This means it will host etcd along with the Kubernetes controlplane components such as API Server, Controller Manager, Scheduler.\nTYPE_WORKER\t3\tTypeWorker designates the node as a worker node. This means it will be an available compute node for scheduling workloads.\n\nNethelpersADSelect\n\nNethelpersADSelect is ADSelect.\n\nName\tNumber\tDescription\nAD_SELECT_STABLE\t0\t\nAD_SELECT_BANDWIDTH\t1\t\nAD_SELECT_COUNT\t2\t\n\nNethelpersARPAllTargets\n\nNethelpersARPAllTargets is an ARP targets mode.\n\nName\tNumber\tDescription\nARP_ALL_TARGETS_ANY\t0\t\nARP_ALL_TARGETS_ALL\t1\t\n\nNethelpersARPValidate\n\nNethelpersARPValidate is an ARP Validation mode.\n\nName\tNumber\tDescription\nARP_VALIDATE_NONE\t0\t\nARP_VALIDATE_ACTIVE\t1\t\nARP_VALIDATE_BACKUP\t2\t\nARP_VALIDATE_ALL\t3\t\n\nNethelpersAddressFlag\n\nNethelpersAddressFlag wraps IFF_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ADDRESSFLAG_UNSPECIFIED\t0\t\nADDRESS_TEMPORARY\t1\t\nADDRESS_NO_DAD\t2\t\nADDRESS_OPTIMISTIC\t4\t\nADDRESS_DAD_FAILED\t8\t\nADDRESS_HOME\t16\t\nADDRESS_DEPRECATED\t32\t\nADDRESS_TENTATIVE\t64\t\nADDRESS_PERMANENT\t128\t\nADDRESS_MANAGEMENT_TEMP\t256\t\nADDRESS_NO_PREFIX_ROUTE\t512\t\nADDRESS_MC_AUTO_JOIN\t1024\t\nADDRESS_STABLE_PRIVACY\t2048\t\n\nNethelpersBondMode\n\nNethelpersBondMode is a bond mode.\n\nName\tNumber\tDescription\nBOND_MODE_ROUNDROBIN\t0\t\nBOND_MODE_ACTIVE_BACKUP\t1\t\nBOND_MODE_XOR\t2\t\nBOND_MODE_BROADCAST\t3\t\nBOND_MODE8023_AD\t4\t\nBOND_MODE_TLB\t5\t\nBOND_MODE_ALB\t6\t\n\nNethelpersBondXmitHashPolicy\n\nNethelpersBondXmitHashPolicy is a bond hash policy.\n\nName\tNumber\tDescription\nBOND_XMIT_POLICY_LAYER2\t0\t\nBOND_XMIT_POLICY_LAYER34\t1\t\nBOND_XMIT_POLICY_LAYER23\t2\t\nBOND_XMIT_POLICY_ENCAP23\t3\t\nBOND_XMIT_POLICY_ENCAP34\t4\t\n\nNethelpersConntrackState\n\nNethelpersConntrackState is a conntrack state.\n\nName\tNumber\tDescription\nNETHELPERS_CONNTRACKSTATE_UNSPECIFIED\t0\t\nCONNTRACK_STATE_NEW\t8\t\nCONNTRACK_STATE_RELATED\t4\t\nCONNTRACK_STATE_ESTABLISHED\t2\t\nCONNTRACK_STATE_INVALID\t1\t\n\nNethelpersDuplex\n\nNethelpersDuplex wraps ethtool.Duplex for YAML marshaling.\n\nName\tNumber\tDescription\nHALF\t0\t\nFULL\t1\t\nUNKNOWN\t255\t\n\nNethelpersFailOverMAC\n\nNethelpersFailOverMAC is a MAC failover mode.\n\nName\tNumber\tDescription\nFAIL_OVER_MAC_NONE\t0\t\nFAIL_OVER_MAC_ACTIVE\t1\t\nFAIL_OVER_MAC_FOLLOW\t2\t\n\nNethelpersFamily\n\nNethelpersFamily is a network family.\n\nName\tNumber\tDescription\nNETHELPERS_FAMILY_UNSPECIFIED\t0\t\nFAMILY_INET4\t2\t\nFAMILY_INET6\t10\t\n\nNethelpersLACPRate\n\nNethelpersLACPRate is a LACP rate.\n\nName\tNumber\tDescription\nLACP_RATE_SLOW\t0\t\nLACP_RATE_FAST\t1\t\n\nNethelpersLinkType\n\nNethelpersLinkType is a link type.\n\nName\tNumber\tDescription\nLINK_NETROM\t0\t\nLINK_ETHER\t1\t\nLINK_EETHER\t2\t\nLINK_AX25\t3\t\nLINK_PRONET\t4\t\nLINK_CHAOS\t5\t\nLINK_IEE802\t6\t\nLINK_ARCNET\t7\t\nLINK_ATALK\t8\t\nLINK_DLCI\t15\t\nLINK_ATM\t19\t\nLINK_METRICOM\t23\t\nLINK_IEEE1394\t24\t\nLINK_EUI64\t27\t\nLINK_INFINIBAND\t32\t\nLINK_SLIP\t256\t\nLINK_CSLIP\t257\t\nLINK_SLIP6\t258\t\nLINK_CSLIP6\t259\t\nLINK_RSRVD\t260\t\nLINK_ADAPT\t264\t\nLINK_ROSE\t270\t\nLINK_X25\t271\t\nLINK_HWX25\t272\t\nLINK_CAN\t280\t\nLINK_PPP\t512\t\nLINK_CISCO\t513\t\nLINK_HDLC\t513\t\nLINK_LAPB\t516\t\nLINK_DDCMP\t517\t\nLINK_RAWHDLC\t518\t\nLINK_TUNNEL\t768\t\nLINK_TUNNEL6\t769\t\nLINK_FRAD\t770\t\nLINK_SKIP\t771\t\nLINK_LOOPBCK\t772\t\nLINK_LOCALTLK\t773\t\nLINK_FDDI\t774\t\nLINK_BIF\t775\t\nLINK_SIT\t776\t\nLINK_IPDDP\t777\t\nLINK_IPGRE\t778\t\nLINK_PIMREG\t779\t\nLINK_HIPPI\t780\t\nLINK_ASH\t781\t\nLINK_ECONET\t782\t\nLINK_IRDA\t783\t\nLINK_FCPP\t784\t\nLINK_FCAL\t785\t\nLINK_FCPL\t786\t\nLINK_FCFABRIC\t787\t\nLINK_FCFABRIC1\t788\t\nLINK_FCFABRIC2\t789\t\nLINK_FCFABRIC3\t790\t\nLINK_FCFABRIC4\t791\t\nLINK_FCFABRIC5\t792\t\nLINK_FCFABRIC6\t793\t\nLINK_FCFABRIC7\t794\t\nLINK_FCFABRIC8\t795\t\nLINK_FCFABRIC9\t796\t\nLINK_FCFABRIC10\t797\t\nLINK_FCFABRIC11\t798\t\nLINK_FCFABRIC12\t799\t\nLINK_IEE802TR\t800\t\nLINK_IEE80211\t801\t\nLINK_IEE80211PRISM\t802\t\nLINK_IEE80211_RADIOTAP\t803\t\nLINK_IEE8021154\t804\t\nLINK_IEE8021154MONITOR\t805\t\nLINK_PHONET\t820\t\nLINK_PHONETPIPE\t821\t\nLINK_CAIF\t822\t\nLINK_IP6GRE\t823\t\nLINK_NETLINK\t824\t\nLINK6_LOWPAN\t825\t\nLINK_VOID\t65535\t\nLINK_NONE\t65534\t\n\nNethelpersMatchOperator\n\nNethelpersMatchOperator is a netfilter match operator.\n\nName\tNumber\tDescription\nOPERATOR_EQUAL\t0\t\nOPERATOR_NOT_EQUAL\t1\t\n\nNethelpersNfTablesChainHook\n\nNethelpersNfTablesChainHook wraps nftables.ChainHook for YAML marshaling.\n\nName\tNumber\tDescription\nCHAIN_HOOK_PREROUTING\t0\t\nCHAIN_HOOK_INPUT\t1\t\nCHAIN_HOOK_FORWARD\t2\t\nCHAIN_HOOK_OUTPUT\t3\t\nCHAIN_HOOK_POSTROUTING\t4\t\n\nNethelpersNfTablesChainPriority\n\nNethelpersNfTablesChainPriority wraps nftables.ChainPriority for YAML marshaling.\n\nName\tNumber\tDescription\nNETHELPERS_NFTABLESCHAINPRIORITY_UNSPECIFIED\t0\t\nCHAIN_PRIORITY_FIRST\t-2147483648\t\nCHAIN_PRIORITY_CONNTRACK_DEFRAG\t-400\t\nCHAIN_PRIORITY_RAW\t-300\t\nCHAIN_PRIORITY_SE_LINUX_FIRST\t-225\t\nCHAIN_PRIORITY_CONNTRACK\t-200\t\nCHAIN_PRIORITY_MANGLE\t-150\t\nCHAIN_PRIORITY_NAT_DEST\t-100\t\nCHAIN_PRIORITY_FILTER\t0\t\nCHAIN_PRIORITY_SECURITY\t50\t\nCHAIN_PRIORITY_NAT_SOURCE\t100\t\nCHAIN_PRIORITY_SE_LINUX_LAST\t225\t\nCHAIN_PRIORITY_CONNTRACK_HELPER\t300\t\nCHAIN_PRIORITY_LAST\t2147483647\t\n\nNethelpersNfTablesVerdict\n\nNethelpersNfTablesVerdict wraps nftables.Verdict for YAML marshaling.\n\nName\tNumber\tDescription\nVERDICT_DROP\t0\t\nVERDICT_ACCEPT\t1\t\n\nNethelpersOperationalState\n\nNethelpersOperationalState wraps rtnetlink.OperationalState for YAML marshaling.\n\nName\tNumber\tDescription\nOPER_STATE_UNKNOWN\t0\t\nOPER_STATE_NOT_PRESENT\t1\t\nOPER_STATE_DOWN\t2\t\nOPER_STATE_LOWER_LAYER_DOWN\t3\t\nOPER_STATE_TESTING\t4\t\nOPER_STATE_DORMANT\t5\t\nOPER_STATE_UP\t6\t\n\nNethelpersPort\n\nNethelpersPort wraps ethtool.Port for YAML marshaling.\n\nName\tNumber\tDescription\nTWISTED_PAIR\t0\t\nAUI\t1\t\nMII\t2\t\nFIBRE\t3\t\nBNC\t4\t\nDIRECT_ATTACH\t5\t\nNONE\t239\t\nOTHER\t255\t\n\nNethelpersPrimaryReselect\n\nNethelpersPrimaryReselect is an ARP targets mode.\n\nName\tNumber\tDescription\nPRIMARY_RESELECT_ALWAYS\t0\t\nPRIMARY_RESELECT_BETTER\t1\t\nPRIMARY_RESELECT_FAILURE\t2\t\n\nNethelpersProtocol\n\nNethelpersProtocol is a inet protocol.\n\nName\tNumber\tDescription\nNETHELPERS_PROTOCOL_UNSPECIFIED\t0\t\nPROTOCOL_ICMP\t1\t\nPROTOCOL_TCP\t6\t\nPROTOCOL_UDP\t17\t\nPROTOCOL_ICM_PV6\t58\t\n\nNethelpersRouteFlag\n\nNethelpersRouteFlag wraps RTM_F_* constants.\n\nName\tNumber\tDescription\nNETHELPERS_ROUTEFLAG_UNSPECIFIED\t0\t\nROUTE_NOTIFY\t256\t\nROUTE_CLONED\t512\t\nROUTE_EQUALIZE\t1024\t\nROUTE_PREFIX\t2048\t\nROUTE_LOOKUP_TABLE\t4096\t\nROUTE_FIB_MATCH\t8192\t\nROUTE_OFFLOAD\t16384\t\nROUTE_TRAP\t32768\t\n\nNethelpersRouteProtocol\n\nNethelpersRouteProtocol is a routing protocol.\n\nName\tNumber\tDescription\nPROTOCOL_UNSPEC\t0\t\nPROTOCOL_REDIRECT\t1\t\nPROTOCOL_KERNEL\t2\t\nPROTOCOL_BOOT\t3\t\nPROTOCOL_STATIC\t4\t\nPROTOCOL_RA\t9\t\nPROTOCOL_MRT\t10\t\nPROTOCOL_ZEBRA\t11\t\nPROTOCOL_BIRD\t12\t\nPROTOCOL_DNROUTED\t13\t\nPROTOCOL_XORP\t14\t\nPROTOCOL_NTK\t15\t\nPROTOCOL_DHCP\t16\t\nPROTOCOL_MRTD\t17\t\nPROTOCOL_KEEPALIVED\t18\t\nPROTOCOL_BABEL\t42\t\nPROTOCOL_OPENR\t99\t\nPROTOCOL_BGP\t186\t\nPROTOCOL_ISIS\t187\t\nPROTOCOL_OSPF\t188\t\nPROTOCOL_RIP\t189\t\nPROTOCOL_EIGRP\t192\t\n\nNethelpersRouteType\n\nNethelpersRouteType is a route type.\n\nName\tNumber\tDescription\nTYPE_UNSPEC\t0\t\nTYPE_UNICAST\t1\t\nTYPE_LOCAL\t2\t\nTYPE_BROADCAST\t3\t\nTYPE_ANYCAST\t4\t\nTYPE_MULTICAST\t5\t\nTYPE_BLACKHOLE\t6\t\nTYPE_UNREACHABLE\t7\t\nTYPE_PROHIBIT\t8\t\nTYPE_THROW\t9\t\nTYPE_NAT\t10\t\nTYPE_X_RESOLVE\t11\t\n\nNethelpersRoutingTable\n\nNethelpersRoutingTable is a routing table ID.\n\nName\tNumber\tDescription\nTABLE_UNSPEC\t0\t\nTABLE_DEFAULT\t253\t\nTABLE_MAIN\t254\t\nTABLE_LOCAL\t255\t\n\nNethelpersScope\n\nNethelpersScope is an address scope.\n\nName\tNumber\tDescription\nSCOPE_GLOBAL\t0\t\nSCOPE_SITE\t200\t\nSCOPE_LINK\t253\t\nSCOPE_HOST\t254\t\nSCOPE_NOWHERE\t255\t\n\nNethelpersVLANProtocol\n\nNethelpersVLANProtocol is a VLAN protocol.\n\nName\tNumber\tDescription\nNETHELPERS_VLANPROTOCOL_UNSPECIFIED\t0\t\nVLAN_PROTOCOL8021_Q\t33024\t\nVLAN_PROTOCOL8021_AD\t34984\t\n\nNetworkConfigLayer\n\nNetworkConfigLayer describes network configuration layers, with lowest priority first.\n\nName\tNumber\tDescription\nCONFIG_DEFAULT\t0\t\nCONFIG_CMDLINE\t1\t\nCONFIG_PLATFORM\t2\t\nCONFIG_OPERATOR\t3\t\nCONFIG_MACHINE_CONFIGURATION\t4\t\n\nNetworkOperator\n\nNetworkOperator enumerates Talos network operators.\n\nName\tNumber\tDescription\nOPERATOR_DHCP4\t0\t\nOPERATOR_DHCP6\t1\t\nOPERATOR_VIP\t2\t\n\nRuntimeMachineStage\n\nRuntimeMachineStage describes the stage of the machine boot/run process.\n\nName\tNumber\tDescription\nMACHINE_STAGE_UNKNOWN\t0\t\nMACHINE_STAGE_BOOTING\t1\t\nMACHINE_STAGE_INSTALLING\t2\t\nMACHINE_STAGE_MAINTENANCE\t3\t\nMACHINE_STAGE_RUNNING\t4\t\nMACHINE_STAGE_REBOOTING\t5\t\nMACHINE_STAGE_SHUTTING_DOWN\t6\t\nMACHINE_STAGE_RESETTING\t7\t\nMACHINE_STAGE_UPGRADING\t8\t\n\nTop\n\nresource/definitions/etcd/etcd.proto\n\nConfigSpec\n\nConfigSpec describes (some) configuration settings of etcd.\n\nField\tType\tLabel\tDescription\nadvertise_valid_subnets\tstring\trepeated\t\nadvertise_exclude_subnets\tstring\trepeated\t\nimage\tstring\t\t\nextra_args\tConfigSpec.ExtraArgsEntry\trepeated\t\nlisten_valid_subnets\tstring\trepeated\t\nlisten_exclude_subnets\tstring\trepeated\t\n\nConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nMemberSpec\n\nMemberSpec holds information about an etcd member.\n\nField\tType\tLabel\tDescription\nmember_id\tstring\t\t\n\nPKIStatusSpec\n\nPKIStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSpecSpec\n\nSpecSpec describes (some) Specuration settings of etcd.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nadvertised_addresses\tcommon.NetIP\trepeated\t\nimage\tstring\t\t\nextra_args\tSpecSpec.ExtraArgsEntry\trepeated\t\nlisten_peer_addresses\tcommon.NetIP\trepeated\t\nlisten_client_addresses\tcommon.NetIP\trepeated\t\n\nSpecSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nTop\n\nresource/definitions/extensions/extensions.proto\n\nCompatibility\n\nCompatibility describes extension compatibility.\n\nField\tType\tLabel\tDescription\ntalos\tConstraint\t\t\n\nConstraint\n\nConstraint describes compatibility constraint.\n\nField\tType\tLabel\tDescription\nversion\tstring\t\t\n\nLayer\n\nLayer defines overlay mount layer.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nmetadata\tMetadata\t\t\n\nMetadata\n\nMetadata describes base extension metadata.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nversion\tstring\t\t\nauthor\tstring\t\t\ndescription\tstring\t\t\ncompatibility\tCompatibility\t\t\n\nTop\n\nresource/definitions/files/files.proto\n\nEtcFileSpecSpec\n\nEtcFileSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ncontents\tbytes\t\t\nmode\tuint32\t\t\n\nEtcFileStatusSpec\n\nEtcFileStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nspec_version\tstring\t\t\n\nTop\n\nresource/definitions/hardware/hardware.proto\n\nMemoryModuleSpec\n\nMemoryModuleSpec represents a single Memory.\n\nField\tType\tLabel\tDescription\nsize\tuint32\t\t\ndevice_locator\tstring\t\t\nbank_locator\tstring\t\t\nspeed\tuint32\t\t\nmanufacturer\tstring\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\nproduct_name\tstring\t\t\n\nProcessorSpec\n\nProcessorSpec represents a single processor.\n\nField\tType\tLabel\tDescription\nsocket\tstring\t\t\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nmax_speed\tuint32\t\t\nboot_speed\tuint32\t\t\nstatus\tuint32\t\t\nserial_number\tstring\t\t\nasset_tag\tstring\t\t\npart_number\tstring\t\t\ncore_count\tuint32\t\t\ncore_enabled\tuint32\t\t\nthread_count\tuint32\t\t\n\nSystemInformationSpec\n\nSystemInformationSpec represents the system information obtained from smbios.\n\nField\tType\tLabel\tDescription\nmanufacturer\tstring\t\t\nproduct_name\tstring\t\t\nversion\tstring\t\t\nserial_number\tstring\t\t\nuuid\tstring\t\t\nwake_up_type\tstring\t\t\nsku_number\tstring\t\t\n\nTop\n\nresource/definitions/k8s/k8s.proto\n\nAPIServerConfigSpec\n\nAPIServerConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncloud_provider\tstring\t\t\ncontrol_plane_endpoint\tstring\t\t\netcd_servers\tstring\trepeated\t\nlocal_port\tint64\t\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tAPIServerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tAPIServerConfigSpec.EnvironmentVariablesEntry\trepeated\t\npod_security_policy_enabled\tbool\t\t\nadvertised_address\tstring\t\t\nresources\tResources\t\t\n\nAPIServerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAPIServerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nAdmissionControlConfigSpec\n\nAdmissionControlConfigSpec is configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tAdmissionPluginSpec\trepeated\t\n\nAdmissionPluginSpec\n\nAdmissionPluginSpec is a single admission plugin configuration Admission Control plugins.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nconfiguration\tgoogle.protobuf.Struct\t\t\n\nAuditPolicyConfigSpec\n\nAuditPolicyConfigSpec is audit policy configuration for kube-apiserver.\n\nField\tType\tLabel\tDescription\nconfig\tgoogle.protobuf.Struct\t\t\n\nBootstrapManifestsConfigSpec\n\nBootstrapManifestsConfigSpec is configuration for bootstrap manifests.\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\ncluster_domain\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nproxy_enabled\tbool\t\t\nproxy_image\tstring\t\t\nproxy_args\tstring\trepeated\t\ncore_dns_enabled\tbool\t\t\ncore_dns_image\tstring\t\t\ndns_service_ip\tstring\t\t\ndns_service_i_pv6\tstring\t\t\nflannel_enabled\tbool\t\t\nflannel_image\tstring\t\t\nflannel_cni_image\tstring\t\t\npod_security_policy_enabled\tbool\t\t\ntalos_api_service_enabled\tbool\t\t\nflannel_extra_args\tstring\trepeated\t\n\nConfigStatusSpec\n\nConfigStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nControllerManagerConfigSpec\n\nControllerManagerConfigSpec is configuration for kube-controller-manager.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\ncloud_provider\tstring\t\t\npod_cid_rs\tstring\trepeated\t\nservice_cid_rs\tstring\trepeated\t\nextra_args\tControllerManagerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tControllerManagerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\n\nControllerManagerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nControllerManagerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nEndpointSpec\n\nEndpointSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nExtraManifest\n\nExtraManifest defines a single extra manifest to download.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurl\tstring\t\t\npriority\tstring\t\t\nextra_headers\tExtraManifest.ExtraHeadersEntry\trepeated\t\ninline_manifest\tstring\t\t\n\nExtraManifest.ExtraHeadersEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nExtraManifestsConfigSpec\n\nExtraManifestsConfigSpec is configuration for extra bootstrap manifests.\n\nField\tType\tLabel\tDescription\nextra_manifests\tExtraManifest\trepeated\t\n\nExtraVolume\n\nExtraVolume is a configuration of extra volume.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhost_path\tstring\t\t\nmount_path\tstring\t\t\nread_only\tbool\t\t\n\nKubePrismConfigSpec\n\nKubePrismConfigSpec describes KubePrismConfig data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tint64\t\t\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismEndpoint\n\nKubePrismEndpoint holds data for control plane endpoint.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nport\tuint32\t\t\n\nKubePrismEndpointsSpec\n\nKubePrismEndpointsSpec describes KubePrismEndpoints configuration.\n\nField\tType\tLabel\tDescription\nendpoints\tKubePrismEndpoint\trepeated\t\n\nKubePrismStatusesSpec\n\nKubePrismStatusesSpec describes KubePrismStatuses data.\n\nField\tType\tLabel\tDescription\nhost\tstring\t\t\nhealthy\tbool\t\t\n\nKubeletConfigSpec\n\nKubeletConfigSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\ncluster_dns\tstring\trepeated\t\ncluster_domain\tstring\t\t\nextra_args\tKubeletConfigSpec.ExtraArgsEntry\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nextra_config\tgoogle.protobuf.Struct\t\t\ncloud_provider_external\tbool\t\t\ndefault_runtime_seccomp_enabled\tbool\t\t\nskip_node_registration\tbool\t\t\nstatic_pod_list_url\tstring\t\t\ndisable_manifests_directory\tbool\t\t\nenable_fs_quota_monitoring\tbool\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nKubeletConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nKubeletSpecSpec\n\nKubeletSpecSpec holds the source of kubelet configuration.\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\nargs\tstring\trepeated\t\nextra_mounts\ttalos.resource.definitions.proto.Mount\trepeated\t\nexpected_nodename\tstring\t\t\nconfig\tgoogle.protobuf.Struct\t\t\ncredential_provider_config\tgoogle.protobuf.Struct\t\t\n\nManifestSpec\n\nManifestSpec holds the Kubernetes resources spec.\n\nField\tType\tLabel\tDescription\nitems\tSingleManifest\trepeated\t\n\nManifestStatusSpec\n\nManifestStatusSpec describes manifest application status.\n\nField\tType\tLabel\tDescription\nmanifests_applied\tstring\trepeated\t\n\nNodeIPConfigSpec\n\nNodeIPConfigSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\nvalid_subnets\tstring\trepeated\t\nexclude_subnets\tstring\trepeated\t\n\nNodeIPSpec\n\nNodeIPSpec holds the Node IP specification.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIP\trepeated\t\n\nNodeLabelSpecSpec\n\nNodeLabelSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec\n\nNodeStatusSpec describes Kubernetes NodeStatus.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nnode_ready\tbool\t\t\nunschedulable\tbool\t\t\nlabels\tNodeStatusSpec.LabelsEntry\trepeated\t\nannotations\tNodeStatusSpec.AnnotationsEntry\trepeated\t\n\nNodeStatusSpec.AnnotationsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeStatusSpec.LabelsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nNodeTaintSpecSpec\n\nNodeTaintSpecSpec represents a label that’s attached to a Talos node.\n\nField\tType\tLabel\tDescription\nkey\tstring\t\t\neffect\tstring\t\t\nvalue\tstring\t\t\n\nNodenameSpec\n\nNodenameSpec describes Kubernetes nodename.\n\nField\tType\tLabel\tDescription\nnodename\tstring\t\t\nhostname_version\tstring\t\t\nskip_node_registration\tbool\t\t\n\nResources\n\nResources is a configuration of cpu and memory resources.\n\nField\tType\tLabel\tDescription\nrequests\tResources.RequestsEntry\trepeated\t\nlimits\tResources.LimitsEntry\trepeated\t\n\nResources.LimitsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nResources.RequestsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec\n\nSchedulerConfigSpec is configuration for kube-scheduler.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nimage\tstring\t\t\nextra_args\tSchedulerConfigSpec.ExtraArgsEntry\trepeated\t\nextra_volumes\tExtraVolume\trepeated\t\nenvironment_variables\tSchedulerConfigSpec.EnvironmentVariablesEntry\trepeated\t\nresources\tResources\t\t\nconfig\tgoogle.protobuf.Struct\t\t\n\nSchedulerConfigSpec.EnvironmentVariablesEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSchedulerConfigSpec.ExtraArgsEntry\nField\tType\tLabel\tDescription\nkey\tstring\t\t\nvalue\tstring\t\t\n\nSecretsStatusSpec\n\nSecretsStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nversion\tstring\t\t\n\nSingleManifest\n\nSingleManifest is a single manifest.\n\nField\tType\tLabel\tDescription\nobject\tgoogle.protobuf.Struct\t\t\n\nStaticPodServerStatusSpec\n\nStaticPodServerStatusSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\nurl\tstring\t\t\n\nStaticPodSpec\n\nStaticPodSpec describes static pod spec, it contains marshaled *v1.Pod spec.\n\nField\tType\tLabel\tDescription\npod\tgoogle.protobuf.Struct\t\t\n\nStaticPodStatusSpec\n\nStaticPodStatusSpec describes kubelet static pod status.\n\nField\tType\tLabel\tDescription\npod_status\tgoogle.protobuf.Struct\t\t\n\nTop\n\nresource/definitions/kubeaccess/kubeaccess.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\nallowed_api_roles\tstring\trepeated\t\nallowed_kubernetes_namespaces\tstring\trepeated\t\n\nTop\n\nresource/definitions/kubespan/kubespan.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\ncluster_id\tstring\t\t\nshared_secret\tstring\t\t\nforce_routing\tbool\t\t\nadvertise_kubernetes_networks\tbool\t\t\nmtu\tuint32\t\t\nendpoint_filters\tstring\trepeated\t\nharvest_extra_endpoints\tbool\t\t\n\nEndpointSpec\n\nEndpointSpec describes Endpoint state.\n\nField\tType\tLabel\tDescription\naffiliate_id\tstring\t\t\nendpoint\tcommon.NetIPPort\t\t\n\nIdentitySpec\n\nIdentitySpec describes KubeSpan keys and address.\n\nNote: IdentitySpec is persisted on disk in the STATE partition, so YAML serialization should be kept backwards compatible.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nsubnet\tcommon.NetIPPrefix\t\t\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\n\nPeerSpecSpec\n\nPeerSpecSpec describes PeerSpec state.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIP\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\nendpoints\tcommon.NetIPPort\trepeated\t\nlabel\tstring\t\t\n\nPeerStatusSpec\n\nPeerStatusSpec describes PeerStatus state.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.NetIPPort\t\t\nlabel\tstring\t\t\nstate\ttalos.resource.definitions.enums.KubespanPeerState\t\t\nreceive_bytes\tint64\t\t\ntransmit_bytes\tint64\t\t\nlast_handshake_time\tgoogle.protobuf.Timestamp\t\t\nlast_used_endpoint\tcommon.NetIPPort\t\t\nlast_endpoint_change\tgoogle.protobuf.Timestamp\t\t\n\nTop\n\nresource/definitions/network/network.proto\n\nAddressSpecSpec\n\nAddressSpecSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\nannounce_with_arp\tbool\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nAddressStatusSpec\n\nAddressStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\naddress\tcommon.NetIPPrefix\t\t\nlocal\tcommon.NetIP\t\t\nbroadcast\tcommon.NetIP\t\t\nanycast\tcommon.NetIP\t\t\nmulticast\tcommon.NetIP\t\t\nlink_index\tuint32\t\t\nlink_name\tstring\t\t\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\nflags\tuint32\t\t\n\nBondMasterSpec\n\nBondMasterSpec describes bond settings if Kind == “bond”.\n\nField\tType\tLabel\tDescription\nmode\ttalos.resource.definitions.enums.NethelpersBondMode\t\t\nhash_policy\ttalos.resource.definitions.enums.NethelpersBondXmitHashPolicy\t\t\nlacp_rate\ttalos.resource.definitions.enums.NethelpersLACPRate\t\t\narp_validate\ttalos.resource.definitions.enums.NethelpersARPValidate\t\t\narp_all_targets\ttalos.resource.definitions.enums.NethelpersARPAllTargets\t\t\nprimary_index\tuint32\t\t\nprimary_reselect\ttalos.resource.definitions.enums.NethelpersPrimaryReselect\t\t\nfail_over_mac\ttalos.resource.definitions.enums.NethelpersFailOverMAC\t\t\nad_select\ttalos.resource.definitions.enums.NethelpersADSelect\t\t\nmii_mon\tuint32\t\t\nup_delay\tuint32\t\t\ndown_delay\tuint32\t\t\narp_interval\tuint32\t\t\nresend_igmp\tuint32\t\t\nmin_links\tuint32\t\t\nlp_interval\tuint32\t\t\npackets_per_slave\tuint32\t\t\nnum_peer_notif\tfixed32\t\t\ntlb_dynamic_lb\tfixed32\t\t\nall_slaves_active\tfixed32\t\t\nuse_carrier\tbool\t\t\nad_actor_sys_prio\tfixed32\t\t\nad_user_port_key\tfixed32\t\t\npeer_notify_delay\tuint32\t\t\n\nBondSlave\n\nBondSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\nslave_index\tint64\t\t\n\nBridgeMasterSpec\n\nBridgeMasterSpec describes bridge settings if Kind == “bridge”.\n\nField\tType\tLabel\tDescription\nstp\tSTPSpec\t\t\n\nBridgeSlave\n\nBridgeSlave contains a bond’s master name and slave index.\n\nField\tType\tLabel\tDescription\nmaster_name\tstring\t\t\n\nDHCP4OperatorSpec\n\nDHCP4OperatorSpec describes DHCP4 operator options.\n\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nDHCP6OperatorSpec\n\nDHCP6OperatorSpec describes DHCP6 operator options.\n\nField\tType\tLabel\tDescription\nduid\tstring\t\t\nroute_metric\tuint32\t\t\nskip_hostname_request\tbool\t\t\n\nHardwareAddrSpec\n\nHardwareAddrSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nhardware_addr\tbytes\t\t\n\nHostnameSpecSpec\n\nHostnameSpecSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nHostnameStatusSpec\n\nHostnameStatusSpec describes node hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ndomainname\tstring\t\t\n\nLinkRefreshSpec\n\nLinkRefreshSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ngeneration\tint64\t\t\n\nLinkSpecSpec\n\nLinkSpecSpec describes spec for the link.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nlogical\tbool\t\t\nup\tbool\t\t\nmtu\tuint32\t\t\nkind\tstring\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nparent_name\tstring\t\t\nbond_slave\tBondSlave\t\t\nbridge_slave\tBridgeSlave\t\t\nvlan\tVLANSpec\t\t\nbond_master\tBondMasterSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nLinkStatusSpec\n\nLinkStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nindex\tuint32\t\t\ntype\ttalos.resource.definitions.enums.NethelpersLinkType\t\t\nlink_index\tuint32\t\t\nflags\tuint32\t\t\nhardware_addr\tbytes\t\t\nbroadcast_addr\tbytes\t\t\nmtu\tuint32\t\t\nqueue_disc\tstring\t\t\nmaster_index\tuint32\t\t\noperational_state\ttalos.resource.definitions.enums.NethelpersOperationalState\t\t\nkind\tstring\t\t\nslave_kind\tstring\t\t\nbus_path\tstring\t\t\npciid\tstring\t\t\ndriver\tstring\t\t\ndriver_version\tstring\t\t\nfirmware_version\tstring\t\t\nproduct_id\tstring\t\t\nvendor_id\tstring\t\t\nproduct\tstring\t\t\nvendor\tstring\t\t\nlink_state\tbool\t\t\nspeed_megabits\tint64\t\t\nport\ttalos.resource.definitions.enums.NethelpersPort\t\t\nduplex\ttalos.resource.definitions.enums.NethelpersDuplex\t\t\nvlan\tVLANSpec\t\t\nbridge_master\tBridgeMasterSpec\t\t\nbond_master\tBondMasterSpec\t\t\nwireguard\tWireguardSpec\t\t\npermanent_addr\tbytes\t\t\n\nNfTablesAddressMatch\n\nNfTablesAddressMatch describes the match on the IP address.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\ninvert\tbool\t\t\n\nNfTablesChainSpec\n\nNfTablesChainSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\ntype\tstring\t\t\nhook\ttalos.resource.definitions.enums.NethelpersNfTablesChainHook\t\t\npriority\ttalos.resource.definitions.enums.NethelpersNfTablesChainPriority\t\t\nrules\tNfTablesRule\trepeated\t\npolicy\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\n\nNfTablesClampMSS\n\nNfTablesClampMSS describes the TCP MSS clamping operation.\n\nMSS is limited by the MaxMTU so that:\n\nIPv4: MSS = MaxMTU - 40\nIPv6: MSS = MaxMTU - 60.\nField\tType\tLabel\tDescription\nmtu\tfixed32\t\t\n\nNfTablesConntrackStateMatch\n\nNfTablesConntrackStateMatch describes the match on the connection tracking state.\n\nField\tType\tLabel\tDescription\nstates\ttalos.resource.definitions.enums.NethelpersConntrackState\trepeated\t\n\nNfTablesIfNameMatch\n\nNfTablesIfNameMatch describes the match on the interface name.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NethelpersMatchOperator\t\t\ninterface_names\tstring\trepeated\t\n\nNfTablesLayer4Match\n\nNfTablesLayer4Match describes the match on the transport layer protocol.\n\nField\tType\tLabel\tDescription\nprotocol\ttalos.resource.definitions.enums.NethelpersProtocol\t\t\nmatch_source_port\tNfTablesPortMatch\t\t\nmatch_destination_port\tNfTablesPortMatch\t\t\n\nNfTablesLimitMatch\n\nNfTablesLimitMatch describes the match on the packet rate.\n\nField\tType\tLabel\tDescription\npacket_rate_per_second\tuint64\t\t\n\nNfTablesMark\n\nNfTablesMark encodes packet mark match/update operation.\n\nWhen used as a match computes the following condition: (mark & mask) ^ xor == value\n\nWhen used as an update computes the following operation: mark = (mark & mask) ^ xor.\n\nField\tType\tLabel\tDescription\nmask\tuint32\t\t\nxor\tuint32\t\t\nvalue\tuint32\t\t\n\nNfTablesPortMatch\n\nNfTablesPortMatch describes the match on the transport layer port.\n\nField\tType\tLabel\tDescription\nranges\tPortRange\trepeated\t\n\nNfTablesRule\n\nNfTablesRule describes a single rule in the nftables chain.\n\nField\tType\tLabel\tDescription\nmatch_o_if_name\tNfTablesIfNameMatch\t\t\nverdict\ttalos.resource.definitions.enums.NethelpersNfTablesVerdict\t\t\nmatch_mark\tNfTablesMark\t\t\nset_mark\tNfTablesMark\t\t\nmatch_source_address\tNfTablesAddressMatch\t\t\nmatch_destination_address\tNfTablesAddressMatch\t\t\nmatch_layer4\tNfTablesLayer4Match\t\t\nmatch_i_if_name\tNfTablesIfNameMatch\t\t\nclamp_mss\tNfTablesClampMSS\t\t\nmatch_limit\tNfTablesLimitMatch\t\t\nmatch_conntrack_state\tNfTablesConntrackStateMatch\t\t\nanon_counter\tbool\t\t\n\nNodeAddressFilterSpec\n\nNodeAddressFilterSpec describes a filter for NodeAddresses.\n\nField\tType\tLabel\tDescription\ninclude_subnets\tcommon.NetIPPrefix\trepeated\t\nexclude_subnets\tcommon.NetIPPrefix\trepeated\t\n\nNodeAddressSpec\n\nNodeAddressSpec describes a set of node addresses.\n\nField\tType\tLabel\tDescription\naddresses\tcommon.NetIPPrefix\trepeated\t\n\nOperatorSpecSpec\n\nOperatorSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\noperator\ttalos.resource.definitions.enums.NetworkOperator\t\t\nlink_name\tstring\t\t\nrequire_up\tbool\t\t\ndhcp4\tDHCP4OperatorSpec\t\t\ndhcp6\tDHCP6OperatorSpec\t\t\nvip\tVIPOperatorSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nPortRange\n\nPortRange describes a range of ports.\n\nRange is [lo, hi].\n\nField\tType\tLabel\tDescription\nlo\tfixed32\t\t\nhi\tfixed32\t\t\n\nProbeSpecSpec\n\nProbeSpecSpec describes the Probe.\n\nField\tType\tLabel\tDescription\ninterval\tgoogle.protobuf.Duration\t\t\nfailure_threshold\tint64\t\t\ntcp\tTCPProbeSpec\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nProbeStatusSpec\n\nProbeStatusSpec describes the Probe.\n\nField\tType\tLabel\tDescription\nsuccess\tbool\t\t\nlast_error\tstring\t\t\n\nResolverSpecSpec\n\nResolverSpecSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nResolverStatusSpec\n\nResolverStatusSpec describes DNS resolvers.\n\nField\tType\tLabel\tDescription\ndns_servers\tcommon.NetIP\trepeated\t\n\nRouteSpecSpec\n\nRouteSpecSpec describes the route.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\nmtu\tuint32\t\t\n\nRouteStatusSpec\n\nRouteStatusSpec describes status of rendered secrets.\n\nField\tType\tLabel\tDescription\nfamily\ttalos.resource.definitions.enums.NethelpersFamily\t\t\ndestination\tcommon.NetIPPrefix\t\t\nsource\tcommon.NetIP\t\t\ngateway\tcommon.NetIP\t\t\nout_link_index\tuint32\t\t\nout_link_name\tstring\t\t\ntable\ttalos.resource.definitions.enums.NethelpersRoutingTable\t\t\npriority\tuint32\t\t\nscope\ttalos.resource.definitions.enums.NethelpersScope\t\t\ntype\ttalos.resource.definitions.enums.NethelpersRouteType\t\t\nflags\tuint32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersRouteProtocol\t\t\nmtu\tuint32\t\t\n\nSTPSpec\n\nSTPSpec describes Spanning Tree Protocol (STP) settings of a bridge.\n\nField\tType\tLabel\tDescription\nenabled\tbool\t\t\n\nStatusSpec\n\nStatusSpec describes network state.\n\nField\tType\tLabel\tDescription\naddress_ready\tbool\t\t\nconnectivity_ready\tbool\t\t\nhostname_ready\tbool\t\t\netc_files_ready\tbool\t\t\n\nTCPProbeSpec\n\nTCPProbeSpec describes the TCP Probe.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\ntimeout\tgoogle.protobuf.Duration\t\t\n\nTimeServerSpecSpec\n\nTimeServerSpecSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\nconfig_layer\ttalos.resource.definitions.enums.NetworkConfigLayer\t\t\n\nTimeServerStatusSpec\n\nTimeServerStatusSpec describes NTP servers.\n\nField\tType\tLabel\tDescription\nntp_servers\tstring\trepeated\t\n\nVIPEquinixMetalSpec\n\nVIPEquinixMetalSpec describes virtual (elastic) IP settings for Equinix Metal.\n\nField\tType\tLabel\tDescription\nproject_id\tstring\t\t\ndevice_id\tstring\t\t\napi_token\tstring\t\t\n\nVIPHCloudSpec\n\nVIPHCloudSpec describes virtual (elastic) IP settings for Hetzner Cloud.\n\nField\tType\tLabel\tDescription\ndevice_id\tint64\t\t\nnetwork_id\tint64\t\t\napi_token\tstring\t\t\n\nVIPOperatorSpec\n\nVIPOperatorSpec describes virtual IP operator options.\n\nField\tType\tLabel\tDescription\nip\tcommon.NetIP\t\t\ngratuitous_arp\tbool\t\t\nequinix_metal\tVIPEquinixMetalSpec\t\t\nh_cloud\tVIPHCloudSpec\t\t\n\nVLANSpec\n\nVLANSpec describes VLAN settings if Kind == “vlan”.\n\nField\tType\tLabel\tDescription\nvid\tfixed32\t\t\nprotocol\ttalos.resource.definitions.enums.NethelpersVLANProtocol\t\t\n\nWireguardPeer\n\nWireguardPeer describes a single peer.\n\nField\tType\tLabel\tDescription\npublic_key\tstring\t\t\npreshared_key\tstring\t\t\nendpoint\tstring\t\t\npersistent_keepalive_interval\tgoogle.protobuf.Duration\t\t\nallowed_ips\tcommon.NetIPPrefix\trepeated\t\n\nWireguardSpec\n\nWireguardSpec describes Wireguard settings if Kind == “wireguard”.\n\nField\tType\tLabel\tDescription\nprivate_key\tstring\t\t\npublic_key\tstring\t\t\nlisten_port\tint64\t\t\nfirewall_mark\tint64\t\t\npeers\tWireguardPeer\trepeated\t\n\nTop\n\nresource/definitions/perf/perf.proto\n\nCPUSpec\n\nCPUSpec represents the last CPU stats snapshot.\n\nField\tType\tLabel\tDescription\ncpu\tCPUStat\trepeated\t\ncpu_total\tCPUStat\t\t\nirq_total\tuint64\t\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\n\nCPUStat\n\nCPUStat represents a single cpu stat.\n\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nMemorySpec\n\nMemorySpec represents the last Memory stats snapshot.\n\nField\tType\tLabel\tDescription\nmem_total\tuint64\t\t\nmem_used\tuint64\t\t\nmem_available\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswap_cached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactive_anon\tuint64\t\t\ninactive_anon\tuint64\t\t\nactive_file\tuint64\t\t\ninactive_file\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswap_total\tuint64\t\t\nswap_free\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanon_pages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\ns_reclaimable\tuint64\t\t\ns_unreclaim\tuint64\t\t\nkernel_stack\tuint64\t\t\npage_tables\tuint64\t\t\nnf_sunstable\tuint64\t\t\nbounce\tuint64\t\t\nwriteback_tmp\tuint64\t\t\ncommit_limit\tuint64\t\t\ncommitted_as\tuint64\t\t\nvmalloc_total\tuint64\t\t\nvmalloc_used\tuint64\t\t\nvmalloc_chunk\tuint64\t\t\nhardware_corrupted\tuint64\t\t\nanon_huge_pages\tuint64\t\t\nshmem_huge_pages\tuint64\t\t\nshmem_pmd_mapped\tuint64\t\t\ncma_total\tuint64\t\t\ncma_free\tuint64\t\t\nhuge_pages_total\tuint64\t\t\nhuge_pages_free\tuint64\t\t\nhuge_pages_rsvd\tuint64\t\t\nhuge_pages_surp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirect_map4k\tuint64\t\t\ndirect_map2m\tuint64\t\t\ndirect_map1g\tuint64\t\t\n\nTop\n\nresource/definitions/proto/proto.proto\n\nLinuxIDMapping\n\nLinuxIDMapping specifies UID/GID mappings.\n\nField\tType\tLabel\tDescription\ncontainer_id\tuint32\t\t\nhost_id\tuint32\t\t\nsize\tuint32\t\t\n\nMount\n\nMount specifies a mount for a container.\n\nField\tType\tLabel\tDescription\ndestination\tstring\t\t\ntype\tstring\t\t\nsource\tstring\t\t\noptions\tstring\trepeated\t\nuid_mappings\tLinuxIDMapping\trepeated\t\ngid_mappings\tLinuxIDMapping\trepeated\t\n\nTop\n\nresource/definitions/runtime/runtime.proto\n\nDevicesStatusSpec\n\nDevicesStatusSpec is the spec for devices status.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\n\nEventSinkConfigSpec\n\nEventSinkConfigSpec describes configuration of Talos event log streaming.\n\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nKernelModuleSpecSpec\n\nKernelModuleSpecSpec describes Linux kernel module to load.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nparameters\tstring\trepeated\t\n\nKernelParamSpecSpec\n\nKernelParamSpecSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\nignore_errors\tbool\t\t\n\nKernelParamStatusSpec\n\nKernelParamStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\ncurrent\tstring\t\t\ndefault\tstring\t\t\nunsupported\tbool\t\t\n\nKmsgLogConfigSpec\n\nKmsgLogConfigSpec describes configuration for kmsg log streaming.\n\nField\tType\tLabel\tDescription\ndestinations\tcommon.URL\trepeated\t\n\nMachineStatusSpec\n\nMachineStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nstage\ttalos.resource.definitions.enums.RuntimeMachineStage\t\t\nstatus\tMachineStatusStatus\t\t\n\nMachineStatusStatus\n\nMachineStatusStatus describes machine current status at the stage.\n\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tUnmetCondition\trepeated\t\n\nMaintenanceServiceConfigSpec\n\nMaintenanceServiceConfigSpec describes configuration for maintenance service API.\n\nField\tType\tLabel\tDescription\nlisten_address\tstring\t\t\nreachable_addresses\tcommon.NetIP\trepeated\t\n\nMetaKeySpec\n\nMetaKeySpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nvalue\tstring\t\t\n\nMetaLoadedSpec\n\nMetaLoadedSpec is the spec for meta loaded. The Done field is always true when resource exists.\n\nField\tType\tLabel\tDescription\ndone\tbool\t\t\n\nMountStatusSpec\n\nMountStatusSpec describes status of the defined sysctls.\n\nField\tType\tLabel\tDescription\nsource\tstring\t\t\ntarget\tstring\t\t\nfilesystem_type\tstring\t\t\noptions\tstring\trepeated\t\nencrypted\tbool\t\t\nencryption_providers\tstring\trepeated\t\n\nPlatformMetadataSpec\n\nPlatformMetadataSpec describes platform metadata properties.\n\nField\tType\tLabel\tDescription\nplatform\tstring\t\t\nhostname\tstring\t\t\nregion\tstring\t\t\nzone\tstring\t\t\ninstance_type\tstring\t\t\ninstance_id\tstring\t\t\nprovider_id\tstring\t\t\nspot\tbool\t\t\n\nSecurityStateSpec\n\nSecurityStateSpec describes the security state resource properties.\n\nField\tType\tLabel\tDescription\nsecure_boot\tbool\t\t\nuki_signing_key_fingerprint\tstring\t\t\npcr_signing_key_fingerprint\tstring\t\t\n\nUniqueMachineTokenSpec\n\nUniqueMachineTokenSpec is the spec for the machine unique token. Token can be empty if machine wasn’t assigned any.\n\nField\tType\tLabel\tDescription\ntoken\tstring\t\t\n\nUnmetCondition\n\nUnmetCondition is a failure which prevents machine from being ready at the stage.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nTop\n\nresource/definitions/secrets/secrets.proto\n\nAPICertsSpec\n\nAPICertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nclient\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nCertSANSpec\n\nCertSANSpec describes fields of the cert SANs.\n\nField\tType\tLabel\tDescription\ni_ps\tcommon.NetIP\trepeated\t\ndns_names\tstring\trepeated\t\nfqdn\tstring\t\t\n\nEtcdCertsSpec\n\nEtcdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\netcd\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_peer\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_admin\tcommon.PEMEncodedCertificateAndKey\t\t\netcd_api_server\tcommon.PEMEncodedCertificateAndKey\t\t\n\nEtcdRootSpec\n\nEtcdRootSpec describes etcd CA secrets.\n\nField\tType\tLabel\tDescription\netcd_ca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubeletSpec\n\nKubeletSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nendpoint\tcommon.URL\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\n\nKubernetesCertsSpec\n\nKubernetesCertsSpec describes generated Kubernetes certificates.\n\nField\tType\tLabel\tDescription\nscheduler_kubeconfig\tstring\t\t\ncontroller_manager_kubeconfig\tstring\t\t\nlocalhost_admin_kubeconfig\tstring\t\t\nadmin_kubeconfig\tstring\t\t\n\nKubernetesDynamicCertsSpec\n\nKubernetesDynamicCertsSpec describes generated KubernetesCerts certificates.\n\nField\tType\tLabel\tDescription\napi_server\tcommon.PEMEncodedCertificateAndKey\t\t\napi_server_kubelet_client\tcommon.PEMEncodedCertificateAndKey\t\t\nfront_proxy\tcommon.PEMEncodedCertificateAndKey\t\t\n\nKubernetesRootSpec\n\nKubernetesRootSpec describes root Kubernetes secrets.\n\nField\tType\tLabel\tDescription\nname\tstring\t\t\nendpoint\tcommon.URL\t\t\nlocal_endpoint\tcommon.URL\t\t\ncert_sa_ns\tstring\trepeated\t\ndns_domain\tstring\t\t\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nservice_account\tcommon.PEMEncodedKey\t\t\naggregator_ca\tcommon.PEMEncodedCertificateAndKey\t\t\naescbc_encryption_secret\tstring\t\t\nbootstrap_token_id\tstring\t\t\nbootstrap_token_secret\tstring\t\t\nsecretbox_encryption_secret\tstring\t\t\napi_server_ips\tcommon.NetIP\trepeated\t\n\nMaintenanceRootSpec\n\nMaintenanceRootSpec describes maintenance service CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\n\nMaintenanceServiceCertsSpec\n\nMaintenanceServiceCertsSpec describes maintenance service certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nOSRootSpec\n\nOSRootSpec describes operating system CA.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\ncert_sani_ps\tcommon.NetIP\trepeated\t\ncert_sandns_names\tstring\trepeated\t\ntoken\tstring\t\t\n\nTrustdCertsSpec\n\nTrustdCertsSpec describes etcd certs secrets.\n\nField\tType\tLabel\tDescription\nca\tcommon.PEMEncodedCertificateAndKey\t\t\nserver\tcommon.PEMEncodedCertificateAndKey\t\t\n\nTop\n\nresource/definitions/siderolink/siderolink.proto\n\nConfigSpec\n\nConfigSpec describes KubeSpan configuration..\n\nField\tType\tLabel\tDescription\napi_endpoint\tstring\t\t\n\nTop\n\nresource/definitions/time/time.proto\n\nAdjtimeStatusSpec\n\nAdjtimeStatusSpec describes Linux internal adjtime state.\n\nField\tType\tLabel\tDescription\noffset\tgoogle.protobuf.Duration\t\t\nfrequency_adjustment_ratio\tdouble\t\t\nmax_error\tgoogle.protobuf.Duration\t\t\nest_error\tgoogle.protobuf.Duration\t\t\nstatus\tstring\t\t\nconstant\tint64\t\t\nsync_status\tbool\t\t\nstate\tstring\t\t\n\nStatusSpec\n\nStatusSpec describes time sync state.\n\nField\tType\tLabel\tDescription\nsynced\tbool\t\t\nepoch\tint64\t\t\nsync_disabled\tbool\t\t\n\nTop\n\nresource/definitions/v1alpha1/v1alpha1.proto\n\nServiceSpec\n\nServiceSpec describe service state.\n\nField\tType\tLabel\tDescription\nrunning\tbool\t\t\nhealthy\tbool\t\t\nunknown\tbool\t\t\n\nTop\n\ninspect/inspect.proto\n\nControllerDependencyEdge\nField\tType\tLabel\tDescription\ncontroller_name\tstring\t\t\nedge_type\tDependencyEdgeType\t\t\nresource_namespace\tstring\t\t\nresource_type\tstring\t\t\nresource_id\tstring\t\t\n\nControllerRuntimeDependenciesResponse\nField\tType\tLabel\tDescription\nmessages\tControllerRuntimeDependency\trepeated\t\n\nControllerRuntimeDependency\n\nThe ControllerRuntimeDependency message contains the graph of controller-resource dependencies.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nedges\tControllerDependencyEdge\trepeated\t\n\nDependencyEdgeType\nName\tNumber\tDescription\nOUTPUT_EXCLUSIVE\t0\t\nOUTPUT_SHARED\t3\t\nINPUT_STRONG\t1\t\nINPUT_WEAK\t2\t\nINPUT_DESTROY_READY\t4\t\n\nInspectService\n\nThe inspect service definition.\n\nInspectService provides auxiliary API to inspect OS internals.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nControllerRuntimeDependencies\t.google.protobuf.Empty\tControllerRuntimeDependenciesResponse\t\n\nTop\n\nmachine/machine.proto\n\nAddressEvent\n\nAddressEvent reports node endpoints aggregated from k8s.Endpoints and network.Hostname.\n\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\naddresses\tstring\trepeated\t\n\nApplyConfiguration\n\nApplyConfigurationResponse describes the response to a configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nwarnings\tstring\trepeated\tConfiguration validation warnings.\nmode\tApplyConfigurationRequest.Mode\t\tStates which mode was actually chosen.\nmode_details\tstring\t\tHuman-readable message explaining the result of the apply configuration call.\n\nApplyConfigurationRequest\n\nrpc applyConfiguration ApplyConfiguration describes a request to assert a new configuration upon a node.\n\nField\tType\tLabel\tDescription\ndata\tbytes\t\t\nmode\tApplyConfigurationRequest.Mode\t\t\ndry_run\tbool\t\t\ntry_mode_timeout\tgoogle.protobuf.Duration\t\t\n\nApplyConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tApplyConfiguration\trepeated\t\n\nBPFInstruction\nField\tType\tLabel\tDescription\nop\tuint32\t\t\njt\tuint32\t\t\njf\tuint32\t\t\nk\tuint32\t\t\n\nBootstrap\n\nThe bootstrap message containing the bootstrap status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nBootstrapRequest\n\nrpc Bootstrap\n\nField\tType\tLabel\tDescription\nrecover_etcd\tbool\t\tEnable etcd recovery from the snapshot. Snapshot should be uploaded before this call via EtcdRecover RPC.\nrecover_skip_hash_check\tbool\t\tSkip hash check on the snapshot (etcd). Enable this when recovering from data directory copy to skip integrity check.\n\nBootstrapResponse\nField\tType\tLabel\tDescription\nmessages\tBootstrap\trepeated\t\n\nCNIConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\nurls\tstring\trepeated\t\n\nCPUInfo\nField\tType\tLabel\tDescription\nprocessor\tuint32\t\t\nvendor_id\tstring\t\t\ncpu_family\tstring\t\t\nmodel\tstring\t\t\nmodel_name\tstring\t\t\nstepping\tstring\t\t\nmicrocode\tstring\t\t\ncpu_mhz\tdouble\t\t\ncache_size\tstring\t\t\nphysical_id\tstring\t\t\nsiblings\tuint32\t\t\ncore_id\tstring\t\t\ncpu_cores\tuint32\t\t\napic_id\tstring\t\t\ninitial_apic_id\tstring\t\t\nfpu\tstring\t\t\nfpu_exception\tstring\t\t\ncpu_id_level\tuint32\t\t\nwp\tstring\t\t\nflags\tstring\trepeated\t\nbugs\tstring\trepeated\t\nbogo_mips\tdouble\t\t\ncl_flush_size\tuint32\t\t\ncache_alignment\tuint32\t\t\naddress_sizes\tstring\t\t\npower_management\tstring\t\t\n\nCPUInfoResponse\nField\tType\tLabel\tDescription\nmessages\tCPUsInfo\trepeated\t\n\nCPUStat\nField\tType\tLabel\tDescription\nuser\tdouble\t\t\nnice\tdouble\t\t\nsystem\tdouble\t\t\nidle\tdouble\t\t\niowait\tdouble\t\t\nirq\tdouble\t\t\nsoft_irq\tdouble\t\t\nsteal\tdouble\t\t\nguest\tdouble\t\t\nguest_nice\tdouble\t\t\n\nCPUsInfo\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncpu_info\tCPUInfo\trepeated\t\n\nClusterConfig\nField\tType\tLabel\tDescription\nname\tstring\t\t\ncontrol_plane\tControlPlaneConfig\t\t\ncluster_network\tClusterNetworkConfig\t\t\nallow_scheduling_on_control_planes\tbool\t\t\n\nClusterNetworkConfig\nField\tType\tLabel\tDescription\ndns_domain\tstring\t\t\ncni_config\tCNIConfig\t\t\n\nConfigLoadErrorEvent\n\nConfigLoadErrorEvent is reported when the config loading has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConfigValidationErrorEvent\n\nConfigValidationErrorEvent is reported when config validation has failed.\n\nField\tType\tLabel\tDescription\nerror\tstring\t\t\n\nConnectRecord\nField\tType\tLabel\tDescription\nl4proto\tstring\t\t\nlocalip\tstring\t\t\nlocalport\tuint32\t\t\nremoteip\tstring\t\t\nremoteport\tuint32\t\t\nstate\tConnectRecord.State\t\t\ntxqueue\tuint64\t\t\nrxqueue\tuint64\t\t\ntr\tConnectRecord.TimerActive\t\t\ntimerwhen\tuint64\t\t\nretrnsmt\tuint64\t\t\nuid\tuint32\t\t\ntimeout\tuint64\t\t\ninode\tuint64\t\t\nref\tuint64\t\t\npointer\tuint64\t\t\nprocess\tConnectRecord.Process\t\t\nnetns\tstring\t\t\n\nConnectRecord.Process\nField\tType\tLabel\tDescription\npid\tuint32\t\t\nname\tstring\t\t\n\nContainer\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ncontainers\tContainerInfo\trepeated\t\n\nContainerInfo\n\nThe messages message containing the requested containers.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nimage\tstring\t\t\npid\tuint32\t\t\nstatus\tstring\t\t\npod_id\tstring\t\t\nname\tstring\t\t\nnetwork_namespace\tstring\t\t\n\nContainersRequest\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nContainersResponse\nField\tType\tLabel\tDescription\nmessages\tContainer\trepeated\t\n\nControlPlaneConfig\nField\tType\tLabel\tDescription\nendpoint\tstring\t\t\n\nCopyRequest\n\nCopyRequest describes a request to copy data out of Talos node\n\nCopy produces .tar.gz archive which is streamed back to the caller\n\nField\tType\tLabel\tDescription\nroot_path\tstring\t\tRoot path to start copying data out, it might be either a file or directory\n\nDHCPOptionsConfig\nField\tType\tLabel\tDescription\nroute_metric\tuint32\t\t\n\nDiskStat\nField\tType\tLabel\tDescription\nname\tstring\t\t\nread_completed\tuint64\t\t\nread_merged\tuint64\t\t\nread_sectors\tuint64\t\t\nread_time_ms\tuint64\t\t\nwrite_completed\tuint64\t\t\nwrite_merged\tuint64\t\t\nwrite_sectors\tuint64\t\t\nwrite_time_ms\tuint64\t\t\nio_in_progress\tuint64\t\t\nio_time_ms\tuint64\t\t\nio_time_weighted_ms\tuint64\t\t\ndiscard_completed\tuint64\t\t\ndiscard_merged\tuint64\t\t\ndiscard_sectors\tuint64\t\t\ndiscard_time_ms\tuint64\t\t\n\nDiskStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tDiskStat\t\t\ndevices\tDiskStat\trepeated\t\n\nDiskStatsResponse\nField\tType\tLabel\tDescription\nmessages\tDiskStats\trepeated\t\n\nDiskUsageInfo\n\nDiskUsageInfo describes a file or directory’s information for du command\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\n\nDiskUsageRequest\n\nDiskUsageRequest describes a request to list disk usage of directories and regular files\n\nField\tType\tLabel\tDescription\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\nall\tbool\t\tAll write sizes for all files, not just directories.\nthreshold\tint64\t\tThreshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative.\npaths\tstring\trepeated\tDiskUsagePaths is the list of directories to calculate disk usage for.\n\nDmesgRequest\n\ndmesg\n\nField\tType\tLabel\tDescription\nfollow\tbool\t\t\ntail\tbool\t\t\n\nEtcdAlarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarm\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_alarms\tEtcdMemberAlarm\trepeated\t\n\nEtcdAlarmDisarmResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarmDisarm\trepeated\t\n\nEtcdAlarmListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdAlarm\trepeated\t\n\nEtcdDefragment\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdDefragmentResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdDefragment\trepeated\t\n\nEtcdForfeitLeadership\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember\tstring\t\t\n\nEtcdForfeitLeadershipRequest\n\nEtcdForfeitLeadershipResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdForfeitLeadership\trepeated\t\n\nEtcdLeaveCluster\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdLeaveClusterRequest\n\nEtcdLeaveClusterResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdLeaveCluster\trepeated\t\n\nEtcdMember\n\nEtcdMember describes a single etcd member.\n\nField\tType\tLabel\tDescription\nid\tuint64\t\tmember ID.\nhostname\tstring\t\thuman-readable name of the member.\npeer_urls\tstring\trepeated\tthe list of URLs the member exposes to clients for communication.\nclient_urls\tstring\trepeated\tthe list of URLs the member exposes to the cluster for communication.\nis_learner\tbool\t\tlearner flag\n\nEtcdMemberAlarm\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nalarm\tEtcdMemberAlarm.AlarmType\t\t\n\nEtcdMemberListRequest\nField\tType\tLabel\tDescription\nquery_local\tbool\t\t\n\nEtcdMemberListResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdMembers\trepeated\t\n\nEtcdMemberStatus\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\nprotocol_version\tstring\t\t\ndb_size\tint64\t\t\ndb_size_in_use\tint64\t\t\nleader\tuint64\t\t\nraft_index\tuint64\t\t\nraft_term\tuint64\t\t\nraft_applied_index\tuint64\t\t\nerrors\tstring\trepeated\t\nis_learner\tbool\t\t\n\nEtcdMembers\n\nEtcdMembers contains the list of members registered on the host.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nlegacy_members\tstring\trepeated\tlist of member hostnames.\nmembers\tEtcdMember\trepeated\tthe list of etcd members registered on the node.\n\nEtcdRecover\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRecoverResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRecover\trepeated\t\n\nEtcdRemoveMember\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByID\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nEtcdRemoveMemberByIDRequest\nField\tType\tLabel\tDescription\nmember_id\tuint64\t\t\n\nEtcdRemoveMemberByIDResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMemberByID\trepeated\t\n\nEtcdRemoveMemberRequest\nField\tType\tLabel\tDescription\nmember\tstring\t\t\n\nEtcdRemoveMemberResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdRemoveMember\trepeated\t\n\nEtcdSnapshotRequest\n\nEtcdStatus\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmember_status\tEtcdMemberStatus\t\t\n\nEtcdStatusResponse\nField\tType\tLabel\tDescription\nmessages\tEtcdStatus\trepeated\t\n\nEvent\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tgoogle.protobuf.Any\t\t\nid\tstring\t\t\nactor_id\tstring\t\t\n\nEventsRequest\nField\tType\tLabel\tDescription\ntail_events\tint32\t\t\ntail_id\tstring\t\t\ntail_seconds\tint32\t\t\nwith_actor_id\tstring\t\t\n\nFeaturesInfo\n\nFeaturesInfo describes individual Talos features that can be switched on or off.\n\nField\tType\tLabel\tDescription\nrbac\tbool\t\tRBAC is true if role-based access control is enabled.\n\nFileInfo\n\nFileInfo describes a file or directory’s information\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\tName is the name (including prefixed path) of the file or directory\nsize\tint64\t\tSize indicates the number of bytes contained within the file\nmode\tuint32\t\tMode is the bitmap of UNIX mode/permission flags of the file\nmodified\tint64\t\tModified indicates the UNIX timestamp at which the file was last modified\nis_dir\tbool\t\tIsDir indicates that the file is a directory\nerror\tstring\t\tError describes any error encountered while trying to read the file information.\nlink\tstring\t\tLink is filled with symlink target\nrelative_name\tstring\t\tRelativeName is the name of the file or directory relative to the RootPath\nuid\tuint32\t\tOwner uid\ngid\tuint32\t\tOwner gid\n\nGenerateClientConfiguration\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nca\tbytes\t\tPEM-encoded CA certificate.\ncrt\tbytes\t\tPEM-encoded generated client certificate.\nkey\tbytes\t\tPEM-encoded generated client key.\ntalosconfig\tbytes\t\tClient configuration (talosconfig) file content.\n\nGenerateClientConfigurationRequest\nField\tType\tLabel\tDescription\nroles\tstring\trepeated\tRoles in the generated client certificate.\ncrt_ttl\tgoogle.protobuf.Duration\t\tClient certificate TTL.\n\nGenerateClientConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateClientConfiguration\trepeated\t\n\nGenerateConfiguration\n\nGenerateConfiguration describes the response to a generate configuration request.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndata\tbytes\trepeated\t\ntalosconfig\tbytes\t\t\n\nGenerateConfigurationRequest\n\nGenerateConfigurationRequest describes a request to generate a new configuration on a node.\n\nField\tType\tLabel\tDescription\nconfig_version\tstring\t\t\ncluster_config\tClusterConfig\t\t\nmachine_config\tMachineConfig\t\t\noverride_time\tgoogle.protobuf.Timestamp\t\t\n\nGenerateConfigurationResponse\nField\tType\tLabel\tDescription\nmessages\tGenerateConfiguration\trepeated\t\n\nHostname\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nhostname\tstring\t\t\n\nHostnameResponse\nField\tType\tLabel\tDescription\nmessages\tHostname\trepeated\t\n\nImageListRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\n\nImageListResponse\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nname\tstring\t\t\ndigest\tstring\t\t\nsize\tint64\t\t\ncreated_at\tgoogle.protobuf.Timestamp\t\t\n\nImagePull\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nImagePullRequest\nField\tType\tLabel\tDescription\nnamespace\tcommon.ContainerdNamespace\t\tContainerd namespace to use.\nreference\tstring\t\tImage reference to pull.\n\nImagePullResponse\nField\tType\tLabel\tDescription\nmessages\tImagePull\trepeated\t\n\nInstallConfig\nField\tType\tLabel\tDescription\ninstall_disk\tstring\t\t\ninstall_image\tstring\t\t\n\nListRequest\n\nListRequest describes a request to list the contents of a directory.\n\nField\tType\tLabel\tDescription\nroot\tstring\t\tRoot indicates the root directory for the list. If not indicated, ‘/’ is presumed.\nrecurse\tbool\t\tRecurse indicates that subdirectories should be recursed.\nrecursion_depth\tint32\t\tRecursionDepth indicates how many levels of subdirectories should be recursed. The default (0) indicates that no limit should be enforced.\ntypes\tListRequest.Type\trepeated\tTypes indicates what file type should be returned. If not indicated, all files will be returned.\n\nLoadAvg\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nload1\tdouble\t\t\nload5\tdouble\t\t\nload15\tdouble\t\t\n\nLoadAvgResponse\nField\tType\tLabel\tDescription\nmessages\tLoadAvg\trepeated\t\n\nLogsRequest\n\nrpc logs The request message containing the process name.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\nfollow\tbool\t\t\ntail_lines\tint32\t\t\n\nMachineConfig\nField\tType\tLabel\tDescription\ntype\tMachineConfig.MachineType\t\t\ninstall_config\tInstallConfig\t\t\nnetwork_config\tNetworkConfig\t\t\nkubernetes_version\tstring\t\t\n\nMachineStatusEvent\n\nMachineStatusEvent reports changes to the MachineStatus resource.\n\nField\tType\tLabel\tDescription\nstage\tMachineStatusEvent.MachineStage\t\t\nstatus\tMachineStatusEvent.MachineStatus\t\t\n\nMachineStatusEvent.MachineStatus\nField\tType\tLabel\tDescription\nready\tbool\t\t\nunmet_conditions\tMachineStatusEvent.MachineStatus.UnmetCondition\trepeated\t\n\nMachineStatusEvent.MachineStatus.UnmetCondition\nField\tType\tLabel\tDescription\nname\tstring\t\t\nreason\tstring\t\t\n\nMemInfo\nField\tType\tLabel\tDescription\nmemtotal\tuint64\t\t\nmemfree\tuint64\t\t\nmemavailable\tuint64\t\t\nbuffers\tuint64\t\t\ncached\tuint64\t\t\nswapcached\tuint64\t\t\nactive\tuint64\t\t\ninactive\tuint64\t\t\nactiveanon\tuint64\t\t\ninactiveanon\tuint64\t\t\nactivefile\tuint64\t\t\ninactivefile\tuint64\t\t\nunevictable\tuint64\t\t\nmlocked\tuint64\t\t\nswaptotal\tuint64\t\t\nswapfree\tuint64\t\t\ndirty\tuint64\t\t\nwriteback\tuint64\t\t\nanonpages\tuint64\t\t\nmapped\tuint64\t\t\nshmem\tuint64\t\t\nslab\tuint64\t\t\nsreclaimable\tuint64\t\t\nsunreclaim\tuint64\t\t\nkernelstack\tuint64\t\t\npagetables\tuint64\t\t\nnfsunstable\tuint64\t\t\nbounce\tuint64\t\t\nwritebacktmp\tuint64\t\t\ncommitlimit\tuint64\t\t\ncommittedas\tuint64\t\t\nvmalloctotal\tuint64\t\t\nvmallocused\tuint64\t\t\nvmallocchunk\tuint64\t\t\nhardwarecorrupted\tuint64\t\t\nanonhugepages\tuint64\t\t\nshmemhugepages\tuint64\t\t\nshmempmdmapped\tuint64\t\t\ncmatotal\tuint64\t\t\ncmafree\tuint64\t\t\nhugepagestotal\tuint64\t\t\nhugepagesfree\tuint64\t\t\nhugepagesrsvd\tuint64\t\t\nhugepagessurp\tuint64\t\t\nhugepagesize\tuint64\t\t\ndirectmap4k\tuint64\t\t\ndirectmap2m\tuint64\t\t\ndirectmap1g\tuint64\t\t\n\nMemory\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nmeminfo\tMemInfo\t\t\n\nMemoryResponse\nField\tType\tLabel\tDescription\nmessages\tMemory\trepeated\t\n\nMetaDelete\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaDeleteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\n\nMetaDeleteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaDelete\trepeated\t\n\nMetaWrite\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nMetaWriteRequest\nField\tType\tLabel\tDescription\nkey\tuint32\t\t\nvalue\tbytes\t\t\n\nMetaWriteResponse\nField\tType\tLabel\tDescription\nmessages\tMetaWrite\trepeated\t\n\nMountStat\n\nThe messages message containing the requested processes.\n\nField\tType\tLabel\tDescription\nfilesystem\tstring\t\t\nsize\tuint64\t\t\navailable\tuint64\t\t\nmounted_on\tstring\t\t\n\nMounts\n\nThe messages message containing the requested df stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tMountStat\trepeated\t\n\nMountsResponse\nField\tType\tLabel\tDescription\nmessages\tMounts\trepeated\t\n\nNetDev\nField\tType\tLabel\tDescription\nname\tstring\t\t\nrx_bytes\tuint64\t\t\nrx_packets\tuint64\t\t\nrx_errors\tuint64\t\t\nrx_dropped\tuint64\t\t\nrx_fifo\tuint64\t\t\nrx_frame\tuint64\t\t\nrx_compressed\tuint64\t\t\nrx_multicast\tuint64\t\t\ntx_bytes\tuint64\t\t\ntx_packets\tuint64\t\t\ntx_errors\tuint64\t\t\ntx_dropped\tuint64\t\t\ntx_fifo\tuint64\t\t\ntx_collisions\tuint64\t\t\ntx_carrier\tuint64\t\t\ntx_compressed\tuint64\t\t\n\nNetstat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nconnectrecord\tConnectRecord\trepeated\t\n\nNetstatRequest\nField\tType\tLabel\tDescription\nfilter\tNetstatRequest.Filter\t\t\nfeature\tNetstatRequest.Feature\t\t\nl4proto\tNetstatRequest.L4proto\t\t\nnetns\tNetstatRequest.NetNS\t\t\n\nNetstatRequest.Feature\nField\tType\tLabel\tDescription\npid\tbool\t\t\n\nNetstatRequest.L4proto\nField\tType\tLabel\tDescription\ntcp\tbool\t\t\ntcp6\tbool\t\t\nudp\tbool\t\t\nudp6\tbool\t\t\nudplite\tbool\t\t\nudplite6\tbool\t\t\nraw\tbool\t\t\nraw6\tbool\t\t\n\nNetstatRequest.NetNS\nField\tType\tLabel\tDescription\nhostnetwork\tbool\t\t\nnetns\tstring\trepeated\t\nallnetns\tbool\t\t\n\nNetstatResponse\nField\tType\tLabel\tDescription\nmessages\tNetstat\trepeated\t\n\nNetworkConfig\nField\tType\tLabel\tDescription\nhostname\tstring\t\t\ninterfaces\tNetworkDeviceConfig\trepeated\t\n\nNetworkDeviceConfig\nField\tType\tLabel\tDescription\ninterface\tstring\t\t\ncidr\tstring\t\t\nmtu\tint32\t\t\ndhcp\tbool\t\t\nignore\tbool\t\t\ndhcp_options\tDHCPOptionsConfig\t\t\nroutes\tRouteConfig\trepeated\t\n\nNetworkDeviceStats\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ntotal\tNetDev\t\t\ndevices\tNetDev\trepeated\t\n\nNetworkDeviceStatsResponse\nField\tType\tLabel\tDescription\nmessages\tNetworkDeviceStats\trepeated\t\n\nPacketCaptureRequest\nField\tType\tLabel\tDescription\ninterface\tstring\t\tInterface name to perform packet capture on.\npromiscuous\tbool\t\tEnable promiscuous mode.\nsnap_len\tuint32\t\tSnap length in bytes.\nbpf_filter\tBPFInstruction\trepeated\tBPF filter.\n\nPhaseEvent\nField\tType\tLabel\tDescription\nphase\tstring\t\t\naction\tPhaseEvent.Action\t\t\n\nPlatformInfo\nField\tType\tLabel\tDescription\nname\tstring\t\t\nmode\tstring\t\t\n\nProcess\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nprocesses\tProcessInfo\trepeated\t\n\nProcessInfo\nField\tType\tLabel\tDescription\npid\tint32\t\t\nppid\tint32\t\t\nstate\tstring\t\t\nthreads\tint32\t\t\ncpu_time\tdouble\t\t\nvirtual_memory\tuint64\t\t\nresident_memory\tuint64\t\t\ncommand\tstring\t\t\nexecutable\tstring\t\t\nargs\tstring\t\t\n\nProcessesResponse\n\nrpc processes\n\nField\tType\tLabel\tDescription\nmessages\tProcess\trepeated\t\n\nReadRequest\nField\tType\tLabel\tDescription\npath\tstring\t\t\n\nReboot\n\nThe reboot message containing the reboot status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nRebootRequest\n\nrpc reboot\n\nField\tType\tLabel\tDescription\nmode\tRebootRequest.Mode\t\t\n\nRebootResponse\nField\tType\tLabel\tDescription\nmessages\tReboot\trepeated\t\n\nReset\n\nThe reset message containing the restart status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nResetPartitionSpec\n\nrpc reset\n\nField\tType\tLabel\tDescription\nlabel\tstring\t\t\nwipe\tbool\t\t\n\nResetRequest\nField\tType\tLabel\tDescription\ngraceful\tbool\t\tGraceful indicates whether node should leave etcd before the upgrade, it also enforces etcd checks before leaving.\nreboot\tbool\t\tReboot indicates whether node should reboot or halt after resetting.\nsystem_partitions_to_wipe\tResetPartitionSpec\trepeated\tSystem_partitions_to_wipe lists specific system disk partitions to be reset (wiped). If system_partitions_to_wipe is empty, all the partitions are erased.\nuser_disks_to_wipe\tstring\trepeated\tUserDisksToWipe lists specific connected block devices to be reset (wiped).\nmode\tResetRequest.WipeMode\t\tWipeMode defines which devices should be wiped.\n\nResetResponse\nField\tType\tLabel\tDescription\nmessages\tReset\trepeated\t\n\nRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRestartEvent\nField\tType\tLabel\tDescription\ncmd\tint64\t\t\n\nRestartRequest\n\nrpc restart The request message containing the process to restart.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nRestartResponse\n\nThe messages message containing the restart status.\n\nField\tType\tLabel\tDescription\nmessages\tRestart\trepeated\t\n\nRollback\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\n\nRollbackRequest\n\nrpc rollback\n\nRollbackResponse\nField\tType\tLabel\tDescription\nmessages\tRollback\trepeated\t\n\nRouteConfig\nField\tType\tLabel\tDescription\nnetwork\tstring\t\t\ngateway\tstring\t\t\nmetric\tuint32\t\t\n\nSequenceEvent\n\nrpc events\n\nField\tType\tLabel\tDescription\nsequence\tstring\t\t\naction\tSequenceEvent.Action\t\t\nerror\tcommon.Error\t\t\n\nServiceEvent\nField\tType\tLabel\tDescription\nmsg\tstring\t\t\nstate\tstring\t\t\nts\tgoogle.protobuf.Timestamp\t\t\n\nServiceEvents\nField\tType\tLabel\tDescription\nevents\tServiceEvent\trepeated\t\n\nServiceHealth\nField\tType\tLabel\tDescription\nunknown\tbool\t\t\nhealthy\tbool\t\t\nlast_message\tstring\t\t\nlast_change\tgoogle.protobuf.Timestamp\t\t\n\nServiceInfo\nField\tType\tLabel\tDescription\nid\tstring\t\t\nstate\tstring\t\t\nevents\tServiceEvents\t\t\nhealth\tServiceHealth\t\t\n\nServiceList\n\nrpc servicelist\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nservices\tServiceInfo\trepeated\t\n\nServiceListResponse\nField\tType\tLabel\tDescription\nmessages\tServiceList\trepeated\t\n\nServiceRestart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceRestartRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceRestartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceRestart\trepeated\t\n\nServiceStart\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStartRequest\n\nrpc servicestart\n\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStartResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStart\trepeated\t\n\nServiceStateEvent\nField\tType\tLabel\tDescription\nservice\tstring\t\t\naction\tServiceStateEvent.Action\t\t\nmessage\tstring\t\t\nhealth\tServiceHealth\t\t\n\nServiceStop\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nresp\tstring\t\t\n\nServiceStopRequest\nField\tType\tLabel\tDescription\nid\tstring\t\t\n\nServiceStopResponse\nField\tType\tLabel\tDescription\nmessages\tServiceStop\trepeated\t\n\nShutdown\n\nrpc shutdown The messages message containing the shutdown status.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nactor_id\tstring\t\t\n\nShutdownRequest\nField\tType\tLabel\tDescription\nforce\tbool\t\tForce indicates whether node should shutdown without first cordening and draining\n\nShutdownResponse\nField\tType\tLabel\tDescription\nmessages\tShutdown\trepeated\t\n\nSoftIRQStat\nField\tType\tLabel\tDescription\nhi\tuint64\t\t\ntimer\tuint64\t\t\nnet_tx\tuint64\t\t\nnet_rx\tuint64\t\t\nblock\tuint64\t\t\nblock_io_poll\tuint64\t\t\ntasklet\tuint64\t\t\nsched\tuint64\t\t\nhrtimer\tuint64\t\t\nrcu\tuint64\t\t\n\nStat\n\nThe messages message containing the requested stat.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\nid\tstring\t\t\nmemory_usage\tuint64\t\t\ncpu_usage\tuint64\t\t\npod_id\tstring\t\t\nname\tstring\t\t\n\nStats\n\nThe messages message containing the requested stats.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nstats\tStat\trepeated\t\n\nStatsRequest\n\nThe request message containing the containerd namespace.\n\nField\tType\tLabel\tDescription\nnamespace\tstring\t\t\ndriver\tcommon.ContainerDriver\t\tdriver might be default “containerd” or “cri”\n\nStatsResponse\nField\tType\tLabel\tDescription\nmessages\tStats\trepeated\t\n\nSystemStat\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nboot_time\tuint64\t\t\ncpu_total\tCPUStat\t\t\ncpu\tCPUStat\trepeated\t\nirq_total\tuint64\t\t\nirq\tuint64\trepeated\t\ncontext_switches\tuint64\t\t\nprocess_created\tuint64\t\t\nprocess_running\tuint64\t\t\nprocess_blocked\tuint64\t\t\nsoft_irq_total\tuint64\t\t\nsoft_irq\tSoftIRQStat\t\t\n\nSystemStatResponse\nField\tType\tLabel\tDescription\nmessages\tSystemStat\trepeated\t\n\nTaskEvent\nField\tType\tLabel\tDescription\ntask\tstring\t\t\naction\tTaskEvent.Action\t\t\n\nUpgrade\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nack\tstring\t\t\nactor_id\tstring\t\t\n\nUpgradeRequest\n\nrpc upgrade\n\nField\tType\tLabel\tDescription\nimage\tstring\t\t\npreserve\tbool\t\t\nstage\tbool\t\t\nforce\tbool\t\t\nreboot_mode\tUpgradeRequest.RebootMode\t\t\n\nUpgradeResponse\nField\tType\tLabel\tDescription\nmessages\tUpgrade\trepeated\t\n\nVersion\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nversion\tVersionInfo\t\t\nplatform\tPlatformInfo\t\t\nfeatures\tFeaturesInfo\t\tFeatures describe individual Talos features that can be switched on or off.\n\nVersionInfo\nField\tType\tLabel\tDescription\ntag\tstring\t\t\nsha\tstring\t\t\nbuilt\tstring\t\t\ngo_version\tstring\t\t\nos\tstring\t\t\narch\tstring\t\t\n\nVersionResponse\nField\tType\tLabel\tDescription\nmessages\tVersion\trepeated\t\n\nApplyConfigurationRequest.Mode\nName\tNumber\tDescription\nREBOOT\t0\t\nAUTO\t1\t\nNO_REBOOT\t2\t\nSTAGED\t3\t\nTRY\t4\t\n\nConnectRecord.State\nName\tNumber\tDescription\nRESERVED\t0\t\nESTABLISHED\t1\t\nSYN_SENT\t2\t\nSYN_RECV\t3\t\nFIN_WAIT1\t4\t\nFIN_WAIT2\t5\t\nTIME_WAIT\t6\t\nCLOSE\t7\t\nCLOSEWAIT\t8\t\nLASTACK\t9\t\nLISTEN\t10\t\nCLOSING\t11\t\n\nConnectRecord.TimerActive\nName\tNumber\tDescription\nOFF\t0\t\nON\t1\t\nKEEPALIVE\t2\t\nTIMEWAIT\t3\t\nPROBE\t4\t\n\nEtcdMemberAlarm.AlarmType\nName\tNumber\tDescription\nNONE\t0\t\nNOSPACE\t1\t\nCORRUPT\t2\t\n\nListRequest.Type\n\nFile type.\n\nName\tNumber\tDescription\nREGULAR\t0\tRegular file (not directory, symlink, etc).\nDIRECTORY\t1\tDirectory.\nSYMLINK\t2\tSymbolic link.\n\nMachineConfig.MachineType\nName\tNumber\tDescription\nTYPE_UNKNOWN\t0\t\nTYPE_INIT\t1\t\nTYPE_CONTROL_PLANE\t2\t\nTYPE_WORKER\t3\t\n\nMachineStatusEvent.MachineStage\nName\tNumber\tDescription\nUNKNOWN\t0\t\nBOOTING\t1\t\nINSTALLING\t2\t\nMAINTENANCE\t3\t\nRUNNING\t4\t\nREBOOTING\t5\t\nSHUTTING_DOWN\t6\t\nRESETTING\t7\t\nUPGRADING\t8\t\n\nNetstatRequest.Filter\nName\tNumber\tDescription\nALL\t0\t\nCONNECTED\t1\t\nLISTENING\t2\t\n\nPhaseEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nRebootRequest.Mode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nResetRequest.WipeMode\nName\tNumber\tDescription\nALL\t0\t\nSYSTEM_DISK\t1\t\nUSER_DISKS\t2\t\n\nSequenceEvent.Action\nName\tNumber\tDescription\nNOOP\t0\t\nSTART\t1\t\nSTOP\t2\t\n\nServiceStateEvent.Action\nName\tNumber\tDescription\nINITIALIZED\t0\t\nPREPARING\t1\t\nWAITING\t2\t\nRUNNING\t3\t\nSTOPPING\t4\t\nFINISHED\t5\t\nFAILED\t6\t\nSKIPPED\t7\t\n\nTaskEvent.Action\nName\tNumber\tDescription\nSTART\t0\t\nSTOP\t1\t\n\nUpgradeRequest.RebootMode\nName\tNumber\tDescription\nDEFAULT\t0\t\nPOWERCYCLE\t1\t\n\nMachineService\n\nThe machine service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nApplyConfiguration\tApplyConfigurationRequest\tApplyConfigurationResponse\t\nBootstrap\tBootstrapRequest\tBootstrapResponse\tBootstrap method makes control plane node enter etcd bootstrap mode. Node aborts etcd join sequence and creates single-node etcd cluster. If recover_etcd argument is specified, etcd is recovered from a snapshot uploaded with EtcdRecover.\nContainers\tContainersRequest\tContainersResponse\t\nCopy\tCopyRequest\t.common.Data stream\t\nCPUInfo\t.google.protobuf.Empty\tCPUInfoResponse\t\nDiskStats\t.google.protobuf.Empty\tDiskStatsResponse\t\nDmesg\tDmesgRequest\t.common.Data stream\t\nEvents\tEventsRequest\tEvent stream\t\nEtcdMemberList\tEtcdMemberListRequest\tEtcdMemberListResponse\t\nEtcdRemoveMember\tEtcdRemoveMemberRequest\tEtcdRemoveMemberResponse\tEtcdRemoveMember removes a member from the etcd cluster by hostname. Please use EtcdRemoveMemberByID instead.\nEtcdRemoveMemberByID\tEtcdRemoveMemberByIDRequest\tEtcdRemoveMemberByIDResponse\tEtcdRemoveMemberByID removes a member from the etcd cluster identified by member ID. This API should be used to remove members which don’t have an associated Talos node anymore. To remove a member with a running Talos node, use EtcdLeaveCluster API on the node to be removed.\nEtcdLeaveCluster\tEtcdLeaveClusterRequest\tEtcdLeaveClusterResponse\t\nEtcdForfeitLeadership\tEtcdForfeitLeadershipRequest\tEtcdForfeitLeadershipResponse\t\nEtcdRecover\t.common.Data stream\tEtcdRecoverResponse\tEtcdRecover method uploads etcd data snapshot created with EtcdSnapshot to the node. Snapshot can be later used to recover the cluster via Bootstrap method.\nEtcdSnapshot\tEtcdSnapshotRequest\t.common.Data stream\tEtcdSnapshot method creates etcd data snapshot (backup) from the local etcd instance and streams it back to the client. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmList\t.google.protobuf.Empty\tEtcdAlarmListResponse\tEtcdAlarmList lists etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdAlarmDisarm\t.google.protobuf.Empty\tEtcdAlarmDisarmResponse\tEtcdAlarmDisarm disarms etcd alarms for the current node. This method is available only on control plane nodes (which run etcd).\nEtcdDefragment\t.google.protobuf.Empty\tEtcdDefragmentResponse\tEtcdDefragment defragments etcd data directory for the current node. Defragmentation is a resource-heavy operation, so it should only run on a specific node. This method is available only on control plane nodes (which run etcd).\nEtcdStatus\t.google.protobuf.Empty\tEtcdStatusResponse\tEtcdStatus returns etcd status for the current member. This method is available only on control plane nodes (which run etcd).\nGenerateConfiguration\tGenerateConfigurationRequest\tGenerateConfigurationResponse\t\nHostname\t.google.protobuf.Empty\tHostnameResponse\t\nKubeconfig\t.google.protobuf.Empty\t.common.Data stream\t\nList\tListRequest\tFileInfo stream\t\nDiskUsage\tDiskUsageRequest\tDiskUsageInfo stream\t\nLoadAvg\t.google.protobuf.Empty\tLoadAvgResponse\t\nLogs\tLogsRequest\t.common.Data stream\t\nMemory\t.google.protobuf.Empty\tMemoryResponse\t\nMounts\t.google.protobuf.Empty\tMountsResponse\t\nNetworkDeviceStats\t.google.protobuf.Empty\tNetworkDeviceStatsResponse\t\nProcesses\t.google.protobuf.Empty\tProcessesResponse\t\nRead\tReadRequest\t.common.Data stream\t\nReboot\tRebootRequest\tRebootResponse\t\nRestart\tRestartRequest\tRestartResponse\t\nRollback\tRollbackRequest\tRollbackResponse\t\nReset\tResetRequest\tResetResponse\t\nServiceList\t.google.protobuf.Empty\tServiceListResponse\t\nServiceRestart\tServiceRestartRequest\tServiceRestartResponse\t\nServiceStart\tServiceStartRequest\tServiceStartResponse\t\nServiceStop\tServiceStopRequest\tServiceStopResponse\t\nShutdown\tShutdownRequest\tShutdownResponse\t\nStats\tStatsRequest\tStatsResponse\t\nSystemStat\t.google.protobuf.Empty\tSystemStatResponse\t\nUpgrade\tUpgradeRequest\tUpgradeResponse\t\nVersion\t.google.protobuf.Empty\tVersionResponse\t\nGenerateClientConfiguration\tGenerateClientConfigurationRequest\tGenerateClientConfigurationResponse\tGenerateClientConfiguration generates talosctl client configuration (talosconfig).\nPacketCapture\tPacketCaptureRequest\t.common.Data stream\tPacketCapture performs packet capture and streams back pcap file.\nNetstat\tNetstatRequest\tNetstatResponse\tNetstat provides information about network connections.\nMetaWrite\tMetaWriteRequest\tMetaWriteResponse\tMetaWrite writes a META key-value pair.\nMetaDelete\tMetaDeleteRequest\tMetaDeleteResponse\tMetaDelete deletes a META key.\nImageList\tImageListRequest\tImageListResponse stream\tImageList lists images in the CRI.\nImagePull\tImagePullRequest\tImagePullResponse\tImagePull pulls an image into the CRI.\n\nTop\n\nsecurity/security.proto\n\nCertificateRequest\n\nThe request message containing the certificate signing request.\n\nField\tType\tLabel\tDescription\ncsr\tbytes\t\tCertificate Signing Request in PEM format.\n\nCertificateResponse\n\nThe response message containing signed certificate.\n\nField\tType\tLabel\tDescription\nca\tbytes\t\tCertificate of the CA that signed the requested certificate in PEM format.\ncrt\tbytes\t\tSigned X.509 requested certificate in PEM format.\n\nSecurityService\n\nThe security service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nCertificate\tCertificateRequest\tCertificateResponse\t\n\nTop\n\nstorage/storage.proto\n\nDisk\n\nDisk represents a disk.\n\nField\tType\tLabel\tDescription\nsize\tuint64\t\tSize indicates the disk size in bytes.\nmodel\tstring\t\tModel idicates the disk model.\ndevice_name\tstring\t\tDeviceName indicates the disk name (e.g. sda).\nname\tstring\t\tName as in /sys/block/<dev>/device/name.\nserial\tstring\t\tSerial as in /sys/block/<dev>/device/serial.\nmodalias\tstring\t\tModalias as in /sys/block/<dev>/device/modalias.\nuuid\tstring\t\tUuid as in /sys/block/<dev>/device/uuid.\nwwid\tstring\t\tWwid as in /sys/block/<dev>/device/wwid.\ntype\tDisk.DiskType\t\tType is a type of the disk: nvme, ssd, hdd, sd card.\nbus_path\tstring\t\tBusPath is the bus path of the disk.\nsystem_disk\tbool\t\tSystemDisk indicates that the disk is used as Talos system disk.\nsubsystem\tstring\t\tSubsystem is the symlink path in the /sys/block/<dev>/subsystem.\nreadonly\tbool\t\tReadonly specifies if the disk is read only.\n\nDisks\n\nDisksResponse represents the response of the Disks RPC.\n\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\ndisks\tDisk\trepeated\t\n\nDisksResponse\nField\tType\tLabel\tDescription\nmessages\tDisks\trepeated\t\n\nDisk.DiskType\nName\tNumber\tDescription\nUNKNOWN\t0\t\nSSD\t1\t\nHDD\t2\t\nNVME\t3\t\nSD\t4\t\n\nStorageService\n\nStorageService represents the storage service.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nDisks\t.google.protobuf.Empty\tDisksResponse\t\n\nTop\n\ntime/time.proto\n\nTime\nField\tType\tLabel\tDescription\nmetadata\tcommon.Metadata\t\t\nserver\tstring\t\t\nlocaltime\tgoogle.protobuf.Timestamp\t\t\nremotetime\tgoogle.protobuf.Timestamp\t\t\n\nTimeRequest\n\nThe response message containing the ntp server\n\nField\tType\tLabel\tDescription\nserver\tstring\t\t\n\nTimeResponse\n\nThe response message containing the ntp server, time, and offset\n\nField\tType\tLabel\tDescription\nmessages\tTime\trepeated\t\n\nTimeService\n\nThe time service definition.\n\nMethod Name\tRequest Type\tResponse Type\tDescription\nTime\t.google.protobuf.Empty\tTimeResponse\t\nTimeCheck\tTimeRequest\tTimeResponse\t\nScalar Value Types\n.proto Type\tNotes\tC++\tJava\tPython\tGo\tC#\tPHP\tRuby\ndouble\t\tdouble\tdouble\tfloat\tfloat64\tdouble\tfloat\tFloat\nfloat\t\tfloat\tfloat\tfloat\tfloat32\tfloat\tfloat\tFloat\nint32\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint32 instead.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nint64\tUses variable-length encoding. Inefficient for encoding negative numbers – if your field is likely to have negative values, use sint64 instead.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nuint32\tUses variable-length encoding.\tuint32\tint\tint/long\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nuint64\tUses variable-length encoding.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum or Fixnum (as required)\nsint32\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsint64\tUses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nfixed32\tAlways four bytes. More efficient than uint32 if values are often greater than 2^28.\tuint32\tint\tint\tuint32\tuint\tinteger\tBignum or Fixnum (as required)\nfixed64\tAlways eight bytes. More efficient than uint64 if values are often greater than 2^56.\tuint64\tlong\tint/long\tuint64\tulong\tinteger/string\tBignum\nsfixed32\tAlways four bytes.\tint32\tint\tint\tint32\tint\tinteger\tBignum or Fixnum (as required)\nsfixed64\tAlways eight bytes.\tint64\tlong\tint/long\tint64\tlong\tinteger/string\tBignum\nbool\t\tbool\tboolean\tboolean\tbool\tbool\tboolean\tTrueClass/FalseClass\nstring\tA string must always contain UTF-8 encoded or 7-bit ASCII text.\tstring\tString\tstr/unicode\tstring\tstring\tstring\tString (UTF-8)\nbytes\tMay contain any arbitrary sequence of bytes.\tstring\tByteString\tstr\t[]byte\tByteString\tstring\tString (ASCII-8BIT)\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Reference | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/",
    "html": "Documentation\nReference\nReference\nAPI\n\nTalos gRPC API reference.\n\nCLI\n\nTalosctl CLI tool reference.\n\nConfiguration\n\nTalos Linux machine configuration reference.\n\nKernel\n\nLinux kernel reference.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "CLI | Talos Linux",
    "url": "https://www.talos.dev/v1.6/reference/cli/",
    "html": "talosctl apply-config\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl bootstrap\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl cluster create\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl cluster destroy\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl cluster show\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl cluster\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl completion\nSynopsis\nExamples\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config add\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config context\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config contexts\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config endpoint\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config info\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config merge\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config new\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config node\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config remove\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl config\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl conformance kubernetes\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl conformance\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl containers\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl copy\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl dashboard\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl disks\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl dmesg\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl edit\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd alarm disarm\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd alarm list\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd alarm\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd defrag\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd forfeit-leadership\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd leave\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd members\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd remove-member\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd snapshot\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd status\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl etcd\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl events\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen ca\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen config\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen crt\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen csr\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen key\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen keypair\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen secrets\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen secureboot database\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen secureboot pcr\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen secureboot uki\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen secureboot\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl gen\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl get\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl health\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl image default\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl image list\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl image pull\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl image\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl inject serviceaccount\nExamples\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl inject\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl inspect dependencies\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl inspect\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl kubeconfig\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl list\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl logs\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl machineconfig gen\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl machineconfig patch\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl machineconfig\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl memory\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl meta delete\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl meta write\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl meta\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl mounts\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl netstat\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl patch\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl pcap\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl processes\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl read\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl reboot\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl reset\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl restart\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl rollback\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl service\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl shutdown\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl stats\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl support\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl time\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl upgrade\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl upgrade-k8s\nSynopsis\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl usage\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl validate\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl version\nOptions\nOptions inherited from parent commands\nSEE ALSO\ntalosctl\nOptions\nSEE ALSO\nDocumentation\nReference\nCLI\nCLI\nTalosctl CLI tool reference.\ntalosctl apply-config\n\nApply a new configuration to a node\n\ntalosctl apply-config [flags]\n\nOptions\n      --cert-fingerprint strings                                 list of server certificate fingeprints to accept (defaults to no check)\n  -p, --config-patch strings                                     the list of config patches to apply to the local config file before sending it to the node\n      --dry-run                                                  check how the config change will be applied in dry-run mode\n  -f, --file string                                              the filename of the updated configuration\n  -h, --help                                                     help for apply-config\n  -i, --insecure                                                 apply the config using the insecure (encrypted with no auth) maintenance service\n  -m, --mode auto, interactive, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --timeout duration                                         the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl bootstrap\n\nBootstrap the etcd cluster on the specified node.\n\nSynopsis\n\nWhen Talos cluster is created etcd service on control plane nodes enter the join loop waiting to join etcd peers from other control plane nodes. One node should be picked as the boostrap node. When boostrap command is issued, the node aborts join process and bootstraps etcd cluster as a single node cluster. Other control plane nodes will join etcd cluster once Kubernetes is boostrapped on the bootstrap node.\n\nThis command should not be used when “init” type node are used.\n\nTalos etcd cluster can be recovered from a known snapshot with ‘–recover-from=’ flag.\n\ntalosctl bootstrap [flags]\n\nOptions\n  -h, --help                      help for bootstrap\n      --recover-from string       recover etcd cluster from the snapshot\n      --recover-skip-hash-check   skip integrity check when recovering etcd (use when recovering from data directory copy)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create\n\nCreates a local docker-based or QEMU-based kubernetes cluster\n\ntalosctl cluster create [flags]\n\nOptions\n      --arch string                              cluster architecture (default \"amd64\")\n      --bad-rtc                                  launch VM with bad RTC state (QEMU only)\n      --cidr string                              CIDR of the cluster network (IPv4, ULA network for IPv6 is derived in automated way) (default \"10.5.0.0/24\")\n      --cni-bin-path strings                     search path for CNI binaries (VM only) (default [/home/user/.talos/cni/bin])\n      --cni-bundle-url string                    URL to download CNI bundle from (VM only) (default \"https://github.com/siderolabs/talos/releases/download/v1.6.0-alpha.2/talosctl-cni-bundle-${ARCH}.tar.gz\")\n      --cni-cache-dir string                     CNI cache directory path (VM only) (default \"/home/user/.talos/cni/cache\")\n      --cni-conf-dir string                      CNI config directory path (VM only) (default \"/home/user/.talos/cni/conf.d\")\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --control-plane-port int                   control plane port (load balancer and local API port) (default 6443)\n      --controlplanes int                        the number of controlplanes to create (default 1)\n      --cpus string                              the share of CPUs as fraction (each control plane/VM) (default \"2.0\")\n      --cpus-workers string                      the share of CPUs as fraction (each worker/VM) (default \"2.0\")\n      --crashdump                                print debug crashdump to stderr when cluster startup fails\n      --custom-cni-url string                    install custom CNI from the URL (Talos cluster)\n      --disable-dhcp-hostname                    skip announcing hostname via DHCP (QEMU only)\n      --disk int                                 default limit on disk size in MB (each VM) (default 6144)\n      --disk-encryption-key-types stringArray    encryption key types to use for disk encryption (uuid, kms) (default [uuid])\n      --disk-image-path string                   disk image to use\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n      --docker-disable-ipv6                      skip enabling IPv6 in containers (Docker only)\n      --docker-host-ip string                    Host IP to forward exposed ports to (Docker provisioner only) (default \"0.0.0.0\")\n      --encrypt-ephemeral                        enable ephemeral partition encryption\n      --encrypt-state                            enable state partition encryption\n      --endpoint string                          use endpoint instead of provider defaults\n  -p, --exposed-ports string                     Comma-separated list of ports/protocols to expose on init node. Ex -p <hostPort>:<containerPort>/<protocol (tcp or udp)> (Docker provisioner only)\n      --extra-boot-kernel-args string            add extra kernel args to the initial boot from vmlinuz and initramfs (QEMU only)\n      --extra-disks int                          number of extra disks to create for each worker VM\n      --extra-disks-size int                     default limit on disk size in MB (each VM) (default 5120)\n      --extra-uefi-search-paths strings          additional search paths for UEFI firmware (only applies when UEFI is enabled)\n  -h, --help                                     help for create\n      --image string                             the image to use (default \"ghcr.io/siderolabs/talos:latest\")\n      --init-node-as-endpoint                    use init node as endpoint instead of any load balancer endpoint\n      --initrd-path string                       initramfs image to use (default \"_out/initramfs-${ARCH}.xz\")\n  -i, --input-dir string                         location of pre-generated config files\n      --install-image string                     the installer image to use (default \"ghcr.io/siderolabs/installer:latest\")\n      --ipv4                                     enable IPv4 network in the cluster (default true)\n      --ipv6                                     enable IPv6 network in the cluster (QEMU provisioner only)\n      --ipxe-boot-script string                  iPXE boot script (URL) to use\n      --iso-path string                          the ISO path to use for the initial boot (VM only)\n      --kubeprism-port int                       KubePrism port (set to 0 to disable) (default 7445)\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n      --memory int                               the limit on memory usage in MB (each control plane/VM) (default 2048)\n      --memory-workers int                       the limit on memory usage in MB (each worker/VM) (default 2048)\n      --mtu int                                  MTU of the cluster network (default 1500)\n      --nameservers strings                      list of nameservers to use (default [8.8.8.8,1.1.1.1,2001:4860:4860::8888,2606:4700:4700::1111])\n      --registry-insecure-skip-verify strings    list of registry hostnames to skip TLS verification for\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --skip-boot-phase-finished-check           skip waiting for node to finish boot phase\n      --skip-injecting-config                    skip injecting config from embedded metadata server, write config files to current directory\n      --skip-kubeconfig                          skip merging kubeconfig from the created cluster\n      --talos-version string                     the desired Talos version to generate config for (if not set, defaults to image version)\n      --talosconfig string                       The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n      --use-vip                                  use a virtual IP for the controlplane endpoint instead of the loadbalancer\n      --user-disk strings                        list of disks to create for each VM in format: <mount_point1>:<size1>:<mount_point2>:<size2>\n      --vmlinuz-path string                      the compressed kernel image to use (default \"_out/vmlinuz-${ARCH}\")\n      --wait                                     wait for the cluster to be ready before returning (default true)\n      --wait-timeout duration                    timeout to wait for the cluster to be ready (default 20m0s)\n      --wireguard-cidr string                    CIDR of the wireguard network\n      --with-apply-config                        enable apply config when the VM is starting in maintenance mode\n      --with-bootloader                          enable bootloader to load kernel and initramfs from disk image after install (default true)\n      --with-cluster-discovery                   enable cluster discovery (default true)\n      --with-debug                               enable debug in Talos config to send service logs to the console\n      --with-firewall string                     inject firewall rules into the cluster, value is default policy - accept/block (QEMU only)\n      --with-init-node                           create the cluster with an init node\n      --with-kubespan                            enable KubeSpan system\n      --with-network-bandwidth int               specify bandwidth restriction (in kbps) on the bridge interface when creating a qemu cluster\n      --with-network-chaos                       enable to use network chaos parameters when creating a qemu cluster\n      --with-network-jitter duration             specify jitter on the bridge interface when creating a qemu cluster\n      --with-network-latency duration            specify latency on the bridge interface when creating a qemu cluster\n      --with-network-packet-corrupt float        specify percent of corrupt packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-loss float           specify percent of packet loss on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-network-packet-reorder float        specify percent of reordered packets on the bridge interface when creating a qemu cluster. e.g. 50% = 0.50 (default: 0.0)\n      --with-tpm2                                enable TPM2 emulation support using swtpm\n      --with-uefi                                enable UEFI on x86_64 architecture (default true)\n      --workers int                              the number of workers to create (default 1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster destroy\n\nDestroys a local docker-based or firecracker-based kubernetes cluster\n\ntalosctl cluster destroy [flags]\n\nOptions\n  -f, --force   force deletion of cluster directory if there were errors\n  -h, --help    help for destroy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster show\n\nShows info about a local provisioned kubernetes cluster\n\ntalosctl cluster show [flags]\n\nOptions\n  -h, --help   help for show\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --name string          the name of the cluster (default \"talos-default\")\n  -n, --nodes strings        target the specified nodes\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl cluster\n\nA collection of commands for managing local docker-based or QEMU-based clusters\n\nOptions\n  -h, --help                 help for cluster\n      --name string          the name of the cluster (default \"talos-default\")\n      --provisioner string   Talos cluster provisioner to use (default \"docker\")\n      --state string         directory path to store cluster state (default \"/home/user/.talos/clusters\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl cluster create - Creates a local docker-based or QEMU-based kubernetes cluster\ntalosctl cluster destroy - Destroys a local docker-based or firecracker-based kubernetes cluster\ntalosctl cluster show - Shows info about a local provisioned kubernetes cluster\ntalosctl completion\n\nOutput shell completion code for the specified shell (bash, fish or zsh)\n\nSynopsis\n\nOutput shell completion code for the specified shell (bash, fish or zsh). The shell code must be evaluated to provide interactive completion of talosctl commands. This can be done by sourcing it from the .bash_profile.\n\nNote for zsh users: [1] zsh completions are only supported in versions of zsh >= 5.2\n\ntalosctl completion SHELL [flags]\n\nExamples\n# Installing bash completion on macOS using homebrew\n## If running Bash 3.2 included with macOS\n\tbrew install bash-completion\n## or, if running Bash 4.1+\n\tbrew install bash-completion@2\n## If talosctl is installed via homebrew, this should start working immediately.\n## If you've installed via other means, you may need add the completion to your completion directory\n\ttalosctl completion bash > $(brew --prefix)/etc/bash_completion.d/talosctl\n\n# Installing bash completion on Linux\n## If bash-completion is not installed on Linux, please install the 'bash-completion' package\n## via your distribution's package manager.\n## Load the talosctl completion code for bash into the current shell\n\tsource <(talosctl completion bash)\n## Write bash completion code to a file and source if from .bash_profile\n\ttalosctl completion bash > ~/.talos/completion.bash.inc\n\tprintf \"\n\t\t# talosctl shell completion\n\t\tsource '$HOME/.talos/completion.bash.inc'\n\t\t\" >> $HOME/.bash_profile\n\tsource $HOME/.bash_profile\n# Load the talosctl completion code for fish[1] into the current shell\n\ttalosctl completion fish | source\n# Set the talosctl completion code for fish[1] to autoload on startup\n    talosctl completion fish > ~/.config/fish/completions/talosctl.fish\n# Load the talosctl completion code for zsh[1] into the current shell\n\tsource <(talosctl completion zsh)\n# Set the talosctl completion code for zsh[1] to autoload on startup\n    talosctl completion zsh > \"${fpath[1]}/_talosctl\"\n\nOptions\n  -h, --help   help for completion\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add\n\nAdd a new context\n\ntalosctl config add <context> [flags]\n\nOptions\n      --ca string    the path to the CA certificate\n      --crt string   the path to the certificate\n  -h, --help         help for add\n      --key string   the path to the key\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config context\n\nSet the current context\n\ntalosctl config context <context> [flags]\n\nOptions\n  -h, --help   help for context\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config contexts\n\nList defined contexts\n\ntalosctl config contexts [flags]\n\nOptions\n  -h, --help   help for contexts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config endpoint\n\nSet the endpoint(s) for the current context\n\ntalosctl config endpoint <endpoint>... [flags]\n\nOptions\n  -h, --help   help for endpoint\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config info\n\nShow information about the current context\n\ntalosctl config info [flags]\n\nOptions\n  -h, --help            help for info\n  -o, --output string   output format (json|yaml|text). Default text. (default \"text\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config merge\n\nMerge additional contexts from another client configuration file\n\nSynopsis\n\nContexts with the same name are renamed while merging configs.\n\ntalosctl config merge <from> [flags]\n\nOptions\n  -h, --help   help for merge\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config new\n\nGenerate a new client configuration file\n\ntalosctl config new [<path>] [flags]\n\nOptions\n      --crt-ttl duration   certificate TTL (default 87600h0m0s)\n  -h, --help               help for new\n      --roles strings      roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config node\n\nSet the node(s) for the current context\n\ntalosctl config node <endpoint>... [flags]\n\nOptions\n  -h, --help   help for node\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config remove\n\nRemove contexts\n\ntalosctl config remove <context> [flags]\n\nOptions\n      --dry-run     dry run\n  -h, --help        help for remove\n  -y, --noconfirm   do not ask for confirmation\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl config\n\nManage the client configuration file (talosconfig)\n\nOptions\n  -h, --help   help for config\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl config add - Add a new context\ntalosctl config context - Set the current context\ntalosctl config contexts - List defined contexts\ntalosctl config endpoint - Set the endpoint(s) for the current context\ntalosctl config info - Show information about the current context\ntalosctl config merge - Merge additional contexts from another client configuration file\ntalosctl config new - Generate a new client configuration file\ntalosctl config node - Set the node(s) for the current context\ntalosctl config remove - Remove contexts\ntalosctl conformance kubernetes\n\nRun Kubernetes conformance tests\n\ntalosctl conformance kubernetes [flags]\n\nOptions\n  -h, --help          help for kubernetes\n      --mode string   conformance test mode: [fast, certified] (default \"fast\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl conformance - Run conformance tests\ntalosctl conformance\n\nRun conformance tests\n\nOptions\n  -h, --help   help for conformance\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl conformance kubernetes - Run Kubernetes conformance tests\ntalosctl containers\n\nList containers\n\ntalosctl containers [flags]\n\nOptions\n  -h, --help         help for containers\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl copy\n\nCopy data out from the node\n\nSynopsis\n\nCreates an .tar.gz archive at the node starting at and streams it back to the client.\n\nIf ‘-’ is given for , archive is written to stdout. Otherwise archive is extracted to which should be an empty directory or talosctl creates a directory if doesn’t exist. Command doesn’t preserve ownership and access mode for the files in extract mode, while streamed .tar archive captures ownership and permission bits.\n\ntalosctl copy <src-path> -|<local-path> [flags]\n\nOptions\n  -h, --help   help for copy\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dashboard\n\nCluster dashboard with node overview, logs and real-time metrics\n\nSynopsis\n\nProvide a text-based UI to navigate node overview, logs and real-time metrics.\n\nKeyboard shortcuts:\n\nh, <Left> - switch one node to the left\nl, <Right> - switch one node to the right\nj, <Down> - scroll logs/process list down\nk, <Up> - scroll logs/process list up\n<C-d> - scroll logs/process list half page down\n<C-u> - scroll logs/process list half page up\n<C-f> - scroll logs/process list one page down\n<C-b> - scroll logs/process list one page up\ntalosctl dashboard [flags]\n\nOptions\n  -h, --help                       help for dashboard\n  -d, --update-interval duration   interval between updates (default 3s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl disks\n\nGet the list of disks from /sys/block on the machine\n\ntalosctl disks [flags]\n\nOptions\n  -h, --help       help for disks\n  -i, --insecure   get disks using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl dmesg\n\nRetrieve kernel logs\n\ntalosctl dmesg [flags]\n\nOptions\n  -f, --follow   specify if the kernel log should be streamed\n  -h, --help     help for dmesg\n      --tail     specify if only new messages should be sent (makes sense only when combined with --follow)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl edit\n\nEdit a resource from the default editor.\n\nSynopsis\n\nThe edit command allows you to directly edit any API resource you can retrieve via the command line tools.\n\nIt will open the editor defined by your TALOS_EDITOR, or EDITOR environment variables, or fall back to ‘vi’ for Linux or ’notepad’ for Windows.\n\ntalosctl edit <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     do not apply the change after editing and print the change summary instead\n  -h, --help                                        help for edit\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm disarm\n\nDisarm the etcd alarms for the node.\n\ntalosctl etcd alarm disarm [flags]\n\nOptions\n  -h, --help   help for disarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm list\n\nList the etcd alarms for the node.\n\ntalosctl etcd alarm list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd alarm\n\nManage etcd alarms\n\nOptions\n  -h, --help   help for alarm\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd alarm disarm - Disarm the etcd alarms for the node.\ntalosctl etcd alarm list - List the etcd alarms for the node.\ntalosctl etcd defrag\n\nDefragment etcd database on the node\n\nSynopsis\n\nDefragmentation is a maintenance operation that releases unused space from the etcd database file. Defragmentation is a resource heavy operation and should be performed only when necessary on a single node at a time.\n\ntalosctl etcd defrag [flags]\n\nOptions\n  -h, --help   help for defrag\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd forfeit-leadership\n\nTell node to forfeit etcd cluster leadership\n\ntalosctl etcd forfeit-leadership [flags]\n\nOptions\n  -h, --help   help for forfeit-leadership\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd leave\n\nTell nodes to leave etcd cluster\n\ntalosctl etcd leave [flags]\n\nOptions\n  -h, --help   help for leave\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd members\n\nGet the list of etcd cluster members\n\ntalosctl etcd members [flags]\n\nOptions\n  -h, --help   help for members\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd remove-member\n\nRemove the node from etcd cluster\n\nSynopsis\n\nUse this command only if you want to remove a member which is in broken state. If there is no access to the node, or the node can’t access etcd to call etcd leave. Always prefer etcd leave over this command. It’s always better to use member ID than hostname, as hostname might not be set consistently.\n\ntalosctl etcd remove-member <member ID>|<hostname> [flags]\n\nOptions\n  -h, --help   help for remove-member\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd snapshot\n\nStream snapshot of the etcd node to the path.\n\ntalosctl etcd snapshot <path> [flags]\n\nOptions\n  -h, --help   help for snapshot\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd status\n\nGet the status of etcd cluster member\n\nSynopsis\n\nReturns the status of etcd member on the node, use multiple nodes to get status of all members.\n\ntalosctl etcd status [flags]\n\nOptions\n  -h, --help   help for status\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl etcd - Manage etcd\ntalosctl etcd\n\nManage etcd\n\nOptions\n  -h, --help   help for etcd\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl etcd alarm - Manage etcd alarms\ntalosctl etcd defrag - Defragment etcd database on the node\ntalosctl etcd forfeit-leadership - Tell node to forfeit etcd cluster leadership\ntalosctl etcd leave - Tell nodes to leave etcd cluster\ntalosctl etcd members - Get the list of etcd cluster members\ntalosctl etcd remove-member - Remove the node from etcd cluster\ntalosctl etcd snapshot - Stream snapshot of the etcd node to the path.\ntalosctl etcd status - Get the status of etcd cluster member\ntalosctl events\n\nStream runtime events\n\ntalosctl events [flags]\n\nOptions\n      --actor-id string     filter events by the specified actor ID (default is no filter)\n      --duration duration   show events for the past duration interval (one second resolution, default is to show no history)\n  -h, --help                help for events\n      --since string        show events after the specified event ID (default is to show no history)\n      --tail int32          show specified number of past events (use -1 to show full history, default is to show no history)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca\n\nGenerates a self-signed X.509 certificate authority\n\ntalosctl gen ca [flags]\n\nOptions\n  -h, --help                  help for ca\n      --hours int             the hours from now on which the certificate validity period ends (default 87600)\n      --organization string   X.509 distinguished name for the Organization\n      --rsa                   generate in RSA format\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen config\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl gen config <cluster name> <cluster endpoint> [flags]\n\nOptions\n      --additional-sans strings                  additional Subject-Alt-Names for the APIServer certificate\n      --config-patch stringArray                 patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n      --config-patch-control-plane stringArray   patch generated machineconfigs (applied to 'init' and 'controlplane' types)\n      --config-patch-worker stringArray          patch generated machineconfigs (applied to 'worker' type)\n      --dns-domain string                        the dns domain to use for cluster (default \"cluster.local\")\n  -h, --help                                     help for config\n      --install-disk string                      the disk to install to (default \"/dev/sda\")\n      --install-image string                     the image used to perform an installation (default \"ghcr.io/siderolabs/installer:latest\")\n      --kubernetes-version string                desired kubernetes version to run (default \"1.29.0\")\n  -o, --output string                            destination to output generated files. when multiple output types are specified, it must be a directory. for a single output type, it must either be a file path, or \"-\" for stdout\n  -t, --output-types strings                     types of outputs to be generated. valid types are: [\"controlplane\" \"worker\" \"talosconfig\"] (default [controlplane,worker,talosconfig])\n  -p, --persist                                  the desired persist value for configs (default true)\n      --registry-mirror strings                  list of registry mirrors to use in format: <registry host>=<mirror URL>\n      --talos-version string                     the desired Talos version to generate config for (backwards compatibility, e.g. v0.8)\n      --version string                           the desired machine config version to generate (default \"v1alpha1\")\n      --with-cluster-discovery                   enable cluster discovery feature (default true)\n      --with-docs                                renders all machine configs adding the documentation for each field (default true)\n      --with-examples                            renders all machine configs with the commented examples (default true)\n      --with-kubespan                            enable KubeSpan feature\n      --with-secrets string                      use a secrets file generated using 'gen secrets'\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen crt\n\nGenerates an X.509 Ed25519 certificate\n\ntalosctl gen crt [flags]\n\nOptions\n      --ca string     path to the PEM encoded CERTIFICATE\n      --csr string    path to the PEM encoded CERTIFICATE REQUEST\n  -h, --help          help for crt\n      --hours int     the hours from now on which the certificate validity period ends (default 24)\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen csr\n\nGenerates a CSR using an Ed25519 private key\n\ntalosctl gen csr [flags]\n\nOptions\n  -h, --help            help for csr\n      --ip string       generate the certificate for this IP address\n      --key string      path to the PEM encoded EC or RSA PRIVATE KEY\n      --roles strings   roles (default [os:admin])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen key\n\nGenerates an Ed25519 private key\n\ntalosctl gen key [flags]\n\nOptions\n  -h, --help          help for key\n      --name string   the basename of the generated file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen keypair\n\nGenerates an X.509 Ed25519 key pair\n\ntalosctl gen keypair [flags]\n\nOptions\n  -h, --help                  help for keypair\n      --ip string             generate the certificate for this IP address\n      --organization string   X.509 distinguished name for the Organization\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secrets\n\nGenerates a secrets bundle file which can later be used to generate a config\n\ntalosctl gen secrets [flags]\n\nOptions\n      --from-controlplane-config string     use the provided controlplane Talos machine configuration as input\n  -p, --from-kubernetes-pki string          use a Kubernetes PKI directory (e.g. /etc/kubernetes/pki) as input\n  -h, --help                                help for secrets\n  -t, --kubernetes-bootstrap-token string   use the provided bootstrap token as input\n  -o, --output-file string                  path of the output file (default \"secrets.yaml\")\n      --talos-version string                the desired Talos version to generate secrets bundle for (backwards compatibility, e.g. v0.8)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database\n\nGenerates a UEFI database to enroll the signing certificate\n\ntalosctl gen secureboot database [flags]\n\nOptions\n      --enrolled-certificate string   path to the certificate to enroll (default \"_out/uki-signing-cert.pem\")\n  -h, --help                          help for database\n      --signing-certificate string    path to the certificate used to sign the database (default \"_out/uki-signing-cert.pem\")\n      --signing-key string            path to the key used to sign the database (default \"_out/uki-signing-key.pem\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot pcr\n\nGenerates a key which is used to sign TPM PCR values\n\ntalosctl gen secureboot pcr [flags]\n\nOptions\n  -h, --help   help for pcr\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot uki\n\nGenerates a certificate which is used to sign boot assets (UKI)\n\ntalosctl gen secureboot uki [flags]\n\nOptions\n      --common-name string   common name for the certificate (default \"Test UKI Signing Key\")\n  -h, --help                 help for uki\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n  -o, --output string        path to the directory storing the generated files (default \"_out\")\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl gen secureboot\n\nGenerates secrets for the SecureBoot process\n\nOptions\n  -h, --help            help for secureboot\n  -o, --output string   path to the directory storing the generated files (default \"_out\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -f, --force                will overwrite existing files\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl gen secureboot database - Generates a UEFI database to enroll the signing certificate\ntalosctl gen secureboot pcr - Generates a key which is used to sign TPM PCR values\ntalosctl gen secureboot uki - Generates a certificate which is used to sign boot assets (UKI)\ntalosctl gen\n\nGenerate CAs, certificates, and private keys\n\nOptions\n  -f, --force   will overwrite existing files\n  -h, --help    help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl gen ca - Generates a self-signed X.509 certificate authority\ntalosctl gen config - Generates a set of configuration files for Talos cluster\ntalosctl gen crt - Generates an X.509 Ed25519 certificate\ntalosctl gen csr - Generates a CSR using an Ed25519 private key\ntalosctl gen key - Generates an Ed25519 private key\ntalosctl gen keypair - Generates an X.509 Ed25519 key pair\ntalosctl gen secrets - Generates a secrets bundle file which can later be used to generate a config\ntalosctl gen secureboot - Generates secrets for the SecureBoot process\ntalosctl get\n\nGet a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\n\nSynopsis\n\nSimilar to ‘kubectl get’, ’talosctl get’ returns a set of resources from the OS. To get a list of all available resource definitions, issue ’talosctl get rd’\n\ntalosctl get <type> [<id>] [flags]\n\nOptions\n  -h, --help               help for get\n  -i, --insecure           get resources using the insecure (encrypted with no auth) maintenance service\n      --namespace string   resource namespace (default is to use default namespace per resource)\n  -o, --output string      output mode (json, table, yaml, jsonpath) (default \"table\")\n  -w, --watch              watch resource changes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl health\n\nCheck cluster health\n\ntalosctl health [flags]\n\nOptions\n      --control-plane-nodes strings   specify IPs of control plane nodes\n  -h, --help                          help for health\n      --init-node string              specify IPs of init node\n      --k8s-endpoint string           use endpoint instead of kubeconfig default\n      --run-e2e                       run Kubernetes e2e test\n      --server                        run server-side check (default true)\n      --wait-timeout duration         timeout to wait for the cluster to be ready (default 20m0s)\n      --worker-nodes strings          specify IPs of worker nodes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default\n\nList the default images used by Talos\n\ntalosctl image default [flags]\n\nOptions\n  -h, --help   help for default\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image list\n\nList CRI images\n\ntalosctl image list [flags]\n\nOptions\n  -h, --help   help for list\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image pull\n\nPull an image into CRI\n\ntalosctl image pull [flags]\n\nOptions\n  -h, --help   help for pull\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n      --namespace system     namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl image - Manage CRI containter images\ntalosctl image\n\nManage CRI containter images\n\nOptions\n  -h, --help               help for image\n      --namespace system   namespace to use: system (etcd and kubelet images) or `cri` for all Kubernetes workloads (default \"cri\")\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl image default - List the default images used by Talos\ntalosctl image list - List CRI images\ntalosctl image pull - Pull an image into CRI\ntalosctl inject serviceaccount\n\nInject Talos API ServiceAccount into Kubernetes manifests\n\ntalosctl inject serviceaccount [--roles='<ROLE_1>,<ROLE_2>'] -f <manifest.yaml> [flags]\n\nExamples\ntalosctl inject serviceaccount --roles=\"os:admin\" -f deployment.yaml > deployment-injected.yaml\n\nAlternatively, stdin can be piped to the command:\ncat deployment.yaml | talosctl inject serviceaccount --roles=\"os:admin\" -f - > deployment-injected.yaml\n\nOptions\n  -f, --file string     file with Kubernetes manifests to be injected with ServiceAccount\n  -h, --help            help for serviceaccount\n  -r, --roles strings   roles to add to the generated ServiceAccount manifests (default [os:reader])\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inject\n\nInject Talos API resources into Kubernetes manifests\n\nOptions\n  -h, --help   help for inject\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inject serviceaccount - Inject Talos API ServiceAccount into Kubernetes manifests\ntalosctl inspect dependencies\n\nInspect controller-resource dependencies as graphviz graph.\n\nSynopsis\n\nInspect controller-resource dependencies as graphviz graph.\n\nPipe the output of the command through the “dot” program (part of graphviz package) to render the graph:\n\ntalosctl inspect dependencies | dot -Tpng > graph.png\n\ntalosctl inspect dependencies [flags]\n\nOptions\n  -h, --help             help for dependencies\n      --with-resources   display live resource information with dependencies\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl inspect - Inspect internals of Talos\ntalosctl inspect\n\nInspect internals of Talos\n\nOptions\n  -h, --help   help for inspect\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl inspect dependencies - Inspect controller-resource dependencies as graphviz graph.\ntalosctl kubeconfig\n\nDownload the admin kubeconfig from the node\n\nSynopsis\n\nDownload the admin kubeconfig from the node. If merge flag is defined, config will be merged with ~/.kube/config or [local-path] if specified. Otherwise kubeconfig will be written to PWD or [local-path] if specified.\n\ntalosctl kubeconfig [local-path] [flags]\n\nOptions\n  -f, --force                       Force overwrite of kubeconfig if already present, force overwrite on kubeconfig merge\n      --force-context-name string   Force context name for kubeconfig merge\n  -h, --help                        help for kubeconfig\n  -m, --merge                       Merge with existing kubeconfig (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl list\n\nRetrieve a directory listing\n\ntalosctl list [path] [flags]\n\nOptions\n  -d, --depth int32    maximum recursion depth (default 1)\n  -h, --help           help for list\n  -H, --humanize       humanize size and time in the output\n  -l, --long           display additional file details\n  -r, --recurse        recurse into subdirectories\n  -t, --type strings   filter by specified types:\n                       f\tregular file\n                       d\tdirectory\n                       l, L\tsymbolic link\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl logs\n\nRetrieve logs for a service\n\ntalosctl logs <service name> [flags]\n\nOptions\n  -f, --follow       specify if the logs should be streamed\n  -h, --help         help for logs\n  -k, --kubernetes   use the k8s.io containerd namespace\n      --tail int32   lines of log file to display (default is to show from the beginning) (default -1)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen\n\nGenerates a set of configuration files for Talos cluster\n\nSynopsis\n\nThe cluster endpoint is the URL for the Kubernetes API. If you decide to use a control plane node, common in a single node control plane setup, use port 6443 as this is the port that the API server binds to on every control plane node. For an HA setup, usually involving a load balancer, use the IP and port of the load balancer.\n\ntalosctl machineconfig gen <cluster name> <cluster endpoint> [flags]\n\nOptions\n  -h, --help   help for gen\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig patch\n\nPatch a machine config\n\ntalosctl machineconfig patch <machineconfig-file> [flags]\n\nOptions\n  -h, --help                help for patch\n  -o, --output string       output destination. if not specified, output will be printed to stdout\n  -p, --patch stringArray   patch generated machineconfigs (applied to all node types), use @file to read a patch from file\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl machineconfig - Machine config related commands\ntalosctl machineconfig\n\nMachine config related commands\n\nOptions\n  -h, --help   help for machineconfig\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl machineconfig gen - Generates a set of configuration files for Talos cluster\ntalosctl machineconfig patch - Patch a machine config\ntalosctl memory\n\nShow memory usage\n\ntalosctl memory [flags]\n\nOptions\n  -h, --help      help for memory\n  -v, --verbose   display extended memory statistics\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete\n\nDelete a key from the META partition.\n\ntalosctl meta delete key [flags]\n\nOptions\n  -h, --help   help for delete\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta write\n\nWrite a key-value pair to the META partition.\n\ntalosctl meta write key value [flags]\n\nOptions\n  -h, --help   help for write\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -i, --insecure             write|delete meta using the insecure (encrypted with no auth) maintenance service\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl meta - Write and delete keys in the META partition\ntalosctl meta\n\nWrite and delete keys in the META partition\n\nOptions\n  -h, --help       help for meta\n  -i, --insecure   write|delete meta using the insecure (encrypted with no auth) maintenance service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl meta delete - Delete a key from the META partition.\ntalosctl meta write - Write a key-value pair to the META partition.\ntalosctl mounts\n\nList mounts\n\ntalosctl mounts [flags]\n\nOptions\n  -h, --help   help for mounts\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl netstat\n\nShow network connections and sockets\n\nSynopsis\n\nShow network connections and sockets.\n\nYou can pass an optional argument to view a specific pod’s connections. To do this, format the argument as “namespace/pod”. Note that only pods with a pod network namespace are allowed. If you don’t pass an argument, the command will show host connections.\n\ntalosctl netstat [flags]\n\nOptions\n  -a, --all         display all sockets states (default: connected)\n  -x, --extend      show detailed socket information\n  -h, --help        help for netstat\n  -4, --ipv4        display only ipv4 sockets\n  -6, --ipv6        display only ipv6 sockets\n  -l, --listening   display listening server sockets\n  -k, --pods        show sockets used by Kubernetes pods\n  -p, --programs    show process using socket\n  -w, --raw         display only RAW sockets\n  -t, --tcp         display only TCP sockets\n  -o, --timers      display timers\n  -u, --udp         display only UDP sockets\n  -U, --udplite     display only UDPLite sockets\n  -v, --verbose     display sockets of all supported transport protocols\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl patch\n\nUpdate field(s) of a resource using a JSON patch.\n\ntalosctl patch <type> [<id>] [flags]\n\nOptions\n      --dry-run                                     print the change summary and patch preview without applying the changes\n  -h, --help                                        help for patch\n  -m, --mode auto, no-reboot, reboot, staged, try   apply config mode (default auto)\n      --namespace string                            resource namespace (default is to use default namespace per resource)\n  -p, --patch stringArray                           the patch to be applied to the resource file, use @file to read a patch from file.\n      --patch-file string                           a file containing a patch to be applied to the resource.\n      --timeout duration                            the config will be rolled back after specified timeout (if try mode is selected) (default 1m0s)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl pcap\n\nCapture the network packets from the node.\n\nSynopsis\n\nThe command launches packet capture on the node and streams back the packets as raw pcap file.\n\nDefault behavior is to decode the packets with internal decoder to stdout:\n\ntalosctl pcap -i eth0\n\n\nRaw pcap file can be saved with --output flag:\n\ntalosctl pcap -i eth0 --output eth0.pcap\n\n\nOutput can be piped to tcpdump:\n\ntalosctl pcap -i eth0 -o - | tcpdump -vvv -r -\n\n\nBPF filter can be applied, but it has to compiled to BPF instructions first using tcpdump. Correct link type should be specified for the tcpdump: EN10MB for Ethernet links and RAW for e.g. Wireguard tunnels:\n\ntalosctl pcap -i eth0 --bpf-filter \"$(tcpdump -dd -y EN10MB 'tcp and dst port 80')\"\n\ntalosctl pcap -i kubespan --bpf-filter \"$(tcpdump -dd -y RAW 'port 50000')\"\n\n\nAs packet capture is transmitted over the network, it is recommended to filter out the Talos API traffic, e.g. by excluding packets with the port 50000.\n\ntalosctl pcap [flags]\n\nOptions\n      --bpf-filter string   bpf filter to apply, tcpdump -dd format\n      --duration duration   duration of the capture\n  -h, --help                help for pcap\n  -i, --interface string    interface name to capture packets on (default \"eth0\")\n  -o, --output string       if not set, decode packets to stdout; if set write raw pcap data to a file, use '-' for stdout\n      --promiscuous         put interface into promiscuous mode\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl processes\n\nList running processes\n\ntalosctl processes [flags]\n\nOptions\n  -h, --help          help for processes\n  -s, --sort string   Column to sort output by. [rss|cpu] (default \"rss\")\n  -w, --watch         Stream running processes\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl read\n\nRead a file on the machine\n\ntalosctl read <path> [flags]\n\nOptions\n  -h, --help   help for read\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reboot\n\nReboot a node\n\ntalosctl reboot [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n  -h, --help               help for reboot\n  -m, --mode string        select the reboot mode: \"default\", \"powercycle\" (skips kexec) (default \"default\")\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl reset\n\nReset a node\n\ntalosctl reset [flags]\n\nOptions\n      --debug                                    debug operation from kernel logs. --wait is set to true when this flag is set\n      --graceful                                 if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n  -h, --help                                     help for reset\n      --insecure                                 reset using the insecure (encrypted with no auth) maintenance service\n      --reboot                                   if true, reboot the node after resetting instead of shutting down\n      --system-labels-to-wipe strings            if set, just wipe selected system disk partitions by label but keep other partitions intact\n      --timeout duration                         time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --user-disks-to-wipe strings               if set, wipes defined devices in the list\n      --wait                                     wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n      --wipe-mode all, system-disk, user-disks   disk reset mode (default all)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl restart\n\nRestart a process\n\ntalosctl restart <id> [flags]\n\nOptions\n  -h, --help         help for restart\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl rollback\n\nRollback a node to the previous installation\n\ntalosctl rollback [flags]\n\nOptions\n  -h, --help   help for rollback\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl service\n\nRetrieve the state of a service (or all services), control service state\n\nSynopsis\n\nService control command. If run without arguments, lists all the services and their state. If service ID is specified, default action ‘status’ is executed which shows status of a single list service. With actions ‘start’, ‘stop’, ‘restart’, service state is updated respectively.\n\ntalosctl service [<id> [start|stop|restart|status]] [flags]\n\nOptions\n  -h, --help   help for service\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl shutdown\n\nShutdown a node\n\ntalosctl shutdown [flags]\n\nOptions\n      --debug              debug operation from kernel logs. --wait is set to true when this flag is set\n      --force              if true, force a node to shutdown without a cordon/drain\n  -h, --help               help for shutdown\n      --timeout duration   time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait               wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl stats\n\nGet container stats\n\ntalosctl stats [flags]\n\nOptions\n  -h, --help         help for stats\n  -k, --kubernetes   use the k8s.io containerd namespace\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl support\n\nDump debug information about the cluster\n\nSynopsis\n\nGenerated bundle contains the following debug information:\n\nFor each node:\n\nKernel logs.\nAll Talos internal services logs.\nAll kube-system pods logs.\nTalos COSI resources without secrets.\nCOSI runtime state graph.\nProcesses snapshot.\nIO pressure snapshot.\nMounts list.\nPCI devices info.\nTalos version.\n\nFor the cluster:\n\nKubernetes nodes and kube-system pods manifests.\ntalosctl support [flags]\n\nOptions\n  -h, --help              help for support\n  -w, --num-workers int   number of workers per node (default 1)\n  -O, --output string     output file to write support archive to\n  -v, --verbose           verbose output\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl time\n\nGets current server time\n\ntalosctl time [--check server] [flags]\n\nOptions\n  -c, --check string   checks server time against specified ntp server\n  -h, --help           help for time\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade\n\nUpgrade Talos on the target node\n\ntalosctl upgrade [flags]\n\nOptions\n      --debug                debug operation from kernel logs. --wait is set to true when this flag is set\n  -f, --force                force the upgrade (skip checks on etcd health and members, might lead to data loss)\n  -h, --help                 help for upgrade\n  -i, --image string         the container image to use for performing the install (default \"ghcr.io/siderolabs/installer:v1.6.0-alpha.2\")\n      --insecure             upgrade using the insecure (encrypted with no auth) maintenance service\n  -p, --preserve             preserve data\n  -m, --reboot-mode string   select the reboot mode during upgrade. Mode \"powercycle\" bypasses kexec. Valid values are: [\"default\" \"powercycle\"]. (default \"default\")\n  -s, --stage                stage the upgrade to perform it after a reboot\n      --timeout duration     time to wait for the operation is complete if --debug or --wait is set (default 30m0s)\n      --wait                 wait for the operation to complete, tracking its progress. always set to true when --debug is set (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl upgrade-k8s\n\nUpgrade Kubernetes control plane in the Talos cluster.\n\nSynopsis\n\nCommand runs upgrade of Kubernetes control plane components between specified versions.\n\ntalosctl upgrade-k8s [flags]\n\nOptions\n      --dry-run           skip the actual upgrade and show the upgrade plan instead\n      --endpoint string   the cluster control plane endpoint\n      --from string       the Kubernetes control plane version to upgrade from\n  -h, --help              help for upgrade-k8s\n      --pre-pull-images   pre-pull images before upgrade (default true)\n      --to string         the Kubernetes control plane version to upgrade to (default \"1.29.0\")\n      --upgrade-kubelet   upgrade kubelet service (default true)\n      --with-docs         patch all machine configs adding the documentation for each field (default true)\n      --with-examples     patch all machine configs with the commented examples (default true)\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl usage\n\nRetrieve a disk usage\n\ntalosctl usage [path1] [path2] ... [pathN] [flags]\n\nOptions\n  -a, --all             write counts for all files, not just directories\n  -d, --depth int32     maximum recursion depth\n  -h, --help            help for usage\n  -H, --humanize        humanize size and time in the output\n  -t, --threshold int   threshold exclude entries smaller than SIZE if positive, or entries greater than SIZE if negative\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl validate\n\nValidate config\n\ntalosctl validate [flags]\n\nOptions\n  -c, --config string   the path of the config file\n  -h, --help            help for validate\n  -m, --mode string     the mode to validate the config for (valid values are metal, cloud, and container)\n      --strict          treat validation warnings as errors\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl version\n\nPrints the version\n\ntalosctl version [flags]\n\nOptions\n      --client     Print client version only\n  -h, --help       help for version\n  -i, --insecure   use Talos maintenance mode API\n      --short      Print the short version\n\nOptions inherited from parent commands\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl - A CLI for out-of-band management of Kubernetes nodes created by Talos\ntalosctl\n\nA CLI for out-of-band management of Kubernetes nodes created by Talos\n\nOptions\n      --cluster string       Cluster to connect to if a proxy endpoint is used.\n      --context string       Context to be used in command\n  -e, --endpoints strings    override default endpoints in Talos configuration\n  -h, --help                 help for talosctl\n  -n, --nodes strings        target the specified nodes\n      --talosconfig string   The path to the Talos configuration file. Defaults to 'TALOSCONFIG' env variable if set, otherwise '$HOME/.talos/config' and '/var/run/secrets/talos.dev/config' in order.\n\nSEE ALSO\ntalosctl apply-config - Apply a new configuration to a node\ntalosctl bootstrap - Bootstrap the etcd cluster on the specified node.\ntalosctl cluster - A collection of commands for managing local docker-based or QEMU-based clusters\ntalosctl completion - Output shell completion code for the specified shell (bash, fish or zsh)\ntalosctl config - Manage the client configuration file (talosconfig)\ntalosctl conformance - Run conformance tests\ntalosctl containers - List containers\ntalosctl copy - Copy data out from the node\ntalosctl dashboard - Cluster dashboard with node overview, logs and real-time metrics\ntalosctl disks - Get the list of disks from /sys/block on the machine\ntalosctl dmesg - Retrieve kernel logs\ntalosctl edit - Edit a resource from the default editor.\ntalosctl etcd - Manage etcd\ntalosctl events - Stream runtime events\ntalosctl gen - Generate CAs, certificates, and private keys\ntalosctl get - Get a specific resource or list of resources (use ’talosctl get rd’ to see all available resource types).\ntalosctl health - Check cluster health\ntalosctl image - Manage CRI containter images\ntalosctl inject - Inject Talos API resources into Kubernetes manifests\ntalosctl inspect - Inspect internals of Talos\ntalosctl kubeconfig - Download the admin kubeconfig from the node\ntalosctl list - Retrieve a directory listing\ntalosctl logs - Retrieve logs for a service\ntalosctl machineconfig - Machine config related commands\ntalosctl memory - Show memory usage\ntalosctl meta - Write and delete keys in the META partition\ntalosctl mounts - List mounts\ntalosctl netstat - Show network connections and sockets\ntalosctl patch - Update field(s) of a resource using a JSON patch.\ntalosctl pcap - Capture the network packets from the node.\ntalosctl processes - List running processes\ntalosctl read - Read a file on the machine\ntalosctl reboot - Reboot a node\ntalosctl reset - Reset a node\ntalosctl restart - Restart a process\ntalosctl rollback - Rollback a node to the previous installation\ntalosctl service - Retrieve the state of a service (or all services), control service state\ntalosctl shutdown - Shutdown a node\ntalosctl stats - Get container stats\ntalosctl support - Dump debug information about the cluster\ntalosctl time - Gets current server time\ntalosctl upgrade - Upgrade Talos on the target node\ntalosctl upgrade-k8s - Upgrade Kubernetes control plane in the Talos cluster.\ntalosctl usage - Retrieve a disk usage\ntalosctl validate - Validate config\ntalosctl version - Prints the version\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Verifying Images | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/verifying-images/",
    "html": "Verifying Container Image Signatures\nReproducible Builds\nDocumentation\nAdvanced Guides\nVerifying Images\nVerifying Images\nVerifying Talos container image signatures.\n\nSidero Labs signs the container images generated for the Talos release with cosign:\n\nghcr.io/siderolabs/installer (Talos installer)\nghcr.io/siderolabs/talos (Talos image for container runtime)\nghcr.io/siderolabs/talosctl (talosctl client packaged as a container image)\nghcr.io/siderolabs/imager (Talos install image generator)\nall system extension images\nVerifying Container Image Signatures\n\nThe cosign tool can be used to verify the signatures of the Talos container images:\n\nCopy\n$ cosign verify --certificate-identity-regexp '@siderolabs\\.com$' --certificate-oidc-issuer https://accounts.google.com ghcr.io/siderolabs/installer:v1.4.0\n\n\n\nVerification for ghcr.io/siderolabs/installer:v1.4.0 --\n\nThe following checks were performed on each of these signatures:\n\n  - The cosign claims were validated\n\n  - Existence of the claims in the transparency log was verified offline\n\n  - The code-signing certificate was verified using trusted certificate authority certificates\n\n\n\n[{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/siderolabs/installer\"},\"image\":{\"docker-manifest-digest\":\"sha256:f41795cc88f40eb1bc6b3c638c4a3123f6ef3c90627bfc35c04ebab82581e3ee\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://accounts.google.com\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEQCIERkQpgEnPWnfjUHIWO9QxC9Ute3/xJOc7TO5GUnu59xAiBKcFvrDWHoUYChT0/+gaazTrI+r0/GWSbi+Q+sEQ5AKA==\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1681843022,\"logIndex\":18304044,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://accounts.google.com\",\"Subject\":\"andrey.smirnov@siderolabs.com\"}}]\n\n\nThe image should be signed using cosing keyless flow by a Sidero Labs employee with and email from siderolabs.com domain.\n\nReproducible Builds\n\nTalos builds for kernel, initramfs, talosctl, ISO image, and container images are reproducible. So you can verify that the build is the same as the one as provided on GitHub releases page.\n\nSee building Talos images for more details.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Talos API access from Kubernetes | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/talos-api-access-from-k8s/",
    "html": "Enabling the Feature\nInjecting Talos ServiceAccount into manifests\nTesting API Access\nDocumentation\nAdvanced Guides\nTalos API access from Kubernetes\nTalos API access from Kubernetes\nHow to access Talos API from within Kubernetes.\n\nIn this guide, we will enable the Talos feature to access the Talos API from within Kubernetes.\n\nEnabling the Feature\n\nEdit the machine configuration to enable the feature, specifying the Kubernetes namespaces from which Talos API can be accessed and the allowed Talos API roles.\n\nCopy\ntalosctl -n 172.20.0.2 edit machineconfig\n\n\nConfigure the kubernetesTalosAPIAccess like the following:\n\nCopy\nspec:\n\n  machine:\n\n    features:\n\n      kubernetesTalosAPIAccess:\n\n        enabled: true\n\n        allowedRoles:\n\n          - os:reader\n\n        allowedKubernetesNamespaces:\n\n          - default\n\nInjecting Talos ServiceAccount into manifests\n\nCreate the following manifest file deployment.yaml:\n\nCopy\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  template:\n\n    metadata:\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n        - name: talos-api-access\n\n          image: alpine:3\n\n          command:\n\n            - sh\n\n            - -c\n\n            - |\n\n              wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n              chmod +x /usr/local/bin/talosctl\n\n              while true; talosctl -n 172.20.0.2 version; do sleep 1; done              \n\n\nNote: make sure that you replace the IP 172.20.0.2 with a valid Talos node IP.\n\nUse talosctl inject serviceaccount command to inject the Talos ServiceAccount into the manifest.\n\nCopy\ntalosctl inject serviceaccount -f deployment.yaml > deployment-injected.yaml\n\n\nInspect the generated manifest:\n\nCopy\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  creationTimestamp: null\n\n  name: talos-api-access\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      app: talos-api-access\n\n  strategy: {}\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: talos-api-access\n\n    spec:\n\n      containers:\n\n      - command:\n\n        - sh\n\n        - -c\n\n        - |\n\n          wget -O /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/<talos version>/talosctl-linux-amd64\n\n          chmod +x /usr/local/bin/talosctl\n\n          while true; talosctl -n 172.20.0.2 version; do sleep 1; done          \n\n        image: alpine:3\n\n        name: talos-api-access\n\n        resources: {}\n\n        volumeMounts:\n\n        - mountPath: /var/run/secrets/talos.dev\n\n          name: talos-secrets\n\n      tolerations:\n\n      - operator: Exists\n\n      volumes:\n\n      - name: talos-secrets\n\n        secret:\n\n          secretName: talos-api-access-talos-secrets\n\nstatus: {}\n\n---\n\napiVersion: talos.dev/v1alpha1\n\nkind: ServiceAccount\n\nmetadata:\n\n    name: talos-api-access-talos-secrets\n\nspec:\n\n    roles:\n\n        - os:reader\n\n---\n\n\nAs you can notice, your deployment manifest is now injected with the Talos ServiceAccount.\n\nTesting API Access\n\nApply the new manifest into default namespace:\n\nCopy\nkubectl apply -n default -f deployment-injected.yaml\n\n\nFollow the logs of the pods belong to the deployment:\n\nCopy\nkubectl logs -n default -f -l app=talos-api-access\n\n\nYou’ll see a repeating output similar to the following:\n\nCopy\nClient:\n\n    Tag:         <talos version>\n\n    SHA:         ....\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\nServer:\n\n    NODE:        172.20.0.2\n\n    Tag:         <talos version>\n\n    SHA:         ...\n\n    Built:\n\n    Go version:  go1.18.4\n\n    OS/Arch:     linux/amd64\n\n    Enabled:     RBAC\n\n\nThis means that the pod can talk to Talos API of node 172.20.0.2 successfully.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Static Pods | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/static-pods/",
    "html": "Static Pods\nConfiguration\nUsage\nTroubleshooting\nResource Definitions\nDocumentation\nAdvanced Guides\nStatic Pods\nStatic Pods\nUsing Talos Linux to set up static pods in Kubernetes.\nStatic Pods\n\nStatic pods are run directly by the kubelet bypassing the Kubernetes API server checks and validations. Most of the time DaemonSet is a better alternative to static pods, but some workloads need to run before the Kubernetes API server is available or might need to bypass security restrictions imposed by the API server.\n\nSee Kubernetes documentation for more information on static pods.\n\nConfiguration\n\nStatic pod definitions are specified in the Talos machine configuration:\n\nCopy\nmachine:\n\n  pods:\n\n    - apiVersion: v1\n\n       kind: Pod\n\n       metadata:\n\n         name: nginx\n\n       spec:\n\n         containers:\n\n           - name: nginx\n\n             image: nginx\n\n\nTalos renders static pod definitions to the kubelet manifest directory (/etc/kubernetes/manifests), kubelet picks up the definition and launches the pod.\n\nTalos accepts changes to the static pod configuration without a reboot.\n\nUsage\n\nKubelet mirrors pod definition to the API server state, so static pods can be inspected with kubectl get pods, logs can be retrieved with kubectl logs, etc.\n\nCopy\n$ kubectl get pods\n\nNAME                           READY   STATUS    RESTARTS   AGE\n\nnginx-talos-default-controlplane-2   1/1     Running   0          17s\n\n\nIf the API server is not available, status of the static pod can also be inspected with talosctl containers --kubernetes:\n\nCopy\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                      IMAGE                                                   PID    STATUS\n\n172.20.0.3   k8s.io      default/nginx-talos-default-controlplane-2                                              registry.k8s.io/pause:3.6                               4886   SANDBOX_READY\n\n172.20.0.3   k8s.io      └─ default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771                        docker.io/library/nginx:latest\n\n...\n\n\nLogs of static pods can be retrieved with talosctl logs --kubernetes:\n\nCopy\n$ talosctl logs --kubernetes default/nginx-talos-default-controlplane-2:nginx:4183a7d7a771\n\n172.20.0.3: 2022-02-10T15:26:01.289208227Z stderr F 2022/02/10 15:26:01 [notice] 1#1: using the \"epoll\" event method\n\n172.20.0.3: 2022-02-10T15:26:01.2892466Z stderr F 2022/02/10 15:26:01 [notice] 1#1: nginx/1.21.6\n\n172.20.0.3: 2022-02-10T15:26:01.28925723Z stderr F 2022/02/10 15:26:01 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)\n\nTroubleshooting\n\nTalos doesn’t perform any validation on the static pod definitions. If the pod isn’t running, use kubelet logs (talosctl logs kubelet) to find the problem:\n\nCopy\n$ talosctl logs kubelet\n\n172.20.0.2: {\"ts\":1644505520281.427,\"caller\":\"config/file.go:187\",\"msg\":\"Could not process manifest file\",\"path\":\"/etc/kubernetes/manifests/talos-default-nginx-gvisor.yaml\",\"err\":\"invalid pod: [spec.containers: Required value]\"}\n\nResource Definitions\n\nStatic pod definitions are available as StaticPod resources combined with Talos-generated control plane static pods:\n\nCopy\n$ talosctl get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.3   k8s         StaticPod   default-nginx             1\n\n172.20.0.3   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.3   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.3   k8s         StaticPod   kube-scheduler            1\n\n\nTalos assigns ID <namespace>-<name> to the static pods specified in the machine configuration.\n\nOn control plane nodes status of the running static pods is available in the StaticPodStatus resource:\n\nCopy\n$ talosctl get staticpodstatus\n\nNODE         NAMESPACE   TYPE              ID                                                           VERSION   READY\n\n172.20.0.3   k8s         StaticPodStatus   default/nginx-talos-default-controlplane-2                         2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-apiserver-talos-default-controlplane-2            2         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-controller-manager-talos-default-controlplane-2   3         True\n\n172.20.0.3   k8s         StaticPodStatus   kube-system/kube-scheduler-talos-default-controlplane-2            3         True\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Migrating from Kubeadm | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/migrating-from-kubeadm/",
    "html": "Remarks on kube-apiserver load balancer\nPrerequisites\nStep-by-step guide\nDocumentation\nAdvanced Guides\nMigrating from Kubeadm\nMigrating from Kubeadm\nMigrating Kubeadm-based clusters to Talos.\n\nIt is possible to migrate Talos from a cluster that is created using kubeadm to Talos.\n\nHigh-level steps are the following:\n\nCollect CA certificates and a bootstrap token from a control plane node.\nCreate a Talos machine config with the CA certificates with the ones you collected.\nUpdate control plane endpoint in the machine config to point to the existing control plane (i.e. your load balancer address).\nBoot a new Talos machine and apply the machine config.\nVerify that the new control plane node is ready.\nRemove one of the old control plane nodes.\nRepeat the same steps for all control plane nodes.\nVerify that all control plane nodes are ready.\nRepeat the same steps for all worker nodes, using the machine config generated for the workers.\nRemarks on kube-apiserver load balancer\n\nWhile migrating to Talos, you need to make sure that your kube-apiserver load balancer is in place and keeps pointing to the correct set of control plane nodes.\n\nThis process depends on your load balancer setup.\n\nIf you are using an LB that is external to the control plane nodes (e.g. cloud provider LB, F5 BIG-IP, etc.), you need to make sure that you update the backend IPs of the load balancer to point to the control plane nodes as you add Talos nodes and remove kubeadm-based ones.\n\nIf your load balancing is done on the control plane nodes (e.g. keepalived + haproxy on the control plane nodes), you can do the following:\n\nAdd Talos nodes and remove kubeadm-based ones while updating the haproxy backends to point to the newly added nodes except the last kubeadm-based control plane node.\nTurn off keepalived to drop the virtual IP used by the kubeadm-based nodes (introduces kube-apiserver downtime).\nSet up a virtual-IP based new load balancer on the new set of Talos control plane nodes. Use the previous LB IP as the LB virtual IP.\nVerify apiserver connectivity over the Talos-managed virtual IP.\nMigrate the last control-plane node.\nPrerequisites\nAdmin access to the kubeadm-based cluster\nAccess to the /etc/kubernetes/pki directory (e.g. SSH & root permissions) on the control plane nodes of the kubeadm-based cluster\nAccess to kube-apiserver load-balancer configuration\nStep-by-step guide\n\nDownload /etc/kubernetes/pki directory from a control plane node of the kubeadm-based cluster.\n\nCreate a new join token for the new control plane nodes:\n\nCopy\n# inside a control plane node\n\nkubeadm token create --ttl 0\n\n\nCreate Talos secrets from the PKI directory you downloaded on step 1 and the token you generated on step 2:\n\nCopy\ntalosctl gen secrets --kubernetes-bootstrap-token <TOKEN> --from-kubernetes-pki <PKI_DIR>\n\n\nCreate a new Talos config from the secrets:\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml <CLUSTER_NAME> https://<EXISTING_CLUSTER_LB_IP>\n\n\nCollect the information about the kubeadm-based cluster from the kubeadm configmap:\n\nCopy\nkubectl get configmap -n kube-system kubeadm-config -oyaml\n\n\nTake note of the following information in the ClusterConfiguration:\n\n.controlPlaneEndpoint\n.networking.dnsDomain\n.networking.podSubnet\n.networking.serviceSubnet\n\nReplace the following information in the generated controlplane.yaml:\n\n.cluster.network.cni.name with none\n.cluster.network.podSubnets[0] with the value of the networking.podSubnet from the previous step\n.cluster.network.serviceSubnets[0] with the value of the networking.serviceSubnet from the previous step\n.cluster.network.dnsDomain with the value of the networking.dnsDomain from the previous step\n\nGo through the rest of controlplane.yaml and worker.yaml to customize them according to your needs, especially :\n\n.cluster.secretboxEncryptionSecret should be either removed if you don’t currently use EncryptionConfig on your kube-apiserver or set to the correct value\n\nMake sure that, on your current Kubeadm cluster, the first --service-account-issuer= parameter in /etc/kubernetes/manifests/kube-apiserver.yaml is equal to the value of .cluster.controlPlane.endpoint in controlplane.yaml. If it’s not, add a new --service-account-issuer= parameter with the correct value before your current one in /etc/kubernetes/manifests/kube-apiserver.yaml on all of your control planes nodes, and restart the kube-apiserver containers.\n\nBring up a Talos node to be the initial Talos control plane node.\n\nApply the generated controlplane.yaml to the Talos control plane node:\n\nCopy\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file controlplane.yaml\n\n\nWait until the new control plane node joins the cluster and is ready.\n\nCopy\nkubectl get node -owide --watch\n\n\nUpdate your load balancer to point to the new control plane node.\n\nDrain the old control plane node you are replacing:\n\nCopy\nkubectl drain <OLD_NODE> --delete-emptydir-data --force --ignore-daemonsets --timeout=10m\n\n\nRemove the old control plane node from the cluster:\n\nCopy\nkubectl delete node <OLD_NODE>\n\n\nDestroy the old node:\n\nCopy\n# inside the node\n\nsudo kubeadm reset --force\n\n\nRepeat the same steps, starting from step 7, for all control plane nodes.\n\nRepeat the same steps, starting from step 7, for all worker nodes while applying the worker.yaml instead and skipping the LB step:\n\nCopy\ntalosctl --nodes <TALOS_NODE_IP> apply-config --insecure --file worker.yaml\n\n\nYour kubeadm kube-proxy configuration may not be compatible with the one generated by Talos, which will make the Talos Kubernetes upgrades impossible (labels may not be the same, and selector.matchLabels is an immutable field). To be sure, export your current kube-proxy daemonset manifest, check the labels, they have to be:\n\nCopy\ntier: node\n\nk8s-app: kube-proxy\n\n\nIf the are not, modify all the labels fields, save the file, delete your current kube-proxy daemonset, and apply the one you modified.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Proprietary Kernel Modules | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/proprietary-kernel-modules/",
    "html": "Documentation\nAdvanced Guides\nProprietary Kernel Modules\nProprietary Kernel Modules\nAdding a proprietary kernel module to Talos Linux\n\nPatching and building the kernel image\n\nClone the pkgs repository from Github and check out the revision corresponding to your version of Talos Linux\n\nCopy\ngit clone https://github.com/talos-systems/pkgs pkgs && cd pkgs\n\ngit checkout v0.8.0\n\n\nClone the Linux kernel and check out the revision that pkgs uses (this can be found in kernel/kernel-prepare/pkg.yaml and it will be something like the following: https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-x.xx.x.tar.xz)\n\nCopy\ngit clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git && cd linux\n\ngit checkout v5.15\n\n\nYour module will need to be converted to be in-tree. The steps for this are different depending on the complexity of the module to port, but generally it would involve moving the module source code into the drivers tree and creating a new Makefile and Kconfig.\n\nStage your changes in Git with git add -A.\n\nRun git diff --cached --no-prefix > foobar.patch to generate a patch from your changes.\n\nCopy this patch to kernel/kernel/patches in the pkgs repo.\n\nAdd a patch line in the prepare segment of kernel/kernel/pkg.yaml:\n\nCopy\npatch -p0 < /pkg/patches/foobar.patch\n\n\nBuild the kernel image. Make sure you are logged in to ghcr.io before running this command, and you can change or omit PLATFORM depending on what you want to target.\n\nCopy\nmake kernel PLATFORM=linux/amd64 USERNAME=your-username PUSH=true\n\n\nMake a note of the image name the make command outputs.\n\nBuilding the installer image\n\nCopy the following into a new Dockerfile:\n\nCopy\nFROM scratch AS customization\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:<talos version>\n\nCOPY --from=ghcr.io/your-username/kernel:<kernel version> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nRun to build and push the installer:\n\nCopy\nINSTALLER_VERSION=<talos version>\n\nIMAGE_NAME=\"ghcr.io/your-username/talos-installer:$INSTALLER_VERSION\"\n\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t \"$IMAGE_NAME\" . && docker push \"$IMAGE_NAME\"\n\n\nDeploying to your cluster\n\nCopy\ntalosctl upgrade --image ghcr.io/your-username/talos-installer:<talos version> --preserve=true\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Metal Network Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/metal-network-configuration/",
    "html": "Network Configuration Format\nAddresses\nLinks\nBonds\nVLANs\nRoutes\nHostnames\nResolvers\nTime Servers\nSupplying META Network Configuration\nSupplying Network Configuration to a Running Talos Machine\nSupplying Network Configuration to a Talos Disk Image\nSupplying Network Configuration to a Talos ISO/PXE Boot\nGetting Current META Network Configuration\nDocumentation\nAdvanced Guides\nMetal Network Configuration\nMetal Network Configuration\nHow to use META-based network configuration on Talos metal platform.\n\nNote: This is an advanced feature which requires deep understanding of Talos and Linux network configuration.\n\nTalos Linux when running on a cloud platform (e.g. AWS or Azure), uses the platform-provided metadata server to provide initial network configuration to the node. When running on bare-metal, there is no metadata server, so there are several options to provide initial network configuration (before machine configuration is acquired):\n\nuse automatic network configuration via DHCP (Talos default)\nuse initial boot kernel command line parameters to configure networking\nuse automatic network configuration via DHCP just enough to fetch machine configuration and then use machine configuration to set desired advanced configuration.\n\nIf DHCP option is available, it is by far the easiest way to configure networking. The initial boot kernel command line parameters are not very flexible, and they are not persisted after initial Talos installation.\n\nTalos starting with version 1.4.0 offers a new option to configure networking on bare-metal: META-based network configuration.\n\nNote: META-based network configuration is only available on Talos Linux metal platform.\n\nTalos dashboard provides a way to configure META-based network configuration for a machine using the console, but it doesn’t support all kinds of network configuration.\n\nNetwork Configuration Format\n\nTalos META-based network configuration is a YAML file with the following format:\n\nCopy\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n    - address: 10.68.182.1/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nlinks:\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      mtu: 0\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\nroutes:\n\n    - family: inet4\n\n      gateway: 147.75.61.42\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 1024\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nhostnames:\n\n    - hostname: ci-blue-worker-amd64-2\n\n      layer: platform\n\nresolvers: []\n\ntimeServers: []\n\n\nEvery section is optional, so you can configure only the parts you need. The format of each section matches the respective network *Spec resource .spec part, e.g the addresses: section matches the .spec of AddressSpec resource:\n\nCopy\n# talosctl get addressspecs bond0/10.68.182.1/31 -o yaml | yq .spec\n\naddress: 10.68.182.1/31\n\nlinkName: bond0\n\nfamily: inet4\n\nscope: global\n\nflags: permanent\n\nlayer: platform\n\n\nSo one way to prepare the network configuration file is to boot Talos Linux, apply necessary network configuration using Talos machine configuration, and grab the resulting resources from the running Talos instance.\n\nIn this guide we will briefly cover the most common examples of the network configuration.\n\nAddresses\n\nThe addresses configured are usually routable IP addresses assigned to the machine, so the scope: should be set to global and flags: to permanent. Additionally, family: should be set to either inet4 or init6 depending on the address family.\n\nThe linkName: property should match the name of the link the address is assigned to, it might be a physical link, e.g. en9sp0, or the name of a logical link, e.g. bond0, created in the links: section.\n\nExample, IPv4 address:\n\nCopy\naddresses:\n\n    - address: 147.75.61.43/31\n\n      linkName: bond0\n\n      family: inet4\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\n\nExample, IPv6 address:\n\nCopy\naddresses:\n\n    - address: 2604:1380:45f2:6c00::1/127\n\n      linkName: bond0\n\n      family: inet6\n\n      scope: global\n\n      flags: permanent\n\n      layer: platform\n\nLinks\n\nFor physical network interfaces (links), the most usual configuration is to bring the link up:\n\nCopy\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      layer: platform\n\n\nThis will bring the link up, and it will also disable Talos auto-configuration (disables running DHCP on the link).\n\nAnother common case is to set a custom MTU:\n\nCopy\nlinks:\n\n    - name: en9sp0\n\n      up: true\n\n      mtu: 9000\n\n      layer: platform\n\n\nThe order of the links in the links: section is not important.\n\nBonds\n\nFor bonded links, there should be a link resource for the bond itself, and a link resource for each enslaved link:\n\nCopy\nlinks:\n\n    - name: bond0\n\n      logical: true\n\n      up: true\n\n      kind: bond\n\n      type: ether\n\n      bondMaster:\n\n        mode: 802.3ad\n\n        xmitHashPolicy: layer3+4\n\n        lacpRate: slow\n\n        arpValidate: none\n\n        arpAllTargets: any\n\n        primaryReselect: always\n\n        failOverMac: 0\n\n        miimon: 100\n\n        updelay: 200\n\n        downdelay: 200\n\n        resendIgmp: 1\n\n        lpInterval: 1\n\n        packetsPerSlave: 1\n\n        numPeerNotif: 1\n\n        tlbLogicalLb: 1\n\n        adActorSysPrio: 65535\n\n      layer: platform\n\n    - name: eth0\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 0\n\n      layer: platform\n\n    - name: eth1\n\n      up: true\n\n      masterName: bond0\n\n      slaveIndex: 1\n\n      layer: platform\n\n\nThe name of the bond can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: bond - this is a bonded link\ntype: ether - this is an Ethernet link\nbondMaster: - defines bond configuration, please see Linux documentation on the available options\n\nFor each enslaved link, the following properties are important:\n\nmasterName: bond0 - the name of the bond this link is enslaved to\nslaveIndex: 0 - the index of the enslaved link, starting from 0, controls the order of bond slaves\nVLANs\n\nVLANs are logical links which have a parent link, and a VLAN ID and protocol:\n\nCopy\nlinks:\n\n    - name: bond0.35\n\n      logical: true\n\n      up: true\n\n      kind: vlan\n\n      type: ether\n\n      parentName: bond0\n\n      vlan:\n\n        vlanID: 35\n\n        vlanProtocol: 802.1ad\n\n\nThe name of the VLAN link can be anything supported by Linux kernel, but the following properties are important:\n\nlogical: true - this is a logical link, not a physical one\nkind: vlan - this is a VLAN link\ntype: ether - this is an Ethernet link\nparentName: bond0 - the name of the parent link\nvlan: - defines VLAN configuration: vlanID and vlanProtocol\nRoutes\n\nFor route configuration, most of the time table: main, scope: global, type: unicast and protocol: static are used.\n\nThe route most important fields are:\n\ndst: defines the destination network, if left empty means “default gateway”\ngateway: defines the gateway address\npriority: defines the route priority (metric), lower values are preferred for the same dst: network\noutLinkName: defines the name of the link the route is associated with\nsrc: sets the source address for the route (optional)\n\nAdditionally, family: should be set to either inet4 or init6 depending on the address family.\n\nExample, IPv6 default gateway:\n\nCopy\nroutes:\n\n    - family: inet6\n\n      gateway: '2604:1380:45f2:6c00::'\n\n      outLinkName: bond0\n\n      table: main\n\n      priority: 2048\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\n\nExample, IPv4 route to 10/8 via 10.68.182.0 gateway:\n\nCopy\nroutes:\n\n    - family: inet4\n\n      dst: 10.0.0.0/8\n\n      gateway: 10.68.182.0\n\n      outLinkName: bond0\n\n      table: main\n\n      scope: global\n\n      type: unicast\n\n      protocol: static\n\n      layer: platform\n\nHostnames\n\nEven though the section supports multiple hostnames, only a single one should be used:\n\nCopy\nhostnames:\n\n    - hostname: host\n\n      domainname: some.org\n\n      layer: platform\n\n\nThe domainname: is optional.\n\nIf the hostname is not set, Talos will use default generated hostname.\n\nResolvers\n\nThe resolvers: section is used to configure DNS resolvers, only single entry should be used:\n\nCopy\nresolvers:\n\n    - dnsServers:\n\n        - 8.8.8.8\n\n        - 1.1.1.1\n\n      layer: platform\n\n\nIf the dnsServers: is not set, Talos will use default DNS servers.\n\nTime Servers\n\nThe timeServers: section is used to configure NTP time servers, only single entry should be used:\n\nCopy\ntimeServers:\n\n    - timeServers:\n\n        - 169.254.169.254\n\n      layer: platform\n\n\nIf the timeServers: is not set, Talos will use default NTP servers.\n\nSupplying META Network Configuration\n\nOnce the network configuration YAML document is ready, it can be supplied to Talos in one of the following ways:\n\nfor a running Talos machine, using Talos API (requires already established network connectivity)\nfor Talos disk images, it can be embedded into the image\nfor ISO/PXE boot methods, it can be supplied via kernel command line parameters as an environment variable\n\nThe metal network configuration is stored in Talos META partition under the key 0xa (decimal 10).\n\nIn this guide we will assume that the prepared network configuration is stored in the file network.yaml.\n\nNote: as JSON is a subset of YAML, the network configuration can be also supplied as a JSON document.\n\nSupplying Network Configuration to a Running Talos Machine\n\nUse the talosctl to write a network configuration to a running Talos machine:\n\nCopy\ntalosctl meta write 0xa \"$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos Disk Image\n\nFollowing the boot assets guide, create a disk image passing the network configuration as a --meta flag:\n\nCopy\ndocker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 metal --meta \"0xa=$(cat network.yaml)\"\n\nSupplying Network Configuration to a Talos ISO/PXE Boot\n\nAs there is no META partition created yet before Talos Linux is installed, META values can be set as an environment variable INSTALLER_META_BASE64 passed to the initial boot of Talos. The supplied value will be used immediately, and also it will be written to the META partition once Talos is installed.\n\nWhen using imager to create the ISO, the INSTALLER_META_BASE64 environment variable will be automatically generated from the --meta flag:\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --meta \"0xa=$(cat network.yaml)\"\n\n...\n\nkernel command line: ... talos.environment=INSTALLER_META_BASE64=MHhhPWZvbw==\n\n\nWhen PXE booting, the value of INSTALLER_META_BASE64 should be set manually:\n\nCopy\necho -n \"0xa=$(cat network.yaml)\" | base64\n\n\nThe resulting base64 string should be passed as an environment variable INSTALLER_META_BASE64 to the initial boot of Talos: talos.environment=INSTALLER_META_BASE64=<base64-encoded value>.\n\nGetting Current META Network Configuration\n\nTalos exports META keys as resources:\n\nCopy\n# talosctl get meta 0x0a -o yaml\n\n...\n\nspec:\n\n    value: '{\"addresses\": ...}'\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Machine Configuration OAuth2 Authentication | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/machine-config-oauth/",
    "html": "Documentation\nAdvanced Guides\nMachine Configuration OAuth2 Authentication\nMachine Configuration OAuth2 Authentication\nHow to authenticate Talos machine configuration download (talos.config=) on metal platform using OAuth.\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow. The machine configuration is fetched from the URL specified with talos.config kernel argument, and by default this HTTP request is not authenticated. When the OAuth2 authentication is enabled, Talos will authenticate the request using OAuth device flow first, and then pass the token to the machine configuration download endpoint.\n\nPrerequisites\n\nObtain the following information:\n\nOAuth client ID (mandatory)\nOAuth client secret (optional)\nOAuth device endpoint\nOAuth token endpoint\nOAuth scopes, audience (optional)\nOAuth client secret (optional)\nextra Talos variables to send to the device auth endpoint (optional)\nConfiguration\n\nSet the following kernel parameters on the initial Talos boot to enable the OAuth flow:\n\ntalos.config set to the URL of the machine configuration endpoint (which will be authenticated using OAuth)\ntalos.config.oauth.client_id set to the OAuth client ID (required)\ntalos.config.oauth.client_secret set to the OAuth client secret (optional)\ntalos.config.oauth.scope set to the OAuth scopes (optional, repeat the parameter for multiple scopes)\ntalos.config.oauth.audience set to the OAuth audience (optional)\ntalos.config.oauth.device_auth_url set to the OAuth device endpoint (if not set defaults to talos.config URL with the path /device/code)\ntalos.config.oauth.token_url set to the OAuth token endpoint (if not set defaults to talos.config URL with the path /token)\ntalos.config.oauth.extra_variable set to the extra Talos variables to send to the device auth endpoint (optional, repeat the parameter for multiple variables)\n\nThe list of variables supported by the talos.config.oauth.extra_variable parameter is same as the list of variables supported by the talos.config parameter.\n\nFlow\n\nOn the initial Talos boot, when machine configuration is not available, Talos will print the following messages:\n\nCopy\n[talos] downloading config {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"platform\": \"metal\"}\n\n[talos] waiting for network to be ready\n\n[talos] [OAuth] starting the authentication device flow with the following settings:\n\n[talos] [OAuth]  - client ID: \"<REDACTED>\"\n\n[talos] [OAuth]  - device auth URL: \"https://oauth2.googleapis.com/device/code\"\n\n[talos] [OAuth]  - token URL: \"https://oauth2.googleapis.com/token\"\n\n[talos] [OAuth]  - extra variables: [\"uuid\" \"mac\"]\n\n[talos] waiting for variables: [uuid mac]\n\n[talos] waiting for variables: [mac]\n\n[talos] [OAuth] please visit the URL https://www.google.com/device and enter the code <REDACTED>\n\n[talos] [OAuth] waiting for the device to be authorized (expires at 14:46:55)...\n\n\nIf the OAuth service provides the complete verification URL, the QR code to scan is also printed to the console:\n\nCopy\n[talos] [OAuth] or scan the following QR code:\n\n█████████████████████████████████\n\n█████████████████████████████████\n\n████ ▄▄▄▄▄ ██▄▀▀    ▀█ ▄▄▄▄▄ ████\n\n████ █   █ █▄  ▀▄██▄██ █   █ ████\n\n████ █▄▄▄█ ██▀▄██▄  ▀█ █▄▄▄█ ████\n\n████▄▄▄▄▄▄▄█ ▀ █ ▀ █▄█▄▄▄▄▄▄▄████\n\n████   ▀ ▄▄ ▄█  ██▄█   ███▄█▀████\n\n████▀█▄  ▄▄▀▄▄█▀█▄██ ▄▀▄██▄ ▄████\n\n████▄██▀█▄▄▄███▀ ▀█▄▄  ██ █▄ ████\n\n████▄▀▄▄▄ ▄███ ▄ ▀ ▀▀▄▀▄▀█▄ ▄████\n\n████▄█████▄█  █ ██ ▀ ▄▄▄  █▀▀████\n\n████ ▄▄▄▄▄ █ █ ▀█▄█▄ █▄█  █▄ ████\n\n████ █   █ █▄ ▄▀ ▀█▀▄▄▄   ▀█▄████\n\n████ █▄▄▄█ █ ██▄ ▀  ▀███ ▀█▀▄████\n\n████▄▄▄▄▄▄▄█▄▄█▄██▄▄▄▄█▄███▄▄████\n\n█████████████████████████████████\n\n\nOnce the authentication flow is complete on the OAuth provider side, Talos will print the following message:\n\nCopy\n[talos] [OAuth] device authorized\n\n[talos] fetching machine config from: \"http://example.com/config.yaml\"\n\n[talos] machine config loaded successfully {\"component\": \"controller-runtime\", \"controller\": \"config.AcquireController\", \"sources\": [\"metal\"]}\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Extension Services | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/extension-services/",
    "html": "Configuration\nname\ncontainer\ncontainer.mounts\ncontainer.security\ndepends\nrestart\nExample\nDocumentation\nAdvanced Guides\nExtension Services\nExtension Services\nUse extension services in Talos Linux.\n\nTalos provides a way to run additional system services early in the Talos boot process. Extension services should be included into the Talos root filesystem (e.g. using system extensions). Extension services run as privileged containers with ephemeral root filesystem located in the Talos root filesystem.\n\nExtension services can be used to use extend core features of Talos in a way that is not possible via static pods or Kubernetes DaemonSets.\n\nPotential extension services use-cases:\n\nstorage: Open iSCSI, software RAID, etc.\nnetworking: BGP FRR, etc.\nplatform integration: VMWare open VM tools, etc.\nConfiguration\n\nTalos on boot scans directory /usr/local/etc/containers for *.yaml files describing the extension services to run. Format of the extension service config:\n\nCopy\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello-world\n\n  # an optional path to a file containing environment variables\n\n  environmentFile: /var/etc/hello-world/env\n\n  environment:\n\n    - XDG_RUNTIME_DIR=/run\n\n  args:\n\n     - -f\n\n  mounts:\n\n     - # OCI Mount Spec\n\ndepends:\n\n   - service: cri\n\n   - path: /run/machined/machined.sock\n\n   - network:\n\n       - addresses\n\n       - connectivity\n\n       - hostname\n\n       - etcfiles\n\n   - time: true\n\nrestart: never|always|untilSuccess\n\nname\n\nField name sets the service name, valid names are [a-z0-9-_]+. The service container root filesystem path is derived from the name: /usr/local/lib/containers/<name>. The extension service will be registered as a Talos service under an ext-<name> identifier.\n\ncontainer\nentrypoint defines the container entrypoint relative to the container root filesystem (/usr/local/lib/containers/<name>)\nenvironmentFile defines the path to a file containing environment variables, the service waits for the file to exist before starting\nenvironment defines the container environment variables, overrides the variables from environmentFile\nargs defines the additional arguments to pass to the entrypoint\nmounts defines the volumes to be mounted into the container root\ncontainer.mounts\n\nThe section mounts uses the standard OCI spec:\n\nCopy\n- source: /var/log/audit\n\n  destination: /var/log/audit\n\n  type: bind\n\n  options:\n\n    - rshared\n\n    - bind\n\n    - ro\n\n\nAll requested directories will be mounted into the extension service container mount namespace. If the source directory doesn’t exist in the host filesystem, it will be created (only for writable paths in the Talos root filesystem).\n\ncontainer.security\n\nThe section security follows this example:\n\nCopy\nmaskedPaths:\n\n  - \"/should/be/masked\"\n\nreadonlyPaths:\n\n  - \"/path/that/should/be/readonly\"\n\n  - \"/another/readonly/path\"\n\nwriteableRootfs: true\n\nwriteableSysfs: true\n\nrootfsPropagation: shared\n\nThe rootfs is readonly by default unless writeableRootfs: true is set.\nThe sysfs is readonly by default unless writeableSysfs: true is set.\nMasked paths if not set defaults to containerd defaults. Masked paths will be mounted to /dev/null. To set empty masked paths use:\nCopy\ncontainer:\n\n  security:\n\n    maskedPaths: []\n\nRead Only paths if not set defaults to containerd defaults. Read-only paths will be mounted to /dev/null. To set empty read only paths use:\nCopy\ncontainer:\n\n  security:\n\n    readonlyPaths: []\n\nRootfs propagation is not set by default (container mounts are private).\ndepends\n\nThe depends section describes extension service start dependencies: the service will not be started until all dependencies are met.\n\nAvailable dependencies:\n\nservice: <name>: wait for the service <name> to be running and healthy\npath: <path>: wait for the <path> to exist\nnetwork: [addresses, connectivity, hostname, etcfiles]: wait for the specified network readiness checks to succeed\ntime: true: wait for the NTP time sync\nrestart\n\nField restart defines the service restart policy, it allows to either configure an always running service or a one-shot service:\n\nalways: restart service always\nnever: start service only once and never restart\nuntilSuccess: restart failing service, stop restarting on successful run\nExample\n\nExample layout of the Talos root filesystem contents for the extension service:\n\nCopy\n/\n\n└── usr\n\n    └── local\n\n        ├── etc\n\n        │   └── containers\n\n        │       └── hello-world.yaml\n\n        └── lib\n\n            └── containers\n\n                └── hello-world\n\n                    ├── hello\n\n                    └── config.ini\n\n\nTalos discovers the extension service configuration in /usr/local/etc/containers/hello-world.yaml:\n\nCopy\nname: hello-world\n\ncontainer:\n\n  entrypoint: ./hello\n\n  args:\n\n    - --config\n\n    - config.ini\n\ndepends:\n\n  - network:\n\n    - addresses\n\nrestart: always\n\n\nTalos starts the container for the extension service with container root filesystem at /usr/local/lib/containers/hello-world:\n\nCopy\n/\n\n├── hello\n\n└── config.ini\n\n\nExtension service is registered as ext-hello-world in talosctl services:\n\nCopy\n$ talosctl service ext-hello-world\n\nNODE     172.20.0.5\n\nID       ext-hello-world\n\nSTATE    Running\n\nHEALTH   ?\n\nEVENTS   [Running]: Started task ext-hello-world (PID 1100) for container ext-hello-world (2m47s ago)\n\n         [Preparing]: Creating service runner (2m47s ago)\n\n         [Preparing]: Running pre state (2m47s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\" (2m48s ago)\n\n         [Waiting]: Waiting for service \"containerd\" to be \"up\", network (2m49s ago)\n\n\nAn extension service can be started, restarted and stopped using talosctl service ext-hello-world start|restart|stop. Use talosctl logs ext-hello-world to get the logs of the service.\n\nComplete example of the extension service can be found in the extensions repository.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "etcd Maintenance | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/etcd-maintenance/",
    "html": "Space Quota\nDefragmentation\nSnapshotting\nDocumentation\nAdvanced Guides\netcd Maintenance\netcd Maintenance\nOperational instructions for etcd database.\n\netcd database backs Kubernetes control plane state, so etcd health is critical for Kubernetes availability.\n\nSpace Quota\n\netcd default database space quota is set to 2 GiB by default. If the database size exceeds the quota, etcd will stop operations until the issue is resolved.\n\nThis condition can be checked with talosctl etcd alarm list command:\n\nCopy\n$ talosctl -n <IP> etcd alarm list\n\nNODE         MEMBER             ALARM\n\n172.20.0.2   a49c021e76e707db   NOSPACE\n\n\nIf the Kubernetes database contains lots of resources, space quota can be increased to match the actual usage. The recommended maximum size is 8 GiB.\n\nTo increase the space quota, edit the etcd section in the machine configuration:\n\nCopy\nmachine:\n\n  etcd:\n\n    extraArgs:\n\n      quota-backend-bytes: 4294967296 # 4 GiB\n\n\nOnce the node is rebooted with the new configuration, use talosctl etcd alarm disarm to clear the NOSPACE alarm.\n\nDefragmentation\n\netcd database can become fragmented over time if there are lots of writes and deletes. Kubernetes API server performs automatic compaction of the etcd database, which marks deleted space as free and ready to be reused. However, the space is not actually freed until the database is defragmented.\n\nIf the database is heavily fragmented (in use/db size ratio is less than 0.5), defragmentation might increase the performance. If the database runs over the space quota (see above), but the actual in use database size is small, defragmentation is required to bring the on-disk database size below the limit.\n\nCurrent database size can be checked with talosctl etcd status command:\n\nCopy\n$ talosctl -n <CP1>,<CP2>,<CP3> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE            LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.3   ecebb05b59a776f1   21 MB     6.0 MB (29.08%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.2   a49c021e76e707db   17 MB     4.5 MB (26.10%)   ecebb05b59a776f1   53391        4           53391                false\n\n172.20.0.4   eb47fb33e59bf0e2   20 MB     5.9 MB (28.96%)   ecebb05b59a776f1   53391        4           53391                false\n\n\nIf any of the nodes are over database size quota, alarms will be printed in the ERRORS column.\n\nTo defragment the database, run talosctl etcd defrag command:\n\nCopy\ntalosctl -n <CP1> etcd defrag\n\n\nNote: defragmentation is a resource-intensive operation, so it is recommended to run it on a single node at a time. Defragmentation to a live member blocks the system from reading and writing data while rebuilding its state.\n\nOnce the defragmentation is complete, the database size will match closely to the in use size:\n\nCopy\n$ talosctl -n <CP1> etcd status\n\nNODE         MEMBER             DB SIZE   IN USE             LEADER             RAFT INDEX   RAFT TERM   RAFT APPLIED INDEX   LEARNER   ERRORS\n\n172.20.0.2   a49c021e76e707db   4.5 MB    4.5 MB (100.00%)   ecebb05b59a776f1   56065        4           56065                false\n\nSnapshotting\n\nRegular backups of etcd database should be performed to ensure that the cluster can be restored in case of a failure. This procedure is described in the disaster recovery guide.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Disaster Recovery | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/disaster-recovery/",
    "html": "Backup\nSnapshotting etcd Database\nDisaster Database Snapshot\nMachine Configuration\nRecovery\nLatest Etcd Snapshot\nInit Node\nPreparing Control Plane Nodes\nRecovering from the Backup\nSingle Control Plane Node Cluster\nDocumentation\nAdvanced Guides\nDisaster Recovery\nDisaster Recovery\nProcedure for snapshotting etcd database and recovering from catastrophic control plane failure.\n\netcd database backs Kubernetes control plane state, so if the etcd service is unavailable, the Kubernetes control plane goes down, and the cluster is not recoverable until etcd is recovered. etcd builds around the consensus protocol Raft, so highly-available control plane clusters can tolerate the loss of nodes so long as more than half of the members are running and reachable. For a three control plane node Talos cluster, this means that the cluster tolerates a failure of any single node, but losing more than one node at the same time leads to complete loss of service. Because of that, it is important to take routine backups of etcd state to have a snapshot to recover the cluster from in case of catastrophic failure.\n\nBackup\nSnapshotting etcd Database\n\nCreate a consistent snapshot of etcd database with talosctl etcd snapshot command:\n\nCopy\n$ talosctl -n <IP> etcd snapshot db.snapshot\n\netcd snapshot saved to \"db.snapshot\" (2015264 bytes)\n\nsnapshot info: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: filename db.snapshot is arbitrary.\n\nThis database snapshot can be taken on any healthy control plane node (with IP address <IP> in the example above), as all etcd instances contain exactly same data. It is recommended to configure etcd snapshots to be created on some schedule to allow point-in-time recovery using the latest snapshot.\n\nDisaster Database Snapshot\n\nIf the etcd cluster is not healthy (for example, if quorum has already been lost), the talosctl etcd snapshot command might fail. In that case, copy the database snapshot directly from the control plane node:\n\nCopy\ntalosctl -n <IP> cp /var/lib/etcd/member/snap/db .\n\n\nThis snapshot might not be fully consistent (if the etcd process is running), but it allows for disaster recovery when latest regular snapshot is not available.\n\nMachine Configuration\n\nMachine configuration might be required to recover the node after hardware failure. Backup Talos node machine configuration with the command:\n\nCopy\ntalosctl -n IP get mc v1alpha1 -o yaml | yq eval '.spec' -\n\nRecovery\n\nBefore starting a disaster recovery procedure, make sure that etcd cluster can’t be recovered:\n\nget etcd cluster member list on all healthy control plane nodes with talosctl -n IP etcd members command and compare across all members.\nquery etcd health across control plane nodes with talosctl -n IP service etcd.\n\nIf the quorum can be restored, restoring quorum might be a better strategy than performing full disaster recovery procedure.\n\nLatest Etcd Snapshot\n\nGet hold of the latest etcd database snapshot. If a snapshot is not fresh enough, create a database snapshot (see above), even if the etcd cluster is unhealthy.\n\nInit Node\n\nMake sure that there are no control plane nodes with machine type init:\n\nCopy\n$ talosctl -n <IP1>,<IP2>,... get machinetype\n\nNODE         NAMESPACE   TYPE          ID             VERSION   TYPE\n\n172.20.0.2   config      MachineType   machine-type   2         controlplane\n\n172.20.0.4   config      MachineType   machine-type   2         controlplane\n\n172.20.0.3   config      MachineType   machine-type   2         controlplane\n\n\nInit node type is deprecated, and are incompatible with etcd recovery procedure. init node can be converted to controlplane type with talosctl edit mc --mode=staged command followed by node reboot with talosctl reboot command.\n\nPreparing Control Plane Nodes\n\nIf some control plane nodes experienced hardware failure, replace them with new nodes.\n\nUse machine configuration backup to re-create the nodes with the same secret material and control plane settings to allow workers to join the recovered control plane.\n\nIf a control plane node is up but etcd isn’t, wipe the node’s EPHEMERAL partition to remove the etcd data directory (make sure a database snapshot is taken before doing this):\n\nCopy\ntalosctl -n <IP> reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL\n\n\nAt this point, all control plane nodes should boot up, and etcd service should be in the Preparing state.\n\nThe Kubernetes control plane endpoint should be pointed to the new control plane nodes if there were changes to the node addresses.\n\nRecovering from the Backup\n\nMake sure all etcd service instances are in Preparing state:\n\nCopy\n$ talosctl -n <IP> service etcd\n\nNODE     172.20.0.2\n\nID       etcd\n\nSTATE    Preparing\n\nHEALTH   ?\n\nEVENTS   [Preparing]: Running pre state (17s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", time sync (18s ago)\n\n         [Waiting]: Waiting for service \"cri\" to be \"up\", service \"networkd\" to be \"up\", time sync (20s ago)\n\n\nExecute the bootstrap command against any control plane node passing the path to the etcd database snapshot:\n\nCopy\n$ talosctl -n <IP> bootstrap --recover-from=./db.snapshot\n\nrecovering from snapshot \"./db.snapshot\": hash c25fd181, revision 4193, total keys 1287, total size 3035136\n\n\nNote: if database snapshot was copied out directly from the etcd data directory using talosctl cp, add flag --recover-skip-hash-check to skip integrity check on restore.\n\nTalos node should print matching information in the kernel log:\n\nrecovering etcd from snapshot: hash c25fd181, revision 4193, total keys 1287, total size 3035136\n{\"level\":\"info\",\"msg\":\"restoring snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/li}\n{\"level\":\"info\",\"msg\":\"restored last compact revision\",\"meta-bucket-name\":\"meta\",\"meta-bucket-name-key\":\"finishedCompactRev\",\"restored-compact-revision\":3360}\n{\"level\":\"info\",\"msg\":\"added member\",\"cluster-id\":\"a3390e43eb5274e2\",\"local-member-id\":\"0\",\"added-peer-id\":\"eb4f6f534361855e\",\"added-peer-peer-urls\":[\"https:/}\n{\"level\":\"info\",\"msg\":\"restored snapshot\",\"path\":\"/var/lib/etcd.snapshot\",\"wal-dir\":\"/var/lib/etcd/member/wal\",\"data-dir\":\"/var/lib/etcd\",\"snap-dir\":\"/var/lib/etcd/member/snap\"}\n\n\nNow etcd service should become healthy on the bootstrap node, Kubernetes control plane components should start and control plane endpoint should become available. Remaining control plane nodes join etcd cluster once control plane endpoint is up.\n\nSingle Control Plane Node Cluster\n\nThis guide applies to the single control plane clusters as well. In fact, it is much more important to take regular snapshots of the etcd database in single control plane node case, as loss of the control plane node might render the whole cluster irrecoverable without a backup.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Developing Talos | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/developing-talos/",
    "html": "Prepare\nRunning Talos cluster\nConsole Logs\nInteracting with Talos\nQuick Reboot\nDevelopment Cycle\nRunning Integration Tests\nBuild Flavors\nDestroying Cluster\nOptional\nUnit tests\nGo Profiling\nTesting Air-gapped Environments\nRunning Upgrade Integration Tests\nDocumentation\nAdvanced Guides\nDeveloping Talos\nDeveloping Talos\nLearn how to set up a development environment for local testing and hacking on Talos itself!\n\nThis guide outlines steps and tricks to develop Talos operating systems and related components. The guide assumes Linux operating system on the development host. Some steps might work under Mac OS X, but using Linux is highly advised.\n\nPrepare\n\nCheck out the Talos repository.\n\nTry running make help to see available make commands. You would need Docker and buildx installed on the host.\n\nNote: Usually it is better to install up to date Docker from Docker apt repositories, e.g. Ubuntu instructions.\n\nIf buildx plugin is not available with OS docker packages, it can be installed as a plugin from GitHub releases.\n\nSet up a builder with access to the host network:\n\nCopy\n docker buildx create --driver docker-container  --driver-opt network=host --name local1 --buildkitd-flags '--allow-insecure-entitlement security.insecure' --use\n\n\nNote: network=host allows buildx builder to access host network, so that it can push to a local container registry (see below).\n\nMake sure the following steps work:\n\nmake talosctl\nmake initramfs kernel\n\nSet up a local docker registry:\n\nCopy\ndocker run -d -p 5005:5000 \\\n\n    --restart always \\\n\n    --name local registry:2\n\n\nTry to build and push to local registry an installer image:\n\nCopy\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRecord the image name output in the step above.\n\nNote: it is also possible to force a stable image tag by using TAG variable: make installer IMAGE_REGISTRY=127.0.0.1:5005 TAG=v1.0.0-alpha.1 PUSH=true.\n\nRunning Talos cluster\n\nSet up local caching docker registries (this speeds up Talos cluster boot a lot), script is in the Talos repo:\n\nCopy\nbash hack/start-registry-proxies.sh\n\n\nStart your local cluster with:\n\nCopy\nsudo --preserve-env=HOME _out/talosctl-linux-amd64 cluster create \\\n\n    --provisioner=qemu \\\n\n    --cidr=172.20.0.0/24 \\\n\n    --registry-mirror docker.io=http://172.20.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.20.0.1:5001  \\\n\n    --registry-mirror gcr.io=http://172.20.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.20.0.1:5004 \\\n\n    --registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005 \\\n\n    --install-image=127.0.0.1:5005/siderolabs/installer:<RECORDED HASH from the build step> \\\n\n    --controlplanes 3 \\\n\n    --workers 2 \\\n\n    --with-bootloader=false\n\n--provisioner selects QEMU vs. default Docker\ncustom --cidr to make QEMU cluster use different network than default Docker setup (optional)\n--registry-mirror uses the caching proxies set up above to speed up boot time a lot, last one adds your local registry (installer image was pushed to it)\n--install-image is the image you built with make installer above\n--controlplanes & --workers configure cluster size, choose to match your resources; 3 controlplanes give you HA control plane; 1 controlplane is enough, never do 2 controlplanes\n--with-bootloader=false disables boot from disk (Talos will always boot from _out/vmlinuz-amd64 and _out/initramfs-amd64.xz). This speeds up development cycle a lot - no need to rebuild installer and perform install, rebooting is enough to get new code.\n\nNote: as boot loader is not used, it’s not necessary to rebuild installer each time (old image is fine), but sometimes it’s needed (when configuration changes are done and old installer doesn’t validate the config).\n\ntalosctl cluster create derives Talos machine configuration version from the install image tag, so sometimes early in the development cycle (when new minor tag is not released yet), machine config version can be overridden with --talos-version=v1.6.\n\nIf the --with-bootloader=false flag is not enabled, for Talos cluster to pick up new changes to the code (in initramfs), it will require a Talos upgrade (so new installer should be built). With --with-bootloader=false flag, Talos always boots from initramfs in _out/ directory, so simple reboot is enough to pick up new code changes.\n\nIf the installation flow needs to be tested, --with-bootloader=false shouldn’t be used.\n\nConsole Logs\n\nWatching console logs is easy with tail:\n\nCopy\ntail -F ~/.talos/clusters/talos-default/talos-default-*.log\n\nInteracting with Talos\n\nOnce talosctl cluster create finishes successfully, talosconfig and kubeconfig will be set up automatically to point to your cluster.\n\nStart playing with talosctl:\n\nCopy\ntalosctl -n 172.20.0.2 version\n\ntalosctl -n 172.20.0.3,172.20.0.4 dashboard\n\ntalosctl -n 172.20.0.4 get members\n\n\nSame with kubectl:\n\nCopy\nkubectl get nodes -o wide\n\n\nYou can deploy some Kubernetes workloads to the cluster.\n\nYou can edit machine config on the fly with talosctl edit mc --immediate, config patches can be applied via --config-patch flags, also many features have specific flags in talosctl cluster create.\n\nQuick Reboot\n\nTo reboot whole cluster quickly (e.g. to pick up a change made in the code):\n\nCopy\nfor socket in ~/.talos/clusters/talos-default/talos-default-*.monitor; do echo \"q\" | sudo socat - unix-connect:$socket; done\n\n\nSending q to a single socket allows to reboot a single node.\n\nNote: This command performs immediate reboot (as if the machine was powered down and immediately powered back up), for normal Talos reboot use talosctl reboot.\n\nDevelopment Cycle\n\nFast development cycle:\n\nbring up a cluster\nmake code changes\nrebuild initramfs with make initramfs\nreboot a node to pick new initramfs\nverify code changes\nmore code changes…\n\nSome aspects of Talos development require to enable bootloader (when working on installer itself), in that case quick development cycle is no longer possible, and cluster should be destroyed and recreated each time.\n\nRunning Integration Tests\n\nIf integration tests were changed (or when running them for the first time), first rebuild the integration test binary:\n\nCopy\nrm -f  _out/integration-test-linux-amd64; make _out/integration-test-linux-amd64\n\n\nRunning short tests against QEMU provisioned cluster:\n\nCopy\n_out/integration-test-linux-amd64 \\\n\n    -talos.provisioner=qemu \\\n\n    -test.v \\\n\n    -talos.crashdump=false \\\n\n    -test.short \\\n\n    -talos.talosctlpath=$PWD/_out/talosctl-linux-amd64\n\n\nWhole test suite can be run removing -test.short flag.\n\nSpecfic tests can be run with -test.run=TestIntegration/api.ResetSuite.\n\nBuild Flavors\n\nmake <something> WITH_RACE=1 enables Go race detector, Talos runs slower and uses more memory, but memory races are detected.\n\nmake <something> WITH_DEBUG=1 enables Go profiling and other debug features, useful for local development.\n\nDestroying Cluster\nCopy\nsudo --preserve-env=HOME ../talos/_out/talosctl-linux-amd64 cluster destroy --provisioner=qemu\n\n\nThis command stops QEMU and helper processes, tears down bridged network on the host, and cleans up cluster state in ~/.talos/clusters.\n\nNote: if the host machine is rebooted, QEMU instances and helpers processes won’t be started back. In that case it’s required to clean up files in ~/.talos/clusters/<cluster-name> directory manually.\n\nOptional\n\nSet up cross-build environment with:\n\nCopy\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n\n\nNote: the static qemu binaries which come with Ubuntu 21.10 seem to be broken.\n\nUnit tests\n\nUnit tests can be run in buildx with make unit-tests, on Ubuntu systems some tests using loop devices will fail because Ubuntu uses low-index loop devices for snaps.\n\nMost of the unit-tests can be run standalone as well, with regular go test, or using IDE integration:\n\nCopy\ngo test -v ./internal/pkg/circular/\n\n\nThis provides much faster feedback loop, but some tests require either elevated privileges (running as root) or additional binaries available only in Talos rootfs (containerd tests).\n\nRunning tests as root can be done with -exec flag to go test, but this is risky, as test code has root access and can potentially make undesired changes:\n\nCopy\ngo test -exec sudo  -v ./internal/app/machined/pkg/controllers/network/...\n\nGo Profiling\n\nBuild initramfs with debug enabled: make initramfs WITH_DEBUG=1.\n\nLaunch Talos cluster with bootloader disabled, and use go tool pprof to capture the profile and show the output in your browser:\n\nCopy\ngo tool pprof http://172.20.0.2:9982/debug/pprof/heap\n\n\nThe IP address 172.20.0.2 is the address of the Talos node, and port :9982 depends on the Go application to profile:\n\n9981: apid\n9982: machined\n9983: trustd\nTesting Air-gapped Environments\n\nThere is a hidden talosctl debug air-gapped command which launches two components:\n\nHTTP proxy capable of proxying HTTP and HTTPS requests\nHTTPS server with a self-signed certificate\n\nThe command also writes down Talos machine configuration patch to enable the HTTP proxy and add a self-signed certificate to the list of trusted certificates:\n\nCopy\n$ talosctl debug air-gapped --advertised-address 172.20.0.1\n\n2022/08/04 16:43:14 writing config patch to air-gapped-patch.yaml\n\n2022/08/04 16:43:14 starting HTTP proxy on :8002\n\n2022/08/04 16:43:14 starting HTTPS server with self-signed cert on :8001\n\n\nThe --advertised-address should match the bridge IP of the Talos node.\n\nGenerated machine configuration patch looks like:\n\nCopy\nmachine:\n\n    files:\n\n        - content: |\n\n            -----BEGIN CERTIFICATE-----\n\n            MIIBijCCAS+gAwIBAgIBATAKBggqhkjOPQQDAjAUMRIwEAYDVQQKEwlUZXN0IE9u\n\n            bHkwHhcNMjIwODA0MTI0MzE0WhcNMjIwODA1MTI0MzE0WjAUMRIwEAYDVQQKEwlU\n\n            ZXN0IE9ubHkwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQfOJdaOFSOI1I+EeP1\n\n            RlMpsDZJaXjFdoo5zYM5VYs3UkLyTAXAmdTi7JodydgLhty0pwLEWG4NUQAEvip6\n\n            EmzTo3IwcDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsG\n\n            AQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFCwxL+BjG0pDwaH8QgKW\n\n            Ex0J2mVXMA8GA1UdEQQIMAaHBKwUAAEwCgYIKoZIzj0EAwIDSQAwRgIhAJoW0z0D\n\n            JwpjFcgCmj4zT1SbBFhRBUX64PHJpAE8J+LgAiEAvfozZG8Or6hL21+Xuf1x9oh4\n\n            /4Hx3jozbSjgDyHOLk4=\n\n            -----END CERTIFICATE-----            \n\n          permissions: 0o644\n\n          path: /etc/ssl/certs/ca-certificates\n\n          op: append\n\n    env:\n\n        http_proxy: http://172.20.0.1:8002\n\n        https_proxy: http://172.20.0.1:8002\n\n        no_proxy: 172.20.0.1/24\n\ncluster:\n\n    extraManifests:\n\n        - https://172.20.0.1:8001/debug.yaml\n\n\nThe first section appends a self-signed certificate of the HTTPS server to the list of trusted certificates, followed by the HTTP proxy setup (in-cluster traffic is excluded from the proxy). The last section adds an extra Kubernetes manifest hosted on the HTTPS server.\n\nThe machine configuration patch can now be used to launch a test Talos cluster:\n\nCopy\ntalosctl cluster create ... --config-patch @air-gapped-patch.yaml\n\n\nThe following lines should appear in the output of the talosctl debug air-gapped command:\n\nCONNECT discovery.talos.dev:443: the HTTP proxy is used to talk to the discovery service\nhttp: TLS handshake error from 172.20.0.2:53512: remote error: tls: bad certificate: an expected error on Talos side, as self-signed cert is not written yet to the file\nGET /debug.yaml: Talos successfully fetches the extra manifest successfully\n\nThere might be more output depending on the registry caches being used or not.\n\nRunning Upgrade Integration Tests\n\nTalos has a separate set of provision upgrade tests, which create a cluster on older versions of Talos, perform an upgrade, and verify that the cluster is still functional.\n\nBuild the test binary:\n\nCopy\nrm -f  _out/integration-test-provision-linux-amd64; make _out/integration-test-provision-linux-amd64\n\n\nPrepare the test artifacts for the upgrade test:\n\nCopy\nmake release-artifacts\n\n\nBuild and push an installer image for the development version of Talos:\n\nCopy\nmake installer IMAGE_REGISTRY=127.0.0.1:5005 PUSH=true\n\n\nRun the tests (the tests will create the cluster on the older version of Talos, perform an upgrade, and verify that the cluster is still functional):\n\nCopy\nsudo --preserve-env=HOME _out/integration-test-provision-linux-amd64 \\\n\n    -test.v \\\n\n    -talos.talosctlpath _out/talosctl-linux-amd64 \\\n\n    -talos.provision.target-installer-registry=127.0.0.1:5005 \\\n\n    -talos.provision.registry-mirror 127.0.0.1:5005=http://172.20.0.1:5005,docker.io=http://172.20.0.1:5000,registry.k8s.io=http://172.20.0.1:5001,quay.io=http://172.20.0.1:5002,gcr.io=http://172.20.0.1:5003,ghcr.io=http://172.20.0.1:5004 \\\n\n    -talos.provision.cidr 172.20.0.0/24\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Customizing the Root Filesystem | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/customizing-the-root-filesystem/",
    "html": "Documentation\nAdvanced Guides\nCustomizing the Root Filesystem\nCustomizing the Root Filesystem\nHow to add your own content to the immutable root file system of Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nFor example, say we have an image that contains the contents of a library we wish to add to the Talos rootfs. We need to define a stage with the name customization:\n\nCopy\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nCopy\nFROM scratch AS customization\n\nCOPY --from=<name|index> <src> <dest>\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nNote: <dest> is the path relative to the rootfs that you wish to place the contents of <src>.\n\nTo build the image, run:\n\nCopy\ndocker build --squash -t <organization>/installer:latest .\n\n\nIn the case that you need to perform some cleanup before adding additional files to the rootfs, you can specify the RM build-time variable:\n\nCopy\ndocker build --squash --build-arg RM=\"[<path> ...]\" -t <organization>/installer:latest .\n\n\nThis will perform a rm -rf on the specified paths relative to the rootfs.\n\nNote: RM must be a whitespace delimited list.\n\nThe resulting image can be used to:\n\ngenerate an image for any of the supported providers\nperform bare-metall installs\nperform upgrades\n\nWe will step through common customizations in the remainder of this section.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Customizing the Kernel | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/customizing-the-kernel/",
    "html": "Documentation\nAdvanced Guides\nCustomizing the Kernel\nCustomizing the Kernel\nGuide on how to customize the kernel used by Talos Linux.\n\nThe installer image contains ONBUILD instructions that handle the following:\n\nthe decompression, and unpacking of the initramfs.xz\nthe unsquashing of the rootfs\nthe copying of new rootfs files\nthe squashing of the new rootfs\nand the packing, and compression of the new initramfs.xz\n\nWhen used as a base image, the installer will perform the above steps automatically with the requirement that a customization stage be defined in the Dockerfile.\n\nBuild and push your own kernel:\n\nCopy\ngit clone https://github.com/talos-systems/pkgs.git\n\ncd pkgs\n\nmake kernel-menuconfig USERNAME=_your_github_user_name_\n\n\n\ndocker login ghcr.io --username _your_github_user_name_\n\nmake kernel USERNAME=_your_github_user_name_ PUSH=true\n\n\nUsing a multi-stage Dockerfile we can define the customization stage and build FROM the installer image:\n\nCopy\nFROM scratch AS customization\n\n# this is needed so that Talos copies base kernel modules info and default modules shipped with Talos\n\nCOPY --from=<custom kernel image> /lib/modules /kernel/lib/modules\n\n# this copies over the custom modules\n\nCOPY --from=<custom kernel image> /lib/modules /lib/modules\n\n\n\nFROM ghcr.io/siderolabs/installer:latest\n\nCOPY --from=<custom kernel image> /boot/vmlinuz /usr/install/${TARGETARCH}/vmlinuz\n\n\nWhen building the image, the customization stage will automatically be copied into the rootfs. The customization stage is not limited to a single COPY instruction. In fact, you can do whatever you would like in this stage, but keep in mind that everything in / will be copied into the rootfs.\n\nTo build the image, run:\n\nCopy\nDOCKER_BUILDKIT=0 docker build --build-arg RM=\"/lib/modules\" -t installer:kernel .\n\n\nNote: buildkit has a bug #816, to disable it use DOCKER_BUILDKIT=0\n\nNow that we have a custom installer we can build Talos for the specific platform we wish to deploy to.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Building Custom Talos Images | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/building-images/",
    "html": "Checkout Talos Source\nSet up the Build Environment\nArchitectures\nCustomizations\nBuilding Kernel and Initramfs\nBuilding Container Images\nBuilding ISO\nBuilding Disk Images\nDocumentation\nAdvanced Guides\nBuilding Custom Talos Images\nBuilding Custom Talos Images\nHow to build a custom Talos image from source.\n\nThere might be several reasons to build Talos images from source:\n\nverifying the image integrity\nbuilding an image with custom configuration\nCheckout Talos Source\nCopy\ngit clone https://github.com/siderolabs/talos.git\n\n\nIf building for a specific release, checkout the corresponding tag:\n\nCopy\ngit checkout v1.6.2\n\nSet up the Build Environment\n\nSee Developing Talos for details on setting up the buildkit builder.\n\nArchitectures\n\nBy default, Talos builds for linux/amd64, but you can customize that by passing PLATFORM variable to make:\n\nCopy\nmake <target> PLATFORM=linux/arm64 # build for arm64 only\n\nmake <target> PLATFORM=linux/arm64,linux/amd64 # build for arm64 and amd64, container images will be multi-arch\n\nCustomizations\n\nSome of the build parameters can be customized by passing environment variables to make, e.g. GOAMD64=v1 can be used to build Talos images compatible with old AMD64 CPUs:\n\nCopy\nmake <target> GOAMD64=v1\n\nBuilding Kernel and Initramfs\n\nThe most basic boot assets can be built with:\n\nCopy\nmake kernel initramfs\n\n\nBuild result will be stored as _out/vmlinuz-<arch> and _out/initramfs-<arch>.xz.\n\nBuilding Container Images\n\nTalos container images should be pushed to the registry as the result of the build process.\n\nThe default settings are:\n\nIMAGE_REGISTRY is set to ghcr.io\nUSERNAME is set to the siderolabs (or value of environment variable USERNAME if it is set)\n\nThe image can be pushed to any registry you have access to, but the access credentials should be stored in ~/.docker/config.json file (e.g. with docker login).\n\nBuilding and pushing the image can be done with:\n\nCopy\nmake installer PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nmake imager PUSH=true IMAGE_REGISTRY=docker.io USERNAME=<username> # ghcr.io/siderolabs/installer\n\nBuilding ISO\n\nThe ISO image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nCopy\nmake iso\n\n\nThe ISO image will be stored as _out/talos-<arch>.iso.\n\nIf ISO image should be built with the custom imager image, it can be specified with IMAGE_REGISTRY/USERNAME variables:\n\nCopy\nmake iso IMAGE_REGISTRY=docker.io USERNAME=<username>\n\nBuilding Disk Images\n\nThe disk image is built with the help of imager container image, by default ghcr.io/siderolabs/imager will be used with the matching tag:\n\nCopy\nmake image-metal\n\n\nAvailable disk images are encoded in the image-% target, e.g. make image-aws. Same as with ISO image, the custom imager image can be specified with IMAGE_REGISTRY/USERNAME variables.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Advanced Networking | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/advanced-networking/",
    "html": "Static Addressing\nAdditional Addresses for an Interface\nBonding\nSetting Up a Bridge\nVLANs\nDocumentation\nAdvanced Guides\nAdvanced Networking\nAdvanced Networking\nHow to configure advanced networking options on Talos Linux.\nStatic Addressing\n\nStatic addressing is comprised of specifying addresses, routes ( remember to add your default gateway ), and interface. Most likely you’ll also want to define the nameservers so you have properly functioning DNS.\n\nCopy\nmachine:\n\n  network:\n\n    hostname: talos\n\n    nameservers:\n\n      - 10.0.0.1\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.201/8\n\n        mtu: 8765\n\n        routes:\n\n          - network: 0.0.0.0/0\n\n            gateway: 10.0.0.1\n\n      - interface: eth1\n\n        ignore: true\n\n  time:\n\n    servers:\n\n      - time.cloudflare.com\n\nAdditional Addresses for an Interface\n\nIn some environments you may need to set additional addresses on an interface. In the following example, we set two additional addresses on the loopback interface.\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: lo\n\n        addresses:\n\n          - 192.168.0.21/24\n\n          - 10.2.2.2/24\n\nBonding\n\nThe following example shows how to create a bonded interface.\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        dhcp: true\n\n        bond:\n\n          mode: 802.3ad\n\n          lacpRate: fast\n\n          xmitHashPolicy: layer3+4\n\n          miimon: 100\n\n          updelay: 200\n\n          downdelay: 200\n\n          interfaces:\n\n            - eth0\n\n            - eth1\n\nSetting Up a Bridge\n\nThe following example shows how to set up a bridge between two interfaces with an assigned static address.\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: br0\n\n        addresses:\n\n          - 192.168.0.42/24\n\n        bridge:\n\n          stp:\n\n            enabled: true\n\n          interfaces:\n\n              - eth0\n\n              - eth1\n\nVLANs\n\nTo setup vlans on a specific device use an array of VLANs to add. The master device may be configured without addressing by setting dhcp to false.\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        vlans:\n\n          - vlanId: 100\n\n            addresses:\n\n              - \"192.168.2.10/28\"\n\n            routes:\n\n              - network: 0.0.0.0/0\n\n                gateway: 192.168.2.1\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Air-gapped Environments | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/air-gapped/",
    "html": "Requirements\nIdentifying Images\nPreparing the Internal Registry\nLaunching Talos in an Air-gapped Environment\nClosing Notes\nDocumentation\nAdvanced Guides\nAir-gapped Environments\nAir-gapped Environments\nSetting up Talos Linux to work in environments with no internet access.\n\nIn this guide we will create a Talos cluster running in an air-gapped environment with all the required images being pulled from an internal registry. We will use the QEMU provisioner available in talosctl to create a local cluster, but the same approach could be used to deploy Talos in bigger air-gapped networks.\n\nRequirements\n\nThe follow are requirements for this guide:\n\nDocker 18.03 or greater\nRequirements for the Talos QEMU cluster\nIdentifying Images\n\nIn air-gapped environments, access to the public Internet is restricted, so Talos can’t pull images from public Docker registries (docker.io, ghcr.io, etc.) We need to identify the images required to install and run Talos. The same strategy can be used for images required by custom workloads running on the cluster.\n\nThe talosctl image default command provides a list of default images used by the Talos cluster (with default configuration settings). To print the list of images, run:\n\nCopy\ntalosctl image default\n\n\nThis list contains images required by a default deployment of Talos. There might be additional images required for the workloads running on this cluster, and those should be added to this list.\n\nPreparing the Internal Registry\n\nAs access to the public registries is restricted, we have to run an internal Docker registry. In this guide, we will launch the registry on the same machine using Docker:\n\nCopy\n$ docker run -d -p 6000:5000 --restart always --name registry-airgapped registry:2\n\n1bf09802bee1476bc463d972c686f90a64640d87dacce1ac8485585de69c91a5\n\n\nThis registry will be accepting connections on port 6000 on the host IPs. The registry is empty by default, so we have fill it with the images required by Talos.\n\nFirst, we pull all the images to our local Docker daemon:\n\nCopy\n$ for image in `talosctl image default`; do docker pull $image; done\n\nv0.15.1: Pulling from coreos/flannel\n\nDigest: sha256:9a296fbb67790659adc3701e287adde3c59803b7fcefe354f1fc482840cdb3d9\n\n...\n\n\nAll images are now stored in the Docker daemon store:\n\nCopy\n$ docker images\n\nREPOSITORY                               TAG                                        IMAGE ID       CREATED         SIZE\n\ngcr.io/etcd-development/etcd             v3.5.3                                     604d4f022632   6 days ago      181MB\n\nghcr.io/siderolabs/install-cni           v1.0.0-2-gc5d3ab0                          4729e54f794d   6 days ago      76MB\n\n...\n\n\nNow we need to re-tag them so that we can push them to our local registry. We are going to replace the first component of the image name (before the first slash) with our registry endpoint 127.0.0.1:6000:\n\nCopy\n$ for image in `talosctl image default`; do \\\n\n    docker tag $image `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nAs the next step, we push images to the internal registry:\n\nCopy\n$ for image in `talosctl image default`; do \\\n\n    docker push `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \\\n\n  done\n\n\nWe can now verify that the images are pushed to the registry:\n\nCopy\n$ curl http://127.0.0.1:6000/v2/_catalog\n\n{\"repositories\":[\"coredns/coredns\",\"coreos/flannel\",\"etcd-development/etcd\",\"kube-apiserver\",\"kube-controller-manager\",\"kube-proxy\",\"kube-scheduler\",\"pause\",\"siderolabs/install-cni\",\"siderolabs/installer\",\"siderolabs/kubelet\"]}\n\n\nNote: images in the registry don’t have the registry endpoint prefix anymore.\n\nLaunching Talos in an Air-gapped Environment\n\nFor Talos to use the internal registry, we use the registry mirror feature to redirect all image pull requests to the internal registry. This means that the registry endpoint (as the first component of the image reference) gets ignored, and all pull requests are sent directly to the specified endpoint.\n\nWe are going to use a QEMU-based Talos cluster for this guide, but the same approach works with Docker-based clusters as well. As QEMU-based clusters go through the Talos install process, they can be used better to model a real air-gapped environment.\n\nIdentify all registry prefixes from talosctl image default, for example:\n\ndocker.io\ngcr.io\nghcr.io\nregistry.k8s.io\n\nThe talosctl cluster create command provides conveniences for common configuration options. The only required flag for this guide is --registry-mirror <endpoint>=http://10.5.0.1:6000 which redirects every pull request to the internal registry, this flag needs to be repeated for each of the identified registry prefixes above. The endpoint being used is 10.5.0.1, as this is the default bridge interface address which will be routable from the QEMU VMs (127.0.0.1 IP will be pointing to the VM itself).\n\nCopy\n$ sudo --preserve-env=HOME talosctl cluster create --provisioner=qemu --install-image=ghcr.io/siderolabs/installer:v1.6.2 \\\n\n  --registry-mirror docker.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror gcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror ghcr.io=http://10.5.0.1:6000 \\\n\n  --registry-mirror registry.k8s.io=http://10.5.0.1:6000 \\\n\nvalidating CIDR and reserving IPs\n\ngenerating PKI and tokens\n\ncreating state directory in \"/home/user/.talos/clusters/talos-default\"\n\ncreating network talos-default\n\ncreating load balancer\n\ncreating dhcpd\n\ncreating master nodes\n\ncreating worker nodes\n\nwaiting for API\n\n...\n\n\nNote: --install-image should match the image which was copied into the internal registry in the previous step.\n\nYou can be verify that the cluster is air-gapped by inspecting the registry logs: docker logs -f registry-airgapped.\n\nClosing Notes\n\nRunning in an air-gapped environment might require additional configuration changes, for example using custom settings for DNS and NTP servers.\n\nWhen scaling this guide to the bare-metal environment, following Talos config snippet could be used as an equivalent of the --registry-mirror flag above:\n\nCopy\nmachine:\n\n  ...\n\n  registries:\n\n      mirrors:\n\n        docker.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        gcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        ghcr.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n        registry.k8s.io:\n\n          endpoints:\n\n          - http://10.5.0.1:6000/\n\n...\n\n\nOther implementations of Docker registry can be used in place of the Docker registry image used above to run the registry. If required, auth can be configured for the internal registry (and custom TLS certificates if needed).\n\nPlease see pull-through cache guide for an example using Harbor container registry with Talos.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Upgrading Kubernetes | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/upgrading-kubernetes/",
    "html": "Automated Kubernetes Upgrade\nManual Kubernetes Upgrade\nKubeconfig\nAPI Server\nController Manager\nScheduler\nProxy\nBootstrap Manifests\nkubelet\nDocumentation\nKubernetes Guides\nUpgrading Kubernetes\nUpgrading Kubernetes\nGuide on how to upgrade the Kubernetes cluster from Talos Linux.\n\nThis guide covers upgrading Kubernetes on Talos Linux clusters.\n\nFor a list of Kubernetes versions compatible with each Talos release, see the Support Matrix.\n\nFor upgrading the Talos Linux operating system, see Upgrading Talos\n\nVideo Walkthrough\n\nTo see a demo of this process, watch this video:\n\nAutomated Kubernetes Upgrade\n\nThe recommended method to upgrade Kubernetes is to use the talosctl upgrade-k8s command. This will automatically update the components needed to upgrade Kubernetes safely. Upgrading Kubernetes is non-disruptive to the cluster workloads.\n\nTo trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to ugprade to, such as:\n\ntalosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nNote that the --nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded.\n\nTo check what will be upgraded you can run talosctl upgrade-k8s with the --dry-run flag:\n\nCopy\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0 --dry-run\n\nWARNING: found resources which are going to be deprecated/migrated in the version 1.29.0\n\nRESOURCE                                                               COUNT\n\nvalidatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io   4\n\nmutatingwebhookconfigurations.v1beta1.admissionregistration.k8s.io     3\n\ncustomresourcedefinitions.v1beta1.apiextensions.k8s.io                 25\n\napiservices.v1beta1.apiregistration.k8s.io                             54\n\nleases.v1beta1.coordination.k8s.io                                     4\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.4\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\nupdating \"kube-controller-manager\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-controller-manager: v1.28.3 -> 1.29.0\n\n > skipped in dry-run\n\n > \"172.20.0.3\": starting update\n\n\n\n<snip>\n\n\n\nupdating manifests\n\n > apply manifest Secret bootstrap-token-3lb63t\n\n > apply skipped in dry run\n\n > apply manifest ClusterRoleBinding system-bootstrap-approve-node-client-csr\n\n > apply skipped in dry run\n\n<snip>\n\n\nTo upgrade Kubernetes from v1.28.3 to v1.29.0 run:\n\nCopy\n$ talosctl --nodes <controlplane node> upgrade-k8s --to 1.29.0\n\nautomatically detected the lowest Kubernetes version 1.28.3\n\nchecking for resource APIs to be deprecated in version 1.29.0\n\ndiscovered controlplane nodes [\"172.20.0.2\" \"172.20.0.3\" \"172.20.0.4\"]\n\ndiscovered worker nodes [\"172.20.0.5\" \"172.20.0.6\"]\n\nupdating \"kube-apiserver\" to version \"1.29.0\"\n\n > \"172.20.0.2\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n > \"172.20.0.2\": machine configuration patched\n\n > \"172.20.0.2\": waiting for API server state pod update\n\n < \"172.20.0.2\": successfully updated\n\n > \"172.20.0.3\": starting update\n\n > update kube-apiserver: v1.28.3 -> 1.29.0\n\n<snip>\n\n\nThis command runs in several phases:\n\nImages for new Kubernetes components are pre-pulled to the nodes to minimize downtime and test for image availability.\nEvery control plane node machine configuration is patched with the new image version for each control plane component. Talos renders new static pod definitions on the configuration update which is picked up by the kubelet. The command waits for the change to propagate to the API server state.\nThe command updates the kube-proxy daemonset with the new image version.\nOn every node in the cluster, the kubelet version is updated. The command then waits for the kubelet service to be restarted and become healthy. The update is verified by checking the Node resource state.\nKubernetes bootstrap manifests are re-applied to the cluster. Updated bootstrap manifests might come with a new Talos version (e.g. CoreDNS version update), or might be the result of machine configuration change.\n\nNote: The upgrade-k8s command never deletes any resources from the cluster: they should be deleted manually.\n\nIf the command fails for any reason, it can be safely restarted to continue the upgrade process from the moment of the failure.\n\nManual Kubernetes Upgrade\n\nKubernetes can be upgraded manually by following the steps outlined below. They are equivalent to the steps performed by the talosctl upgrade-k8s command.\n\nKubeconfig\n\nIn order to edit the control plane, you need a working kubectl config. If you don’t already have one, you can get one by running:\n\nCopy\ntalosctl --nodes <controlplane node> kubeconfig\n\nAPI Server\n\nPatch machine configuration using talosctl patch command:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need to be adjusted if current machine configuration is missing .cluster.apiServer.image key.\n\nAlso the machine configuration can be edited manually with talosctl -n <IP> edit mc --mode=no-reboot.\n\nCapture the new version of kube-apiserver config with:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-apiserver -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-apiserver\n\n    version: 5\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-apiserver:v1.29.0\n\n    cloudProvider: \"\"\n\n    controlPlaneEndpoint: https://172.20.0.1:6443\n\n    etcdServers:\n\n        - https://127.0.0.1:2379\n\n    localPort: 6443\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, the new version is 5. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n5\n\n\nCheck that the pod is running:\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-apiserver --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-apiserver-talos-default-controlplane-1   1/1     Running   0          16m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nController Manager\n\nPatch machine configuration using talosctl patch command:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/controllerManager/image\", \"value\": \"registry.k8s.io/kube-controller-manager:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nThe JSON patch might need be adjusted if current machine configuration is missing .cluster.controllerManager.image key.\n\nCapture new version of kube-controller-manager config with:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-controller-manager -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-controller-manager\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-controller-manager:v1.29.0\n\n    cloudProvider: \"\"\n\n    podCIDR: 10.244.0.0/16\n\n    serviceCIDR: 10.96.0.0/12\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-controller-manager --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\nkube-controller-manager-talos-default-controlplane-1   1/1     Running   0          35m\n\n\nRepeat this process for every control plane node, verifying that state propagated successfully between each node update.\n\nScheduler\n\nPatch machine configuration using talosctl patch command:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/scheduler/image\", \"value\": \"registry.k8s.io/kube-scheduler:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nJSON patch might need be adjusted if current machine configuration is missing .cluster.scheduler.image key.\n\nCapture new version of kube-scheduler config with:\n\nCopy\n$ talosctl -n <CONTROL_PLANE_IP_1> get kcpc kube-scheduler -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: config\n\n    type: KubernetesControlPlaneConfigs.config.talos.dev\n\n    id: kube-scheduler\n\n    version: 3\n\n    phase: running\n\nspec:\n\n    image: registry.k8s.io/kube-scheduler:v1.29.0\n\n    extraArgs: {}\n\n    extraVolumes: []\n\n\nIn this example, new version is 3. Wait for the new pod definition to propagate to the API server state (replace talos-default-controlplane-1 with the node name):\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1 -o jsonpath='{.items[0].metadata.annotations.talos\\.dev/config\\-version}'\n\n3\n\n\nCheck that the pod is running:\n\nCopy\n$ kubectl get pod -n kube-system -l k8s-app=kube-scheduler --field-selector spec.nodeName=talos-default-controlplane-1\n\nNAME                                    READY   STATUS    RESTARTS   AGE\n\nkube-scheduler-talos-default-controlplane-1   1/1     Running   0          39m\n\n\nRepeat this process for every control plane node, verifying that state got propagated successfully between each node update.\n\nProxy\n\nIn the proxy’s DaemonSet, change:\n\nCopy\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n\nto:\n\nCopy\nkind: DaemonSet\n\n...\n\nspec:\n\n  ...\n\n  template:\n\n    ...\n\n    spec:\n\n      containers:\n\n        - name: kube-proxy\n\n          image: registry.k8s.io/kube-proxy:v1.29.0\n\n      tolerations:\n\n        - ...\n\n        - key: node-role.kubernetes.io/control-plane\n\n          operator: Exists\n\n          effect: NoSchedule\n\n\nTo edit the DaemonSet, run:\n\nCopy\nkubectl edit daemonsets -n kube-system kube-proxy\n\nBootstrap Manifests\n\nBootstrap manifests can be retrieved in a format which works for kubectl with the following command:\n\nCopy\ntalosctl -n <controlplane IP> get manifests -o yaml | yq eval-all '.spec | .[] | splitDoc' - > manifests.yaml\n\n\nDiff the manifests with the cluster:\n\nCopy\nkubectl diff -f manifests.yaml\n\n\nApply the manifests:\n\nCopy\nkubectl apply -f manifests.yaml\n\n\nNote: if some bootstrap resources were removed, they have to be removed from the cluster manually.\n\nkubelet\n\nFor every node, patch machine configuration with new kubelet version, wait for the kubelet to restart with new version:\n\nCopy\n$ talosctl -n <IP> patch mc --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node 172.20.0.2\n\n\nOnce kubelet restarts with the new configuration, confirm upgrade with kubectl get nodes <name>:\n\nCopy\n$ kubectl get nodes talos-default-controlplane-1\n\nNAME                           STATUS   ROLES                  AGE    VERSION\n\ntalos-default-controlplane-1   Ready    control-plane          123m   v1.29.0\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Advanced Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/advanced/",
    "html": "Documentation\nAdvanced Guides\nAdvanced Guides\nAdvanced Networking\n\nHow to configure advanced networking options on Talos Linux.\n\nAir-gapped Environments\n\nSetting up Talos Linux to work in environments with no internet access.\n\nBuilding Custom Talos Images\n\nHow to build a custom Talos image from source.\n\nCustomizing the Kernel\n\nGuide on how to customize the kernel used by Talos Linux.\n\nCustomizing the Root Filesystem\n\nHow to add your own content to the immutable root file system of Talos Linux.\n\nDeveloping Talos\n\nLearn how to set up a development environment for local testing and hacking on Talos itself!\n\nDisaster Recovery\n\nProcedure for snapshotting etcd database and recovering from catastrophic control plane failure.\n\netcd Maintenance\n\nOperational instructions for etcd database.\n\nExtension Services\n\nUse extension services in Talos Linux.\n\nMachine Configuration OAuth2 Authentication\n\nHow to authenticate Talos machine configuration download (talos.config=) on metal platform using OAuth.\n\nMetal Network Configuration\n\nHow to use META-based network configuration on Talos metal platform.\n\nMigrating from Kubeadm\n\nMigrating Kubeadm-based clusters to Talos.\n\nProprietary Kernel Modules\n\nAdding a proprietary kernel module to Talos Linux\n\nStatic Pods\n\nUsing Talos Linux to set up static pods in Kubernetes.\n\nTalos API access from Kubernetes\n\nHow to access Talos API from within Kubernetes.\n\nVerifying Images\n\nVerifying Talos container image signatures.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Deploying Cilium CNI | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/network/deploying-cilium/",
    "html": "Machine config preparation\nInstallation using Cilium CLI\nWith kube-proxy\nWithout kube-proxy\nInstallation using Helm\nMethod 1: Helm install\nMethod 2: Helm manifests install\nMethod 3: Helm manifests hosted install\nMethod 4: Helm manifests inline install\nKnown issues\nOther things to know\nDocumentation\nKubernetes Guides\nNetwork\nDeploying Cilium CNI\nDeploying Cilium CNI\nIn this guide you will learn how to set up Cilium CNI on Talos.\n\nCilium can be installed either via the cilium cli or using helm.\n\nThis documentation will outline installing Cilium CNI v1.14.0 on Talos in six different ways. Adhering to Talos principles we’ll deploy Cilium with IPAM mode set to Kubernetes, and using the cgroupv2 and bpffs mount that talos already provides. As Talos does not allow loading kernel modules by Kubernetes workloads, SYS_MODULE capability needs to be dropped from the Cilium default set of values, this override can be seen in the helm/cilium cli install commands. Each method can either install Cilium using kube proxy (default) or without: Kubernetes Without kube-proxy\n\nIn this guide we assume that KubePrism is enabled and configured to use the port 7445.\n\nMachine config preparation\n\nWhen generating the machine config for a node set the CNI to none. For example using a config patch:\n\nCreate a patch.yaml file with the following contents:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nOr if you want to deploy Cilium without kube-proxy, you also need to disable kube proxy:\n\nCreate a patch.yaml file with the following contents:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\nInstallation using Cilium CLI\n\nNote: It is recommended to template the cilium manifest using helm and use it as part of Talos machine config, but if you want to install Cilium using the Cilium CLI, you can follow the steps below.\n\nInstall the Cilium CLI following the steps here.\n\nWith kube-proxy\nCopy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=disabled \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup\n\nWithout kube-proxy\nCopy\ncilium install \\\n\n    --helm-set=ipam.mode=kubernetes \\\n\n    --helm-set=kubeProxyReplacement=true \\\n\n    --helm-set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --helm-set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --helm-set=cgroup.autoMount.enabled=false \\\n\n    --helm-set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --helm-set=k8sServiceHost=localhost \\\n\n    --helm-set=k8sServicePort=7445\n\nInstallation using Helm\n\nRefer to Installing with Helm for more information.\n\nFirst we’ll need to add the helm repo for Cilium.\n\nCopy\nhelm repo add cilium https://helm.cilium.io/\n\nhelm repo update\n\nMethod 1: Helm install\n\nAfter applying the machine config and bootstrapping Talos will appear to hang on phase 18/19 with the message: retrying error: node not ready. This happens because nodes in Kubernetes are only marked as ready once the CNI is up. As there is no CNI defined, the boot process is pending and will reboot the node to retry after 10 minutes, this is expected behavior.\n\nDuring this window you can install Cilium manually by running the following:\n\nCopy\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup\n\n\nOr if you want to deploy Cilium without kube-proxy, also set some extra paramaters:\n\nCopy\nhelm install \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445\n\n\nAfter Cilium is installed the boot process should continue and complete successfully.\n\nMethod 2: Helm manifests install\n\nInstead of directly installing Cilium you can instead first generate the manifest and then apply it:\n\nCopy\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=disabled \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\n\nWithout kube-proxy:\n\nCopy\nhelm template \\\n\n    cilium \\\n\n    cilium/cilium \\\n\n    --version 1.14.0 \\\n\n    --namespace kube-system \\\n\n    --set ipam.mode=kubernetes \\\n\n    --set=kubeProxyReplacement=true \\\n\n    --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n\n    --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n\n    --set=cgroup.autoMount.enabled=false \\\n\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n\n    --set=k8sServiceHost=localhost \\\n\n    --set=k8sServicePort=7445 > cilium.yaml\n\n\n\nkubectl apply -f cilium.yaml\n\nMethod 3: Helm manifests hosted install\n\nAfter generating cilium.yaml using helm template, instead of applying this manifest directly during the Talos boot window (before the reboot timeout). You can also host this file somewhere and patch the machine config to apply this manifest automatically during bootstrap. To do this patch your machine configuration to include this config instead of the above:\n\nCreate a patch.yaml file with the following contents:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      name: custom\n\n      urls:\n\n        - https://server.yourdomain.tld/some/path/cilium.yaml\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nHowever, beware of the fact that the helm generated Cilium manifest contains sensitive key material. As such you should definitely not host this somewhere publicly accessible.\n\nMethod 4: Helm manifests inline install\n\nA more secure option would be to include the helm template output manifest inside the machine configuration. The machine config should be generated with CNI set to none\n\nCreate a patch.yaml file with the following contents:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nif deploying Cilium with kube-proxy disabled, you can also include the following:\n\nCreate a patch.yaml file with the following contents:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      name: none\n\n  proxy:\n\n    disabled: true\n\nmachine:\n\n  features:\n\n    kubePrism:\n\n      enabled: true\n\n      port: 7445\n\nCopy\ntalosctl gen config \\\n\n    my-cluster https://mycluster.local:6443 \\\n\n    --config-patch @patch.yaml\n\n\nTo do so patch this into your machine configuration:\n\nCopy\ninlineManifests:\n\n    - name: cilium\n\n      contents: |\n\n        --\n\n        # Source: cilium/templates/cilium-agent/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        metadata:\n\n          name: \"cilium\"\n\n          namespace: kube-system\n\n        ---\n\n        # Source: cilium/templates/cilium-operator/serviceaccount.yaml\n\n        apiVersion: v1\n\n        kind: ServiceAccount\n\n        -> Your cilium.yaml file will be pretty long....        \n\n\nThis will install the Cilium manifests at just the right time during bootstrap.\n\nBeware though:\n\nChanging the namespace when templating with Helm does not generate a manifest containing the yaml to create that namespace. As the inline manifest is processed from top to bottom make sure to manually put the namespace yaml at the start of the inline manifest.\nOnly add the Cilium inline manifest to the control plane nodes machine configuration.\nMake sure all control plane nodes have an identical configuration.\nIf you delete any of the generated resources they will be restored whenever a control plane node reboots.\nAs a safety messure Talos only creates missing resources from inline manifests, it never deletes or updates anything.\nIf you need to update a manifest make sure to first edit all control plane machine configurations and then run talosctl upgrade-k8s as it will take care of updating inline manifests.\nKnown issues\nThere are some gotchas when using Talos and Cilium on the Google cloud platform when using internal load balancers. For more details: GCP ILB support / support scope local routes to be configured\nOther things to know\nTalos has full kernel module support for eBPF, See:\nCilium System Requirements\nTalos Kernel Config AMD64\nTalos Kernel Config ARM64\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Storage | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/storage/",
    "html": "Public Cloud\nStorage Clusters\nRook/Ceph\nMayastor\nVideo Walkthrough\nPrep Nodes\nDeploy Mayastor\nPiraeus / LINSTOR\nInstall Piraeus Operator V2\nCreate first storage pool and PVC\nNFS\nObject storage\nOthers (iSCSI)\nDocumentation\nKubernetes Guides\nConfiguration\nStorage\nStorage\nSetting up storage for a Kubernetes cluster\n\nIn Kubernetes, using storage in the right way is well-facilitated by the API. However, unless you are running in a major public cloud, that API may not be hooked up to anything. This frequently sends users down a rabbit hole of researching all the various options for storage backends for their platform, for Kubernetes, and for their workloads. There are a lot of options out there, and it can be fairly bewildering.\n\nFor Talos, we try to limit the options somewhat to make the decision-making easier.\n\nPublic Cloud\n\nIf you are running on a major public cloud, use their block storage. It is easy and automatic.\n\nStorage Clusters\n\nSidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\nRedundancy, scaling capabilities, reliability, speed, maintenance load, and ease of use are all factors you must consider when managing your own storage.\n\nRunning a storage cluster can be a very good choice when managing your own storage, and there are two projects we recommend, depending on your situation.\n\nIf you need vast amounts of storage composed of more than a dozen or so disks, we recommend you use Rook to manage Ceph. Also, if you need both mount-once and mount-many capabilities, Ceph is your answer. Ceph also bundles in an S3-compatible object store. The down side of Ceph is that there are a lot of moving parts.\n\nPlease note that most people should never use mount-many semantics. NFS is pervasive because it is old and easy, not because it is a good idea. While it may seem like a convenience at first, there are all manner of locking, performance, change control, and reliability concerns inherent in any mount-many situation, so we strongly recommend you avoid this method.\n\nIf your storage needs are small enough to not need Ceph, use Mayastor.\n\nRook/Ceph\n\nCeph is the grandfather of open source storage clusters. It is big, has a lot of pieces, and will do just about anything. It scales better than almost any other system out there, open source or proprietary, being able to easily add and remove storage over time with no downtime, safely and easily. It comes bundled with RadosGW, an S3-compatible object store; CephFS, a NFS-like clustered filesystem; and RBD, a block storage system.\n\nWith the help of Rook, the vast majority of the complexity of Ceph is hidden away by a very robust operator, allowing you to control almost everything about your Ceph cluster from fairly simple Kubernetes CRDs.\n\nSo if Ceph is so great, why not use it for everything?\n\nCeph can be rather slow for small clusters. It relies heavily on CPUs and massive parallelisation to provide good cluster performance, so if you don’t have much of those dedicated to Ceph, it is not going to be well-optimised for you. Also, if your cluster is small, just running Ceph may eat up a significant amount of the resources you have available.\n\nTroubleshooting Ceph can be difficult if you do not understand its architecture. There are lots of acronyms and the documentation assumes a fair level of knowledge. There are very good tools for inspection and debugging, but this is still frequently seen as a concern.\n\nMayastor\n\nMayastor is an OpenEBS project built in Rust utilising the modern NVMEoF system. (Despite the name, Mayastor does not require you to have NVME drives.) It is fast and lean but still cluster-oriented and cloud native. Unlike most of the other OpenEBS project, it is not built on the ancient iSCSI system.\n\nUnlike Ceph, Mayastor is just a block store. It focuses on block storage and does it well. It is much less complicated to set up than Ceph, but you probably wouldn’t want to use it for more than a few dozen disks.\n\nMayastor is new, maybe too new. If you’re looking for something well-tested and battle-hardened, this is not it. However, if you’re looking for something lean, future-oriented, and simpler than Ceph, it might be a great choice.\n\nVideo Walkthrough\n\nTo see a live demo of this section, see the video below:\n\nPrep Nodes\n\nEither during initial cluster creation or on running worker nodes, several machine config values should be edited. (This information is gathered from the Mayastor documentation.) We need to set the vm.nr_hugepages sysctl and add openebs.io/engine=mayastor labels to the nodes which are meant to be storage nodes. This can be done with talosctl patch machineconfig or via config patches during talosctl gen config.\n\nSome examples are shown below: modify as needed.\n\nFirst create a config patch file named mayastor-patch.yaml with the following contents:\n\nCopy\n- op: add\n\n  path: /machine/sysctls\n\n  value:\n\n    vm.nr_hugepages: \"1024\"\n\n- op: add\n\n  path: /machine/nodeLabels\n\n  value:\n\n    openebs.io/engine: mayastor\n\n\nUsing gen config\n\nCopy\ntalosctl gen config my-cluster https://mycluster.local:6443 --config-patch @mayastor-patch.yaml\n\n\nPatching an existing node\n\nCopy\ntalosctl patch --mode=no-reboot machineconfig -n <node ip> --patch @mayastor-patch.yaml\n\n\nNote: If you are adding/updating the vm.nr_hugepages on a node which already had the openebs.io/engine=mayastor label set, you’d need to restart kubelet so that it picks up the new value, by issuing the following command\n\nCopy\ntalosctl -n <node ip> service kubelet restart\n\nDeploy Mayastor\n\nContinue setting up Mayastor using the official documentation.\n\nPiraeus / LINSTOR\nPiraeus-Operator\nLINSTOR\nDRBD Extension\nInstall Piraeus Operator V2\n\nThere is already a how-to for Talos: Link\n\nCreate first storage pool and PVC\n\nBefore proceeding, install linstor plugin for kubectl: https://github.com/piraeusdatastore/kubectl-linstor\n\nOr use krew: kubectl krew install linstor\n\nCopy\n# Create device pool on a blank (no partitation table!) disk on node01\n\nkubectl linstor physical-storage create-device-pool --pool-name nvme_lvm_pool LVM node01 /dev/nvme0n1 --storage-pool nvme_pool\n\n\npiraeus-sc.yml\n\nCopy\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  name: simple-nvme\n\nparameters:\n\n  csi.storage.k8s.io/fstype: xfs\n\n  linstor.csi.linbit.com/autoPlace: \"3\"\n\n  linstor.csi.linbit.com/storagePool: nvme_pool\n\nprovisioner: linstor.csi.linbit.com\n\nvolumeBindingMode: WaitForFirstConsumer\n\nCopy\n# Create storage class\n\nkubectl apply -f piraeus-sc.yml\n\nNFS\n\nNFS is an old pack animal long past its prime. NFS is slow, has all kinds of bottlenecks involving contention, distributed locking, single points of service, and more. However, it is supported by a wide variety of systems. You don’t want to use it unless you have to, but unfortunately, that “have to” is too frequent.\n\nThe NFS client is part of the kubelet image maintained by the Talos team. This means that the version installed in your running kubelet is the version of NFS supported by Talos. You can reduce some of the contention problems by parceling Persistent Volumes from separate underlying directories.\n\nObject storage\n\nCeph comes with an S3-compatible object store, but there are other options, as well. These can often be built on top of other storage backends. For instance, you may have your block storage running with Mayastor but assign a Pod a large Persistent Volume to serve your object store.\n\nOne of the most popular open source add-on object stores is MinIO.\n\nOthers (iSCSI)\n\nThe most common remaining systems involve iSCSI in one form or another. These include the original OpenEBS, Rancher’s Longhorn, and many proprietary systems. iSCSI in Linux is facilitated by open-iscsi. This system was designed long before containers caught on, and it is not well suited to the task, especially when coupled with a read-only host operating system.\n\niSCSI support in Talos is now supported via the iscsi-tools system extension installed. The extension enables compatibility with OpenEBS Jiva - refer to the local storage installation guide for more information.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/network/",
    "html": "Documentation\nKubernetes Guides\nNetwork\nNetwork\nManaging the Kubernetes cluster networking\nDeploying Cilium CNI\n\nIn this guide you will learn how to set up Cilium CNI on Talos.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Seccomp Profiles | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/seccomp-profiles/",
    "html": "Preparing the nodes\nCreate a Kubernetes workload that uses the custom Seccomp Profile\nCleanup\nDocumentation\nKubernetes Guides\nConfiguration\nSeccomp Profiles\nSeccomp Profiles\nUsing custom Seccomp Profiles with Kubernetes workloads.\n\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.\n\nRefer the Kubernetes Seccomp Guide for more details.\n\nIn this guide we are going to configure a custom Seccomp Profile that logs all syscalls made by the workload.\n\nPreparing the nodes\n\nCreate a machine config path with the contents below and save as patch.yaml\n\nCopy\nmachine:\n\n  seccompProfiles:\n\n    - name: audit.json\n\n      value:\n\n        defaultAction: SCMP_ACT_LOG\n\n\nApply the machine config to all the nodes using talosctl:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThis would create a seccomp profile name audit.json on the node at /var/lib/kubelet/seccomp/profiles.\n\nThe profiles can be used by Kubernetes pods by specfying the pod securityContext as below:\n\nCopy\nspec:\n\n  securityContext:\n\n    seccompProfile:\n\n      type: Localhost\n\n      localhostProfile: profiles/audit.json\n\n\nNote that the localhostProfile uses the name of the profile created under profiles directory. So make sure to use path as profiles/<profile-name.json>\n\nThis can be verfied by running the below commands:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get seccompprofiles\n\n\nAn output similar to below can be observed:\n\nCopy\nNODE       NAMESPACE   TYPE             ID           VERSION\n\n10.5.0.3   cri         SeccompProfile   audit.json   1\n\n\nThe content of the seccomp profile can be viewed by running the below command:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> read /var/lib/kubelet/seccomp/profiles/audit.json\n\n\nAn output similar to below can be observed:\n\nCopy\n{\"defaultAction\":\"SCMP_ACT_LOG\"}\n\nCreate a Kubernetes workload that uses the custom Seccomp Profile\n\nHere we’ll be using an example workload from the Kubernetes documentation.\n\nFirst open up a second terminal and run the following talosctl command so that we can view the Syscalls being logged in realtime:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> dmesg --follow --tail\n\n\nNow deploy the example workload from the Kubernetes documentation:\n\nCopy\nkubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml\n\n\nOnce the pod starts running the terminal running talosctl dmesg command from above should log similar to below:\n\nCopy\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.489473063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.490852063Z]: cni0: port 1(veth32488a86) entered disabled state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.492470063Z]: device veth32488a86 entered promiscuous mode\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503105063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.503944063Z]: IPv6: ADDRCONF(NETDEV_CHANGE): veth32488a86: link becomes ready\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.504764063Z]: cni0: port 1(veth32488a86) entered blocking state\n\n10.5.0.3: kern:    info: [2022-07-28T11:49:42.505423063Z]: cni0: port 1(veth32488a86) entered forwarding state\n\n10.5.0.3: kern: warning: [2022-07-28T11:49:44.873616063Z]: kauditd_printk_skb: 14 callbacks suppressed\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.873619063Z]: audit: type=1326 audit(1659008985.445:25): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.876609063Z]: audit: type=1326 audit(1659008985.445:26): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.878789063Z]: audit: type=1326 audit(1659008985.449:27): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=257 compat=0 ip=0x55ec0657bdaa code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.886693063Z]: audit: type=1326 audit(1659008985.461:28): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.888764063Z]: audit: type=1326 audit(1659008985.461:29): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=202 compat=0 ip=0x55ec06532b43 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.891009063Z]: audit: type=1326 audit(1659008985.461:30): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=1 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.893162063Z]: audit: type=1326 audit(1659008985.461:31): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=3 compat=0 ip=0x55ec0657bd3b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.895365063Z]: audit: type=1326 audit(1659008985.461:32): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=39 compat=0 ip=0x55ec066eb68b code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.898306063Z]: audit: type=1326 audit(1659008985.461:33): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"runc:[2:INIT]\" exe=\"/\" sig=0 arch=c000003e syscall=59 compat=0 ip=0x55ec0657be16 code=0x7ffc0000\n\n10.5.0.3: kern:  notice: [2022-07-28T11:49:44.901518063Z]: audit: type=1326 audit(1659008985.473:34): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=2784 comm=\"http-echo\" exe=\"/http-echo\" sig=0 arch=c000003e syscall=158 compat=0 ip=0x455f35 code=0x7ffc0000\n\nCleanup\n\nYou can clean up the test resources by running the following command:\n\nCopy\nkubectl delete pod audit-pod\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Replicated Local Storage | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/replicated-local-storage-with-openebs-jiva/",
    "html": "Preparing the nodes\nInstall OpenEBS Jiva\nPatching the Namespace\nNumber of Replicas\nPatching the jiva installation\nTesting a simple workload\nDocumentation\nKubernetes Guides\nConfiguration\nReplicated Local Storage\nReplicated Local Storage\nUsing local storage with OpenEBS Jiva\n\nIf you want to use replicated storage leveraging disk space from a local disk with Talos Linux installed, OpenEBS Jiva is a great option. This requires installing the iscsi-tools system extension.\n\nSince OpenEBS Jiva is a replicated storage, it’s recommended to have at least three nodes where sufficient local disk space is available. The documentation will follow installing OpenEBS Jiva via the offical Helm chart. Since Talos is different from standard Operating Systems, the OpenEBS components need a little tweaking after the Helm installation. Refer to the OpenEBS Jiva documentation if you need further customization.\n\nNB: Also note that the Talos nodes need to be upgraded with --preserve set while running OpenEBS Jiva, otherwise you risk losing data. Even though it’s possible to recover data from other replicas if the node is wiped during an upgrade, this can require extra operational knowledge to recover, so it’s highly recommended to use --preserve to avoid data loss.\n\nPreparing the nodes\n\nCreate the boot assets which includes the iscsi-tools system extensions (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nCreate a machine config patch with the contents below and save as patch.yaml\n\nCopy\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/openebs/local\n\n        type: bind\n\n        source: /var/openebs/local\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nApply the machine config to all the nodes using talosctl:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> patch mc -p @patch.yaml\n\n\nThe extension status can be verified by running the following command:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> get extensions\n\n\nAn output similar to below can be observed:\n\nCopy\nNODE            NAMESPACE   TYPE              ID                                          VERSION   NAME          VERSION\n\n192.168.20.61   runtime     ExtensionStatus   000.ghcr.io-siderolabs-iscsi-tools-v0.1.1   1         iscsi-tools   v0.1.1\n\n\nThe service status can be checked by running the following command:\n\nCopy\ntalosctl -e <endpoint ip/hostname> -n <node ip/hostname> services\n\n\nYou should see that the ext-tgtd and the ext-iscsid services are running.\n\nCopy\nNODE            SERVICE      STATE     HEALTH   LAST CHANGE     LAST EVENT\n\n192.168.20.51   apid         Running   OK       64h57m15s ago   Health check successful\n\n192.168.20.51   containerd   Running   OK       64h57m23s ago   Health check successful\n\n192.168.20.51   cri          Running   OK       64h57m20s ago   Health check successful\n\n192.168.20.51   etcd         Running   OK       64h55m29s ago   Health check successful\n\n192.168.20.51   ext-iscsid   Running   ?        64h57m19s ago   Started task ext-iscsid (PID 4040) for container ext-iscsid\n\n192.168.20.51   ext-tgtd     Running   ?        64h57m19s ago   Started task ext-tgtd (PID 3999) for container ext-tgtd\n\n192.168.20.51   kubelet      Running   OK       38h14m10s ago   Health check successful\n\n192.168.20.51   machined     Running   ?        64h57m29s ago   Service started as goroutine\n\n192.168.20.51   trustd       Running   OK       64h57m19s ago   Health check successful\n\n192.168.20.51   udevd        Running   OK       64h57m21s ago   Health check successful\n\nInstall OpenEBS Jiva\nCopy\nhelm repo add openebs-jiva https://openebs.github.io/jiva-operator\n\nhelm repo update\n\nhelm upgrade --install --create-namespace --namespace openebs --version 3.2.0 openebs-jiva openebs-jiva/jiva\n\n\nThis will create a storage class named openebs-jiva-csi-default which can be used for workloads. The storage class named openebs-hostpath is used by jiva to create persistent volumes backed by local storage and then used for replicated storage by the jiva controller.\n\nPatching the Namespace\n\nwhen using the default Pod Security Admissions created by Talos you need the following labels on your namespace:\n\nCopy\n    pod-security.kubernetes.io/audit: privileged\n\n    pod-security.kubernetes.io/enforce: privileged\n\n    pod-security.kubernetes.io/warn: privileged\n\n\nor via kubectl:\n\nCopy\nkubectl label ns openebs pod-security.kubernetes.io/audit=privileged pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/warn=privileged\n\nNumber of Replicas\n\nBy Default Jiva uses 3 replicas if your cluster consists of lesser nodes consider setting defaultPolicy.replicas to the number of nodes in your cluster e.g. 2.\n\nPatching the jiva installation\n\nSince Jiva assumes iscisd to be running natively on the host and not as a Talos extension service, we need to modify the CSI node daemonset to enable it to find the PID of the iscsid service. The default config map used by Jiva also needs to be modified so that it can execute iscsiadm commands inside the PID namespace of the iscsid service.\n\nStart by creating a configmap definition named config.yaml as below:\n\nCopy\napiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\n  labels:\n\n    app.kubernetes.io/managed-by: pulumi\n\n  name: openebs-jiva-csi-iscsiadm\n\n  namespace: openebs\n\ndata:\n\n  iscsiadm: |\n\n    #!/bin/sh\n\n    iscsid_pid=$(pgrep iscsid)\n\n\n\n    nsenter --mount=\"/proc/${iscsid_pid}/ns/mnt\" --net=\"/proc/${iscsid_pid}/ns/net\" -- /usr/local/sbin/iscsiadm \"$@\"    \n\n\nReplace the existing config map with the above config map by running the following command:\n\nCopy\nkubectl --namespace openebs apply --filename config.yaml\n\n\nNow we need to update the jiva CSI daemonset to run with hostPID: true so it can find the PID of the iscsid service, by running the following command:\n\nCopy\nkubectl --namespace openebs patch daemonset openebs-jiva-csi-node --type=json --patch '[{\"op\": \"add\", \"path\": \"/spec/template/spec/hostPID\", \"value\": true}]'\n\nTesting a simple workload\n\nIn order to test the Jiva installation, let’s first create a PVC referencing the openebs-jiva-csi-default storage class:\n\nCopy\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: example-jiva-csi-pvc\n\nspec:\n\n  storageClassName: openebs-jiva-csi-default\n\n  accessModes:\n\n    - ReadWriteOnce\n\n  resources:\n\n    requests:\n\n      storage: 4Gi\n\n\nand then create a deployment using the above PVC:\n\nCopy\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\n  name: fio\n\nspec:\n\n  selector:\n\n    matchLabels:\n\n      name: fio\n\n  replicas: 1\n\n  strategy:\n\n    type: Recreate\n\n    rollingUpdate: null\n\n  template:\n\n    metadata:\n\n      labels:\n\n        name: fio\n\n    spec:\n\n      containers:\n\n      - name: perfrunner\n\n        image: openebs/tests-fio\n\n        command: [\"/bin/bash\"]\n\n        args: [\"-c\", \"while true ;do sleep 50; done\"]\n\n        volumeMounts:\n\n        - mountPath: /datadir\n\n          name: fio-vol\n\n      volumes:\n\n      - name: fio-vol\n\n        persistentVolumeClaim:\n\n          claimName: example-jiva-csi-pvc\n\n\nYou can clean up the test resources by running the following command:\n\nCopy\nkubectl delete deployment fio\n\nkubectl delete pvc example-jiva-csi-pvc\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Pod Security | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/pod-security/",
    "html": "Documentation\nKubernetes Guides\nConfiguration\nPod Security\nPod Security\nEnabling Pod Security Admission plugin to configure Pod Security Standards.\n\nKubernetes deprecated Pod Security Policy as of v1.21, and it was removed in v1.25.\n\nPod Security Policy was replaced with Pod Security Admission, which is enabled by default starting with Kubernetes v1.23.\n\nTalos Linux by default enables and configures Pod Security Admission plugin to enforce Pod Security Standards with the baseline profile as the default enforced with the exception of kube-system namespace which enforces privileged profile.\n\nSome applications (e.g. Prometheus node exporter or storage solutions) require more relaxed Pod Security Standards, which can be configured by either updating the Pod Security Admission plugin configuration, or by using the pod-security.kubernetes.io/enforce label on the namespace level:\n\nCopy\nkubectl label namespace NAMESPACE-NAME pod-security.kubernetes.io/enforce=privileged\n\nConfiguration\n\nTalos provides default Pod Security Admission in the machine configuration:\n\nCopy\napiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\nkind: PodSecurityConfiguration\n\ndefaults:\n\n    enforce: \"baseline\"\n\n    enforce-version: \"latest\"\n\n    audit: \"restricted\"\n\n    audit-version: \"latest\"\n\n    warn: \"restricted\"\n\n    warn-version: \"latest\"\n\nexemptions:\n\n    usernames: []\n\n    runtimeClasses: []\n\n    namespaces: [kube-system]\n\n\nThis is a cluster-wide configuration for the Pod Security Admission plugin:\n\nby default baseline Pod Security Standard profile is enforced\nmore strict restricted profile is not enforced, but API server warns about found issues\n\nThis default policy can be modified by updating the generated machine configuration before the cluster is created or on the fly by using the talosctl CLI utility.\n\nVerify current admission plugin configuration with:\n\nCopy\n$ talosctl get admissioncontrolconfigs.kubernetes.talos.dev admission-control -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: AdmissionControlConfigs.kubernetes.talos.dev\n\n    id: admission-control\n\n    version: 1\n\n    owner: config.K8sControlPlaneController\n\n    phase: running\n\n    created: 2022-02-22T20:28:21Z\n\n    updated: 2022-02-22T20:28:21Z\n\nspec:\n\n    config:\n\n        - name: PodSecurity\n\n          configuration:\n\n            apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n            defaults:\n\n                audit: restricted\n\n                audit-version: latest\n\n                enforce: baseline\n\n                enforce-version: latest\n\n                warn: restricted\n\n                warn-version: latest\n\n            exemptions:\n\n                namespaces:\n\n                    - kube-system\n\n                runtimeClasses: []\n\n                usernames: []\n\n            kind: PodSecurityConfiguration\n\nUsage\n\nCreate a deployment that satisfies the baseline policy but gives warnings on restricted policy:\n\nCopy\n$ kubectl create deployment nginx --image=nginx\n\nWarning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndeployment.apps/nginx created\n\n$ kubectl get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nnginx-85b98978db-j68l8   1/1     Running   0          2m3s\n\n\nCreate a daemonset which fails to meet requirements of the baseline policy:\n\nCopy\napiVersion: apps/v1\n\nkind: DaemonSet\n\nmetadata:\n\n  labels:\n\n    app: debug-container\n\n  name: debug-container\n\n  namespace: default\n\nspec:\n\n  revisionHistoryLimit: 10\n\n  selector:\n\n    matchLabels:\n\n      app: debug-container\n\n  template:\n\n    metadata:\n\n      creationTimestamp: null\n\n      labels:\n\n        app: debug-container\n\n    spec:\n\n      containers:\n\n      - args:\n\n        - \"360000\"\n\n        command:\n\n        - /bin/sleep\n\n        image: ubuntu:latest\n\n        imagePullPolicy: IfNotPresent\n\n        name: debug-container\n\n        resources: {}\n\n        securityContext:\n\n          privileged: true\n\n        terminationMessagePath: /dev/termination-log\n\n        terminationMessagePolicy: File\n\n      dnsPolicy: ClusterFirstWithHostNet\n\n      hostIPC: true\n\n      hostPID: true\n\n      hostNetwork: true\n\n      restartPolicy: Always\n\n      schedulerName: default-scheduler\n\n      securityContext: {}\n\n      terminationGracePeriodSeconds: 30\n\n  updateStrategy:\n\n    rollingUpdate:\n\n      maxSurge: 0\n\n      maxUnavailable: 1\n\n    type: RollingUpdate\n\nCopy\n$ kubectl apply -f debug.yaml\n\nWarning: would violate PodSecurity \"restricted:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true), allowPrivilegeEscalation != false (container \"debug-container\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"debug-container\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"debug-container\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"debug-container\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n\ndaemonset.apps/debug-container created\n\n\nDaemonset debug-container gets created, but no pods are scheduled:\n\nCopy\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   0         0         0       0            0           <none>          34s\n\n\nPod Security Admission plugin errors are in the daemonset events:\n\nCopy\n$ kubectl describe ds debug-container\n\n...\n\n  Warning  FailedCreate  92s                daemonset-controller  Error creating: pods \"debug-container-kwzdj\" is forbidden: violates PodSecurity \"baseline:latest\": host namespaces (hostNetwork=true, hostPID=true, hostIPC=true), privileged (container \"debug-container\" must not set securityContext.privileged=true)\n\n\nPod Security Admission configuration can also be overridden on a namespace level:\n\nCopy\n$ kubectl label ns default pod-security.kubernetes.io/enforce=privileged\n\nnamespace/default labeled\n\n$ kubectl get ds\n\nNAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\ndebug-container   2         2         0       2            0           <none>          4s\n\n\nAs enforce policy was updated to the privileged for the default namespace, debug-container is now successfully running.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Local Storage | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/local-storage/",
    "html": "Documentation\nKubernetes Guides\nConfiguration\nLocal Storage\nLocal Storage\nUsing local storage for Kubernetes workloads.\n\nUsing local storage for Kubernetes workloads implies that the pod will be bound to the node where the local storage is available. Local storage is not replicated, so in case of a machine failure contents of the local storage will be lost.\n\nNote: when using EPHEMERAL Talos partition (/var), make sure to use --preserve set while performing upgrades, otherwise you risk losing data.\n\nhostPath mounts\n\nThe simplest way to use local storage is to use hostPath mounts. When using hostPath mounts, make sure the root directory of the mount is mounted into the kubelet container:\n\nCopy\nmachine:\n\n  kubelet:\n\n    extraMounts:\n\n      - destination: /var/mnt\n\n        type: bind\n\n        source: /var/mnt\n\n        options:\n\n          - bind\n\n          - rshared\n\n          - rw\n\n\nBoth EPHEMERAL partition and user disks can be used for hostPath mounts.\n\nLocal Path Provisioner\n\nLocal Path Provisioner can be used to dynamically provision local storage. Make sure to update its configuration to use a path under /var, e.g. /var/local-path-provisioner as the root path for the local storage. (In Talos Linux default local path provisioner path /opt/local-path-provisioner is read-only).\n\nFor example, Local Path Provisioner can be installed using kustomize with the following configuration:\n\nCopy\n# kustomization.yaml\n\napiVersion: kustomize.config.k8s.io/v1beta1\n\nkind: Kustomization\n\nresources:\n\n- github.com/rancher/local-path-provisioner/deploy?ref=v0.0.26\n\npatches:\n\n- patch: |-\n\n    kind: ConfigMap\n\n    apiVersion: v1\n\n    metadata:\n\n      name: local-path-config\n\n      namespace: local-path-storage\n\n    data:\n\n      config.json: |-\n\n        {\n\n                \"nodePathMap\":[\n\n                {\n\n                        \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n\n                        \"paths\":[\"/var/local-path-provisioner\"]\n\n                }\n\n                ]\n\n        }    \n\n- patch: |-\n\n    apiVersion: storage.k8s.io/v1\n\n    kind: StorageClass\n\n    metadata:\n\n      name: local-path\n\n      annotations:\n\n        storageclass.kubernetes.io/is-default-class: \"true\"    \n\n- patch: |-\n\n    apiVersion: v1\n\n    kind: Namespace\n\n    metadata:\n\n      name: local-path-storage\n\n      labels:\n\n        pod-security.kubernetes.io/enforce: privileged    \n\n\nPut kustomization.yaml into a new directory, and run kustomize build | kubectl apply -f - to install Local Path Provisioner to a Talos Linux cluster. There are three patches applied:\n\nchange default /opt/local-path-provisioner path to /var/local-path-provisioner\nmake local-path storage class the default storage class (optional)\nlabel the local-path-storage namespace as privileged to allow privileged pods to be scheduled there\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "KubePrism | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/kubeprism/",
    "html": "Enabling KubePrism\nHow it works\nNotes\nDocumentation\nKubernetes Guides\nConfiguration\nKubePrism\nKubePrism\nEnabling in-cluster highly-available controlplane endpoint.\n\nKubernetes pods running in CNI mode can use the kubernetes.default.svc service endpoint to access the Kubernetes API server, while pods running in host networking mode can only use the external cluster endpoint to access the Kubernetes API server.\n\nKubernetes controlplane components run in host networking mode, and it is critical for them to be able to access the Kubernetes API server, same as CNI components (when CNI requires access to Kubernetes API).\n\nThe external cluster endpoint might be unavailable due to misconfiguration or network issues, or it might have higher latency than the internal endpoint. A failure to access the Kubernetes API server might cause a series of issues in the cluster: pods are not scheduled, service IPs stop working, etc.\n\nKubePrism feature solves this problem by enabling in-cluster highly-available controlplane endpoint on every node in the cluster.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nEnabling KubePrism\n\nAs of Talos 1.6, KubePrism is enabled by default with port 7445.\n\nNote: the port specified should be available on every node in the cluster.\n\nHow it works\n\nTalos spins up a TCP loadbalancer on every machine on the localhost on the specified port which automatically picks up one of the endpoints:\n\nthe external cluster endpoint as specified in the machine configuration\nfor controlplane machines: https://localhost:<api-server-local-port> (http://localhost:6443 in the default configuration)\nhttps://<controlplane-address>:<api-server-port> for every controlplane machine (based on the information from Cluster Discovery)\n\nKubePrism automatically filters out unhealthy (or unreachable) endpoints, and prefers lower-latency endpoints over higher-latency endpoints.\n\nTalos automatically reconfigures kubelet, kube-scheduler and kube-controller-manager to use the KubePrism endpoint. The kube-proxy manifest is also reconfigured to use the KubePrism endpoint by default, but when enabling KubePrism for a running cluster the manifest should be updated with talosctl upgrade-k8s command.\n\nWhen using CNI components that require access to the Kubernetes API server, the KubePrism endpoint should be passed to the CNI configuration (e.g. Cilium, Calico CNIs).\n\nNotes\n\nAs the list of endpoints for KubePrism includes the external cluster endpoint, KubePrism in the worst case scenario will behave the same as the external cluster endpoint. For controlplane nodes, the KubePrism should pick up the localhost endpoint of the kube-apiserver, minimizing the latency. Worker nodes might use direct address of the controlplane endpoint if the latency is lower than the latency of the external cluster endpoint.\n\nKubePrism listen endpoint is bound to localhost address, so it can’t be used outside the cluster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "iSCSI Storage with Synology CSI | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/synology-csi/",
    "html": "Background\nPrerequisites\nSetting up the Synology user account\nSetting up the Synology CSI\nConfigure connection info\nBuild the Talos-compatible image\nConfigure the CSI driver\nApply YAML manifests\nRun performance tests\nDocumentation\nKubernetes Guides\nConfiguration\niSCSI Storage with Synology CSI\niSCSI Storage with Synology CSI\nAutomatically provision iSCSI volumes on a Synology NAS with the synology-csi driver.\nBackground\n\nSynology is a company that specializes in Network Attached Storage (NAS) devices. They provide a number of features within a simple web OS, including an LDAP server, Docker support, and (perhaps most relevant to this guide) function as an iSCSI host. The focus of this guide is to allow a Kubernetes cluster running on Talos to provision Kubernetes storage (both dynamic or static) on a Synology NAS using a direct integration, rather than relying on an intermediary layer like Rook/Ceph or Maystor.\n\nThis guide assumes a very basic familiarity with iSCSI terminology (LUN, iSCSI target, etc.).\n\nPrerequisites\nSynology NAS running DSM 7.0 or above\nProvisioned Talos cluster running Kubernetes v1.20 or above\n(Optional) Both Volume Snapshot CRDs and the common snapshot controller must be installed in your Kubernetes cluster if you want to use the Snapshot feature\nSetting up the Synology user account\n\nThe synology-csi controller interacts with your NAS in two different ways: via the API and via the iSCSI protocol. Actions such as creating a new iSCSI target or deleting an old one are accomplished via the Synology API, and require administrator access. On the other hand, mounting the disk to a pod and reading from / writing to it will utilize iSCSI. Because you can only authenticate with one account per DSM configured, that account needs to have admin privileges. In order to minimize access in the case of these credentials being compromised, you should configure the account with the lease possible amount of access – explicitly specify “No Access” on all volumes when configuring the user permissions.\n\nSetting up the Synology CSI\n\nNote: this guide is paraphrased from the Synology CSI readme. Please consult the readme for more in-depth instructions and explanations.\n\nClone the git repository.\n\nCopy\ngit clone https://github.com/zebernst/synology-csi-talos.git\n\n\nWhile Synology provides some automated scripts to deploy the CSI driver, they can be finicky especially when making changes to the source code. We will be configuring and deploying things manually in this guide.\n\nThe relevant files we will be touching are in the following locations:\n\nCopy\n.\n\n├── Dockerfile\n\n├── Makefile\n\n├── config\n\n│   └── client-info-template.yml\n\n└── deploy\n\n    └── kubernetes\n\n        └── v1.20\n\n            ├── controller.yml\n\n            ├── csi-driver.yml\n\n            ├── namespace.yml\n\n            ├── node.yml\n\n            ├── snapshotter\n\n            │   ├── snapshotter.yaml\n\n            │   └── volume-snapshot-class.yml\n\n            └── storage-class.yml\n\nConfigure connection info\n\nUse config/client-info-template.yml as an example to configure the connection information for DSM. You can specify one or more storage systems on which the CSI volumes will be created. See below for an example:\n\nCopy\n---\n\nclients:\n\n- host: 192.168.1.1   # ipv4 address or domain of the DSM\n\n  port: 5000          # port for connecting to the DSM\n\n  https: false        # set this true to use https. you need to specify the port to DSM HTTPS port as well\n\n  username: username  # username\n\n  password: password  # password\n\n\nCreate a Kubernetes secret using the client information config file.\n\nCopy\nkubectl create secret -n synology-csi generic client-info-secret --from-file=config/client-info.yml\n\n\nNote that if you rename the secret to something other than client-info-secret, make sure you update the corresponding references in the deployment manifests as well.\n\nBuild the Talos-compatible image\n\nModify the Makefile so that the image is built and tagged under your GitHub Container Registry username:\n\nCopy\nREGISTRY_NAME=ghcr.io/<username>\n\n\nWhen you run make docker-build or make docker-build-multiarch, it will push the resulting image to ghcr.io/<username>/synology-csi:v1.1.0. Ensure that you find and change any reference to synology/synology-csi:v1.1.0 to point to your newly-pushed image within the deployment manifests.\n\nConfigure the CSI driver\n\nBy default, the deployment manifests include one storage class and one volume snapshot class. See below for examples:\n\nCopy\n---\n\napiVersion: storage.k8s.io/v1\n\nkind: StorageClass\n\nmetadata:\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\n  name: syno-storage\n\nprovisioner: csi.san.synology.com\n\nparameters:\n\n  fsType: 'ext4'\n\n  dsm: '192.168.1.1'\n\n  location: '/volume1'\n\nreclaimPolicy: Retain\n\nallowVolumeExpansion: true\n\n---\n\napiVersion: snapshot.storage.k8s.io/v1\n\nkind: VolumeSnapshotClass\n\nmetadata:\n\n  name: syno-snapshot\n\n  annotations:\n\n    storageclass.kubernetes.io/is-default-class: \"false\"\n\ndriver: csi.san.synology.com\n\ndeletionPolicy: Delete\n\nparameters:\n\n  description: 'Kubernetes CSI'\n\n\nIt can be useful to configure multiple different StorageClasses. For example, a popular strategy is to create two nearly identical StorageClasses, with one configured with reclaimPolicy: Retain and the other with reclaimPolicy: Delete. Alternately, a workload may require a specific filesystem, such as ext4. If a Synology NAS is going to be the most common way to configure storage on your cluster, it can be convenient to add the storageclass.kubernetes.io/is-default-class: \"true\" annotation to one of your StorageClasses.\n\nThe following table details the configurable parameters for the Synology StorageClass.\n\nName\tType\tDescription\tDefault\tSupported protocols\ndsm\tstring\tThe IPv4 address of your DSM, which must be included in the client-info.yml for the CSI driver to log in to DSM\t-\tiSCSI, SMB\nlocation\tstring\tThe location (/volume1, /volume2, …) on DSM where the LUN for PersistentVolume will be created\t-\tiSCSI, SMB\nfsType\tstring\tThe formatting file system of the PersistentVolumes when you mount them on the pods. This parameter only works with iSCSI. For SMB, the fsType is always ‘cifs‘.\text4\tiSCSI\nprotocol\tstring\tThe backing storage protocol. Enter ‘iscsi’ to create LUNs or ‘smb‘ to create shared folders on DSM.\tiscsi\tiSCSI, SMB\ncsi.storage.k8s.io/node-stage-secret-name\tstring\tThe name of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\ncsi.storage.k8s.io/node-stage-secret-namespace\tstring\tThe namespace of node-stage-secret. Required if DSM shared folder is accessed via SMB.\t-\tSMB\n\nThe VolumeSnapshotClass can be similarly configured with the following parameters:\n\nName\tType\tDescription\tDefault\tSupported protocols\ndescription\tstring\tThe description of the snapshot on DSM\t-\tiSCSI\nis_locked\tstring\tWhether you want to lock the snapshot on DSM\tfalse\tiSCSI, SMB\nApply YAML manifests\n\nOnce you have created the desired StorageClass(es) and VolumeSnapshotClass(es), the final step is to apply the Kubernetes manifests against the cluster. The easiest way to apply them all at once is to create a kustomization.yaml file in the same directory as the manifests and use Kustomize to apply:\n\nCopy\nkubectl apply -k path/to/manifest/directory\n\n\nAlternately, you can apply each manifest one-by-one:\n\nCopy\nkubectl apply -f <file>\n\nRun performance tests\n\nIn order to test the provisioning, mounting, and performance of using a Synology NAS as Kubernetes persistent storage, use the following command:\n\nCopy\nkubectl apply -f speedtest.yaml\n\n\nContent of speedtest.yaml (source)\n\nCopy\nkind: PersistentVolumeClaim\n\napiVersion: v1\n\nmetadata:\n\n  name: test-claim\n\nspec:\n\n#  storageClassName: syno-storage\n\n  accessModes:\n\n  - ReadWriteMany\n\n  resources:\n\n    requests:\n\n      storage: 5G\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: read\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: read\n\n      labels:\n\n        app: speedtest\n\n        job: read\n\n    spec:\n\n      containers:\n\n      - name: read\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/mnt/pv/test.img\",\"of=/dev/null\",\"bs=8k\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n---\n\napiVersion: batch/v1\n\nkind: Job\n\nmetadata:\n\n  name: write\n\nspec:\n\n  template:\n\n    metadata:\n\n      name: write\n\n      labels:\n\n        app: speedtest\n\n        job: write\n\n    spec:\n\n      containers:\n\n      - name: write\n\n        image: ubuntu:xenial\n\n        command: [\"dd\",\"if=/dev/zero\",\"of=/mnt/pv/test.img\",\"bs=1G\",\"count=1\",\"oflag=dsync\"]\n\n        volumeMounts:\n\n        - mountPath: \"/mnt/pv\"\n\n          name: test-volume\n\n      volumes:\n\n      - name: test-volume\n\n        persistentVolumeClaim:\n\n          claimName: test-claim\n\n      restartPolicy: Never\n\n\nIf these two jobs complete successfully, use the following commands to get the results of the speed tests:\n\nCopy\n# Pod logs for read test:\n\nkubectl logs -l app=speedtest,job=read\n\n\n\n# Pod logs for write test:\n\nkubectl logs -l app=speedtest,job=write\n\n\nWhen you’re satisfied with the results of the test, delete the artifacts created from the speedtest:\n\nCopy\nkubectl delete -f speedtest.yaml\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Deploying Metrics Server | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/deploy-metrics-server/",
    "html": "Node Configuration\nInstall During Bootstrap\nInstall After Bootstrap\nDocumentation\nKubernetes Guides\nConfiguration\nDeploying Metrics Server\nDeploying Metrics Server\nIn this guide you will learn how to set up metrics-server.\n\nMetrics Server enables use of the Horizontal Pod Autoscaler and Vertical Pod Autoscaler. It does this by gathering metrics data from the kubelets in a cluster. By default, the certificates in use by the kubelets will not be recognized by metrics-server. This can be solved by either configuring metrics-server to do no validation of the TLS certificates, or by modifying the kubelet configuration to rotate its certificates and use ones that will be recognized by metrics-server.\n\nNode Configuration\n\nTo enable kubelet certificate rotation, all nodes should have the following Machine Config snippet:\n\nCopy\nmachine:\n\n  kubelet:\n\n    extraArgs:\n\n      rotate-server-certificates: true\n\nInstall During Bootstrap\n\nWe will want to ensure that new certificates for the kubelets are approved automatically. This can easily be done with the Kubelet Serving Certificate Approver, which will automatically approve the Certificate Signing Requests generated by the kubelets.\n\nWe can have Kubelet Serving Certificate Approver and metrics-server installed on the cluster automatically during bootstrap by adding the following snippet to the Cluster Config of the node that will be handling the bootstrap process:\n\nCopy\ncluster:\n\n  extraManifests:\n\n    - https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\n    - https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nInstall After Bootstrap\n\nIf you choose not to use extraManifests to install Kubelet Serving Certificate Approver and metrics-server during bootstrap, you can install them once the cluster is online using kubectl:\n\nCopy\nkubectl apply -f https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Ceph Storage cluster with Rook | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/ceph-with-rook/",
    "html": "Preparation\nInstallation\nTalos Linux Considerations\nCleaning Up\nRook Ceph Cluster Removal\nTalos Linux Rook Metadata Removal\nDocumentation\nKubernetes Guides\nConfiguration\nCeph Storage cluster with Rook\nCeph Storage cluster with Rook\nGuide on how to create a simple Ceph storage cluster with Rook for Kubernetes\nPreparation\n\nTalos Linux reserves an entire disk for the OS installation, so machines with multiple available disks are needed for a reliable Ceph cluster with Rook and Talos Linux. Rook requires that the block devices or partitions used by Ceph have no partitions or formatted filesystems before use. Rook also requires a minimum Kubernetes version of v1.16 and Helm v3.0 for installation of charts. It is highly recommended that the Rook Ceph overview is read and understood before deploying a Ceph cluster with Rook.\n\nInstallation\n\nCreating a Ceph cluster with Rook requires two steps; first the Rook Operator needs to be installed which can be done with a Helm Chart. The example below installs the Rook Operator into the rook-ceph namespace, which is the default for a Ceph cluster with Rook.\n\nCopy\n$ helm repo add rook-release https://charts.rook.io/release\n\n\"rook-release\" has been added to your repositories\n\n\n\n$ helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph\n\nW0327 17:52:44.277830   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nW0327 17:52:44.612243   54987 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nNAME: rook-ceph\n\nLAST DEPLOYED: Sun Mar 27 17:52:42 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Rook Operator has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get pods -l \"app=rook-ceph-operator\"\n\n\n\nVisit https://rook.io/docs/rook/latest for instructions on how to create and configure Rook clusters\n\n\n\nImportant Notes:\n\n- You must customize the 'CephCluster' resource in the sample manifests for your cluster.\n\n- Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the namespace.\n\n- The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.\n\n- The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.\n\n- Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).\n\n\nOnce that is complete, the Ceph cluster can be installed with the official Helm Chart. The Chart can be installed with default values, which will attempt to use all nodes in the Kubernetes cluster, and all unused disks on each node for Ceph storage, and make available block storage, object storage, as well as a shared filesystem. Generally more specific node/device/cluster configuration is used, and the Rook documentation explains all the available options in detail. For this example the defaults will be adequate.\n\nCopy\n$ helm install --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster\n\nNAME: rook-ceph-cluster\n\nLAST DEPLOYED: Sun Mar 27 18:12:46 2022\n\nNAMESPACE: rook-ceph\n\nSTATUS: deployed\n\nREVISION: 1\n\nTEST SUITE: None\n\nNOTES:\n\nThe Ceph Cluster has been installed. Check its status by running:\n\n  kubectl --namespace rook-ceph get cephcluster\n\n\n\nVisit https://rook.github.io/docs/rook/latest/ceph-cluster-crd.html for more information about the Ceph CRD.\n\n\n\nImportant Notes:\n\n- You can only deploy a single cluster per namespace\n\n- If you wish to delete this cluster and start fresh, you will also have to wipe the OSD disks using `sfdisk`\n\n\nNow the Ceph cluster configuration has been created, the Rook operator needs time to install the Ceph cluster and bring all the components online. The progression of the Ceph cluster state can be followed with the following command.\n\nCopy\n$ watch kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nEvery 2.0s: kubectl --namespace rook-ceph get cephcluster rook-ceph\n\n\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                 HEALTH   EXTERNAL\n\nrook-ceph   /var/lib/rook     3          57s   Progressing   Configuring Ceph Mons\n\n\nDepending on the size of the Ceph cluster and the availability of resources the Ceph cluster should become available, and with it the storage classes that can be used with Kubernetes Physical Volumes.\n\nCopy\n$ kubectl --namespace rook-ceph get cephcluster rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          40m   Ready   Cluster created successfully   HEALTH_OK\n\n\n\n$ kubectl  get storageclass\n\nNAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n\nceph-block (default)   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   77m\n\nceph-bucket            rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  77m\n\nceph-filesystem        rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   77m\n\nTalos Linux Considerations\n\nIt is important to note that a Rook Ceph cluster saves cluster information directly onto the node (by default dataDirHostPath is set to /var/lib/rook). If running only a single mon instance, cluster management is little bit more involved, as any time a Talos Linux node is reconfigured or upgraded, the partition that stores the /var file system is wiped, but the --preserve option of talosctl upgrade will ensure that doesn’t happen.\n\nBy default, Rook configues Ceph to have 3 mon instances, in which case the data stored in dataDirHostPath can be regenerated from the other mon instances. So when performing maintenance on a Talos Linux node with a Rook Ceph cluster (e.g. upgrading the Talos Linux version), it is imperative that care be taken to maintain the health of the Ceph cluster. Before upgrading, you should always check the health status of the Ceph cluster to ensure that it is healthy.\n\nCopy\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io rook-ceph\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH      EXTERNAL\n\nrook-ceph   /var/lib/rook     3          98m   Ready   Cluster created successfully   HEALTH_OK\n\n\nIf it is, you can begin the upgrade process for the Talos Linux node, during which time the Ceph cluster will become unhealthy as the node is reconfigured. Before performing any other action on the Talos Linux nodes, the Ceph cluster must return to a healthy status.\n\nCopy\n$ talosctl upgrade --nodes 172.20.15.5 --image ghcr.io/talos-systems/installer:v0.14.3\n\nNODE          ACK                        STARTED\n\n172.20.15.5   Upgrade request received   2022-03-27 20:29:55.292432887 +0200 CEST m=+10.050399758\n\n\n\n$ kubectl --namespace rook-ceph get cephclusters.ceph.rook.io\n\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE         MESSAGE                   HEALTH        EXTERNAL\n\nrook-ceph   /var/lib/rook     3          99m   Progressing   Configuring Ceph Mgr(s)   HEALTH_WARN\n\n\n\n$ kubectl --namespace rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n\ncephcluster.ceph.rook.io/rook-ceph condition met\n\n\nThe above steps need to be performed for each Talos Linux node undergoing maintenance, one at a time.\n\nCleaning Up\nRook Ceph Cluster Removal\n\nRemoving a Rook Ceph cluster requires a few steps, starting with signalling to Rook that the Ceph cluster is really being destroyed. Then all Persistent Volumes (and Claims) backed by the Ceph cluster must be deleted, followed by the Storage Classes and the Ceph storage types.\n\nCopy\n$ kubectl --namespace rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\ncephcluster.ceph.rook.io/rook-ceph patched\n\n\n\n$ kubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nstorageclass.storage.k8s.io \"ceph-block\" deleted\n\nstorageclass.storage.k8s.io \"ceph-bucket\" deleted\n\nstorageclass.storage.k8s.io \"ceph-filesystem\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephblockpools ceph-blockpool\n\ncephblockpool.ceph.rook.io \"ceph-blockpool\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephobjectstore ceph-objectstore\n\ncephobjectstore.ceph.rook.io \"ceph-objectstore\" deleted\n\n\n\n$ kubectl --namespace rook-ceph delete cephfilesystem ceph-filesystem\n\ncephfilesystem.ceph.rook.io \"ceph-filesystem\" deleted\n\n\nOnce that is complete, the Ceph cluster itself can be removed, along with the Rook Ceph cluster Helm chart installation.\n\nCopy\n$ kubectl --namespace rook-ceph delete cephcluster rook-ceph\n\ncephcluster.ceph.rook.io \"rook-ceph\" deleted\n\n\n\n$ helm --namespace rook-ceph uninstall rook-ceph-cluster\n\nrelease \"rook-ceph-cluster\" uninstalled\n\n\nIf needed, the Rook Operator can also be removed along with all the Custom Resource Definitions that it created.\n\nCopy\n$ helm --namespace rook-ceph uninstall rook-ceph\n\nW0328 12:41:14.998307  147203 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n\nThese resources were kept due to the resource policy:\n\n[CustomResourceDefinition] cephblockpools.ceph.rook.io\n\n[CustomResourceDefinition] cephbucketnotifications.ceph.rook.io\n\n[CustomResourceDefinition] cephbuckettopics.ceph.rook.io\n\n[CustomResourceDefinition] cephclients.ceph.rook.io\n\n[CustomResourceDefinition] cephclusters.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemmirrors.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystems.ceph.rook.io\n\n[CustomResourceDefinition] cephfilesystemsubvolumegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephnfses.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectrealms.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstores.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectstoreusers.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzonegroups.ceph.rook.io\n\n[CustomResourceDefinition] cephobjectzones.ceph.rook.io\n\n[CustomResourceDefinition] cephrbdmirrors.ceph.rook.io\n\n[CustomResourceDefinition] objectbucketclaims.objectbucket.io\n\n[CustomResourceDefinition] objectbuckets.objectbucket.io\n\n\n\nrelease \"rook-ceph\" uninstalled\n\n\n\n$ kubectl delete crds cephblockpools.ceph.rook.io cephbucketnotifications.ceph.rook.io cephbuckettopics.ceph.rook.io \\\n\n                      cephclients.ceph.rook.io cephclusters.ceph.rook.io cephfilesystemmirrors.ceph.rook.io \\\n\n                      cephfilesystems.ceph.rook.io cephfilesystemsubvolumegroups.ceph.rook.io \\\n\n                      cephnfses.ceph.rook.io cephobjectrealms.ceph.rook.io cephobjectstores.ceph.rook.io \\\n\n                      cephobjectstoreusers.ceph.rook.io cephobjectzonegroups.ceph.rook.io cephobjectzones.ceph.rook.io \\\n\n                      cephrbdmirrors.ceph.rook.io objectbucketclaims.objectbucket.io objectbuckets.objectbucket.io\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephblockpools.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbucketnotifications.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephbuckettopics.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclients.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephclusters.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystems.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephfilesystemsubvolumegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephnfses.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectrealms.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstores.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectstoreusers.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzonegroups.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephobjectzones.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"cephrbdmirrors.ceph.rook.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbucketclaims.objectbucket.io\" deleted\n\ncustomresourcedefinition.apiextensions.k8s.io \"objectbuckets.objectbucket.io\" deleted\n\nTalos Linux Rook Metadata Removal\n\nIf the Rook Operator is cleanly removed following the above process, the node metadata and disks should be clean and ready to be re-used. In the case of an unclean cluster removal, there may be still a few instances of metadata stored on the system disk, as well as the partition information on the storage disks. First the node metadata needs to be removed, make sure to update the nodeName with the actual name of a storage node that needs cleaning, and path with the Rook configuration dataDirHostPath set when installing the chart. The following will need to be repeated for each node used in the Rook Ceph cluster.\n\nCopy\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-clean\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  volumes:\n\n  - name: rook-data-dir\n\n    hostPath:\n\n      path: <dataDirHostPath>\n\n  containers:\n\n  - name: disk-clean\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    volumeMounts:\n\n    - name: rook-data-dir\n\n      mountPath: /node/rook-data\n\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\n\nEOF\n\npod/disk-clean created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\n\npod/disk-clean condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-clean\" deleted\n\n\nLastly, the disks themselves need the partition and filesystem data wiped before they can be reused. Again, the following as to be repeated for each node and disk used in the Rook Ceph cluster, updating nodeName and of= in the command as needed.\n\nCopy\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\n  name: disk-wipe\n\nspec:\n\n  restartPolicy: Never\n\n  nodeName: <storage-node-name>\n\n  containers:\n\n  - name: disk-wipe\n\n    image: busybox\n\n    securityContext:\n\n      privileged: true\n\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=<device>\"]\n\nEOF\n\npod/disk-wipe created\n\n\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\n\npod/disk-wipe condition met\n\n\n\n$ kubectl delete pod disk-clean\n\npod \"disk-wipe\" deleted\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Upgrading Talos Linux | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/upgrading-talos/",
    "html": "Supported Upgrade Paths\nBefore Upgrade to v1.6.2\nVideo Walkthrough\nAfter Upgrade to v1.6.2\ntalosctl upgrade\nMachine Configuration Changes\nUpgrade Sequence\nFAQs\nDocumentation\nTalos Linux Guides\nUpgrading Talos Linux\nUpgrading Talos Linux\nGuide to upgrading a Talos Linux machine.\n\nOS upgrades are effected by an API call, which can be sent via the talosctl CLI utility.\n\nThe upgrade API call passes a node the installer image to use to perform the upgrade. Each Talos version has a corresponding installer image, listed on the release page for the version, for example v1.6.2.\n\nUpgrades use an A-B image scheme in order to facilitate rollbacks. This scheme retains the previous Talos kernel and OS image following each upgrade. If an upgrade fails to boot, Talos will roll back to the previous version. Likewise, Talos may be manually rolled back via API (or talosctl rollback), which will update the boot reference and reboot.\n\nUnless explicitly told to preserve data, an upgrade will cause the node to wipe the EPHEMERAL partition, remove itself from the etcd cluster (if it is a controlplane node), and make itself as pristine as is possible. (This is the desired behavior except in specialised use cases such as single-node clusters.)\n\nNote An upgrade of the Talos Linux OS will not (since v1.0) apply an upgrade to the Kubernetes version by default. Kubernetes upgrades should be managed separately per upgrading kubernetes.\n\nSupported Upgrade Paths\n\nBecause Talos Linux is image based, an upgrade is almost the same as installing Talos, with the difference that the system has already been initialized with a configuration. The supported configuration may change between versions. The upgrade process should handle such changes transparently, but this migration is only tested between adjacent minor releases. Thus the recommended upgrade path is to always upgrade to the latest patch release of all intermediate minor releases.\n\nFor example, if upgrading from Talos 1.0 to Talos 1.2.4, the recommended upgrade path would be:\n\nupgrade from 1.0 to latest patch of 1.0 - to v1.0.6\nupgrade from v1.0.6 to latest patch of 1.1 - to v1.1.2\nupgrade from v1.1.2 to v1.2.4\nBefore Upgrade to v1.6.2\n\nThere are no specific actions to be taken before an upgrade.\n\nPlease review the release notes for any changes that may affect your cluster.\n\nVideo Walkthrough\n\nTo see a live demo of an upgrade of Talos Linux, see the video below:\n\nAfter Upgrade to v1.6.2\n\nThere are no specific actions to be taken after an upgrade.\n\ntalosctl upgrade\n\nTo upgrade a Talos node, specify the node’s IP address and the installer container image for the version of Talos to upgrade to.\n\nFor instance, if your Talos node has the IP address 10.20.30.40 and you want to install the current version, you would enter a command such as:\n\nCopy\n  $ talosctl upgrade --nodes 10.20.30.40 \\\n\n      --image ghcr.io/siderolabs/installer:v1.6.2\n\n\nThere is an option to this command: --preserve, which will explicitly tell Talos to keep ephemeral data intact. In most cases, it is correct to let Talos perform its default action of erasing the ephemeral data. However, for a single-node control-plane, make sure that --preserve=true.\n\nRarely, an upgrade command will fail due to a process holding a file open on disk. In these cases, you can use the --stage flag. This puts the upgrade artifacts on disk, and adds some metadata to a disk partition that gets checked very early in the boot process, then reboots the node. On the reboot, Talos sees that it needs to apply an upgrade, and will do so immediately. Because this occurs in a just rebooted system, there will be no conflict with any files being held open. After the upgrade is applied, the node will reboot again, in order to boot into the new version. Note that because Talos Linux reboots via the kexec syscall, the extra reboot adds very little time.\n\nMachine Configuration Changes\n\nNew configuration documents:\n\nIngress Firewall configuration: NetworkRuleConfig and NetworkDefaultActionConfig.\n\nUpdates in v1alpha1 Config:\n\n.persist option was removed\n.machine.nodeTaints configures Kubernetes node taints\n.machine.kubelet.extraMounts supports new fields uidMappings and gidMappings\n.machine.kubelet.credendtialProviderConfig configures kubelet credential provider\n.machine.network.kubespan.harvestExtraEndpoints to disable harvesting extra endpoints\n.cluster.cni.flannel provides customization for the default Flannel CNI manifest\n.cluster.scheduler.config provides custom kube-scheduler configuration\nUpgrade Sequence\n\nWhen a Talos node receives the upgrade command, it cordons itself in Kubernetes, to avoid receiving any new workload. It then starts to drain its existing workload.\n\nNOTE: If any of your workloads are sensitive to being shut down ungracefully, be sure to use the lifecycle.preStop Pod spec.\n\nOnce all of the workload Pods are drained, Talos will start shutting down its internal processes. If it is a control node, this will include etcd. If preserve is not enabled, Talos will leave etcd membership. (Talos ensures the etcd cluster is healthy and will remain healthy after our node leaves the etcd cluster, before allowing a control plane node to be upgraded.)\n\nOnce all the processes are stopped and the services are shut down, the filesystems will be unmounted. This allows Talos to produce a very clean upgrade, as close as possible to a pristine system. We verify the disk and then perform the actual image upgrade. We set the bootloader to boot once with the new kernel and OS image, then we reboot.\n\nAfter the node comes back up and Talos verifies itself, it will make the bootloader change permanent, rejoin the cluster, and finally uncordon itself to receive new workloads.\n\nFAQs\n\nQ. What happens if an upgrade fails?\n\nA. Talos Linux attempts to safely handle upgrade failures.\n\nThe most common failure is an invalid installer image reference. In this case, Talos will fail to download the upgraded image and will abort the upgrade.\n\nSometimes, Talos is unable to successfully kill off all of the disk access points, in which case it cannot safely unmount all filesystems to effect the upgrade. In this case, it will abort the upgrade and reboot. (upgrade --stage can ensure that upgrades can occur even when the filesytems cannot be unmounted.)\n\nIt is possible (especially with test builds) that the upgraded Talos system will fail to start. In this case, the node will be rebooted, and the bootloader will automatically use the previous Talos kernel and image, thus effectively rolling back the upgrade.\n\nLastly, it is possible that Talos itself will upgrade successfully, start up, and rejoin the cluster but your workload will fail to run on it, for whatever reason. This is when you would use the talosctl rollback command to revert back to the previous Talos version.\n\nQ. Can upgrades be scheduled?\n\nA. Because the upgrade sequence is API-driven, you can easily tie it in to your own business logic to schedule and coordinate your upgrades.\n\nQ. Can the upgrade process be observed?\n\nA. Yes, using the talosctl dmesg -f command. You can also use talosctl upgrade --wait, and optionally talosctl upgrade --wait --debug to observe kernel logs\n\nQ. Are worker node upgrades handled differently from control plane node upgrades?\n\nA. Short answer: no.\n\nLong answer: Both node types follow the same set procedure. From the user’s standpoint, however, the processes are identical. However, since control plane nodes run additional services, such as etcd, there are some extra steps and checks performed on them. For instance, Talos will refuse to upgrade a control plane node if that upgrade would cause a loss of quorum for etcd. If multiple control plane nodes are asked to upgrade at the same time, Talos will protect the Kubernetes cluster by ensuring only one control plane node actively upgrades at any time, via checking etcd quorum. If running a single-node cluster, and you want to force an upgrade despite the loss of quorum, you can set preserve to true.\n\nQ. Can I break my cluster by upgrading everything at once?\n\nA. Possibly - it’s not recommended.\n\nNothing prevents the user from sending near-simultaneous upgrades to each node of the cluster - and while Talos Linux and Kubernetes can generally deal with this situation, other components of the cluster may not be able to recover from more than one node rebooting at a time. (e.g. any software that maintains a quorum or state across nodes, such as Rook/Ceph)\n\nQ. Which version of talosctl should I use to update a cluster?\n\nA. We recommend using the version that matches the current running version of the cluster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/configuration/",
    "html": "Documentation\nKubernetes Guides\nConfiguration\nConfiguration\nHow to configure components of the Kubernetes cluster itself.\nCeph Storage cluster with Rook\n\nGuide on how to create a simple Ceph storage cluster with Rook for Kubernetes\n\nDeploying Metrics Server\n\nIn this guide you will learn how to set up metrics-server.\n\niSCSI Storage with Synology CSI\n\nAutomatically provision iSCSI volumes on a Synology NAS with the synology-csi driver.\n\nKubePrism\n\nEnabling in-cluster highly-available controlplane endpoint.\n\nLocal Storage\n\nUsing local storage for Kubernetes workloads.\n\nPod Security\n\nEnabling Pod Security Admission plugin to configure Pod Security Standards.\n\nReplicated Local Storage\n\nUsing local storage with OpenEBS Jiva\n\nSeccomp Profiles\n\nUsing custom Seccomp Profiles with Kubernetes workloads.\n\nStorage\n\nSetting up storage for a Kubernetes cluster\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Interactive Dashboard | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/interactive-dashboard/",
    "html": "Summary Screen (F1)\nMonitor Screen (F2)\nNetwork Config Screen (F3)\nDocumentation\nTalos Linux Guides\nInteractive Dashboard\nInteractive Dashboard\nA tool to inspect the running Talos machine state on the physical video console.\n\nInteractive dashboard is enabled for all Talos platforms except for SBC images. The dashboard can be disabled with kernel parameter talos.dashboard.disabled=1.\n\nThe dashboard runs only on the physical video console (not serial console) on the 2nd virtual TTY. The first virtual TTY shows kernel logs same as in Talos <1.4.0. The virtual TTYs can be switched with <Alt+F1> and <Alt+F2> keys.\n\nKeys <F1> - <Fn> can be used to switch between different screens of the dashboard.\n\nThe dashboard is using either UEFI framebuffer or VGA/VESA framebuffer (for legacy BIOS boot). For legacy BIOS boot screen resolution can be controlled with the vga= kernel parameter.\n\nSummary Screen (F1)\n\nInteractive Dashboard Summary Screen\n\nThe header shows brief information about the node:\n\nhostname\nTalos version\nuptime\nCPU and memory hardware information\nCPU and memory load, number of processes\n\nTable view presents summary information about the machine:\n\nUUID (from SMBIOS data)\nCluster name (when the machine config is available)\nMachine stage: Installing, Upgrading, Booting, Maintenance, Running, Rebooting, Shutting down, etc.\nMachine stage readiness: checks Talos service status, static pod status, etc. (for Running stage)\nMachine type: controlplane/worker\nNumber of members discovered in the cluster\nKubernetes version\nStatus of Kubernetes components: kubelet and Kubernetes controlplane components (only on controlplane machines)\nNetwork information: Hostname, Addresses, Gateway, Connectivity, DNS and NTP servers\n\nBottom part of the screen shows kernel logs, same as on the virtual TTY 1.\n\nMonitor Screen (F2)\n\nInteractive Dashboard Monitor Screen\n\nMonitor screen provides live view of the machine resource usage: CPU, memory, disk, network and processes.\n\nNetwork Config Screen (F3)\n\nNote: network config screen is only available for metal platform.\n\nInteractive Dashboard Network Config Screen\n\nNetwork config screen provides editing capabilities for the metal platform network configuration.\n\nThe screen is split into three sections:\n\nthe leftmost section provides a way to enter network configuration: hostname, DNS and NTP servers, configure the network interface either via DHCP or static IP address, etc.\nthe middle section shows the current network configuration.\nthe rightmost section shows the network configuration which will be applied after pressing “Save” button.\n\nOnce the platform network configuration is saved, it is immediately applied to the machine.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Kubernetes Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/kubernetes-guides/",
    "html": "Documentation\nKubernetes Guides\nKubernetes Guides\nManagement of a Kubernetes Cluster hosted by Talos Linux\nConfiguration\n\nHow to configure components of the Kubernetes cluster itself.\n\nNetwork\n\nManaging the Kubernetes cluster networking\n\nUpgrading Kubernetes\n\nGuide on how to upgrade the Kubernetes cluster from Talos Linux.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Resetting a Machine | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/resetting-a-machine/",
    "html": "Documentation\nTalos Linux Guides\nResetting a Machine\nResetting a Machine\nSteps on how to reset a Talos Linux machine to a clean state.\n\nFrom time to time, it may be beneficial to reset a Talos machine to its “original” state. Bear in mind that this is a destructive action for the given machine. Doing this means removing the machine from Kubernetes, etcd (if applicable), and clears any data on the machine that would normally persist a reboot.\n\nCLI\n\nWARNING: Running a talosctl reset on cloud VM’s might result in the VM being unable to boot as this wipes the entire disk. It might be more useful to just wipe the STATE and EPHEMERAL partitions on a cloud VM if not booting via iPXE. talosctl reset --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL\n\nThe API command for doing this is talosctl reset. There are a couple of flags as part of this command:\n\nCopy\nFlags:\n\n      --graceful                        if true, attempt to cordon/drain node and leave etcd (if applicable) (default true)\n\n      --reboot                          if true, reboot the node after resetting instead of shutting down\n\n      --system-labels-to-wipe strings   if set, just wipe selected system disk partitions by label but keep other partitions intact keep other partitions intact\n\n\nThe graceful flag is especially important when considering HA vs. non-HA Talos clusters. If the machine is part of an HA cluster, a normal, graceful reset should work just fine right out of the box as long as the cluster is in a good state. However, if this is a single node cluster being used for testing purposes, a graceful reset is not an option since Etcd cannot be “left” if there is only a single member. In this case, reset should be used with --graceful=false to skip performing checks that would normally block the reset.\n\nKernel Parameter\n\nAnother way to reset a machine is to specify talos.experimental.wipe=system kernel parameter. If the machine got stuck in the boot loop and you access to the console you can use GRUB to specify this kernel argument. Then when Talos boots for the next time it will reset system disk and reboot.\n\nNext steps can be to install Talos either using PXE boot or by mounting an ISO.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Discovery Service | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/discovery/",
    "html": "Registries\nDiscovery Service\nResource Definitions\nDiscovery\nIdentities\nAffiliates\nMembers\nDocumentation\nTalos Linux Guides\nDiscovery Service\nDiscovery Service\nTalos Linux Node discovery services\n\nTalos Linux includes node-discovery capabilities that depend on a discovery registry. This allows you to see the members of your cluster, and the associated IP addresses of the nodes.\n\nCopy\ntalosctl get members\n\nNODE       NAMESPACE   TYPE     ID                             VERSION   HOSTNAME                       MACHINE TYPE   OS               ADDRESSES\n\n10.5.0.2   cluster     Member   talos-default-controlplane-1   1         talos-default-controlplane-1   controlplane   Talos (v1.2.3)   [\"10.5.0.2\"]\n\n10.5.0.2   cluster     Member   talos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.2.3)   [\"10.5.0.3\"]\n\n\nThere are currently two supported discovery services: a Kubernetes registry (which stores data in the cluster’s etcd service) and an external registry service. Sidero Labs runs a public external registry service, which is enabled by default. The Kubernetes registry service is disabled by default. The advantage of the external registry service is that it is not dependent on etcd, and thus can inform you of cluster membership even when Kubernetes is down.\n\nVideo Walkthrough\n\nTo see a live demo of Cluster Discovery, see the video below:\n\nRegistries\n\nPeers are aggregated from enabled registries. By default, Talos will use the service registry, while the kubernetes registry is disabled. To disable a registry, set disabled to true (this option is the same for all registries): For example, to disable the service registry:\n\nCopy\ncluster:\n\n  discovery:\n\n    enabled: true\n\n    registries:\n\n      service:\n\n        disabled: true\n\n\nDisabling all registries effectively disables member discovery.\n\nNote: An enabled discovery service is required for KubeSpan to function correctly.\n\nThe Kubernetes registry uses Kubernetes Node resource data and additional Talos annotations:\n\nCopy\n$ kubectl describe node <nodename>\n\nAnnotations:        cluster.talos.dev/node-id: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n                    networking.talos.dev/assigned-prefixes: 10.244.0.0/32,10.244.0.1/24\n\n                    networking.talos.dev/self-ips: 172.20.0.2,fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\n\n...\n\n\nThe Service registry by default uses a public external Discovery Service to exchange encrypted information about cluster members.\n\nNote: Talos supports operations when Discovery Service is disabled, but some features will rely on Kubernetes API availability to discover controlplane endpoints, so in case of a failure disabled Discovery Service makes troubleshooting much harder.\n\nDiscovery Service\n\nSidero Labs maintains a public discovery service at https://discovery.talos.dev/ whereby cluster members use a shared key that is globally unique to coordinate basic connection information (i.e. the set of possible “endpoints”, or IP:port pairs). We call this data “affiliate data.”\n\nNote: If KubeSpan is enabled the data has the addition of the WireGuard public key.\n\nData sent to the discovery service is encrypted with AES-GCM encryption and endpoint data is separately encrypted with AES in ECB mode so that endpoints coming from different sources can be deduplicated server-side. Each node submits its own data, plus the endpoints it sees from other peers, to the discovery service. The discovery service aggregates the data, deduplicates the endpoints, and sends updates to each connected peer. Each peer receives information back from the discovery service, decrypts it and uses it to drive KubeSpan and cluster discovery.\n\nData is stored in memory only. The cluster ID is used as a key to select the affiliates (so that different clusters see different affiliates).\n\nTo summarize, the discovery service knows the client version, cluster ID, the number of affiliates, some encrypted data for each affiliate, and a list of encrypted endpoints. The discovery service doesn’t see actual node information – it only stores and updates encrypted blobs. Discovery data is encrypted/decrypted by the clients – the cluster members. The discovery service does not have the encryption key.\n\nThe discovery service may, with a commercial license, be operated by your organization and can be downloaded here. In order for nodes to communicate to the discovery service, they must be able to connect to it on TCP port 443.\n\nResource Definitions\n\nTalos provides resources that can be used to introspect the discovery and KubeSpan features.\n\nDiscovery\nIdentities\n\nThe node’s unique identity (base62 encoded random 32 bytes) can be obtained with:\n\nNote: Using base62 allows the ID to be URL encoded without having to use the ambiguous URL-encoding version of base64.\n\nCopy\n$ talosctl get identities -o yaml\n\n...\n\nspec:\n\n    nodeId: Utoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd\n\n\nNode identity is used as the unique Affiliate identifier.\n\nNode identity resource is preserved in the STATE partition in node-identity.yaml file. Node identity is preserved across reboots and upgrades, but it is regenerated if the node is reset (wiped).\n\nAffiliates\n\nAn affiliate is a proposed member: the node has the same cluster ID and secret.\n\nCopy\n$ talosctl get affiliates\n\nID                                             VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\n2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    2         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\n6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nNVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nUtoh3O0ZneV0kT2IUBrh7TgdouRcUW2yzaaMl4VXnCd    4         talos-default-controlplane-1   controlplane   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\nb3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   2         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nOne of the Affiliates with the ID matching node identity is populated from the node data, other Affiliates are pulled from the registries. Enabled discovery registries run in parallel and discovered data is merged to build the list presented above.\n\nDetails about data coming from each registry can be queried from the cluster-raw namespace:\n\nCopy\n$ talosctl get affiliates --namespace=cluster-raw\n\nID                                                     VERSION   HOSTNAME                       MACHINE TYPE   ADDRESSES\n\nk8s/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF        3         talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nk8s/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA       2         talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nk8s/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB       2         talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nk8s/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C       3         talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\nservice/2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF    23        talos-default-controlplane-2   controlplane   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\nservice/6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA   26        talos-default-worker-1         worker         [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\nservice/NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB   20        talos-default-worker-2         worker         [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\nservice/b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C   14        talos-default-controlplane-3   controlplane   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\n\nEach Affiliate ID is prefixed with k8s/ for data coming from the Kubernetes registry and with service/ for data coming from the discovery service.\n\nMembers\n\nA member is an affiliate that has been approved to join the cluster. The members of the cluster can be obtained with:\n\nCopy\n$ talosctl get members\n\nID                             VERSION   HOSTNAME                       MACHINE TYPE   OS                ADDRESSES\n\ntalos-default-controlplane-1   2         talos-default-controlplane-1   controlplane   Talos (v1.6.2)   [\"172.20.0.2\",\"fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94\"]\n\ntalos-default-controlplane-2   1         talos-default-controlplane-2   controlplane   Talos (v1.6.2)   [\"172.20.0.3\",\"fd83:b1f7:fcb5:2802:986b:7eff:fec5:889d\"]\n\ntalos-default-controlplane-3   1         talos-default-controlplane-3   controlplane   Talos (v1.6.2)   [\"172.20.0.4\",\"fd83:b1f7:fcb5:2802:248f:1fff:fe5c:c3f\"]\n\ntalos-default-worker-1         1         talos-default-worker-1         worker         Talos (v1.6.2)   [\"172.20.0.5\",\"fd83:b1f7:fcb5:2802:cc80:3dff:fece:d89d\"]\n\ntalos-default-worker-2         1         talos-default-worker-2         worker         Talos (v1.6.2)   [\"172.20.0.6\",\"fd83:b1f7:fcb5:2802:2805:fbff:fe80:5ed2\"]\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Virtual (shared) IP | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/vip/",
    "html": "Requirements\nVideo Walkthrough\nChoose your Shared IP\nConfigure your Talos Machines\nCaveats\nDocumentation\nTalos Linux Guides\nNetwork\nVirtual (shared) IP\nVirtual (shared) IP\nUsing Talos Linux to set up a floating virtual IP address for cluster access.\n\nOne of the pain points when building a high-availability controlplane is giving clients a single IP or URL at which they can reach any of the controlplane nodes. The most common approaches - reverse proxy, load balancer, BGP, and DNS - all require external resources, and add complexity in setting up Kubernetes.\n\nTo simplify cluster creation, Talos Linux supports a “Virtual” IP (VIP) address to access the Kubernetes API server, providing high availability with no other resources required.\n\nWhat happens is that the controlplane machines vie for control of the shared IP address using etcd elections. There can be only one owner of the IP address at any given time. If that owner disappears or becomes non-responsive, another owner will be chosen, and it will take up the IP address.\n\nRequirements\n\nThe controlplane nodes must share a layer 2 network, and the virtual IP must be assigned from that shared network subnet. In practical terms, this means that they are all connected via a switch, with no router in between them. Note that the virtual IP election depends on etcd being up, as Talos uses etcd for elections and leadership (control) of the IP address.\n\nThe virtual IP is not restricted by ports - you can access any port that the control plane nodes are listening on, on that IP address. Thus it is possible to access the Talos API over the VIP, but it is not recommended, as you cannot access the VIP when etcd is down - and then you could not access the Talos API to recover etcd.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nChoose your Shared IP\n\nThe Virtual IP should be a reserved, unused IP address in the same subnet as your controlplane nodes. It should not be assigned or assignable by your DHCP server.\n\nFor our example, we will assume that the controlplane nodes have the following IP addresses:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nWe then choose our shared IP to be:\n\n192.168.0.15\nConfigure your Talos Machines\n\nThe shared IP setting is only valid for controlplane nodes.\n\nFor the example above, each of the controlplane nodes should have the following Machine Config snippet:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n\nVirtual IP’s can also be configured on a VLAN interface.\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\n      vlans:\n\n        - vlanId: 100\n\n          dhcp: true\n\n          vip:\n\n            ip: 192.168.1.15\n\n\nFor your own environment, the interface and the DHCP setting may differ, or you may use static addressing (adresses) instead of DHCP.\n\nWhen using predictable interface names, the interface name might not be eth0.\n\nIf the machine has a single network interface, it can be selected using a dummy device selector:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n      dhcp: true\n\n      vip:\n\n        ip: 192.168.0.15\n\nCaveats\n\nSince VIP functionality relies on etcd for elections, the shared IP will not come alive until after you have bootstrapped Kubernetes.\n\nDon’t use the VIP as the endpoint in the talosconfig, as the VIP is bound to etcd and kube-apiserver health, and you will not be able to recover from a failure of either of those components using Talos API.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Wireguard Network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/wireguard-network/",
    "html": "Configuring Wireguard Network\nQuick Start\nManual Configuration\nKey Generation\nSetting up Device\nDocumentation\nTalos Linux Guides\nNetwork\nWireguard Network\nWireguard Network\nA guide on how to set up Wireguard network using Kernel module.\nConfiguring Wireguard Network\nQuick Start\n\nThe quickest way to try out Wireguard is to use talosctl cluster create command:\n\nCopy\ntalosctl cluster create --wireguard-cidr 10.1.0.0/24\n\n\nIt will automatically generate Wireguard network configuration for each node with the following network topology:\n\nWhere all controlplane nodes will be used as Wireguard servers which listen on port 51111. All controlplanes and workers will connect to all controlplanes. It also sets PersistentKeepalive to 5 seconds to establish controlplanes to workers connection.\n\nAfter the cluster is deployed it should be possible to verify Wireguard network connectivity. It is possible to deploy a container with hostNetwork enabled, then do kubectl exec <container> /bin/bash and either do:\n\nCopy\nping 10.1.0.2\n\n\nOr install wireguard-tools package and run:\n\nCopy\nwg show\n\n\nWireguard show should output something like this:\n\nCopy\ninterface: wg0\n\n  public key: OMhgEvNIaEN7zeCLijRh4c+0Hwh3erjknzdyvVlrkGM=\n\n  private key: (hidden)\n\n  listening port: 47946\n\n\n\npeer: 1EsxUygZo8/URWs18tqB5FW2cLVlaTA+lUisKIf8nh4=\n\n  endpoint: 10.5.0.2:51111\n\n  allowed ips: 10.1.0.0/24\n\n  latest handshake: 1 minute, 55 seconds ago\n\n  transfer: 3.17 KiB received, 3.55 KiB sent\n\n  persistent keepalive: every 5 seconds\n\n\nIt is also possible to use generated configuration as a reference by pulling generated config files using:\n\nCopy\ntalosctl read -n 10.5.0.2 /system/state/config.yaml > controlplane.yaml\n\ntalosctl read -n 10.5.0.3 /system/state/config.yaml > worker.yaml\n\nManual Configuration\n\nAll Wireguard configuration can be done by changing Talos machine config files. As an example we will use this official Wireguard quick start tutorial.\n\nKey Generation\n\nThis part is exactly the same:\n\nCopy\nwg genkey | tee privatekey | wg pubkey > publickey\n\nSetting up Device\n\nInline comments show relations between configs and wg quickstart tutorial commands:\n\nCopy\n...\n\nnetwork:\n\n  interfaces:\n\n    ...\n\n      # ip link add dev wg0 type wireguard\n\n    - interface: wg0\n\n      mtu: 1500\n\n      # ip address add dev wg0 192.168.2.1/24\n\n      addresses:\n\n        - 192.168.2.1/24\n\n      # wg set wg0 listen-port 51820 private-key /path/to/private-key peer ABCDEF... allowed-ips 192.168.88.0/24 endpoint 209.202.254.14:8172\n\n      wireguard:\n\n        privateKey: <privatekey file contents>\n\n        listenPort: 51820\n\n        peers:\n\n          allowedIPs:\n\n            - 192.168.88.0/24\n\n          endpoint: 209.202.254.14.8172\n\n          publicKey: ABCDEF...\n\n...\n\n\nWhen networkd gets this configuration it will create the device, configure it and will bring it up (equivalent to ip link set up dev wg0).\n\nAll supported config parameters are described in the Machine Config Reference.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "KubeSpan | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/kubespan/",
    "html": "Network Requirements\nCaveats\nKubernetes API Endpoint Limitations\nDigital Ocean Limitations\nEnabling\nCreating a New Cluster\nEnabling for an Existing Cluster\nConfiguration\nResource Definitions\nKubeSpanIdentities\nKubeSpanPeerSpecs\nKubeSpanPeerStatuses\nKubeSpanEndpoints\nDocumentation\nTalos Linux Guides\nNetwork\nKubeSpan\nKubeSpan\nLearn to use KubeSpan to connect Talos Linux machines securely across networks.\n\nKubeSpan is a feature of Talos that automates the setup and maintenance of a full mesh WireGuard network for your cluster, giving you the ability to operate hybrid Kubernetes clusters that can span the edge, datacenter, and cloud. Management of keys and discovery of peers can be completely automated, making it simple and easy to create hybrid clusters.\n\nKubeSpan consists of client code in Talos Linux, as well as a discovery service that enables clients to securely find each other. Sidero Labs operates a free Discovery Service, but the discovery service may, with a commercial license, be operated by your organization and can be downloaded here.\n\nVideo Walkthrough\n\nTo see a live demo of KubeSpan, see one the videos below:\n\n \nNetwork Requirements\n\nKubeSpan uses UDP port 51820 to carry all KubeSpan encrypted traffic. Because UDP traversal of firewalls is often lenient, and the Discovery Service communicates the apparent IP address of all peers to all other peers, KubeSpan will often work automatically, even when each nodes is behind their own firewall. However, when both ends of a KubeSpan connection are behind firewalls, it is possible the connection may not be established correctly - it depends on each end sending out packets in a limited time window.\n\nThus best practice is to ensure that one end of all possible node-node communication allows UDP port 51820, inbound.\n\nFor example, if control plane nodes are running in a corporate data center, behind firewalls, KubeSpan connectivity will work correctly so long as worker nodes on the public Internet can receive packets on UDP port 51820. (Note the workers will also need to receive TCP port 50000 for initial configuration via talosctl).\n\nAn alternative topology would be to run control plane nodes in a public cloud, and allow inbound UDP port 51820 to the control plane nodes. Workers could be behind firewalls, and KubeSpan connectivity will be established. Note that if workers are in different locations, behind different firewalls, the KubeSpan connectivity between workers should be correctly established, but may require opening the KubeSpan UDP port on the local firewall also.\n\nCaveats\nKubernetes API Endpoint Limitations\n\nWhen the K8s endpoint is an IP address that is not part of Kubespan, but is an address that is forwarded on to the Kubespan address of a control plane node, without changing the source address, then worker nodes will fail to join the cluster. In such a case, the control plane node has no way to determine whether the packet arrived on the private Kubespan address, or the public IP address. If the source of the packet was a Kubespan member, the reply will be Kubespan encapsulated, and thus not translated to the public IP, and so the control plane will reply to the session with the wrong address.\n\nThis situation is seen, for example, when the Kubernetes API endpoint is the public IP of a VM in GCP or Azure for a single node control plane. The control plane will receive packets on the public IP, but will reply from it’s KubeSpan address. The workaround is to create a load balancer to terminate the Kubernetes API endpoint.\n\nDigital Ocean Limitations\n\nDigital Ocean assigns an “Anchor IP” address to each droplet. Talos Linux correctly identifies this as a link-local address, and configures KubeSpan correctly, but this address will often be selected by Flannel or other CNIs as a node’s private IP. Because this address is not routable, nor advertised via KubeSpan, it will break pod-pod communication between nodes. This can be worked-around by assigning a non-Anchor private IP:\n\nkubectl annotate node do-worker flannel.alpha.coreos.com/public-ip-overwrite=10.116.X.X\n\nThen restarting flannel: kubectl delete pods -n kube-system -l k8s-app=flannel\n\nEnabling\nCreating a New Cluster\n\nTo enable KubeSpan for a new cluster, we can use the --with-kubespan flag in talosctl gen config. This will enable peer discovery and KubeSpan.\n\nCopy\nmachine:\n\n    network:\n\n        kubespan:\n\n            enabled: true # Enable the KubeSpan feature.\n\ncluster:\n\n    discovery:\n\n        enabled: true\n\n        # Configure registries used for cluster member discovery.\n\n        registries:\n\n            kubernetes: # Kubernetes registry is problematic with KubeSpan, if the control plane endpoint is routeable itself via KubeSpan.\n\n              disabled: true\n\n            service: {}\n\n\nThe default discovery service is an external service hosted by Sidero Labs at https://discovery.talos.dev/. Contact Sidero Labs if you need to run this service privately.\n\nEnabling for an Existing Cluster\n\nIn order to enable KubeSpan on an existing cluster, enable kubespan and discovery settings in the machine config for each machine in the cluster (discovery is enabled by default):\n\nCopy\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: true\n\ncluster:\n\n  discovery:\n\n    enabled: true\n\nConfiguration\n\nKubeSpan will automatically discovery all cluster members, exchange Wireguard public keys and establish a full mesh network.\n\nThere are configuration options available which are not usually required:\n\nCopy\nmachine:\n\n  network:\n\n    kubespan:\n\n      enabled: false\n\n      advertiseKubernetesNetworks: false\n\n      allowDownPeerBypass: false\n\n      mtu: 1420\n\n      filters:\n\n        endpoints:\n\n          - 0.0.0.0/0\n\n          - ::/0\n\n\nThe setting advertiseKubernetesNetworks controls whether the node will advertise Kubernetes service and pod networks to other nodes in the cluster over KubeSpan. It defaults to being disabled, which means KubeSpan only controls the node-to-node traffic, while pod-to-pod traffic is routed and encapsulated by CNI. This setting should not be enabled with Calico and Cilium CNI plugins, as they do their own pod IP allocation which is not visible to KubeSpan.\n\nThe setting allowDownPeerBypass controls whether the node will allow traffic to bypass WireGuard if the destination is not connected over KubeSpan. If enabled, there is a risk that traffic will be routed unencrypted if the destination is not connected over KubeSpan, but it allows a workaround for the case where a node is not connected to the KubeSpan network, but still needs to access the cluster.\n\nThe mtu setting configures the Wireguard MTU, which defaults to 1420. This default value of 1420 is safe to use when the underlying network MTU is 1500, but if the underlying network MTU is smaller, the KubeSpanMTU should be adjusted accordingly: KubeSpanMTU = UnderlyingMTU - 80.\n\nThe filters setting allows hiding some endpoints from being advertised over KubeSpan. This is useful when some endpoints are known to be unreachable between the nodes, so that KubeSpan doesn’t try to establish a connection to them. Another use-case is hiding some endpoints if nodes can connect on multiple networks, and some of the networks are more preferable than others.\n\nResource Definitions\nKubeSpanIdentities\n\nA node’s WireGuard identities can be obtained with:\n\nCopy\n$ talosctl get kubespanidentities -o yaml\n\n...\n\nspec:\n\n    address: fd83:b1f7:fcb5:2802:8c13:71ff:feaf:7c94/128\n\n    subnet: fd83:b1f7:fcb5:2802::/64\n\n    privateKey: gNoasoKOJzl+/B+uXhvsBVxv81OcVLrlcmQ5jQwZO08=\n\n    publicKey: NzW8oeIH5rJyY5lefD9WRoHWWRr/Q6DwsDjMX+xKjT4=\n\n\nTalos automatically configures unique IPv6 address for each node in the cluster-specific IPv6 ULA prefix.\n\nThe Wireguard private key is generated and never leaves the node, while the public key is published through the cluster discovery.\n\nKubeSpanIdentity is persisted across reboots and upgrades in STATE partition in the file kubespan-identity.yaml.\n\nKubeSpanPeerSpecs\n\nA node’s WireGuard peers can be obtained with:\n\nCopy\n$ talosctl get kubespanpeerspecs\n\nID                                             VERSION   LABEL                          ENDPOINTS\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   2         talos-default-controlplane-2   [\"172.20.0.3:51820\"]\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   2         talos-default-controlplane-3   [\"172.20.0.4:51820\"]\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   2         talos-default-worker-2         [\"172.20.0.6:51820\"]\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   2         talos-default-worker-1         [\"172.20.0.5:51820\"]\n\n\nThe peer ID is the Wireguard public key. KubeSpanPeerSpecs are built from the cluster discovery data.\n\nKubeSpanPeerStatuses\n\nThe status of a node’s WireGuard peers can be obtained with:\n\nCopy\n$ talosctl get kubespanpeerstatuses\n\nID                                             VERSION   LABEL                          ENDPOINT           STATE   RX         TX\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   63        talos-default-controlplane-2   172.20.0.3:51820   up      15043220   17869488\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   62        talos-default-controlplane-3   172.20.0.4:51820   up      14573208   18157680\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   60        talos-default-worker-2         172.20.0.6:51820   up      130072     46888\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   60        talos-default-worker-1         172.20.0.5:51820   up      130044     46556\n\n\nKubeSpan peer status includes following information:\n\nthe actual endpoint used for peer communication\nlink state:\nunknown: the endpoint was just changed, link state is not known yet\nup: there is a recent handshake from the peer\ndown: there is no handshake from the peer\nnumber of bytes sent/received over the Wireguard link with the peer\n\nIf the connection state goes down, Talos will be cycling through the available endpoints until it finds the one which works.\n\nPeer status information is updated every 30 seconds.\n\nKubeSpanEndpoints\n\nA node’s WireGuard endpoints (peer addresses) can be obtained with:\n\nCopy\n$ talosctl get kubespanendpoints\n\nID                                             VERSION   ENDPOINT           AFFILIATE ID\n\n06D9QQOydzKrOL7oeLiqHy9OWE8KtmJzZII2A5/FLFI=   1         172.20.0.3:51820   2VfX3nu67ZtZPl57IdJrU87BMjVWkSBJiL9ulP9TCnF\n\nTHtfKtfNnzJs1nMQKs5IXqK0DFXmM//0WMY+NnaZrhU=   1         172.20.0.4:51820   b3DebkPaCRLTLLWaeRF1ejGaR0lK3m79jRJcPn0mfA6C\n\nnVHu7l13uZyk0AaI1WuzL2/48iG8af4WRv+LWmAax1M=   1         172.20.0.6:51820   NVtfu1bT1QjhNq5xJFUZl8f8I8LOCnnpGrZfPpdN9WlB\n\nzXP0QeqRo+CBgDH1uOBiQ8tA+AKEQP9hWkqmkE/oDlc=   1         172.20.0.5:51820   6EVq8RHIne03LeZiJ60WsJcoQOtttw1ejvTS6SOBzhUA\n\n\nThe endpoint ID is the base64 encoded WireGuard public key.\n\nThe observed endpoints are submitted back to the discovery service (if enabled) so that other peers can try additional endpoints to establish the connection.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Predictable Interface Names | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/predictable-interface-names/",
    "html": "Documentation\nTalos Linux Guides\nNetwork\nPredictable Interface Names\nPredictable Interface Names\nHow to use predictable interface naming.\n\nStarting with version Talos 1.5, network interfaces are renamed to predictable names same way as systemd does that in other Linux distributions.\n\nThe naming schema enx78e7d1ea46da (based on MAC addresses) is enabled by default, the order of interface naming decisions is:\n\nfirmware/BIOS provided index numbers for on-board devices (example: eno1)\nfirmware/BIOS provided PCI Express hotplug slot index numbers (example: ens1)\nphysical/geographical location of the connector of the hardware (example: enp2s0)\ninterfaces’s MAC address (example: enx78e7d1ea46da)\n\nThe predictable network interface names features can be disabled by specifying net.ifnames=0 in the kernel command line.\n\nNote: Talos automatically adds the net.ifnames=0 kernel argument when upgrading from Talos versions before 1.5, so upgrades to 1.5 don’t require any manual intervention.\n\n“Cloud” platforms, like AWS, still use old eth0 naming scheme as Talos automatically adds net.ifnames=0 to the kernel command line.\n\nSingle Network Interface\n\nWhen running Talos on a machine with a single network interface, predictable interface names might be confusing, as it might come up as enxSOMETHING which is hard to address. There are two ways to solve this:\n\ndisable the feature by supplying net.ifnames=0 to the initial boot of Talos, Talos will persist net.ifnames=0 over installs/upgrades.\n\nuse device selectors:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          busPath: \"0*\" # should select any hardware network device, if you have just one, it will be selected\n\n        # any configuration can follow, e.g:\n\n        addresses: [10.3.4.5/24]\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network Device Selector | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/device-selector/",
    "html": "Configuring Network Device Using Device Selector\nUsing Device Selector for Bonding\nDocumentation\nTalos Linux Guides\nNetwork\nNetwork Device Selector\nNetwork Device Selector\nHow to configure network devices by selecting them using hardware information\nConfiguring Network Device Using Device Selector\n\ndeviceSelector is an alternative method of configuring a network device:\n\nCopy\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - deviceSelector:\n\n          driver: virtio\n\n          hardwareAddr: \"00:00:*\"\n\n        address: 192.168.88.21\n\n\nSelector has the following traits:\n\nqualifiers match a device by reading the hardware information in /sys/class/net/...\nqualifiers are applied using logical AND\nmachine.network.interfaces.deviceConfig option is mutually exclusive with machine.network.interfaces.interface\nif the selector matches multiple devices, the controller will apply config to all of them\n\nThe available hardware information used in the selector can be observed in the LinkStatus resource (works in maintenance mode):\n\nCopy\n# talosctl get links eth0 -o yaml\n\nspec:\n\n  ...\n\n  hardwareAddr: 4e:95:8e:8f:e4:47\n\n  busPath: 0000:06:00.0\n\n  driver: alx\n\n  pciID: 1969:E0B1\n\nUsing Device Selector for Bonding\n\nDevice selectors can be used to configure bonded interfaces:\n\nCopy\nmachine:\n\n  ...\n\n  network:\n\n    interfaces:\n\n      - interface: bond0\n\n        bond:\n\n          mode: balance-rr\n\n          deviceSelectors:\n\n            - hardwareAddr: '00:50:56:8e:8f:e4'\n\n            - hardwareAddr: '00:50:57:9c:2c:2d'\n\n\nIn this example, the bond0 interface will be created and bonded using two devices with the specified hardware addresses.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Ingress Firewall | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/ingress-firewall/",
    "html": "Configuration\nRecommended Rules\nControlplane\nWorker\nLearn More\nDocumentation\nTalos Linux Guides\nNetwork\nIngress Firewall\nIngress Firewall\nLearn to use Talos Linux Ingress Firewall to limit access to the host services.\n\nTalos Linux Ingress Firewall is a simple and effective way to limit access to the services running on the host, which includes both Talos standard services (e.g. apid and kubelet), and any additional workloads that may be running on the host. Talos Linux Ingress Firewall doesn’t affect the traffic between the Kubernetes pods/services, please use CNI Network Policies for that.\n\nConfiguration\n\nIngress rules are configured as extra documents NetworkDefaultActionConfig and NetworkRuleConfig in the Talos machine configuration:\n\nCopy\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 172.20.0.0/24\n\n    except: 172.20.0.1/32\n\n\nThe first document configures the default action for the ingress traffic, which can be either accept or block, with the default being accept. If the default action is set to accept, then all the ingress traffic will be allowed, unless there is a matching rule that blocks it. If the default action is set to block, then all the ingress traffic will be blocked, unless there is a matching rule that allows it.\n\nWith either accept or block, the traffic is always allowed on the following network interfaces:\n\nlo\nsiderolink\nkubespan\n\nIn the block mode:\n\nICMP and ICMPv6 traffic is also allowed with a rate limit of 5 packets per second\ntraffic between Kubernetes pod/service subnets is allowed (for native routing CNIs)\n\nThe second document defines an ingress rule for a set of ports and protocols on the host. The NetworkRuleConfig might be repeated many times to define multiple rules, but each document must have a unique name.\n\nThe ports field accepts either a single port or a port range:\n\nCopy\nportSelector:\n\n  ports:\n\n    - 10250\n\n    - 10260\n\n    - 10300-10400\n\n\nThe protocol might be either tcp or udp.\n\nThe ingress specifies the list of subnets that are allowed to access the host services, with the optional except field to exclude a set of addresses from the subnet.\n\nNote: incorrect configuration of the ingress firewall might result in the host becoming inaccessible over Talos API. The configuration might be applied in --mode=try to make sure it gets reverted in case of a mistake.\n\nRecommended Rules\n\nThe following rules improve the security of the cluster and cover only standard Talos services. If there are additional services running with host networking in the cluster, they should be covered by additional rules.\n\nIn the block mode, the ingress firewall will also block encapsulated traffic (e.g. VXLAN) between the nodes, which needs to be explicitly allowed for the Kubernetes networking to function properly. Please refer to the CNI documentation for the specifics, some default configurations are listed below:\n\nFlannel, Calico: vxlan UDP port 4789\nCilium: vxlan UDP port 8472\n\nIn the examples we assume following template variables to describe the cluster:\n\n$CLUSTER_SUBNET, e.g. 172.20.0.0/24 - the subnet which covers all machines in the cluster\n$CP1, $CP2, $CP3 - the IP addresses of the controlplane nodes\n$VXLAN_PORT - the UDP port used by the CNI for encapsulated traffic\nControlplane\napid and Kubernetes API are wide open\nkubelet and trustd API is only accessible within the cluster\netcd API is limited to controlplane nodes\nCopy\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: trustd-ingress\n\nportSelector:\n\n  ports:\n\n    - 50001\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubernetes-api-ingress\n\nportSelector:\n\n  ports:\n\n    - 6443\n\n  protocol: tcp\n\ningress:\n\n  - subnet: 0.0.0.0/0\n\n  - subnet: ::/0\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: etcd-ingress\n\nportSelector:\n\n  ports:\n\n    - 2379-2380\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CP1/32\n\n  - subnet: $CP2/32\n\n  - subnet: $CP3/32\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nWorker\nkubelet and apid API is only accessible within the cluster\nCopy\napiVersion: v1alpha1\n\nkind: NetworkDefaultActionConfig\n\ningress: block\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: kubelet-ingress\n\nportSelector:\n\n  ports:\n\n    - 10250\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: apid-ingress\n\nportSelector:\n\n  ports:\n\n    - 50000\n\n  protocol: tcp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\n---\n\napiVersion: v1alpha1\n\nkind: NetworkRuleConfig\n\nname: cni-vxlan\n\nportSelector:\n\n  ports:\n\n    - $VXLAN_PORT\n\n  protocol: udp\n\ningress:\n\n  - subnet: $CLUSTER_SUBNET\n\nLearn More\n\nTalos Linux Ingress Firewall is using the nftables to perform the filtering.\n\nWith the default action set to accept, the following rules are applied (example):\n\nCopy\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy accept;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ip saddr != { 172.20.0.0/24 } tcp dport { 10250 } drop\n\n    meta nfproto ipv6 tcp dport { 10250 } drop\n\n  }\n\n}\n\n\nWith the default action set to block, the following rules are applied (example):\n\nCopy\ntable inet talos {\n\n  chain ingress {\n\n    type filter hook input priority filter; policy drop;\n\n    iifname { \"lo\", \"siderolink\", \"kubespan\" }  accept\n\n    ct state { established, related } accept\n\n    ct state invalid drop\n\n    meta l4proto icmp limit rate 5/second accept\n\n    meta l4proto ipv6-icmp limit rate 5/second accept\n\n    ip saddr { 172.20.0.0/24 } tcp dport { 10250 }  accept\n\n    meta nfproto ipv4 tcp dport { 50000 } accept\n\n    meta nfproto ipv6 tcp dport { 50000 } accept\n\n  }\n\n}\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/",
    "html": "Documentation\nTalos Linux Guides\nNetwork\nNetwork\nSet up networking layers for Talos Linux\nCorporate Proxies\n\nHow to configure Talos Linux to use proxies in a corporate environment\n\nIngress Firewall\n\nLearn to use Talos Linux Ingress Firewall to limit access to the host services.\n\nKubeSpan\n\nLearn to use KubeSpan to connect Talos Linux machines securely across networks.\n\nNetwork Device Selector\n\nHow to configure network devices by selecting them using hardware information\n\nPredictable Interface Names\n\nHow to use predictable interface naming.\n\nVirtual (shared) IP\n\nUsing Talos Linux to set up a floating virtual IP address for cluster access.\n\nWireguard Network\n\nA guide on how to set up Wireguard network using Kernel module.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Corporate Proxies | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/network/corporate-proxies/",
    "html": "Appending the Certificate Authority of MITM Proxies\nConfiguring a Machine to Use the Proxy\nDocumentation\nTalos Linux Guides\nNetwork\nCorporate Proxies\nCorporate Proxies\nHow to configure Talos Linux to use proxies in a corporate environment\nAppending the Certificate Authority of MITM Proxies\n\nPut into each machine the PEM encoded certificate:\n\nCopy\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\nConfiguring a Machine to Use the Proxy\n\nTo make use of a proxy:\n\nCopy\nmachine:\n\n  env:\n\n    http_proxy: <http proxy>\n\n    https_proxy: <https proxy>\n\n    no_proxy: <no proxy>\n\n\nAdditionally, configure the DNS nameservers, and NTP servers:\n\nCopy\nmachine:\n\n  env:\n\n  ...\n\n  time:\n\n    servers:\n\n      - <server 1>\n\n      - <server ...>\n\n      - <server n>\n\n  ...\n\n  network:\n\n    nameservers:\n\n      - <ip 1>\n\n      - <ip ...>\n\n      - <ip n>\n\n\nIf a proxy is required before Talos machine configuration is applied, use kernel command line arguments:\n\nCopy\ntalos.environment=http_proxy=<http-proxy> talos.environment=https_proxy=<https-proxy>\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How to scale up a Talos cluster | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/scaling-up/",
    "html": "Documentation\nTalos Linux Guides\nHow Tos\nHow to scale up a Talos cluster\nHow to scale up a Talos cluster\nHow to add more nodes to a Talos Linux cluster.\n\nTo add more nodes to a Talos Linux cluster, follow the same procedure as when initially creating the cluster:\n\nboot the new machines to install Talos Linux\napply the worker.yaml or controlplane.yaml configuration files to the new machines\n\nYou need the controlplane.yaml and worker.yaml that were created when you initially deployed your cluster. These contain the certificates that enable new machines to join.\n\nOnce you have the IP address, you can then apply the correct configuration for each machine you are adding, either worker or controlplane.\n\nCopy\n  talosctl apply-config --insecure \\\n\n    --nodes [NODE IP] \\\n\n    --file controlplane.yaml\n\n\nThe insecure flag is necessary because the PKI infrastructure has not yet been made available to the node.\n\nYou do not need to bootstrap the new node. Regardless of whether you are adding a control plane or worker node, it will now join the cluster in its role.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How to scale down a Talos cluster | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/scaling-down/",
    "html": "Documentation\nTalos Linux Guides\nHow Tos\nHow to scale down a Talos cluster\nHow to scale down a Talos cluster\nHow to remove nodes from a Talos Linux cluster.\n\nTo remove nodes from a Talos Linux cluster:\n\ntalosctl -n <IP.of.node.to.remove> reset\nkubectl delete node <nodename>\n\nThe command talosctl reset will cordon and drain the node, leaving etcd if required, and then erase its disks and power down the system.\n\nThis command will also remove the node from registration with the discovery service, so it will no longer show up in talosctl get members.\n\nIt is still necessary to remove the node from Kubernetes, as noted above.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How to manage certificate lifetimes with Talos Linux | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/cert-management/",
    "html": "Documentation\nTalos Linux Guides\nHow Tos\nHow to manage certificate lifetimes with Talos Linux\nHow to manage certificate lifetimes with Talos Linux\n\nTalos Linux automatically manages and rotates all server side certs for etcd, Kubernetes, and the Talos API. Note however that the kubelet needs to be restarted at least once a year in order for the certificates to be rotated. Any upgrade/reboot of the node will suffice for this effect.\n\nClient certs (talosconfig and kubeconfig) are the user’s responsibility. Each time you download the kubeconfig file from a Talos Linux cluster, the client certificate is regenerated giving you a kubeconfig which is valid for a year.\n\nThe talosconfig file should be renewed at least once a year, using the talosctl config new command.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How to enable workers on your control plane nodes | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/workers-on-controlplane/",
    "html": "Documentation\nTalos Linux Guides\nHow Tos\nHow to enable workers on your control plane nodes\nHow to enable workers on your control plane nodes\nHow to enable workers on your control plane nodes.\n\nBy default, Talos Linux taints control plane nodes so that workloads are not schedulable on them.\n\nIn order to allow workloads to run on the control plane nodes (useful for single node clusters, or non-production clusters), follow the procedure below.\n\nModify the MachineConfig for the controlplane nodes to add allowSchedulingOnControlPlanes: true:\n\nCopy\ncluster:\n\n    allowSchedulingOnControlPlanes: true\n\n\nThis may be done via editing the controlplane.yaml file before it is applied to the control plane nodes, by editing the machine config, or by patching the machine config.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "How Tos | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/howto/",
    "html": "Documentation\nTalos Linux Guides\nHow Tos\nHow Tos\nHow to guide for common tasks in Talos Linux\nHow to enable workers on your control plane nodes\n\nHow to enable workers on your control plane nodes.\n\nHow to manage certificate lifetimes with Talos Linux\n\nHow to scale down a Talos cluster\n\nHow to remove nodes from a Talos Linux cluster.\n\nHow to scale up a Talos cluster\n\nHow to add more nodes to a Talos Linux cluster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Pull Through Image Cache | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/pull-through-cache/",
    "html": "Requirements\nLaunch the Caching Docker Registry Proxies\nUsing Caching Registries with QEMU Local Cluster\nUsing Caching Registries with docker Local Cluster\nMachine Configuration\nCleaning Up\nUsing Harbor as a Caching Registry\nDocumentation\nTalos Linux Guides\nConfiguration\nPull Through Image Cache\nPull Through Image Cache\nHow to set up local transparent container images caches.\n\nIn this guide we will create a set of local caching Docker registry proxies to minimize local cluster startup time.\n\nWhen running Talos locally, pulling images from container registries might take a significant amount of time. We spin up local caching pass-through registries to cache images and configure a local Talos cluster to use those proxies. A similar approach might be used to run Talos in production in air-gapped environments. It can be also used to verify that all the images are available in local registries.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\n\nThe follow are requirements for creating the set of caching proxies:\n\nDocker 18.03 or greater\nLocal cluster requirements for either docker or QEMU.\nLaunch the Caching Docker Registry Proxies\n\nTalos pulls from docker.io, registry.k8s.io, gcr.io, and ghcr.io by default. If your configuration is different, you might need to modify the commands below:\n\nCopy\ndocker run -d -p 5000:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \\\n\n    --restart always \\\n\n    --name registry-docker.io registry:2\n\n\n\ndocker run -d -p 5001:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://registry.k8s.io \\\n\n    --restart always \\\n\n    --name registry-registry.k8s.io registry:2\n\n\n\ndocker run -d -p 5003:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://gcr.io \\\n\n    --restart always \\\n\n    --name registry-gcr.io registry:2\n\n\n\ndocker run -d -p 5004:5000 \\\n\n    -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \\\n\n    --restart always \\\n\n    --name registry-ghcr.io registry:2\n\n\nNote: Proxies are started as docker containers, and they’re automatically configured to start with Docker daemon.\n\nAs a registry container can only handle a single upstream Docker registry, we launch a container per upstream, each on its own host port (5000, 5001, 5002, 5003 and 5004).\n\nUsing Caching Registries with QEMU Local Cluster\n\nWith a QEMU local cluster, a bridge interface is created on the host. As registry containers expose their ports on the host, we can use bridge IP to direct proxy requests.\n\nCopy\nsudo talosctl cluster create --provisioner qemu \\\n\n    --registry-mirror docker.io=http://10.5.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://10.5.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://10.5.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://10.5.0.1:5004\n\n\nThe Talos local cluster should now start pulling via caching registries. This can be verified via registry logs, e.g. docker logs -f registry-docker.io. The first time cluster boots, images are pulled and cached, so next cluster boot should be much faster.\n\nNote: 10.5.0.1 is a bridge IP with default network (10.5.0.0/24), if using custom --cidr, value should be adjusted accordingly.\n\nUsing Caching Registries with docker Local Cluster\n\nWith a docker local cluster we can use docker bridge IP, default value for that IP is 172.17.0.1. On Linux, the docker bridge address can be inspected with ip addr show docker0.\n\nCopy\ntalosctl cluster create --provisioner docker \\\n\n    --registry-mirror docker.io=http://172.17.0.1:5000 \\\n\n    --registry-mirror registry.k8s.io=http://172.17.0.1:5001 \\\n\n    --registry-mirror gcr.io=http://172.17.0.1:5003 \\\n\n    --registry-mirror ghcr.io=http://172.17.0.1:5004\n\nMachine Configuration\n\nThe caching registries can be configured via machine configuration patch, equivalent to the command line flags above:\n\nCopy\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5000\n\n      gcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5003\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5004\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://10.5.0.1:5001\n\nCleaning Up\n\nTo cleanup, run:\n\nCopy\ndocker rm -f registry-docker.io\n\ndocker rm -f registry-registry.k8s.io\n\ndocker rm -f registry-gcr.io\n\ndocker rm -f registry-ghcr.io\n\n\nNote: Removing docker registry containers also removes the image cache. So if you plan to use caching registries, keep the containers running.\n\nUsing Harbor as a Caching Registry\n\nHarbor is an open source container registry that can be used as a caching proxy. Harbor supports configuring multiple upstream registries, so it can be used to cache multiple registries at once behind a single endpoint.\n\nAs Harbor puts a registry name in the pull image path, we need to set overridePath: true to prevent Talos and containerd from appending /v2 to the path.\n\nCopy\nmachine:\n\n  registries:\n\n    mirrors:\n\n      docker.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-docker.io\n\n        overridePath: true\n\n      ghcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-ghcr.io\n\n        overridePath: true\n\n      gcr.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-gcr.io\n\n        overridePath: true\n\n      registry.k8s.io:\n\n        endpoints:\n\n          - http://harbor/v2/proxy-registry.k8s.io\n\n        overridePath: true\n\n\nThe Harbor external endpoint (http://harbor in this example) can be configured with authentication or custom TLS:\n\nCopy\nmachine:\n\n  registries:\n\n    config:\n\n      harbor:\n\n        auth:\n\n          username: admin\n\n          password: password\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "System Extensions | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/system-extensions/",
    "html": "Installing System Extensions\nExample: Booting from an ISO\nExample: Disk Image\nAuthoring System Extensions\nResource Definitions\nExample: gVisor\nDocumentation\nTalos Linux Guides\nConfiguration\nSystem Extensions\nSystem Extensions\nCustomizing the Talos Linux immutable root file system.\n\nSystem extensions allow extending the Talos root filesystem, which enables a variety of features, such as including custom container runtimes, loading additional firmware, etc.\n\nSystem extensions are only activated during the installation or upgrade of Talos Linux. With system extensions installed, the Talos root filesystem is still immutable and read-only.\n\nInstalling System Extensions\n\nNote: the way to install system extensions in the .machine.install section of the machine configuration is now deprecated.\n\nStarting with Talos v1.5.0, Talos supports generation of boot media with system extensions included, this removes the need to rebuild the initramfs.xz on the machine itself during the installation or upgrade.\n\nThere are two kinds of boot assets that Talos can generate:\n\ninitial boot assets (ISO, PXE, etc.) that are used to boot the machine\ndisk images that have Talos pre-installed\ninstaller container images that can be used to install or upgrade Talos on a machine (installation happens when booted from ISO or PXE)\n\nDepending on the nature of the system extension (e.g. network device driver or containerd plugin), it may be necessary to include the extension in both initial boot assets and disk images/installer, or just the installer.\n\nThe process of generating boot assets with extensions included is described in the boot assets guide.\n\nExample: Booting from an ISO\n\nLet’s assume NVIDIA extension is required on a bare metal machine which is going to be booted from an ISO. As NVIDIA extension is not required for the initial boot and install step, it is sufficient to include the extension in the installer image only.\n\nUse a generic Talos ISO to boot the machine.\nPrepare a custom installer container image with NVIDIA extension included, push the image to a registry.\nEnsure that machine configuration field .machine.install.image points to the custom installer image.\nBoot the machine using the ISO, apply the machine configuration.\nTalos pulls a custom installer image from the registry (containing NVIDIA extension), installs Talos on the machine, and reboots.\n\nWhen it’s time to upgrade Talos, generate a custom installer container for a new version of Talos, push it to a registry, and perform upgrade pointing to the custom installer image.\n\nExample: Disk Image\n\nLet’s assume NVIDIA extension is required on AWS VM.\n\nPrepare an AWS disk image with NVIDIA extension included.\nUpload the image to AWS, register it as an AMI.\nUse the AMI to launch a VM.\nTalos boots with NVIDIA extension included.\n\nWhen it’s time to upgrade Talos, either repeat steps 1-4 to replace the VM with a new AMI, or like in the previous example, generate a custom installer and use it to upgrade Talos in-place.\n\nAuthoring System Extensions\n\nA Talos system extension is a container image with the specific folder structure. System extensions can be built and managed using any tool that produces container images, e.g. docker build.\n\nSidero Labs maintains a repository of system extensions.\n\nResource Definitions\n\nUse talosctl get extensions to get a list of system extensions:\n\nCopy\n$ talosctl get extensions\n\nNODE         NAMESPACE   TYPE              ID                                              VERSION   NAME          VERSION\n\n172.20.0.2   runtime     ExtensionStatus   000.ghcr.io-talos-systems-gvisor-54b831d        1         gvisor        20220117.0-v1.0.0\n\n172.20.0.2   runtime     ExtensionStatus   001.ghcr.io-talos-systems-intel-ucode-54b831d   1         intel-ucode   microcode-20210608-v1.0.0\n\n\nUse YAML or JSON format to see additional details about the extension:\n\nCopy\n$ talosctl -n 172.20.0.2 get extensions 001.ghcr.io-talos-systems-intel-ucode-54b831d -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: runtime\n\n    type: ExtensionStatuses.runtime.talos.dev\n\n    id: 001.ghcr.io-talos-systems-intel-ucode-54b831d\n\n    version: 1\n\n    owner: runtime.ExtensionStatusController\n\n    phase: running\n\n    created: 2022-02-10T18:25:04Z\n\n    updated: 2022-02-10T18:25:04Z\n\nspec:\n\n    image: 001.ghcr.io-talos-systems-intel-ucode-54b831d.sqsh\n\n    metadata:\n\n        name: intel-ucode\n\n        version: microcode-20210608-v1.0.0\n\n        author: Spencer Smith\n\n        description: |\n\n            This system extension provides Intel microcode binaries.\n\n        compatibility:\n\n            talos:\n\n                version: '>= v1.0.0'\n\nExample: gVisor\n\nSee readme of the gVisor extension.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Role-based access control (RBAC) | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/rbac/",
    "html": "Documentation\nTalos Linux Guides\nConfiguration\nRole-based access control (RBAC)\nRole-based access control (RBAC)\nSet up RBAC on the Talos Linux API.\n\nTalos v0.11 introduced initial support for role-based access control (RBAC). This guide will explain what that is and how to enable it without losing access to the cluster.\n\nRBAC in Talos\n\nTalos uses certificates to authorize users. The certificate subject’s organization field is used to encode user roles. There is a set of predefined roles that allow access to different API methods:\n\nos:admin grants access to all methods;\nos:operator grants everything os:reader role does, plus additional methods: rebooting, shutting down, etcd backup, etcd alarm management, and so on;\nos:reader grants access to “safe” methods (for example, that includes the ability to list files, but does not include the ability to read files content);\nos:etcd:backup grants access to /machine.MachineService/EtcdSnapshot method.\n\nRoles in the current talosconfig can be checked with the following command:\n\nCopy\n$ talosctl config info\n\n\n\n[...]\n\nRoles:               os:admin\n\n[...]\n\n\nRBAC is enabled by default in new clusters created with talosctl v0.11+ and disabled otherwise.\n\nEnabling RBAC\n\nFirst, both the Talos cluster and talosctl tool should be upgraded. Then the talosctl config new command should be used to generate a new client configuration with the os:admin role. Additional configurations and certificates for different roles can be generated by passing --roles flag:\n\nCopy\ntalosctl config new --roles=os:reader reader\n\n\nThat command will create a new client configuration file reader with a new certificate with os:reader role.\n\nAfter that, RBAC should be enabled in the machine configuration:\n\nCopy\nmachine:\n\n  features:\n\n    rbac: true\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "NVIDIA GPU (Proprietary drivers) | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/nvidia-gpu-proprietary/",
    "html": "Enabling the NVIDIA modules and the system extension\nDeploying NVIDIA device plugin\n(Optional) Setting the default runtime class as nvidia\nTesting the runtime class\nDocumentation\nTalos Linux Guides\nConfiguration\nNVIDIA GPU (Proprietary drivers)\nNVIDIA GPU (Proprietary drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using proprietary drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA system extensions:\n\nnonfree-kmod-nvidia\nnvidia-container-toolkit\n\nTo build a NVIDIA driver version not published by SideroLabs follow the instructions here\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nonfree-kmod-nvidia and nvidia-container-toolkit extensions. The nonfree-kmod-nvidia extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA modules and the system extension\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nCopy\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\nCopy\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\nCopy\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nCopy\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\nCopy\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nCopy\nNODE           NAMESPACE   TYPE              ID                                                                 VERSION   NAME                       VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-frezbo-nvidia-container-toolkit-510.60.02-v1.9.0       1         nvidia-container-toolkit   510.60.02-v1.9.0\n\nCopy\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nCopy\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  510.60.02  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 11.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\nCopy\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nCopy\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\nCopy\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\nCopy\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nCopy\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "NVIDIA GPU (OSS drivers) | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/nvidia-gpu/",
    "html": "Enabling the NVIDIA OSS modules\nDeploying NVIDIA device plugin\n(Optional) Setting the default runtime class as nvidia\nTesting the runtime class\nDocumentation\nTalos Linux Guides\nConfiguration\nNVIDIA GPU (OSS drivers)\nNVIDIA GPU (OSS drivers)\nIn this guide we’ll follow the procedure to support NVIDIA GPU using OSS drivers on Talos.\n\nEnabling NVIDIA GPU support on Talos is bound by NVIDIA EULA. The Talos published NVIDIA OSS drivers are bound to a specific Talos release. The extensions versions also needs to be updated when upgrading Talos.\n\nWe will be using the following NVIDIA OSS system extensions:\n\nnvidia-open-gpu-kernel-modules\nnvidia-container-toolkit\n\nCreate the boot assets which includes the system extensions mentioned above (or create a custom installer and perform a machine upgrade if Talos is already installed).\n\nMake sure the driver version matches for both the nvidia-open-gpu-kernel-modules and nvidia-container-toolkit extensions. The nvidia-open-gpu-kernel-modules extension is versioned as <nvidia-driver-version>-<talos-release-version> and the nvidia-container-toolkit extension is versioned as <nvidia-driver-version>-<nvidia-container-toolkit-version>.\n\nEnabling the NVIDIA OSS modules\n\nPatch Talos machine configuration using the patch gpu-worker-patch.yaml:\n\nCopy\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\nCopy\ntalosctl patch mc --patch @gpu-worker-patch.yaml\n\n\nThe NVIDIA modules should be loaded and the system extension should be installed.\n\nThis can be confirmed by running:\n\nCopy\ntalosctl read /proc/modules\n\n\nwhich should produce an output similar to below:\n\nCopy\nnvidia_uvm 1146880 - - Live 0xffffffffc2733000 (PO)\n\nnvidia_drm 69632 - - Live 0xffffffffc2721000 (PO)\n\nnvidia_modeset 1142784 - - Live 0xffffffffc25ea000 (PO)\n\nnvidia 39047168 - - Live 0xffffffffc00ac000 (PO)\n\nCopy\ntalosctl get extensions\n\n\nwhich should produce an output similar to below:\n\nCopy\nNODE           NAMESPACE   TYPE              ID                                                                           VERSION   NAME                             VERSION\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-container-toolkit-515.65.01-v1.10.0            1         nvidia-container-toolkit         515.65.01-v1.10.0\n\n172.31.41.27   runtime     ExtensionStatus   000.ghcr.io-siderolabs-nvidia-open-gpu-kernel-modules-515.65.01-v1.2.0       1         nvidia-open-gpu-kernel-modules   515.65.01-v1.2.0\n\nCopy\ntalosctl read /proc/driver/nvidia/version\n\n\nwhich should produce an output similar to below:\n\nCopy\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  515.65.01  Wed Mar 16 11:24:05 UTC 2022\n\nGCC version:  gcc version 12.2.0 (GCC)\n\nDeploying NVIDIA device plugin\n\nFirst we need to create the RuntimeClass\n\nApply the following manifest to create a runtime class that uses the extension:\n\nCopy\n---\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\n  name: nvidia\n\nhandler: nvidia\n\n\nInstall the NVIDIA device plugin:\n\nCopy\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n\nhelm repo update\n\nhelm install nvidia-device-plugin nvdp/nvidia-device-plugin --version=0.13.0 --set=runtimeClassName=nvidia\n\n(Optional) Setting the default runtime class as nvidia\n\nDo note that this will set the default runtime class to nvidia for all pods scheduled on the node.\n\nCreate a patch yaml nvidia-default-runtimeclass.yaml to update the machine config similar to below:\n\nCopy\n- op: add\n\n  path: /machine/files\n\n  value:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n\n              default_runtime_name = \"nvidia\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow apply the patch to all Talos nodes in the cluster having NVIDIA GPU’s installed:\n\nCopy\ntalosctl patch mc --patch @nvidia-default-runtimeclass.yaml\n\nTesting the runtime class\n\nNote the spec.runtimeClassName being explicitly set to nvidia in the pod spec.\n\nRun the following command to test the runtime class:\n\nCopy\nkubectl run \\\n\n  nvidia-test \\\n\n  --restart=Never \\\n\n  -ti --rm \\\n\n  --image nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 \\\n\n  --overrides '{\"spec\": {\"runtimeClassName\": \"nvidia\"}}' \\\n\n  nvidia-smi\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "NVIDIA Fabric Manager | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/nvidia-fabricmanager/",
    "html": "Documentation\nTalos Linux Guides\nConfiguration\nNVIDIA Fabric Manager\nNVIDIA Fabric Manager\nIn this guide we’ll follow the procedure to enable NVIDIA Fabric Manager.\n\nNVIDIA GPUs that have nvlink support (for eg: A100) will need the nvidia-fabricmanager system extension also enabled in addition to the NVIDIA drivers. For more information on Fabric Manager refer https://docs.nvidia.com/datacenter/tesla/fabric-manager-user-guide/index.html\n\nThe published versions of the NVIDIA fabricmanager system extensions is available here\n\nThe nvidia-fabricmanager extension version has to match with the NVIDIA driver version in use.\n\nEnabling the NVIDIA fabricmanager system extension\n\nCreate the boot assets or a custom installer and perform a machine upgrade which include the following system extensions:\n\nCopy\nghcr.io/siderolabs/nvidia-open-gpu-kernel-modules:535.129.03-v1.6.2\n\nghcr.io/siderolabs/nvidia-container-toolkit:535.129.03-v1.13.5\n\nghcr.io/siderolabs/nvidia-fabricmanager:535.129.03\n\n\nPatch the machine configuration to load the required modules:\n\nCopy\nmachine:\n\n  kernel:\n\n    modules:\n\n      - name: nvidia\n\n      - name: nvidia_uvm\n\n      - name: nvidia_drm\n\n      - name: nvidia_modeset\n\n  sysctls:\n\n    net.core.bpf_jit_harden: 1\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Managing Talos PKI | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/managing-pki/",
    "html": "Generating New Client Configuration\nUsing Controlplane Node\nFrom Secrets Bundle\nFrom Control Plane Machine Configuration\nRenewing an Expired Administrator Certificate\nDocumentation\nTalos Linux Guides\nConfiguration\nManaging Talos PKI\nManaging Talos PKI\nHow to manage Public Key Infrastructure\nGenerating New Client Configuration\nUsing Controlplane Node\n\nIf you have a valid (not expired) talosconfig with os:admin role, a new client configuration file can be generated with talosctl config new against any controlplane node:\n\nCopy\ntalosctl -n CP1 config new talosconfig-reader --roles os:reader --crt-ttl 24h\n\n\nA specific role and certificate lifetime can be specified.\n\nFrom Secrets Bundle\n\nIf a secrets bundle (secrets.yaml from talosctl gen secrets) was saved while generating machine configuration:\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml --output-types talosconfig -o talosconfig <cluster-name> https://<cluster-endpoint>\n\n\nNote: <cluster-name> and <cluster-endpoint> arguments don’t matter, as they are not used for talosconfig.\n\nFrom Control Plane Machine Configuration\n\nIn order to create a new key pair for client configuration, you will need the root Talos API CA. The base64 encoded CA can be found in the control plane node’s configuration file. Save the the CA public key, and CA private key as ca.crt, and ca.key respectively:\n\nCopy\nyq eval .machine.ca.crt controlplane.yaml | base64 -d > ca.crt\n\nyq eval .machine.ca.key controlplane.yaml | base64 -d > ca.key\n\n\nNow, run the following commands to generate a certificate:\n\nCopy\ntalosctl gen key --name admin\n\ntalosctl gen csr --key admin.key --ip 127.0.0.1\n\ntalosctl gen crt --ca ca --csr admin.csr --name admin\n\n\nPut the base64-encoded files to the respective location to the talosconfig:\n\nCopy\ncontext: mycluster\n\ncontexts:\n\n    mycluster:\n\n        endpoints:\n\n            - CP1\n\n            - CP2\n\n        ca: <base64-encoded ca.crt>\n\n        crt: <base64-encoded admin.crt>\n\n        key: <base64-encoded admin.key>\n\nRenewing an Expired Administrator Certificate\n\nBy default admin talosconfig certificate is valid for 365 days, while cluster CAs are valid for 10 years. In order to prevent admin talosconfig from expiring, renew the client config before expiration using talosctl config new command described above.\n\nIf the talosconfig is expired or lost, you can still generate a new one using either the secrets.yaml secrets bundle or the control plane node’s configuration file using methods described above.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Logging | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/logging/",
    "html": "Viewing logs\nSending logs\nService logs\nKernel logs\nFilebeat example\nFluent-bit example\nVector example\nDocumentation\nTalos Linux Guides\nConfiguration\nLogging\nLogging\nDealing with Talos Linux logs.\nViewing logs\n\nKernel messages can be retrieved with talosctl dmesg command:\n\nCopy\n$ talosctl -n 172.20.1.2 dmesg\n\n\n\n172.20.1.2: kern:    info: [2021-11-10T10:09:37.662764956Z]: Command line: init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 console=ttyS0 reboot=k panic=1 talos.shutdown=halt talos.platform=metal talos.config=http://172.20.1.1:40101/config.yaml\n\n[...]\n\n\nService logs can be retrieved with talosctl logs command:\n\nCopy\n$ talosctl -n 172.20.1.2 services\n\n\n\nNODE         SERVICE      STATE     HEALTH   LAST CHANGE   LAST EVENT\n\n172.20.1.2   apid         Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   containerd   Running   OK       19m29s ago    Health check successful\n\n172.20.1.2   cri          Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   etcd         Running   OK       19m22s ago    Health check successful\n\n172.20.1.2   kubelet      Running   OK       19m20s ago    Health check successful\n\n172.20.1.2   machined     Running   ?        19m30s ago    Service started as goroutine\n\n172.20.1.2   trustd       Running   OK       19m27s ago    Health check successful\n\n172.20.1.2   udevd        Running   OK       19m28s ago    Health check successful\n\n\n\n$ talosctl -n 172.20.1.2 logs machined\n\n\n\n172.20.1.2: [talos] task setupLogger (1/1): done, 106.109µs\n\n172.20.1.2: [talos] phase logger (1/7): done, 564.476µs\n\n[...]\n\n\nContainer logs for Kubernetes pods can be retrieved with talosctl logs -k command:\n\nCopy\n$ talosctl -n 172.20.1.2 containers -k\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                         PID    STATUS\n\n172.20.1.2   k8s.io      kube-system/kube-flannel-dk6d5                                  registry.k8s.io/pause:3.6                                     1329   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-cni:f1d4cf68feb9      ghcr.io/siderolabs/install-cni:v0.7.0-alpha.0-1-g2bb2efc      0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:install-config:bc39fec3cbac   quay.io/coreos/flannel:v0.13.0                                0      CONTAINER_EXITED\n\n172.20.1.2   k8s.io      └─ kube-system/kube-flannel-dk6d5:kube-flannel:5c3989353b98     quay.io/coreos/flannel:v0.13.0                                1610   CONTAINER_RUNNING\n\n172.20.1.2   k8s.io      kube-system/kube-proxy-gfkqj                                    registry.k8s.io/pause:3.5                                     1311   SANDBOX_READY\n\n172.20.1.2   k8s.io      └─ kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f         registry.k8s.io/kube-proxy:v1.29.0                            1379   CONTAINER_RUNNING\n\n\n\n$ talosctl -n 172.20.1.2 logs -k kube-system/kube-proxy-gfkqj:kube-proxy:ad5e8ddc7e7f\n\n172.20.1.2: 2021-11-30T19:13:20.567825192Z stderr F I1130 19:13:20.567737       1 server_others.go:138] \"Detected node IP\" address=\"172.20.0.3\"\n\n172.20.1.2: 2021-11-30T19:13:20.599684397Z stderr F I1130 19:13:20.599613       1 server_others.go:206] \"Using iptables Proxier\"\n\n[...]\n\nSending logs\nService logs\n\nYou can enable logs sendings in machine configuration:\n\nCopy\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"udp://127.0.0.1:12345/\"\n\n        format: \"json_lines\"\n\n      - endpoint: \"tcp://host:5044/\"\n\n        format: \"json_lines\"\n\n\nSeveral destinations can be specified. Supported protocols are UDP and TCP. The only currently supported format is json_lines:\n\nCopy\n{\n\n  \"msg\": \"[talos] apply config request: immediate true, on reboot false\",\n\n  \"talos-level\": \"info\",\n\n  \"talos-service\": \"machined\",\n\n  \"talos-time\": \"2021-11-10T10:48:49.294858021Z\"\n\n}\n\n\nMessages are newline-separated when sent over TCP. Over UDP messages are sent with one message per packet. msg, talos-level, talos-service, and talos-time fields are always present; there may be additional fields.\n\nKernel logs\n\nKernel log delivery can be enabled with the talos.logging.kernel kernel command line argument, which can be specified in the .machine.installer.extraKernelArgs:\n\nCopy\nmachine:\n\n  install:\n\n    extraKernelArgs:\n\n      - talos.logging.kernel=tcp://host:5044/\n\n\nAlso kernel logs delivery can be configured using the document in machine configuration:\n\nCopy\napiVersion: v1alpha1\n\nkind: KmsgLogConfig\n\nname: remote-log\n\nurl: tcp://host:5044/\n\n\nKernel log destination is specified in the same way as service log endpoint. The only supported format is json_lines.\n\nSample message:\n\nCopy\n{\n\n  \"clock\":6252819, // time relative to the kernel boot time\n\n  \"facility\":\"user\",\n\n  \"msg\":\"[talos] task startAllServices (1/1): waiting for 6 services\\n\",\n\n  \"priority\":\"warning\",\n\n  \"seq\":711,\n\n  \"talos-level\":\"warn\", // Talos-translated `priority` into common logging level\n\n  \"talos-time\":\"2021-11-26T16:53:21.3258698Z\" // Talos-translated `clock` using current time\n\n}\n\n\nextraKernelArgs in the machine configuration are only applied on Talos upgrades, not just by applying the config. (Upgrading to the same version is fine).\n\nFilebeat example\n\nTo forward logs to other Log collection services, one way to do this is sending them to a Filebeat running in the cluster itself (in the host network), which takes care of forwarding it to other endpoints (and the necessary transformations).\n\nIf Elastic Cloud on Kubernetes is being used, the following Beat (custom resource) configuration might be helpful:\n\nCopy\napiVersion: beat.k8s.elastic.co/v1beta1\n\nkind: Beat\n\nmetadata:\n\n  name: talos\n\nspec:\n\n  type: filebeat\n\n  version: 7.15.1\n\n  elasticsearchRef:\n\n    name: talos\n\n  config:\n\n    filebeat.inputs:\n\n      - type: \"udp\"\n\n        host: \"127.0.0.1:12345\"\n\n        processors:\n\n          - decode_json_fields:\n\n              fields: [\"message\"]\n\n              target: \"\"\n\n          - timestamp:\n\n              field: \"talos-time\"\n\n              layouts:\n\n                - \"2006-01-02T15:04:05.999999999Z07:00\"\n\n          - drop_fields:\n\n              fields: [\"message\", \"talos-time\"]\n\n          - rename:\n\n              fields:\n\n                - from: \"msg\"\n\n                  to: \"message\"\n\n\n\n  daemonSet:\n\n    updateStrategy:\n\n      rollingUpdate:\n\n        maxUnavailable: 100%\n\n    podTemplate:\n\n      spec:\n\n        dnsPolicy: ClusterFirstWithHostNet\n\n        hostNetwork: true\n\n        securityContext:\n\n          runAsUser: 0\n\n        containers:\n\n          - name: filebeat\n\n            ports:\n\n              - protocol: UDP\n\n                containerPort: 12345\n\n                hostPort: 12345\n\n\nThe input configuration ensures that messages and timestamps are extracted properly. Refer to the Filebeat documentation on how to forward logs to other outputs.\n\nAlso note the hostNetwork: true in the daemonSet configuration.\n\nThis ensures filebeat uses the host network, and listens on 127.0.0.1:12345 (UDP) on every machine, which can then be specified as a logging endpoint in the machine configuration.\n\nFluent-bit example\n\nFirst, we’ll create a value file for the fluentd-bit Helm chart.\n\nCopy\n# fluentd-bit.yaml\n\n\n\npodAnnotations:\n\n  fluentbit.io/exclude: 'true'\n\n\n\nextraPorts:\n\n  - port: 12345\n\n    containerPort: 12345\n\n    protocol: TCP\n\n    name: talos\n\n\n\nconfig:\n\n  service: |\n\n    [SERVICE]\n\n      Flush         5\n\n      Daemon        Off\n\n      Log_Level     warn\n\n      Parsers_File  custom_parsers.conf    \n\n\n\n  inputs: |\n\n    [INPUT]\n\n      Name          tcp\n\n      Listen        0.0.0.0\n\n      Port          12345\n\n      Format        json\n\n      Tag           talos.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         kubernetes\n\n      Path          /var/log/containers/*.log\n\n      Parser        containerd\n\n      Tag           kubernetes.*\n\n\n\n    [INPUT]\n\n      Name          tail\n\n      Alias         audit\n\n      Path          /var/log/audit/kube/*.log\n\n      Parser        audit\n\n      Tag           audit.*    \n\n\n\n  filters: |\n\n    [FILTER]\n\n      Name                kubernetes\n\n      Alias               kubernetes\n\n      Match               kubernetes.*\n\n      Kube_Tag_Prefix     kubernetes.var.log.containers.\n\n      Use_Kubelet         Off\n\n      Merge_Log           On\n\n      Merge_Log_Trim      On\n\n      Keep_Log            Off\n\n      K8S-Logging.Parser  Off\n\n      K8S-Logging.Exclude On\n\n      Annotations         Off\n\n      Labels              On\n\n\n\n    [FILTER]\n\n      Name          modify\n\n      Match         kubernetes.*\n\n      Add           source kubernetes\n\n      Remove        logtag    \n\n\n\n  customParsers: |\n\n    [PARSER]\n\n      Name          audit\n\n      Format        json\n\n      Time_Key      requestReceivedTimestamp\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z\n\n\n\n    [PARSER]\n\n      Name          containerd\n\n      Format        regex\n\n      Regex         ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$\n\n      Time_Key      time\n\n      Time_Format   %Y-%m-%dT%H:%M:%S.%L%z    \n\n\n\n  outputs: |\n\n    [OUTPUT]\n\n      Name    stdout\n\n      Alias   stdout\n\n      Match   *\n\n      Format  json_lines    \n\n\n\n  # If you wish to ship directly to Loki from Fluentbit,\n\n  # Uncomment the following output, updating the Host with your Loki DNS/IP info as necessary.\n\n  # [OUTPUT]\n\n  # Name loki\n\n  # Match *\n\n  # Host loki.loki.svc\n\n  # Port 3100\n\n  # Labels job=fluentbit\n\n  # Auto_Kubernetes_Labels on\n\n\n\ndaemonSetVolumes:\n\n  - name: varlog\n\n    hostPath:\n\n      path: /var/log\n\n\n\ndaemonSetVolumeMounts:\n\n  - name: varlog\n\n    mountPath: /var/log\n\n\n\ntolerations:\n\n  - operator: Exists\n\n    effect: NoSchedule\n\n\nNext, we will add the helm repo for FluentBit, and deploy it to the cluster.\n\nCopy\nhelm repo add fluent https://fluent.github.io/helm-charts\n\nhelm upgrade -i --namespace=kube-system -f fluentd-bit.yaml fluent-bit fluent/fluent-bit\n\n\nNow we need to find the service IP.\n\nCopy\n$ kubectl -n kube-system get svc -l app.kubernetes.io/name=fluent-bit\n\n\n\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n\nfluent-bit   ClusterIP   10.200.0.138   <none>        2020/TCP,5170/TCP   108m\n\n\nFinally, we will change talos log destination with the command talosctl edit mc.\n\nCopy\nmachine:\n\n  logging:\n\n    destinations:\n\n      - endpoint: \"tcp://10.200.0.138:5170\"\n\n        format: \"json_lines\"\n\n\nThis example configuration was well tested with Cilium CNI, and it should work with iptables/ipvs based CNI plugins too.\n\nVector example\n\nVector is a lightweight observability pipeline ideal for a Kubernetes environment. It can ingest (source) logs from multiple sources, perform remapping on the logs (transform), and forward the resulting pipeline to multiple destinations (sinks). As it is an end to end platform, it can be run as a single-deployment ‘aggregator’ as well as a replicaSet of ‘Agents’ that run on each node.\n\nAs Talos can be set as above to send logs to a destination, we can run Vector as an Aggregator, and forward both kernel and service to a UDP socket in-cluster.\n\nBelow is an excerpt of a source/sink setup for Talos, with a ‘sink’ destination of an in-cluster Grafana Loki log aggregation service. As Loki can create labels from the log input, we have set up the Loki sink to create labels based on the host IP, service and facility of the inbound logs.\n\nNote that a method of exposing the Vector service will be required which may vary depending on your setup - a LoadBalancer is a good option.\n\nCopy\nrole: \"Stateless-Aggregator\"\n\n\n\n# Sources\n\nsources:\n\n  talos_kernel_logs:\n\n    address: 0.0.0.0:6050\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n  talos_service_logs:\n\n    address: 0.0.0.0:6051\n\n    type: socket\n\n    mode: udp\n\n    max_length: 102400\n\n    decoding:\n\n      codec: json\n\n    host_key: __host\n\n\n\n# Sinks\n\nsinks:\n\n  talos_kernel:\n\n    type: loki\n\n    inputs:\n\n      - talos_kernel_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 1048576\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      facility: >-\n\n                {{`{{ facility }}`}}\n\n\n\n  talos_service:\n\n    type: loki\n\n    inputs:\n\n      - talos_service_logs_xform\n\n    endpoint: http://loki.system-monitoring:3100\n\n    encoding:\n\n      codec: json\n\n      except_fields:\n\n        - __host\n\n    batch:\n\n      max_bytes: 400000\n\n    out_of_order_action: rewrite_timestamp\n\n    labels:\n\n      hostname: >-\n\n                {{`{{ __host }}`}}\n\n      service: >-\n\n                {{`{{ \"talos-service\" }}`}}\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Editing Machine Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/editing-machine-configuration/",
    "html": "talosctl apply-config\ntaloctl edit machineconfig\ntalosctl patch machineconfig\nRecovering from Node Boot Failures\nDocumentation\nTalos Linux Guides\nConfiguration\nEditing Machine Configuration\nEditing Machine Configuration\nHow to edit and patch Talos machine configuration, with reboot, immediately, or stage update on reboot.\n\nTalos node state is fully defined by machine configuration. Initial configuration is delivered to the node at bootstrap time, but configuration can be updated while the node is running.\n\nThere are three talosctl commands which facilitate machine configuration updates:\n\ntalosctl apply-config to apply configuration from the file\ntalosctl edit machineconfig to launch an editor with existing node configuration, make changes and apply configuration back\ntalosctl patch machineconfig to apply automated machine configuration via JSON patch\n\nEach of these commands can operate in one of four modes:\n\napply change in automatic mode (default): reboot if the change can’t be applied without a reboot, otherwise apply the change immediately\napply change with a reboot (--mode=reboot): update configuration, reboot Talos node to apply configuration change\napply change immediately (--mode=no-reboot flag): change is applied immediately without a reboot, fails if the change contains any fields that can not be updated without a reboot\napply change on next reboot (--mode=staged): change is staged to be applied after a reboot, but node is not rebooted\napply change with automatic revert (--mode=try): change is applied immediately (if not possible, returns an error), and reverts it automatically in 1 minute if no configuration update is applied\napply change in the interactive mode (--mode=interactive; only for talosctl apply-config): launches TUI based interactive installer\n\nNote: applying change on next reboot (--mode=staged) doesn’t modify current node configuration, so next call to talosctl edit machineconfig --mode=staged will not see changes\n\nAdditionally, there is also talosctl get machineconfig -o yaml, which retrieves the current node configuration API resource and contains the machine configuration in the .spec field. It can be used to modify the configuration locally before being applied to the node.\n\nThe list of config changes allowed to be applied immediately in Talos v1.6.2:\n\n.debug\n.cluster\n.machine.time\n.machine.certCANs\n.machine.install (configuration is only applied during install/upgrade)\n.machine.network\n.machine.nodeLabels\n.machine.sysfs\n.machine.sysctls\n.machine.logging\n.machine.controlplane\n.machine.kubelet\n.machine.pods\n.machine.kernel\n.machine.registries (CRI containerd plugin will not pick up the registry authentication settings without a reboot)\n.machine.features.kubernetesTalosAPIAccess\ntalosctl apply-config\n\nThis command is traditionally used to submit initial machine configuration generated by talosctl gen config to the node.\n\nIt can also be used to apply configuration to running nodes. The initial YAML for this is typically obtained using talosctl get machineconfig -o yaml | yq eval .spec >machs.yaml. (We must use yq because for historical reasons, get returns the configuration as a full resource, while apply-config only accepts the raw machine config directly.)\n\nExample:\n\nCopy\ntalosctl -n <IP> apply-config -f config.yaml\n\n\nCommand apply-config can also be invoked as apply machineconfig:\n\nCopy\ntalosctl -n <IP> apply machineconfig -f config.yaml\n\n\nApplying machine configuration immediately (without a reboot):\n\nCopy\ntalosctl -n IP apply machineconfig -f config.yaml --mode=no-reboot\n\n\nStarting the interactive installer:\n\nCopy\ntalosctl -n IP apply machineconfig --mode=interactive\n\n\nNote: when a Talos node is running in the maintenance mode it’s necessary to provide --insecure (-i) flag to connect to the API and apply the config.\n\ntaloctl edit machineconfig\n\nCommand talosctl edit loads current machine configuration from the node and launches configured editor to modify the config. If config hasn’t been changed in the editor (or if updated config is empty), update is not applied.\n\nNote: Talos uses environment variables TALOS_EDITOR, EDITOR to pick up the editor preference. If environment variables are missing, vi editor is used by default.\n\nExample:\n\nCopy\ntalosctl -n <IP> edit machineconfig\n\n\nConfiguration can be edited for multiple nodes if multiple IP addresses are specified:\n\nCopy\ntalosctl -n <IP1>,<IP2>,... edit machineconfig\n\n\nApplying machine configuration change immediately (without a reboot):\n\nCopy\ntalosctl -n <IP> edit machineconfig --mode=no-reboot\n\ntalosctl patch machineconfig\n\nCommand talosctl patch works similar to talosctl edit command - it loads current machine configuration, but instead of launching configured editor it applies a set of JSON patches to the configuration and writes the result back to the node.\n\nExample, updating kubelet version (in auto mode):\n\nCopy\n$ talosctl -n <IP> patch machineconfig -p '[{\"op\": \"replace\", \"path\": \"/machine/kubelet/image\", \"value\": \"ghcr.io/siderolabs/kubelet:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nUpdating kube-apiserver version in immediate mode (without a reboot):\n\nCopy\n$ talosctl -n <IP> patch machineconfig --mode=no-reboot -p '[{\"op\": \"replace\", \"path\": \"/cluster/apiServer/image\", \"value\": \"registry.k8s.io/kube-apiserver:v1.29.0\"}]'\n\npatched mc at the node <IP>\n\n\nA patch might be applied to multiple nodes when multiple IPs are specified:\n\nCopy\ntalosctl -n <IP1>,<IP2>,... patch machineconfig -p '[{...}]'\n\n\nPatches can also be sourced from files using @file syntax:\n\nCopy\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.json -p @manifest-patch.json\n\n\nIt might be easier to store patches in YAML format vs. the default JSON format. Talos can detect file format automatically:\n\nCopy\n# kubelet-patch.yaml\n\n- op: replace\n\n  path: /machine/kubelet/image\n\n  value: ghcr.io/siderolabs/kubelet:v1.29.0\n\nCopy\ntalosctl -n <IP> patch machineconfig -p @kubelet-patch.yaml\n\nRecovering from Node Boot Failures\n\nIf a Talos node fails to boot because of wrong configuration (for example, control plane endpoint is incorrect), configuration can be updated to fix the issue.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Disk Encryption | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/disk-encryption/",
    "html": "Configuration\nEncryption Keys\nEncryption Key Kinds\nKey Rotation\nGoing from Unencrypted to Encrypted and Vice Versa\nEphemeral Partition\nState Partition\nDocumentation\nTalos Linux Guides\nConfiguration\nDisk Encryption\nDisk Encryption\nGuide on using system disk encryption\n\nIt is possible to enable encryption for system disks at the OS level. Currently, only STATE and EPHEMERAL partitions can be encrypted. STATE contains the most sensitive node data: secrets and certs. The EPHEMERAL partition may contain sensitive workload data. Data is encrypted using LUKS2, which is provided by the Linux kernel modules and cryptsetup utility. The operating system will run additional setup steps when encryption is enabled.\n\nIf the disk encryption is enabled for the STATE partition, the system will:\n\nSave STATE encryption config as JSON in the META partition.\nBefore mounting the STATE partition, load encryption configs either from the machine config or from the META partition. Note that the machine config is always preferred over the META one.\nBefore mounting the STATE partition, format and encrypt it. This occurs only if the STATE partition is empty and has no filesystem.\n\nIf the disk encryption is enabled for the EPHEMERAL partition, the system will:\n\nGet the encryption config from the machine config.\nBefore mounting the EPHEMERAL partition, encrypt and format it.\n\nThis occurs only if the EPHEMERAL partition is empty and has no filesystem.\n\nTalos Linux supports four encryption methods, which can be combined together for a single partition:\n\nstatic - encrypt with the static passphrase (weakest protection, for STATE partition encryption it means that the passphrase will be stored in the META partition).\nnodeID - encrypt with the key derived from the node UUID (weak, it is designed to protect against data being leaked or recovered from a drive that has been removed from a Talos Linux node).\nkms - encrypt using key sealed with network KMS (strong, but requires network access to decrypt the data.)\ntpm - encrypt with the key derived from the TPM (strong, when used with SecureBoot).\n\nNote: nodeID encryption is not designed to protect against attacks where physical access to the machine, including the drive, is available. It uses the hardware characteristics of the machine in order to decrypt the data, so drives that have been removed, or recycled from a cloud environment or attached to a different virtual machine, will maintain their protection and encryption.\n\nConfiguration\n\nDisk encryption is disabled by default. To enable disk encryption you should modify the machine configuration with the following options:\n\nCopy\nmachine:\n\n  ...\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - nodeID: {}\n\n          slot: 0\n\nEncryption Keys\n\nNote: What the LUKS2 docs call “keys” are, in reality, a passphrase. When this passphrase is added, LUKS2 runs argon2 to create an actual key from that passphrase.\n\nLUKS2 supports up to 32 encryption keys and it is possible to specify all of them in the machine configuration. Talos always tries to sync the keys list defined in the machine config with the actual keys defined for the LUKS2 partition. So if you update the keys list, keep at least one key that is not changed to be used for key management.\n\nWhen you define a key you should specify the key kind and the slot:\n\nCopy\nmachine:\n\n  ...\n\n  state:\n\n    keys:\n\n      - nodeID: {} # key kind\n\n        slot: 1\n\n\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: supersecret\n\n        slot: 0\n\n\nTake a note that key order does not play any role on which key slot is used. Every key must always have a slot defined.\n\nEncryption Key Kinds\n\nTalos supports two kinds of keys:\n\nnodeID which is generated using the node UUID and the partition label (note that if the node UUID is not really random it will fail the entropy check).\nstatic which you define right in the configuration.\nkms which is sealed with the network KMS.\ntpm which is sealed using the TPM and protected with SecureBoot.\n\nNote: Use static keys only if your STATE partition is encrypted and only for the EPHEMERAL partition. For the STATE partition it will be stored in the META partition, which is not encrypted.\n\nKey Rotation\n\nIn order to completely rotate keys, it is necessary to do talosctl apply-config a couple of times, since there is a need to always maintain a single working key while changing the other keys around it.\n\nSo, for example, first add a new key:\n\nCopy\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: oldkey\n\n        slot: 0\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\nCopy\ntalosctl apply-config -n <node> -f config.yaml\n\n\nThen remove the old key:\n\nCopy\nmachine:\n\n  ...\n\n  ephemeral:\n\n    keys:\n\n      - static:\n\n          passphrase: newkey\n\n        slot: 1\n\n  ...\n\n\nRun:\n\nCopy\ntalosctl apply-config -n <node> -f config.yaml\n\nGoing from Unencrypted to Encrypted and Vice Versa\nEphemeral Partition\n\nThere is no in-place encryption support for the partitions right now, so to avoid losing data only empty partitions can be encrypted.\n\nAs such, migration from unencrypted to encrypted needs some additional handling, especially around explicitly wiping partitions.\n\napply-config should be called with --mode=staged.\nPartition should be wiped after apply-config, but before the reboot.\n\nEdit your machine config and add the encryption configuration:\n\nCopy\nvim config.yaml\n\n\nApply the configuration with --mode=staged:\n\nCopy\ntalosctl apply-config -f config.yaml -n <node ip> --mode=staged\n\n\nWipe the partition you’re going to encrypt:\n\nCopy\ntalosctl reset --system-labels-to-wipe EPHEMERAL -n <node ip> --reboot=true\n\n\nThat’s it! After you run the last command, the partition will be wiped and the node will reboot. During the next boot the system will encrypt the partition.\n\nState Partition\n\nCalling wipe against the STATE partition will make the node lose the config, so the previous flow is not going to work.\n\nThe flow should be to first wipe the STATE partition:\n\nCopy\ntalosctl reset  --system-labels-to-wipe STATE -n <node ip> --reboot=true\n\n\nNode will enter into maintenance mode, then run apply-config with --insecure flag:\n\nCopy\ntalosctl apply-config --insecure -n <node ip> -f config.yaml\n\n\nAfter installation is complete the node should encrypt the STATE partition.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Custom Certificate Authorities | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/certificate-authorities/",
    "html": "Documentation\nTalos Linux Guides\nConfiguration\nCustom Certificate Authorities\nCustom Certificate Authorities\nHow to supply custom certificate authorities\nAppending the Certificate Authority\n\nPut into each machine the PEM encoded certificate:\n\nCopy\nmachine:\n\n  ...\n\n  files:\n\n    - content: |\n\n        -----BEGIN CERTIFICATE-----\n\n        ...\n\n        -----END CERTIFICATE-----        \n\n      permissions: 0644\n\n      path: /etc/ssl/certs/ca-certificates\n\n      op: append\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Omni SaaS | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/omni/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nOmni SaaS\nOmni SaaS\nOmni is a project created by the Talos team that has native support for Talos Linux.\n\nOmni allows you to start with bare metal, virtual machines or a cloud provider, and create clusters spanning all of your locations, with a few clicks.\n\nYou provide the machines – edge compute, bare metal, VMs, or in your cloud account. Boot from an Omni Talos Linux image. Click to allocate to a cluster. That’s it!\n\nVanilla Kubernetes, on your machines, under your control.\nElegant UI for management and operations\nSecurity taken care of – ties into your Enterprise ID provider\nHighly Available Kubernetes API end point built in\nFirewall friendly: manage Edge nodes securely\nFrom single-node clusters to the largest scale\nSupport for GPUs and most CSIs.\n\nThe Omni SaaS is available to run locally, to support air-gapped security and data sovereignty concerns.\n\nOmni handles the lifecycle of Talos Linux machines, provides unified access to the Talos and Kubernetes API tied to the identity provider of your choice, and provides a UI for cluster management and operations. Omni automates scaling the clusters up and down, and provides a unified view of the state of your clusters.\n\nSee more in the Omni documentation.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Containerd | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/containerd/",
    "html": "Examples\nExposing Metrics\nPause Image\nDocumentation\nTalos Linux Guides\nConfiguration\nContainerd\nContainerd\nCustomize Containerd Settings\n\nThe base containerd configuration expects to merge in any additional configs present in /etc/cri/conf.d/20-customization.part.\n\nExamples\nExposing Metrics\n\nPatch the machine config by adding the following:\n\nCopy\nmachine:\n\n  files:\n\n    - content: |\n\n        [metrics]\n\n          address = \"0.0.0.0:11234\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nOnce the server reboots, metrics are now available:\n\nCopy\n$ curl ${IP}:11234/v1/metrics\n\n# HELP container_blkio_io_service_bytes_recursive_bytes The blkio io service bytes recursive\n\n# TYPE container_blkio_io_service_bytes_recursive_bytes gauge\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Async\"} 0\n\ncontainer_blkio_io_service_bytes_recursive_bytes{container_id=\"0677d73196f5f4be1d408aab1c4125cf9e6c458a4bea39e590ac779709ffbe14\",device=\"/dev/dm-0\",major=\"253\",minor=\"0\",namespace=\"k8s.io\",op=\"Discard\"} 0\n\n...\n\n...\n\nPause Image\n\nThis change is often required for air-gapped environments, as containerd CRI plugin has a reference to the pause image which is used to create pods, and it can’t be controlled with Kubernetes pod definitions.\n\nCopy\nmachine:\n\n  files:\n\n    - content: |\n\n        [plugins]\n\n          [plugins.\"io.containerd.grpc.v1.cri\"]\n\n            sandbox_image = \"registry.k8s.io/pause:3.8\"        \n\n      path: /etc/cri/conf.d/20-customization.part\n\n      op: create\n\n\nNow the pause image is set to registry.k8s.io/pause:3.8:\n\nCopy\n$ talosctl containers --kubernetes\n\nNODE         NAMESPACE   ID                                                              IMAGE                                                      PID    STATUS\n\n172.20.0.5   k8s.io      kube-system/kube-flannel-6hfck                                  registry.k8s.io/pause:3.8                                  1773   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-cni:bc39fec3cbac      ghcr.io/siderolabs/install-cni:v1.3.0-alpha.0-2-gb155fa0   0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:install-config:5c3989353b98   ghcr.io/siderolabs/flannel:v0.20.1                         0      CONTAINER_EXITED\n\n172.20.0.5   k8s.io      └─ kube-system/kube-flannel-6hfck:kube-flannel:116c67b50da8     ghcr.io/siderolabs/flannel:v0.20.1                         2092   CONTAINER_RUNNING\n\n172.20.0.5   k8s.io      kube-system/kube-proxy-xp7jq                                    registry.k8s.io/pause:3.8                                  1780   SANDBOX_READY\n\n172.20.0.5   k8s.io      └─ kube-system/kube-proxy-xp7jq:kube-proxy:84fc77c59e17         registry.k8s.io/kube-proxy:v1.26.0-alpha.3                 1843   CONTAINER_RUNNING\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration Patches | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/patching/",
    "html": "Configuration Patch Formats\nStrategic Merge patches\nRFC6902 (JSON Patches)\nExamples\nMachine Network\nCluster Network\nKubelet\nAdmission Control: Pod Security Policy\nConfiguration Patching with talosctl CLI\nDocumentation\nTalos Linux Guides\nConfiguration\nConfiguration Patches\nConfiguration Patches\nIn this guide, we’ll patch the generated machine configuration.\n\nTalos generates machine configuration for two types of machines: controlplane and worker machines. Many configuration options can be adjusted using talosctl gen config but not all of them. Configuration patching allows modifying machine configuration to fit it for the cluster or a specific machine.\n\nConfiguration Patch Formats\n\nTalos supports two configuration patch formats:\n\nstrategic merge patches\nRFC6902 (JSON patches)\n\nStrategic merge patches are the easiest to use, but JSON patches allow more precise configuration adjustments.\n\nNote: Talos 1.5+ supports multi-document machine configuration. JSON patches don’t support multi-document machine configuration, while strategic merge patches do.\n\nStrategic Merge patches\n\nStrategic merge patches look like incomplete machine configuration files:\n\nCopy\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nWhen applied to the machine configuration, the patch gets merged with the respective section of the machine configuration:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        addresses:\n\n          - 10.0.0.2/24\n\n    hostname: worker1\n\n\nIn general, machine configuration contents are merged with the contents of the strategic merge patch, with strategic merge patch values overriding machine configuration values. There are some special rules:\n\nIf the field value is a list, the patch value is appended to the list, with the following exceptions:\nvalues of the fields cluster.network.podSubnets and cluster.network.serviceSubnets are overwritten on merge\nnetwork.interfaces section is merged with the value in the machine config if there is a match on interface: or deviceSelector: keys\nnetwork.interfaces.vlans section is merged with the value in the machine config if there is a match on the vlanId: key\ncluster.apiServer.auditPolicy value is replaced on merge\n\nWhen patching a multi-document machine configuration, following rules apply:\n\nfor each document in the patch, the document is merged with the respective document in the machine configuration (matching by kind, apiVersion and name for named documents)\nif the patch document doesn’t exist in the machine configuration, it is appended to the machine configuration\n\nThe strategic merge patch itself might be a multi-document YAML, and each document will be applied as a patch to the base machine configuration.\n\nRFC6902 (JSON Patches)\n\nJSON patches can be written either in JSON or YAML format. A proper JSON patch requires an op field that depends on the machine configuration contents: whether the path already exists or not.\n\nFor example, the strategic merge patch from the previous section can be written either as:\n\nCopy\n- op: replace\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nor:\n\nCopy\n- op: add\n\n  path: /machine/network/hostname\n\n  value: worker1\n\n\nThe correct op depends on whether the /machine/network/hostname section exists already in the machine config or not.\n\nExamples\nMachine Network\n\nBase machine configuration:\n\nCopy\n# ...\n\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n\nThe goal is to add a virtual IP 192.168.10.50 to the eth0 interface and add another interface eth1 with DHCP enabled.\n\nStrategic merge patch\nJSON patch\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nPatched machine configuration:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: false\n\n        addresses:\n\n          - 192.168.10.3/24\n\n        vip:\n\n          ip: 192.168.10.50\n\n      - interface: eth1\n\n        dhcp: true\n\nCluster Network\n\nBase machine configuration:\n\nCopy\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 10.244.0.0/16\n\n    serviceSubnets:\n\n      - 10.96.0.0/12\n\n\nThe goal is to update pod and service subnets and disable default CNI (Flannel).\n\nStrategic merge patch\nJSON patch\nCopy\ncluster:\n\n  network:\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nPatched machine configuration:\n\nCopy\ncluster:\n\n  network:\n\n    dnsDomain: cluster.local\n\n    podSubnets:\n\n      - 192.168.0.0/16\n\n    serviceSubnets:\n\n      - 192.0.0.0/12\n\n    cni:\n\n      name: none\n\nKubelet\n\nBase machine configuration:\n\nCopy\n# ...\n\nmachine:\n\n  kubelet: {}\n\n\nThe goal is to set the kubelet node IP to come from the subnet 192.168.10.0/24.\n\nStrategic merge patch\nJSON patch\nCopy\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nPatched machine configuration:\n\nCopy\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.10.0/24\n\nAdmission Control: Pod Security Policy\n\nBase machine configuration:\n\nCopy\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\n\nThe goal is to add an exemption for the namespace rook-ceph.\n\nStrategic merge patch\nJSON patch\nCopy\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          exemptions:\n\n            namespaces:\n\n              - rook-ceph\n\nPatched machine configuration:\n\nCopy\ncluster:\n\n  apiServer:\n\n    admissionControl:\n\n      - name: PodSecurity\n\n        configuration:\n\n          apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n\n          defaults:\n\n            audit: restricted\n\n            audit-version: latest\n\n            enforce: baseline\n\n            enforce-version: latest\n\n            warn: restricted\n\n            warn-version: latest\n\n          exemptions:\n\n            namespaces:\n\n              - kube-system\n\n              - rook-ceph\n\n            runtimeClasses: []\n\n            usernames: []\n\n          kind: PodSecurityConfiguration\n\nConfiguration Patching with talosctl CLI\n\nSeveral talosctl commands accept config patches as command-line flags. Config patches might be passed either as an inline value or as a reference to a file with @file.patch syntax:\n\nCopy\ntalosctl ... --patch '[{\"op\": \"add\", \"path\": \"/machine/network/hostname\", \"value\": \"worker1\"}]' --patch @file.patch\n\n\nIf multiple config patches are specified, they are applied in the order of appearance. The format of the patch (JSON patch or strategic merge patch) is detected automatically.\n\nTalos machine configuration can be patched at the moment of generation with talosctl gen config:\n\nCopy\ntalosctl gen config test-cluster https://172.20.0.1:6443 --config-patch @all.yaml --config-patch-control-plane @cp.yaml --config-patch-worker @worker.yaml\n\n\nGenerated machine configuration can also be patched after the fact with talosctl machineconfig patch\n\nCopy\ntalosctl machineconfig patch worker.yaml --patch @patch.yaml -o worker1.yaml\n\n\nMachine configuration on the running Talos node can be patched with talosctl patch:\n\nCopy\ntalosctl patch mc --nodes 172.20.0.2 --patch @patch.yaml\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/configuration/",
    "html": "Documentation\nTalos Linux Guides\nConfiguration\nConfiguration\nGuides on how to configure Talos Linux machines\nConfiguration Patches\n\nIn this guide, we’ll patch the generated machine configuration.\n\nContainerd\n\nCustomize Containerd Settings\n\nCustom Certificate Authorities\n\nHow to supply custom certificate authorities\n\nDisk Encryption\n\nGuide on using system disk encryption\n\nEditing Machine Configuration\n\nHow to edit and patch Talos machine configuration, with reboot, immediately, or stage update on reboot.\n\nLogging\n\nDealing with Talos Linux logs.\n\nManaging Talos PKI\n\nHow to manage Public Key Infrastructure\n\nNVIDIA Fabric Manager\n\nIn this guide we’ll follow the procedure to enable NVIDIA Fabric Manager.\n\nNVIDIA GPU (OSS drivers)\n\nIn this guide we’ll follow the procedure to support NVIDIA GPU using OSS drivers on Talos.\n\nNVIDIA GPU (Proprietary drivers)\n\nIn this guide we’ll follow the procedure to support NVIDIA GPU using proprietary drivers on Talos.\n\nPull Through Image Cache\n\nHow to set up local transparent container images caches.\n\nRole-based access control (RBAC)\n\nSet up RBAC on the Talos Linux API.\n\nSystem Extensions\n\nCustomizing the Talos Linux immutable root file system.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Boot Assets | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/boot-assets/",
    "html": "Image Factory\nExample: Bare-metal with Image Factory\nExample: AWS with Image Factory\nImager\nExtension Image Reference\nExample: Bare-metal with Imager\nExample: AWS with Imager\nDocumentation\nTalos Linux Guides\nInstallation\nBoot Assets\nBoot Assets\nCreating customized Talos boot assets, disk images, ISO and installer images.\n\nTalos Linux provides a set of pre-built images on the release page, but these images can be customized further for a specific use case:\n\nadding system extensions\nupdating kernel command line arguments\nusing custom META contents, e.g. for metal network configuration\ngenerating SecureBoot images signed with a custom key\n\nThere are two ways to generate Talos boot assets:\n\nusing Image Factory service (recommended)\nmanually using imager container image (advanced)\n\nImage Factory is easier to use, but it only produces images for official Talos Linux releases and official Talos Linux system extensions. The imager container can be used to generate images from main branch, with local changes, or with custom system extensions.\n\nImage Factory\n\nImage Factory is a service that generates Talos boot assets on-demand. Image Factory allows to generate boot assets for the official Talos Linux releases and official Talos Linux system extensions.\n\nThe main concept of the Image Factory is a schematic which defines the customization of the boot asset. Once the schematic is configured, Image Factory can be used to pull various Talos Linux images, ISOs, installer images, PXE booting bare-metal machines across different architectures, versions of Talos and platforms.\n\nSidero Labs maintains a public Image Factory instance at https://factory.talos.dev. Image Factory provides a simple UI to prepare schematics and retrieve asset links.\n\nExample: Bare-metal with Image Factory\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument.\n\nFirst, let’s create the schematic file bare-metal.yaml:\n\nCopy\n# bare-metal.yaml\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n      - siderolabs/intel-ucode\n\n\nThe schematic doesn’t contain system extension versions, Image Factory will pick the correct version matching Talos Linux release.\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\nCopy\n$ curl -X POST --data-binary @bare-metal.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176\"}\n\n\nThe returned schematic ID b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176 we will use to generate the boot assets.\n\nThe schematic ID is based on the schematic contents, so uploading the same schematic will return the same ID.\n\nNow we have two options to boot our bare-metal machine:\n\nusing ISO image: https://factory.talos.dev/image/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64.iso (download it and burn to a CD/DVD or USB stick)\nPXE booting via iPXE script: https://factory.talos.dev/pxe/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176/v1.6.2/metal-amd64\n\nThe Image Factory URL contains both schematic ID and Talos version, and both can be changed to generate different boot assets.\n\nOnce the bare-metal machine is booted up for the first time, it will require Talos Linux installer image to be installed on the disk. The installer image will be produced by the Image Factory as well:\n\nCopy\n# Talos machine configuration patch\n\nmachine:\n\n  install:\n\n    image: factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:v1.6.2\n\n\nOnce installed, the machine can be upgraded to a new version of Talos by referencing new installer image:\n\nCopy\ntalosctl upgrade --image factory.talos.dev/installer/b8e8fbbe1b520989e6c52c8dc8303070cb42095997e76e812fa8892393e1d176:<new_version>\n\n\nSame way upgrade process can be used to transition to a new set of system extensions: generate new schematic with the new set of system extensions, and upgrade the machine to the new schematic ID:\n\nCopy\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nExample: AWS with Image Factory\n\nTalos Linux is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required. Let’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s create the schematic file aws.yaml:\n\nCopy\n# aws.yaml\n\ncustomization:\n\n  systemExtensions:\n\n    officialExtensions:\n\n      - siderolabs/gvisor\n\n\nAnd now we can upload the schematic to the Image Factory to retrieve its ID:\n\nCopy\n$ curl -X POST --data-binary @aws.yaml https://factory.talos.dev/schematics\n\n{\"id\":\"d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5\"}\n\n\nThe returned schematic ID d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5 we will use to generate the boot assets.\n\nNow we can download the AWS disk image from the Image Factory:\n\nCopy\ncurl -LO https://factory.talos.dev/image/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5/v1.6.2/aws-amd64.raw.xz\n\n\nNow the aws-amd64.raw.xz file contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nOnce the AWS VM is created from the AMI, it can be upgraded to a different Talos version or a different schematic using talosctl upgrade:\n\nCopy\n# upgrade to a new Talos version\n\ntalosctl upgrade --image factory.talos.dev/installer/d9ff89777e246792e7642abd3220a616afb4e49822382e4213a2e528ab826fe5:<new_version>\n\n# upgrade to a new schematic\n\ntalosctl upgrade --image factory.talos.dev/installer/<new_schematic_id>:v1.6.2\n\nImager\n\nA custom disk image, boot asset can be generated by using the Talos Linux imager container: ghcr.io/siderolabs/imager:v1.6.2. The imager container image can be checked by verifying its signature.\n\nThe generation process can be run with a simple docker run command:\n\nCopy\ndocker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 <image-kind> [optional: customization]\n\n\nA quick guide to the flags used for docker run:\n\n--rm flag removes the container after the run (as it’s not going to be used anymore)\n-t attaches a terminal for colorized output, it can be removed if used in scripts\n-v $PWD/_out:/secureboot:ro mounts the SecureBoot keys into the container (can be skipped if not generating SecureBoot image)\n-v $PWD/_out:/out mounts the output directory (where the generated image will be placed) into the container\n-v /dev:/dev --privileged is required to generate disk images (loop devices are used), but not required for ISOs, installer container images\n\nThe <image-kind> argument to the imager defines the base profile to be used for the image generation. There are several built-in profiles:\n\niso builds a Talos ISO image (see ISO)\nsecureboot-iso builds a Talos ISO image with SecureBoot (see SecureBoot)\nmetal builds a generic disk image for bare-metal machines\nsecureboot-metal builds a generic disk image for bare-metal machines with SecureBoot\nsecureboot-installer builds an installer container image with SecureBoot (see SecureBoot)\naws, gcp, azure, etc. builds a disk image for a specific Talos platform\n\nThe base profile can be customized with the additional flags to the imager:\n\n--arch specifies the architecture of the image to be generated (default: host architecture)\n--meta allows to set initial META values\n--extra-kernel-arg allows to customize the kernel command line arguments. Default kernel arg can be removed by prefixing the argument with a -. For example -console removes all console=<value> arguments, whereas -console=tty0 removes the console=tty0 default argument.\n--system-extension-image allows to install a system extension into the image\nExtension Image Reference\n\nWhile Image Factory automatically resolves the extension name into a matching container image for a specific version of Talos, imager requires the full explicit container image reference. The imager also allows to install custom extensions which are not part of the official Talos Linux system extensions.\n\nTo get the official Talos Linux system extension container image reference matching a Talos release, use the following command:\n\nCopy\ncrane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep EXTENSION-NAME\n\n\nNote: this command is using crane tool, but any other tool which allows to export the image contents can be used.\n\nFor each Talos release, the ghcr.io/siderolabs/extensions:VERSION image contains a pinned reference to each system extension container image.\n\nExample: Bare-metal with Imager\n\nLet’s assume we want to boot Talos on a bare-metal machine with Intel CPU and add a gvisor container runtime to the image. Also we want to disable predictable network interface names with net.ifnames=0 kernel argument and replace the Talos default console arguments and add a custom console arg.\n\nFirst, let’s lookup extension images for Intel CPU microcode updates and gvisor container runtime in the extensions repository:\n\nCopy\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep -E 'gvisor|intel-ucode'\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\nghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow we can generate the ISO image with the following command:\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d --extra-kernel-arg net.ifnames=0 --extra-kernel-arg=-console --extra-kernel-arg=console=ttyS1\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: false\n\nversion: v1.6.2\n\ncustomization:\n\n  extraKernelArgs:\n\n    - net.ifnames=0\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  systemExtensions:\n\n    - imageRef: ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n    - imageRef: ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\ninitramfs ready\n\nkernel command line: talos.platform=metal console=ttyS1 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 net.ifnames=0\n\nISO ready\n\noutput asset path: /out/metal-amd64.iso\n\n\nNow the _out/metal-amd64.iso contains the customized Talos ISO image.\n\nIf the machine is going to be booted using PXE, we can instead generate kernel and initramfs images:\n\nCopy\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind kernel\n\ndocker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 iso --output-kind initramfs --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n\nNow the _out/kernel-amd64 and _out/initramfs-amd64 contain the customized Talos kernel and initramfs images.\n\nNote: the extra kernel args are not used now, as they are set via the PXE boot process, and can’t be embedded into the kernel or initramfs.\n\nAs the next step, we should generate a custom installer image which contains all required system extensions (kernel args can’t be specified with the installer image, but they are set in the machine configuration):\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 installer --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e --system-extension-image ghcr.io/siderolabs/intel-ucode:20231114@sha256:ea564094402b12a51045173c7523f276180d16af9c38755a894cf355d72c249d\n\n...\n\noutput asset path: /out/metal-amd64-installer.tar\n\n\nThe installer container image should be pushed to the container registry:\n\nCopy\ncrane push _out/metal-amd64-installer.tar ghcr.io/<username></username>/installer:v1.6.2\n\n\nNow we can use the customized installer image to install Talos on the bare-metal machine.\n\nWhen it’s time to upgrade a machine, a new installer image can be generated using the new version of imager, and updating the system extension images to the matching versions. The custom installer image can now be used to upgrade Talos machine.\n\nExample: AWS with Imager\n\nTalos is installed on AWS from a disk image (AWS AMI), so only a single boot asset is required.\n\nLet’s assume we want to boot Talos on AWS with gvisor container runtime system extension.\n\nFirst, let’s lookup extension images for the gvisor container runtime in the extensions repository:\n\nCopy\n$ crane export ghcr.io/siderolabs/extensions:v1.6.2 | tar x -O image-digests | grep gvisor\n\nghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n\nNext, let’s generate AWS disk image with that system extension:\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/out -v /dev:/dev --privileged ghcr.io/siderolabs/imager:v1.6.2 aws --system-extension-image ghcr.io/siderolabs/gvisor:20231214.0-v1.6.2@sha256:548b2b121611424f6b1b6cfb72a1669421ffaf2f1560911c324a546c7cee655e\n\n...\n\noutput asset path: /out/aws-amd64.raw\n\ncompression done: /out/aws-amd64.raw.xz\n\n\nNow the _out/aws-amd64.raw.xz contains the customized Talos AWS disk image which can be uploaded as an AMI to the AWS.\n\nIf the AWS machine is later going to be upgraded to a new version of Talos (or a new set of system extensions), generate a customized installer image following the steps above, and upgrade Talos to that installer image.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Raspberry Pi Series | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/rpi_generic/",
    "html": "Prerequisites\nUpdating the EEPROM\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nTroubleshooting\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nRaspberry Pi Series\nRaspberry Pi Series\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\nTalos disk image for the Raspberry Pi generic should in theory work for the boards supported by u-boot rpi_arm64_defconfig. This has only been officialy tested on the Raspberry Pi 4 and community tested on one variant of the Compute Module 4 using Super 6C boards. If you have tested this on other Raspberry Pi boards, please let us know.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nUpdating the EEPROM\n\nUse Raspberry Pi Imager to write an EEPROM update image to a spare SD card. Select Misc utility images under the Operating System tab.\n\nRemove the SD card from your local machine and insert it into the Raspberry Pi. Power the Raspberry Pi on, and wait at least 10 seconds. If successful, the green LED light will blink rapidly (forever), otherwise an error pattern will be displayed. If an HDMI display is attached to the port closest to the power/USB-C port, the screen will display green for success or red if a failure occurs. Power off the Raspberry Pi and remove the SD card from it.\n\nNote: Updating the bootloader only needs to be done once.\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rpi_generic-arm64.raw.xz\n\nxz -d metal-rpi_generic-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-rpi_generic-arm64.raw of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nNote: if you have an HDMI display attached and it shows only a rainbow splash, please use the other HDMI port, the one closest to the power/USB-C port.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\nTroubleshooting\n\nThe following table can be used to troubleshoot booting issues:\n\nLong Flashes\tShort Flashes\tStatus\n0\t3\tGeneric failure to boot\n0\t4\tstart*.elf not found\n0\t7\tKernel image not found\n0\t8\tSDRAM failure\n0\t9\tInsufficient SDRAM\n0\t10\tIn HALT state\n2\t1\tPartition not FAT\n2\t2\tFailed to read from partition\n2\t3\tExtended partition not FAT\n2\t4\tFile signature/hash mismatch - Pi 4\n4\t4\tUnsupported board type\n4\t5\tFatal firmware error\n4\t6\tPower failure type A\n4\t7\tPower failure type B\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Radxa ROCK PI 4C | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/rockpi_4c/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBooting from SD card or eMMC\nBooting from USB or nVME\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nRadxa ROCK PI 4C\nRadxa ROCK PI 4C\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-rockpi_4c-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nCopy\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4c/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4c-arm64.raw.xz\n\nxz -d metal-rockpi_4c-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4c-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Radxa ROCK PI 4 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/rockpi_4/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBooting from SD card or eMMC\nBooting from USB or nVME\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nRadxa ROCK PI 4\nRadxa ROCK PI 4\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card or an eMMC or USB drive or an nVME drive\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card/eMMC/USB/nVME can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-rockpi_4-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\n\nThe user has two options to proceed:\n\nbooting from a SD card or eMMC\nbooting from a USB or nVME (requires the RockPi board to have the SPI flash)\nBooting from SD card or eMMC\n\nInsert the SD card into the board, turn it on and proceed to bootstrapping the node.\n\nBooting from USB or nVME\n\nThis requires the user to flash the RockPi SPI flash with u-boot.\n\nThis requires the user has access to crane CLI, a spare SD card and optionally access to the RockPi serial console.\n\nFlash the Rock PI 4c variant of Debian to the SD card.\nBoot into the debian image\nCheck that /dev/mtdblock0 exists otherwise the command will silently fail; e.g. lsblk.\nDownload u-boot image from talos u-boot:\nCopy\nmkdir _out\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C _out rockpi_4/rkspi_loader.img\n\nsudo dd if=rkspi_loader.img of=/dev/mtdblock0 bs=4K\n\nOptionally, you can also write Talos image to the SSD drive right from your Rock PI board:\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-rockpi_4-arm64.raw.xz\n\nsudo dd if=metal-rockpi_4-arm64.raw.xz of=/dev/nvme0n1\n\nremove SD card and reboot.\n\nAfter these steps, Talos will boot from the nVME/USB and enter maintenance mode. Proceed to bootstrapping the node.\n\nBootstrapping the Node\n\nWait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Pine64 Rock64 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/rock64/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nPine64 Rock64\nPine64 Rock64\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rock64-arm64.raw.xz\n\nxz -d metal-rock64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-rock64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Libre Computer Board ALL-H3-CC | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/libretech_all_h3_cc_h5/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nLibre Computer Board ALL-H3-CC\nLibre Computer Board ALL-H3-CC\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nxz -d metal-libretech_all_h3_cc_h5-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-libretech_all_h3_cc_h5-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Pine64 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/pine64/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nPine64\nPine64\nInstalling Talos on a Pine64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-pine64-arm64.raw.xz\n\nxz -d metal-pine64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-pine64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Jetson Nano | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/jetson_nano/",
    "html": "Prerequisites\nFlashing the firmware to on-board SPI flash\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nJetson Nano\nJetson Nano\nInstalling Talos on Jetson Nano SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card/USB drive\ncrane CLI\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nFlashing the firmware to on-board SPI flash\n\nFlashing the firmware only needs to be done once.\n\nWe will use the R32.7.2 release for the Jetson Nano. Most of the instructions is similar to this doc except that we’d be using a upstream version of u-boot with patches from NVIDIA u-boot so that USB boot also works.\n\nBefore flashing we need the following:\n\nA USB-A to micro USB cable\nA jumper wire to enable recovery mode\nA HDMI monitor to view the logs if the USB serial adapter is not available\nA USB to Serial adapter with 3.3V TTL (optional)\nA 5V DC barrel jack\n\nIf you’re planning to use the serial console follow the documentation here\n\nFirst start by downloading the Jetson Nano L4T release.\n\nCopy\ncurl -SLO https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/t210/jetson-210_linux_r32.7.2_aarch64.tbz2\n\n\nNext we will extract the L4T release and replace the u-boot binary with the patched version.\n\nCopy\ntar xf jetson-210_linux_r32.6.1_aarch64.tbz2\n\ncd Linux_for_Tegra\n\ncrane --platform=linux/arm64 export ghcr.io/siderolabs/u-boot:v1.3.0-alpha.0-25-g0ac7773 - | tar xf - --strip-components=1 -C bootloader/t210ref/p3450-0000/ jetson_nano/u-boot.bin\n\n\nNext we will flash the firmware to the Jetson Nano SPI flash. In order to do that we need to put the Jetson Nano into Force Recovery Mode (FRC). We will use the instructions from here\n\nEnsure that the Jetson Nano is powered off. There is no need for the SD card/USB storage/network cable to be connected\nConnect the micro USB cable to the micro USB port on the Jetson Nano, don’t plug the other end to the PC yet\nEnable Force Recovery Mode (FRC) by placing a jumper across the FRC pins on the Jetson Nano\nFor board revision A02, these are pins 3 and 4 of header J40\nFor board revision B01, these are pins 9 and 10 of header J50\nPlace another jumper across J48 to enable power from the DC jack and connect the Jetson Nano to the DC jack J25\nNow connect the other end of the micro USB cable to the PC and remove the jumper wire from the FRC pins\n\nNow the Jetson Nano is in Force Recovery Mode (FRC) and can be confirmed by running the following command\n\nCopy\nlsusb | grep -i \"nvidia\"\n\n\nNow we can move on the flashing the firmware.\n\nCopy\nsudo ./flash p3448-0000-max-spi external\n\n\nThis will flash the firmware to the Jetson Nano SPI flash and you’ll see a lot of output. If you’ve connected the serial console you’ll also see the progress there. Once the flashing is done you can disconnect the USB cable and power off the Jetson Nano.\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-jetson_nano-arm64.raw.xz\n\nxz -d metal-jetson_nano-arm64.raw.xz\n\nWriting the Image\n\nNow dd the image to your SD card/USB storage:\n\nCopy\nsudo dd if=metal-jetson_nano-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M status=progress\n\n\n| Replace /dev/mmcblk0 with the name of your SD card/USB storage.\n\nBootstrapping the Node\n\nInsert the SD card/USB storage to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Friendlyelec Nano PI R4S | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/nanopi_r4s/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nFriendlyelec Nano PI R4S\nFriendlyelec Nano PI R4S\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-rockpi_4-arm64.raw.xz\n\nxz -d metal-nanopi_r4s-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-nanopi_r4s-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Banana Pi M64 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/bananapi_m64/",
    "html": "Prerequisites\nDownload the Image\nWriting the Image\nBootstrapping the Node\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nBanana Pi M64\nBanana Pi M64\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\nPrerequisites\n\nYou will need\n\ntalosctl\nan SD card\n\nDownload the latest talosctl.\n\nCopy\ncurl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/v1.6.2/talosctl-$(uname -s | tr \"[:upper:]\" \"[:lower:]\")-amd64\n\nchmod +x /usr/local/bin/talosctl\n\nDownload the Image\n\nDownload the image and decompress it:\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-bananapi_m64-arm64.raw.xz\n\nxz -d metal-bananapi_m64-arm64.raw.xz\n\nWriting the Image\n\nThe path to your SD card can be found using fdisk on Linux or diskutil on macOS. In this example, we will assume /dev/mmcblk0.\n\nNow dd the image to your SD card:\n\nCopy\nsudo dd if=metal-bananapi_m64-arm64.img of=/dev/mmcblk0 conv=fsync bs=4M\n\nBootstrapping the Node\n\nInsert the SD card to your board, turn it on and wait for the console to show you the instructions for bootstrapping the node. Following the instructions in the console output to connect to the interactive installer:\n\nCopy\ntalosctl apply-config --insecure --mode=interactive --nodes <node IP or DNS name>\n\n\nOnce the interactive installation is applied, the cluster will form and you can then use kubectl.\n\nRetrieve the kubeconfig\n\nRetrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "VirtualBox | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/local-platforms/virtualbox/",
    "html": "Installation\nHow to Get VirtualBox\nInstall talosctl\nDownload ISO Image\nCreate VMs\nStart Control Plane Node\nGenerate Machine Configurations\nCreate Control Plane Node\nCreate Worker Node\nUsing the Cluster\nBootstrap Etcd\nRetrieve the kubeconfig\nCleaning Up\nDocumentation\nTalos Linux Guides\nInstallation\nLocal Platforms\nVirtualBox\nVirtualBox\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\nIn this guide we will create a Kubernetes cluster using VirtualBox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get VirtualBox\n\nInstall VirtualBox with your operating system package manager or from the website. For example, on Ubuntu for x86:\n\nCopy\napt install virtualbox\n\nInstall talosctl\n\nYou can download talosctl via\n\nCopy\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nDownload the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nCopy\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nCopy\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nCreate VMs\n\nStart by creating a new VM by clicking the “New” button in the VirtualBox UI:\n\nSupply a name for this VM, and specify the Type and Version:\n\nEdit the memory to supply at least 2GB of RAM for the VM:\n\nProceed through the disk settings, keeping the defaults. You can increase the disk space if desired.\n\nOnce created, select the VM and hit “Settings”:\n\nIn the “System” section, supply at least 2 CPUs:\n\nIn the “Network” section, switch the network “Attached To” section to “Bridged Adapter”:\n\nFinally, in the “Storage” section, select the optical drive and, on the right, select the ISO by browsing your filesystem:\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”. Once the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\nCopy\ntalosctl gen config talos-vbox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\nCopy\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the VirtualBox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\nCopy\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nCopy\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig $TALOSCONFIG config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig $TALOSCONFIG config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig $TALOSCONFIG bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig $TALOSCONFIG kubeconfig .\n\n\nYou can then use kubectl in this fashion:\n\nCopy\nkubectl get nodes\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the VirtualBox UI.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Single Board Computers | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/single-board-computers/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nSingle Board Computers\nSingle Board Computers\nInstallation of Talos Linux on single-board computers.\nBanana Pi M64\n\nInstalling Talos on Banana Pi M64 SBC using raw disk image.\n\nFriendlyelec Nano PI R4S\n\nInstalling Talos on a Nano PI R4S SBC using raw disk image.\n\nJetson Nano\n\nInstalling Talos on Jetson Nano SBC using raw disk image.\n\nLibre Computer Board ALL-H3-CC\n\nInstalling Talos on Libre Computer Board ALL-H3-CC SBC using raw disk image.\n\nPine64\n\nInstalling Talos on a Pine64 SBC using raw disk image.\n\nPine64 Rock64\n\nInstalling Talos on Pine64 Rock64 SBC using raw disk image.\n\nRadxa ROCK PI 4\n\nInstalling Talos on Radxa ROCK PI 4a/4b SBC using raw disk image.\n\nRadxa ROCK PI 4C\n\nInstalling Talos on Radxa ROCK PI 4c SBC using raw disk image.\n\nRaspberry Pi Series\n\nInstalling Talos on Raspberry Pi SBC’s using raw disk image.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "QEMU | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/local-platforms/qemu/",
    "html": "Requirements\nInstallation\nHow to get QEMU\nInstall talosctl\nInstall Talos kernel and initramfs\nCreate the Cluster\nUsing the Cluster\nCleaning Up\nManual Clean Up\nRemove VM Launchers\nStopping VMs\nRemove load balancer\nRemove DHCP server\nRemove network\nRemove state directory\nTroubleshooting\nLogs\nDocumentation\nTalos Linux Guides\nInstallation\nLocal Platforms\nQEMU\nQEMU\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nIn this guide we will create a Kubernetes cluster using QEMU.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, see the video below:\n\nRequirements\nLinux\na kernel with\nKVM enabled (/dev/kvm must exist)\nCONFIG_NET_SCH_NETEM enabled\nCONFIG_NET_SCH_INGRESS enabled\nat least CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities\nQEMU\nbridge, static and firewall CNI plugins from the standard CNI plugins, and tc-redirect-tap CNI plugin from the awslabs tc-redirect-tap installed to /opt/cni/bin (installed automatically by talosctl)\niptables\n/var/run/netns directory should exist\nInstallation\nHow to get QEMU\n\nInstall QEMU with your operating system package manager. For example, on Ubuntu for x86:\n\nCopy\napt install qemu-system-x86 qemu-kvm\n\nInstall talosctl\n\nDownload talosctl via\n\nCopy\ncurl -sL https://talos.dev/install | sh\n\nInstall Talos kernel and initramfs\n\nQEMU provisioner depends on Talos kernel (vmlinuz) and initramfs (initramfs.xz). These files can be downloaded from the Talos release:\n\nCopy\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/vmlinuz-<arch> -L -o _out/vmlinuz-<arch>\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/initramfs-<arch>.xz -L -o _out/initramfs-<arch>.xz\n\n\nFor example version v1.6.2:\n\nCopy\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/vmlinuz-amd64 -L -o _out/vmlinuz-amd64\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/initramfs-amd64.xz -L -o _out/initramfs-amd64.xz\n\nCreate the Cluster\n\nFor the first time, create root state directory as your user so that you can inspect the logs as non-root user:\n\nCopy\nmkdir -p ~/.talos/clusters\n\n\nCreate the cluster:\n\nCopy\nsudo --preserve-env=HOME talosctl cluster create --provisioner qemu\n\n\nBefore the first cluster is created, talosctl will download the CNI bundle for the VM provisioning and install it to ~/.talos/cni directory.\n\nOnce the above finishes successfully, your talosconfig (~/.talos/config) will be configured to point to the new cluster, and kubeconfig will be downloaded and merged into default kubectl config location (~/.kube/config).\n\nCluster provisioning process can be optimized with registry pull-through caches.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl -n 10.5.0.2 containers for a list of containers in the system namespace, or talosctl -n 10.5.0.2 containers -k for the k8s.io namespace. To view the logs of a container, use talosctl -n 10.5.0.2 logs <container> or talosctl -n 10.5.0.2 logs -k <container>.\n\nA bridge interface will be created, and assigned the default IP 10.5.0.1. Each node will be directly accessible on the subnet specified at cluster creation time. A loadbalancer runs on 10.5.0.1 by default, which handles loadbalancing for the Kubernetes APIs.\n\nYou can see a summary of the cluster state by running:\n\nCopy\n$ talosctl cluster show --provisioner qemu\n\nPROVISIONER       qemu\n\nNAME              talos-default\n\nNETWORK NAME      talos-default\n\nNETWORK CIDR      10.5.0.0/24\n\nNETWORK GATEWAY   10.5.0.1\n\nNETWORK MTU       1500\n\n\n\nNODES:\n\n\n\nNAME                           TYPE           IP         CPU    RAM      DISK\n\ntalos-default-controlplane-1   ControlPlane   10.5.0.2   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-2   ControlPlane   10.5.0.3   1.00   1.6 GB   4.3 GB\n\ntalos-default-controlplane-3   ControlPlane   10.5.0.4   1.00   1.6 GB   4.3 GB\n\ntalos-default-worker-1         Worker         10.5.0.5   1.00   1.6 GB   4.3 GB\n\nCleaning Up\n\nTo cleanup, run:\n\nCopy\nsudo --preserve-env=HOME talosctl cluster destroy --provisioner qemu\n\n\nNote: In that case that the host machine is rebooted before destroying the cluster, you may need to manually remove ~/.talos/clusters/talos-default.\n\nManual Clean Up\n\nThe talosctl cluster destroy command depends heavily on the clusters state directory. It contains all related information of the cluster. The PIDs and network associated with the cluster nodes.\n\nIf you happened to have deleted the state folder by mistake or you would like to cleanup the environment, here are the steps how to do it manually:\n\nRemove VM Launchers\n\nFind the process of talosctl qemu-launch:\n\nCopy\nps -elf | grep 'talosctl qemu-launch'\n\n\nTo remove the VMs manually, execute:\n\nCopy\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 157615 and 157617\n\nCopy\nps -elf | grep '[t]alosctl qemu-launch'\n\n0 S root      157615    2835  0  80   0 - 184934 -     07:53 ?        00:00:00 talosctl qemu-launch\n\n0 S root      157617    2835  0  80   0 - 185062 -     07:53 ?        00:00:00 talosctl qemu-launch\n\nsudo kill -s SIGTERM 157615\n\nsudo kill -s SIGTERM 157617\n\nStopping VMs\n\nFind the process of qemu-system:\n\nCopy\nps -elf | grep 'qemu-system'\n\n\nTo stop the VMs manually, execute:\n\nCopy\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where VMs are running with PIDs 158065 and 158216\n\nCopy\nps -elf | grep qemu-system\n\n2 S root     1061663 1061168 26  80   0 - 1786238 -    14:05 ?        01:53:56 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/home/username/.talos/clusters/talos-default/bootstrap-master.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=1e:86:c6:b4:7c:c4 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=7ec0a73c-826e-4eeb-afd1-39ff9f9160ca -machine q35,accel=kvm\n\n2 S root     1061663 1061170 67  80   0 - 621014 -     21:23 ?        00:00:07 qemu-system-x86_64 -m 2048 -drive format=raw,if=virtio,file=/homeusername/.talos/clusters/talos-default/pxe-1.disk -smp cpus=2 -cpu max -nographic -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device virtio-net-pci,netdev=net0,mac=36:f3:2f:c3:9f:06 -device virtio-rng-pci -no-reboot -boot order=cn,reboot-timeout=5000 -smbios type=1,uuid=ce12a0d0-29c8-490f-b935-f6073ab916a6 -machine q35,accel=kvm\n\nsudo kill -s SIGTERM 1061663\n\nsudo kill -s SIGTERM 1061663\n\nRemove load balancer\n\nFind the process of talosctl loadbalancer-launch:\n\nCopy\nps -elf | grep 'talosctl loadbalancer-launch'\n\n\nTo remove the LB manually, execute:\n\nCopy\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nCopy\nps -elf | grep '[t]alosctl loadbalancer-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl loadbalancer-launch --loadbalancer-addr 10.5.0.1 --loadbalancer-upstreams 10.5.0.2\n\nsudo kill -s SIGTERM 157609\n\nRemove DHCP server\n\nFind the process of talosctl dhcpd-launch:\n\nCopy\nps -elf | grep 'talosctl dhcpd-launch'\n\n\nTo remove the LB manually, execute:\n\nCopy\nsudo kill -s SIGTERM <PID>\n\n\nExample output, where loadbalancer is running with PID 157609\n\nCopy\nps -elf | grep '[t]alosctl dhcpd-launch'\n\n4 S root      157609    2835  0  80   0 - 184998 -     07:53 ?        00:00:07 talosctl dhcpd-launch --state-path /home/username/.talos/clusters/talos-default --addr 10.5.0.1 --interface talosbd9c32bc\n\nsudo kill -s SIGTERM 157609\n\nRemove network\n\nThis is more tricky part as if you have already deleted the state folder. If you didn’t then it is written in the state.yaml in the ~/.talos/clusters/<cluster-name> directory.\n\nCopy\nsudo cat ~/.talos/clusters/<cluster-name>/state.yaml | grep bridgename\n\nbridgename: talos<uuid>\n\n\nIf you only had one cluster, then it will be the interface with name talos<uuid>\n\nCopy\n46: talos<uuid>: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n\n    link/ether a6:72:f4:0a:d3:9c brd ff:ff:ff:ff:ff:ff\n\n    inet 10.5.0.1/24 brd 10.5.0.255 scope global talos17c13299\n\n       valid_lft forever preferred_lft forever\n\n    inet6 fe80::a472:f4ff:fe0a:d39c/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n\nTo remove this interface:\n\nCopy\nsudo ip link del talos<uuid>\n\nRemove state directory\n\nTo remove the state directory execute:\n\nCopy\nsudo rm -Rf /home/$USER/.talos/clusters/<cluster-name>\n\nTroubleshooting\nLogs\n\nInspect logs directory\n\nCopy\nsudo cat ~/.talos/clusters/<cluster-name>/*.log\n\n\nLogs are saved under <cluster-name>-<role>-<node-id>.log\n\nFor example in case of k8s cluster name:\n\nCopy\nls -la ~/.talos/clusters/k8s | grep log\n\n-rw-r--r--. 1 root root      69415 Apr 26 20:58 k8s-master-1.log\n\n-rw-r--r--. 1 root root      68345 Apr 26 20:58 k8s-worker-1.log\n\n-rw-r--r--. 1 root root      24621 Apr 26 20:59 lb.log\n\n\nInspect logs during the installation\n\nCopy\ntail -f ~/.talos/clusters/<cluster-name>/*.log\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Docker | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/local-platforms/docker/",
    "html": "Requirements\nCaveats\nCreate the Cluster\nUsing the Cluster\nCleaning Up\nRunning Talos in Docker Manually\nDocumentation\nTalos Linux Guides\nInstallation\nLocal Platforms\nDocker\nDocker\nCreating Talos Kubernetes cluster using Docker.\n\nIn this guide we will create a Kubernetes cluster in Docker, using a containerized version of Talos.\n\nRunning Talos in Docker is intended to be used in CI pipelines, and local testing when you need a quick and easy cluster. Furthermore, if you are running Talos in production, it provides an excellent way for developers to develop against the same version of Talos.\n\nRequirements\n\nThe follow are requirements for running Talos in Docker:\n\nDocker 18.03 or greater\na recent version of talosctl\nCaveats\n\nDue to the fact that Talos will be running in a container, certain APIs are not available. For example upgrade, reset, and similar APIs don’t apply in container mode. Further, when running on a Mac in docker, due to networking limitations, VIPs are not supported.\n\nCreate the Cluster\n\nCreating a local cluster is as simple as:\n\nCopy\ntalosctl cluster create --wait\n\n\nOnce the above finishes successfully, your talosconfig(~/.talos/config) will be configured to point to the new cluster.\n\nNote: Startup times can take up to a minute or more before the cluster is available.\n\nFinally, we just need to specify which nodes you want to communicate with using talosctl. Talosctl can operate on one or all the nodes in the cluster – this makes cluster wide commands much easier.\n\ntalosctl config nodes 10.5.0.2 10.5.0.3\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nCleaning Up\n\nTo cleanup, run:\n\nCopy\ntalosctl cluster destroy\n\nRunning Talos in Docker Manually\n\nTo run Talos in a container manually, run:\n\nCopy\ndocker run --rm -it \\\n\n  --name tutorial \\\n\n  --hostname talos-cp \\\n\n  --read-only \\\n\n  --privileged \\\n\n  --security-opt seccomp=unconfined \\\n\n  --mount type=tmpfs,destination=/run \\\n\n  --mount type=tmpfs,destination=/system \\\n\n  --mount type=tmpfs,destination=/tmp \\\n\n  --mount type=volume,destination=/system/state \\\n\n  --mount type=volume,destination=/var \\\n\n  --mount type=volume,destination=/etc/cni \\\n\n  --mount type=volume,destination=/etc/kubernetes \\\n\n  --mount type=volume,destination=/usr/libexec/kubernetes \\\n\n  --mount type=volume,destination=/usr/etc/udev \\\n\n  --mount type=volume,destination=/opt \\\n\n  -e PLATFORM=container \\\n\n  ghcr.io/siderolabs/talos:v1.6.2\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Local Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/local-platforms/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nLocal Platforms\nLocal Platforms\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\nDocker\n\nCreating Talos Kubernetes cluster using Docker.\n\nQEMU\n\nCreating Talos Kubernetes cluster using QEMU VMs.\n\nVirtualBox\n\nCreating Talos Kubernetes cluster using VurtualBox VMs.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Vultr | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/vultr/",
    "html": "Creating a Cluster using the Vultr CLI\nBoot Options\nUpload an ISO Image\nPXE Booting via Image Factory\nCreate a Load Balancer\nCreate the Machine Configuration\nGenerate Base Configuration\nValidate the Configuration Files\nCreate the Nodes\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nBootstrap etcd\nConfigure Endpoints and Nodes\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nVultr\nVultr\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\nCreating a Cluster using the Vultr CLI\n\nThis guide will demonstrate how to create a highly-available Kubernetes cluster with one worker using the Vultr cloud provider.\n\nVultr have a very well documented REST API, and an open-source CLI tool to interact with the API which will be used in this guide. Make sure to follow installation and authentication instructions for the vultr-cli tool.\n\nBoot Options\nUpload an ISO Image\n\nFirst step is to make the Talos ISO available to Vultr by uploading the latest release of the ISO to the Vultr ISO server.\n\nCopy\nvultr-cli iso create --url https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\n\n\nMake a note of the ID in the output, it will be needed later when creating the instances.\n\nPXE Booting via Image Factory\n\nTalos Linux can be PXE-booted on Vultr using Image Factory, using the vultr platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/vultr-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate a Load Balancer\n\nA load balancer is needed to serve as the Kubernetes endpoint for the cluster.\n\nCopy\nvultr-cli load-balancer create \\\n\n   --region $REGION \\\n\n   --label \"Talos Kubernetes Endpoint\" \\\n\n   --port 6443 \\\n\n   --protocol tcp \\\n\n   --check-interval 10 \\\n\n   --response-timeout 5 \\\n\n   --healthy-threshold 5 \\\n\n   --unhealthy-threshold 3 \\\n\n   --forwarding-rules frontend_protocol:tcp,frontend_port:443,backend_protocol:tcp,backend_port:6443\n\n\nMake a note of the ID of the load balancer from the output of the above command, it will be needed after the control plane instances are created.\n\nCopy\nvultr-cli load-balancer get $LOAD_BALANCER_ID | grep ^IP\n\n\nMake a note of the IP address, it will be needed later when generating the configuration.\n\nCreate the Machine Configuration\nGenerate Base Configuration\n\nUsing the IP address (or DNS name if one was created) of the load balancer created above, generate the machine configuration files for the new cluster.\n\nCopy\ntalosctl gen config talos-kubernetes-vultr https://$LOAD_BALANCER_ADDRESS\n\n\nOnce generated, the machine configuration can be modified as necessary for the new cluster, for instance updating disk installation, or adding SANs for the certificates.\n\nValidate the Configuration Files\nCopy\ntalosctl validate --config controlplane.yaml --mode cloud\n\ntalosctl validate --config worker.yaml --mode cloud\n\nCreate the Nodes\nCreate the Control Plane Nodes\n\nFirst a control plane needs to be created, with the example below creating 3 instances in a loop. The instance type (noted by the --plan vc2-2c-4gb argument) in the example is for a minimum-spec control plane node, and should be updated to suit the cluster being created.\n\nCopy\nfor id in $(seq 3); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-2c-4gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-cp${id} \\\n\n        --label \"Talos Kubernetes Control Plane\" \\\n\n        --tags talos,kubernetes,control-plane\n\ndone\n\n\nMake a note of the instance IDs, as they are needed to attach to the load balancer created earlier.\n\nCopy\nvultr-cli load-balancer update $LOAD_BALANCER_ID --instances $CONTROL_PLANE_1_ID,$CONTROL_PLANE_2_ID,$CONTROL_PLANE_3_ID\n\n\nOnce the nodes are booted and waiting in maintenance mode, the machine configuration can be applied to each one in turn.\n\nCopy\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_1_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_2_ADDRESS --file controlplane.yaml\n\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $CONTROL_PLANE_3_ADDRESS --file controlplane.yaml\n\nCreate the Worker Nodes\n\nNow worker nodes can be created and configured in a similar way to the control plane nodes, the difference being mainly in the machine configuration file. Note that like with the control plane nodes, the instance type (here set by --plan vc2-1-1gb) should be changed for the actual cluster requirements.\n\nCopy\nfor id in $(seq 1); do\n\n    vultr-cli instance create \\\n\n        --plan vc2-1c-1gb \\\n\n        --region $REGION \\\n\n        --iso $TALOS_ISO_ID \\\n\n        --host talos-k8s-worker${id} \\\n\n        --label \"Talos Kubernetes Worker\" \\\n\n        --tags talos,kubernetes,worker\n\ndone\n\n\nOnce the worker is booted and in maintenance mode, the machine configuration can be applied in the following manner.\n\nCopy\ntalosctl --talosconfig talosconfig apply-config --insecure --nodes $WORKER_1_ADDRESS --file worker.yaml\n\nBootstrap etcd\n\nOnce all the cluster nodes are correctly configured, the cluster can be bootstrapped to become functional. It is important that the talosctl bootstrap command be executed only once and against only a single control plane node.\n\nCopy\ntalosctl --talosconfig talosconfig boostrap --endpoints $CONTROL_PLANE_1_ADDRESS --nodes $CONTROL_PLANE_1_ADDRESS\n\nConfigure Endpoints and Nodes\n\nWhile the cluster goes through the bootstrapping process and beings to self-manage, the talosconfig can be updated with the endpoints and nodes.\n\nCopy\ntalosctl --talosconfig talosconfig config endpoints $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS\n\ntalosctl --talosconfig talosconfig config nodes $CONTROL_PLANE_1_ADDRESS $CONTROL_PLANE_2_ADDRESS $CONTROL_PLANE_3_ADDRESS WORKER_1_ADDRESS\n\nRetrieve the kubeconfig\n\nFinally, with the cluster fully running, the administrative kubeconfig can be retrieved from the Talos API to be saved locally.\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nNow the kubeconfig can be used by any of the usual Kubernetes tools to interact with the Talos-based Kubernetes cluster as normal.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "UpCloud | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/upcloud/",
    "html": "Create the Image\nCreating a Cluster via the CLI\nCreate an Endpoint\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nCreate the Servers\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nUpCloud\nUpCloud\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nIn this guide we will create an HA Kubernetes cluster 3 control plane nodes and 1 worker node. We assume some familiarity with UpCloud. If you need more information on UpCloud specifics, please see the official UpCloud documentation.\n\nCreate the Image\n\nThe best way to create an image for UpCloud, is to build one using Hashicorp packer, with the upcloud-amd64.raw.xz image found on the Talos Releases. Using the general ISO is also possible, but the UpCloud image has some UpCloud specific features implemented, such as the fetching of metadata and user data to configure the nodes.\n\nTo create the cluster, you need a few things locally installed:\n\nUpCloud CLI\nHashicorp Packer\n\nNOTE: Make sure your account allows API connections. To do so, log into UpCloud control panel and go to People -> Account -> Permissions -> Allow API connections checkbox. It is recommended to create a separate subaccount for your API access and only set the API permission.\n\nTo use the UpCloud CLI, you need to create a config in $HOME/.config/upctl.yaml\n\nCopy\nusername: your_upcloud_username\n\npassword: your_upcloud_password\n\n\nTo use the UpCloud packer plugin, you need to also export these credentials to your environment variables, by e.g. putting the following in your .bashrc or .zshrc\n\nCopy\nexport UPCLOUD_USERNAME=\"<username>\"\n\nexport UPCLOUD_PASSWORD=\"<password>\"\n\n\nNext create a config file for packer to use:\n\nCopy\n# upcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    upcloud = {\n\n      version = \">=v1.0.0\"\n\n      source  = \"github.com/UpCloudLtd/upcloud\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/upcloud-amd64.raw.xz\"\n\n}\n\n\n\nvariable \"username\" {\n\n  type        = string\n\n  description = \"UpCloud API username\"\n\n  default     = \"${env(\"UPCLOUD_USERNAME\")}\"\n\n}\n\n\n\nvariable \"password\" {\n\n  type        = string\n\n  description = \"UpCloud API password\"\n\n  default     = \"${env(\"UPCLOUD_PASSWORD\")}\"\n\n  sensitive   = true\n\n}\n\n\n\nsource \"upcloud\" \"talos\" {\n\n  username        = \"${var.username}\"\n\n  password        = \"${var.password}\"\n\n  zone            = \"us-nyc1\"\n\n  storage_name    = \"Debian GNU/Linux 11 (Bullseye)\"\n\n  template_name   = \"Talos (${var.talos_version})\"\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.upcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget xz-utils\",\n\n      \"wget -q -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/vda\",\n\n    ]\n\n  }\n\n\n\n  provisioner \"shell-local\" {\n\n      inline = [\n\n      \"upctl server stop --type hard custom\",\n\n      ]\n\n  }\n\n}\n\n\nNow create a new image by issuing the commands shown below.\n\nCopy\npacker init .\n\npacker build .\n\n\nAfter doing this, you can find the custom image in the console interface under storage.\n\nCreating a Cluster via the CLI\nCreate an Endpoint\n\nTo communicate with the Talos cluster you will need a single endpoint that is used to access the cluster. This can either be a loadbalancer that will sit in front of all your control plane nodes, a DNS name with one or more A or AAAA records pointing to the control plane nodes, or directly the IP of a control plane node.\n\nWhich option is best for you will depend on your needs. Endpoint selection has been further documented here.\n\nAfter you decide on which endpoint to use, note down the domain name or IP, as we will need it in the next step.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the endpoint created earlier, generate the base configuration files for the Talos machines:\n\nCopy\n$ talosctl gen config talos-upcloud-tutorial https://<load balancer IP or DNS>:<port> --install-disk /dev/vda\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Depending on the Kubernetes version you want to run, you might need to select a different Talos version, as not all versions are compatible. You can find the support matrix here.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch or yamlpatch which will be applied during the config generation.\n\nValidate the Configuration Files\nCopy\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nRun the following to create three total control plane nodes:\n\nCopy\nfor ID in $(seq 3); do\n\n    upctl server create \\\n\n      --zone us-nyc1 \\\n\n      --title talos-us-nyc1-master-$ID \\\n\n      --hostname talos-us-nyc1-master-$ID \\\n\n      --plan 2xCPU-4GB \\\n\n      --os \"Talos (v1.6.2)\" \\\n\n      --user-data \"$(cat controlplane.yaml)\" \\\n\n      --enable-metada\n\ndone\n\n\nNote: modify the zone and OS depending on your preferences. The OS should match the template name generated with packer in the previous step.\n\nNote the IP address of the first control plane node, as we will need it later.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nCopy\nupctl server create \\\n\n  --zone us-nyc1 \\\n\n  --title talos-us-nyc1-worker-1 \\\n\n  --hostname talos-us-nyc1-worker-1 \\\n\n  --plan 2xCPU-4GB \\\n\n  --os \"Talos (v1.6.2)\" \\\n\n  --user-data \"$(cat worker.yaml)\" \\\n\n  --enable-metada\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP, as noted earlier. We only add one node IP, as that is the entry into our cluster against which our commands will be run. All requests to other nodes are proxied through the endpoint, and therefore not all nodes need to be manually added to the config. You don’t want to run your commands against all nodes, as this can destroy your cluster if you are not careful (further documentation).\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig\n\n\nIt will take a few minutes before Kubernetes has been fully bootstrapped, and is accessible.\n\nYou can check if the nodes are registered in Talos by running\n\nCopy\ntalosctl --talosconfig talosconfig get members\n\n\nTo check if your nodes are ready, run\n\nCopy\nkubectl get nodes\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Scaleway | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/scaleway/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nScaleway\nScaleway\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nTalos is known to work on scaleway.com; however, it is currently undocumented.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Oracle | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/oracle/",
    "html": "Upload image\nCreating a Cluster via the CLI\nCreate a network\nCreate a Load Balancer\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nCreate the Servers\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nOracle\nOracle\nCreating a cluster via the CLI (oci) on OracleCloud.com.\nUpload image\n\nOracle Cloud at the moment does not have a Talos official image. So you can use Bring Your Own Image (BYOI) approach.\n\nOnce the image is uploaded, set the Boot volume type to Paravirtualized mode.\n\nOracleCloud has highly available NTP service, it can be enabled in Talos machine config with:\n\nCopy\nmachine:\n\n  time:\n\n    servers:\n\n      - 169.254.169.254\n\nCreating a Cluster via the CLI\n\nLogin to the console. And open the Cloud Shell.\n\nCreate a network\nCopy\nexport cidr_block=10.0.0.0/16\n\nexport subnet_block=10.0.0.0/24\n\nexport compartment_id=<substitute-value-of-compartment_id> # https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/latest/oci_cli_docs/cmdref/network/vcn/create.html#cmdoption-compartment-id\n\n\n\nexport vcn_id=$(oci network vcn create --cidr-block $cidr_block --display-name talos-example --compartment-id $compartment_id --query data.id --raw-output)\n\nexport rt_id=$(oci network subnet create --cidr-block $subnet_block --display-name kubernetes --compartment-id $compartment_id --vcn-id $vcn_id --query data.route-table-id --raw-output)\n\nexport ig_id=$(oci network internet-gateway create --compartment-id $compartment_id --is-enabled true --vcn-id $vcn_id --query data.id --raw-output)\n\n\n\noci network route-table update --rt-id $rt_id --route-rules \"[{\\\"cidrBlock\\\":\\\"0.0.0.0/0\\\",\\\"networkEntityId\\\":\\\"$ig_id\\\"}]\" --force\n\n\n\n# disable firewall\n\nexport sl_id=$(oci network vcn list --compartment-id $compartment_id --query 'data[0].\"default-security-list-id\"' --raw-output)\n\n\n\noci network security-list update --security-list-id $sl_id --egress-security-rules '[{\"destination\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --ingress-security-rules '[{\"source\": \"0.0.0.0/0\", \"protocol\": \"all\", \"isStateless\": false}]' --force\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nCopy\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer create --compartment-id $compartment_id --display-name controlplane-lb --subnet-id $subnet_id --is-preserve-source-destination false --is-private false --query data.id --raw-output)\n\n\n\ncat <<EOF > talos-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 50000,\n\n  \"protocol\": \"TCP\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://talos-health-checker.json --name talos --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name talos --name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --protocol TCP\n\n\n\ncat <<EOF > controlplane-health-checker.json\n\n{\n\n  \"intervalInMillis\": 10000,\n\n  \"port\": 6443,\n\n  \"protocol\": \"HTTPS\",\n\n  \"returnCode\": 401,\n\n  \"urlPath\": \"/readyz\"\n\n}\n\nEOF\n\n\n\noci nlb backend-set create --health-checker file://controlplane-health-checker.json --name controlplane --network-load-balancer-id $network_load_balancer_id --policy TWO_TUPLE --is-preserve-source false\n\noci nlb listener create --default-backend-set-name controlplane --name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --protocol TCP\n\n\n\n# Save the external IP\n\noci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].\"ip-addresses\"'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\nCopy\n$ talosctl gen config talos-k8s-oracle-tutorial https://<load balancer IP or DNS>:6443 --additional-sans <load balancer IP or DNS>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\nCopy\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nCopy\nexport shape='VM.Standard.A1.Flex'\n\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --shape $shape --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport network_load_balancer_id=$(oci nlb network-load-balancer list --compartment-id $compartment_id --display-name controlplane-lb --query 'data.items[0].id' --raw-output)\n\n\n\ncat <<EOF > shape.json\n\n{\n\n  \"memoryInGBs\": 4,\n\n  \"ocpus\": 1\n\n}\n\nEOF\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-1 --private-ip 10.0.0.11 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-2 --private-ip 10.0.0.12 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\n\n\nexport instance_id=$(oci compute instance launch --shape $shape --shape-config file://shape.json --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name controlplane-3 --private-ip 10.0.0.13 --assign-public-ip true --launch-options '{\"networkType\":\"PARAVIRTUALIZED\"}' --user-data-file controlplane.yaml --query 'data.id' --raw-output)\n\n\n\noci nlb backend create --backend-set-name talos --network-load-balancer-id $network_load_balancer_id --port 50000 --target-id $instance_id\n\noci nlb backend create --backend-set-name controlplane --network-load-balancer-id $network_load_balancer_id --port 6443 --target-id $instance_id\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nCopy\nexport subnet_id=$(oci network subnet list --compartment-id=$compartment_id --display-name kubernetes --query data[0].id --raw-output)\n\nexport image_id=$(oci compute image list --compartment-id $compartment_id --operating-system Talos --limit 1 --query data[0].id --raw-output)\n\nexport availability_domain=$(oci iam availability-domain list --compartment-id=$compartment_id --query data[0].name --raw-output)\n\nexport shape='VM.Standard.E2.1.Micro'\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-1 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-2 --assign-public-ip true --user-data-file worker.yaml\n\n\n\noci compute instance launch --shape $shape --availability-domain $availability_domain --compartment-id $compartment_id --image-id $image_id --subnet-id $subnet_id --display-name worker-3 --assign-public-ip true --user-data-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nCopy\nexport instance_id=$(oci compute instance list --compartment-id $compartment_id --display-name controlplane-1 --query 'data[0].id' --raw-output)\n\n\n\noci compute instance list-vnics --instance-id $instance_id --query 'data[0].\"private-ip\"' --raw-output\n\n\nSet the endpoints and nodes for your talosconfig with:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <load balancer IP or DNS>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Openstack | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/openstack/",
    "html": "Creating a Cluster via the CLI\nEnvironment Setup\nCreate the Image\nUpload the Image\nNetwork Infrastructure\nLoad Balancer and Network Ports\nSecurity Groups\nCluster Configuration\nCompute Creation\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nOpenstack\nOpenstack\nCreating a cluster via the CLI on Openstack.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in Openstack with 1 worker node. We will assume an existing some familiarity with Openstack. If you need more information on Openstack specifics, please see the official Openstack documentation.\n\nEnvironment Setup\n\nYou should have an existing openrc file. This file will provide environment variables necessary to talk to your Openstack cloud. See here for instructions on fetching this file.\n\nCreate the Image\n\nFirst, download the Openstack image from a Talos release. These images are called openstack-$ARCH.tar.gz. Untar this file with tar -xvf openstack-$ARCH.tar.gz. The resulting file will be called disk.raw.\n\nUpload the Image\n\nOnce you have the image, you can upload to Openstack with:\n\nCopy\nopenstack image create --public --disk-format raw --file disk.raw talos\n\nNetwork Infrastructure\nLoad Balancer and Network Ports\n\nOnce the image is prepared, you will need to work through setting up the network. Issue the following to create a load balancer, the necessary network ports for each control plane node, and associations between the two.\n\nCreating loadbalancer:\n\nCopy\n# Create load balancer, updating vip-subnet-id if necessary\n\nopenstack loadbalancer create --name talos-control-plane --vip-subnet-id public\n\n\n\n# Create listener\n\nopenstack loadbalancer listener create --name talos-control-plane-listener --protocol TCP --protocol-port 6443 talos-control-plane\n\n\n\n# Pool and health monitoring\n\nopenstack loadbalancer pool create --name talos-control-plane-pool --lb-algorithm ROUND_ROBIN --listener talos-control-plane-listener --protocol TCP\n\nopenstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP talos-control-plane-pool\n\n\nCreating ports:\n\nCopy\n# Create ports for control plane nodes, updating network name if necessary\n\nopenstack port create --network shared talos-control-plane-1\n\nopenstack port create --network shared talos-control-plane-2\n\nopenstack port create --network shared talos-control-plane-3\n\n\n\n# Create floating IPs for the ports, so that you will have talosctl connectivity to each control plane\n\nopenstack floating ip create --port talos-control-plane-1 public\n\nopenstack floating ip create --port talos-control-plane-2 public\n\nopenstack floating ip create --port talos-control-plane-3 public\n\n\nNote: Take notice of the private and public IPs associated with each of these ports, as they will be used in the next step. Additionally, take node of the port ID, as it will be used in server creation.\n\nAssociate port’s private IPs to loadbalancer:\n\nCopy\n# Create members for each port IP, updating subnet-id and address as necessary.\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-1 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-2 PORT> --protocol-port 6443 talos-control-plane-pool\n\nopenstack loadbalancer member create --subnet-id shared-subnet --address <PRIVATE IP OF talos-control-plane-3 PORT> --protocol-port 6443 talos-control-plane-pool\n\nSecurity Groups\n\nThis example uses the default security group in Openstack. Ports have been opened to ensure that connectivity from both inside and outside the group is possible. You will want to allow, at a minimum, ports 6443 (Kubernetes API server) and 50000 (Talos API) from external sources. It is also recommended to allow communication over all ports from within the subnet.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nCopy\nLB_PUBLIC_IP=$(openstack loadbalancer show talos-control-plane -f json | jq -r .vip_address)\n\n\n\ntalosctl gen config talos-k8s-openstack-tutorial https://${LB_PUBLIC_IP}:6443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our Openstack nodes.\n\nCreate control plane:\n\nCopy\n# Create control planes 2 and 3, substituting the same info.\n\nfor i in $( seq 1 3 ); do\n\n  openstack server create talos-control-plane-$i --flavor m1.small --nic port-id=talos-control-plane-$i --image talos --user-data /path/to/controlplane.yaml\n\ndone\n\n\nCreate worker:\n\nCopy\n# Update network name as necessary.\n\nopenstack server create talos-worker-1 --flavor m1.small --network shared --image talos --user-data /path/to/worker.yaml\n\n\nNote: This step can be repeated to add more workers.\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will use one of the floating IPs we allocated earlier. It does not matter which one.\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Nocloud | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/nocloud/",
    "html": "SMBIOS Serial Number\nSMBIOS: QEMU\nSMBIOS: Proxmox\nCDROM/USB\nExample: QEMU\nExample: Proxmox\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nNocloud\nNocloud\nCreating a cluster via the CLI using qemu.\n\nTalos supports nocloud data source implementation.\n\nThere are two ways to configure Talos server with nocloud platform:\n\nvia SMBIOS “serial number” option\nusing CDROM or USB-flash filesystem\n\nNote: This requires the nocloud image which can be found on the Github Releases page.\n\nSMBIOS Serial Number\n\nThis method requires the network connection to be up (e.g. via DHCP). Configuration is delivered from the HTTP server.\n\nCopy\nds=nocloud-net;s=http://10.10.0.1/configs/;h=HOSTNAME\n\n\nAfter the network initialization is complete, Talos fetches:\n\nthe machine config from http://10.10.0.1/configs/user-data\nthe network config (if available) from http://10.10.0.1/configs/network-config\nSMBIOS: QEMU\n\nAdd the following flag to qemu command line when starting a VM:\n\nCopy\nqemu-system-x86_64 \\\n\n  ...\\\n\n  -smbios type=1,serial=ds=nocloud-net;s=http://10.10.0.1/configs/\n\nSMBIOS: Proxmox\n\nSet the source machine config through the serial number on Proxmox GUI.\n\nThe Proxmox stores the VM config at /etc/pve/qemu-server/$ID.conf ($ID - VM ID number of virtual machine), you will see something like:\n\n...\nsmbios1: uuid=ceae4d10,serial=ZHM9bm9jbG91ZC1uZXQ7cz1odHRwOi8vMTAuMTAuMC4xL2NvbmZpZ3Mv,base64=1\n...\n\n\nWhere serial holds the base64-encoded string version of ds=nocloud-net;s=http://10.10.0.1/configs/.\n\nCDROM/USB\n\nTalos can also get machine config from local attached storage without any prior network connection being established.\n\nYou can provide configs to the server via files on a VFAT or ISO9660 filesystem. The filesystem volume label must be cidata or CIDATA.\n\nExample: QEMU\n\nCreate and prepare Talos machine config:\n\nCopy\nexport CONTROL_PLANE_IP=192.168.1.10\n\n\n\ntalosctl gen config talos-nocloud https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nPrepare cloud-init configs:\n\nCopy\nmkdir -p iso\n\nmv _out/controlplane.yaml iso/user-data\n\necho \"local-hostname: controlplane-1\" > iso/meta-data\n\ncat > iso/network-config << EOF\n\nversion: 1\n\nconfig:\n\n   - type: physical\n\n     name: eth0\n\n     mac_address: \"52:54:00:12:34:00\"\n\n     subnets:\n\n        - type: static\n\n          address: 192.168.1.10\n\n          netmask: 255.255.255.0\n\n          gateway: 192.168.1.254\n\nEOF\n\n\nCreate cloud-init iso image\n\nCopy\ncd iso && genisoimage -output cidata.iso -V cidata -r -J user-data meta-data network-config\n\n\nStart the VM\n\nCopy\nqemu-system-x86_64 \\\n\n    ...\n\n    -cdrom iso/cidata.iso \\\n\n    ...\n\nExample: Proxmox\n\nProxmox can create cloud-init disk for you. Edit the cloud-init config information in Proxmox as follows, substitute your own information as necessary:\n\nand then update cicustom param at /etc/pve/qemu-server/$ID.conf.\n\ncicustom: user=local:snippets/controlplane-1.yml\nipconfig0: ip=192.168.1.10/24,gw=192.168.10.254\nnameserver: 1.1.1.1\nsearchdomain: local\n\n\nNote: snippets/controlplane-1.yml is Talos machine config. It is usually located at /var/lib/vz/snippets/controlplane-1.yml. This file must be placed to this path manually, as Proxmox does not support snippet uploading via API/GUI.\n\nClick on Regenerate Image button after the above changes are made.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Hetzner | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/hetzner/",
    "html": "Upload image\nRescue mode\nPacker\nCreating a Cluster via the CLI\nCreate a Load Balancer\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nCreate the Servers\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nHetzner\nHetzner\nCreating a cluster via the CLI (hcloud) on Hetzner.\nUpload image\n\nHetzner Cloud does not support uploading custom images. You can email their support to get a Talos ISO uploaded by following issues:3599 or you can prepare image snapshot by yourself.\n\nThere are two options to upload your own.\n\nRun an instance in rescue mode and replace the system OS with the Talos image\nUse Hashicorp packer to prepare an image\nRescue mode\n\nCreate a new Server in the Hetzner console. Enable the Hetzner Rescue System for this server and reboot. Upon a reboot, the server will boot a special minimal Linux distribution designed for repair and reinstall. Once running, login to the server using ssh to prepare the system disk by doing the following:\n\nCopy\n# Check that you in Rescue mode\n\ndf\n\n\n\n### Result is like:\n\n# udev                   987432         0    987432   0% /dev\n\n# 213.133.99.101:/nfs 308577696 247015616  45817536  85% /root/.oldroot/nfs\n\n# overlay                995672      8340    987332   1% /\n\n# tmpfs                  995672         0    995672   0% /dev/shm\n\n# tmpfs                  398272       572    397700   1% /run\n\n# tmpfs                    5120         0      5120   0% /run/lock\n\n# tmpfs                  199132         0    199132   0% /run/user/0\n\n\n\n# Download the Talos image\n\ncd /tmp\n\nwget -O /tmp/talos.raw.xz https://github.com/siderolabs/talos/releases/download/v1.6.2/hcloud-amd64.raw.xz\n\n# Replace system\n\nxz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\n\n# shutdown the instance\n\nshutdown -h now\n\n\nTo make sure disk content is consistent, it is recommended to shut the server down before taking an image (snapshot). Once shutdown, simply create an image (snapshot) from the console. You can now use this snapshot to run Talos on the cloud.\n\nPacker\n\nInstall packer to the local machine.\n\nCreate a config file for packer to use:\n\nCopy\n# hcloud.pkr.hcl\n\n\n\npacker {\n\n  required_plugins {\n\n    hcloud = {\n\n      source  = \"github.com/hetznercloud/hcloud\"\n\n      version = \"~> 1\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"talos_version\" {\n\n  type    = string\n\n  default = \"v1.6.2\"\n\n}\n\n\n\nvariable \"arch\" {\n\n  type    = string\n\n  default = \"amd64\"\n\n}\n\n\n\nvariable \"server_type\" {\n\n  type    = string\n\n  default = \"cx11\"\n\n}\n\n\n\nvariable \"server_location\" {\n\n  type    = string\n\n  default = \"hel1\"\n\n}\n\n\n\nlocals {\n\n  image = \"https://github.com/siderolabs/talos/releases/download/${var.talos_version}/hcloud-${var.arch}.raw.xz\"\n\n}\n\n\n\nsource \"hcloud\" \"talos\" {\n\n  rescue       = \"linux64\"\n\n  image        = \"debian-11\"\n\n  location     = \"${var.server_location}\"\n\n  server_type  = \"${var.server_type}\"\n\n  ssh_username = \"root\"\n\n\n\n  snapshot_name   = \"talos system disk - ${var.arch} - ${var.talos_version}\"\n\n  snapshot_labels = {\n\n    type    = \"infra\",\n\n    os      = \"talos\",\n\n    version = \"${var.talos_version}\",\n\n    arch    = \"${var.arch}\",\n\n  }\n\n}\n\n\n\nbuild {\n\n  sources = [\"source.hcloud.talos\"]\n\n\n\n  provisioner \"shell\" {\n\n    inline = [\n\n      \"apt-get install -y wget\",\n\n      \"wget -O /tmp/talos.raw.xz ${local.image}\",\n\n      \"xz -d -c /tmp/talos.raw.xz | dd of=/dev/sda && sync\",\n\n    ]\n\n  }\n\n}\n\n\nAdditionally you could create a file containing\n\nCopy\narch            = \"arm64\"\n\nserver_type     = \"cax11\"\n\nserver_location = \"fsn1\"\n\n\nand build the snapshot for arm64.\n\nCreate a new image by issuing the commands shown below. Note that to create a new API token for your Project, switch into the Hetzner Cloud Console choose a Project, go to Access → Security, and create a new token.\n\nCopy\n# First you need set API Token\n\nexport HCLOUD_TOKEN=${TOKEN}\n\n\n\n# Upload image\n\npacker init .\n\npacker build .\n\n# Save the image ID\n\nexport IMAGE_ID=<image-id-in-packer-output>\n\n\nAfter doing this, you can find the snapshot in the console interface.\n\nCreating a Cluster via the CLI\n\nThis section assumes you have the hcloud console utility on your local machine.\n\nCopy\n# Set hcloud context and api key\n\nhcloud context create talos-tutorial\n\nCreate a Load Balancer\n\nCreate a load balancer by issuing the commands shown below. Save the IP/DNS name, as this info will be used in the next step.\n\nCopy\nhcloud load-balancer create --name controlplane --network-zone eu-central --type lb11 --label 'type=controlplane'\n\n\n\n### Result is like:\n\n# LoadBalancer 484487 created\n\n# IPv4: 49.12.X.X\n\n# IPv6: 2a01:4f8:X:X::1\n\n\n\nhcloud load-balancer add-service controlplane \\\n\n    --listen-port 6443 --destination-port 6443 --protocol tcp\n\nhcloud load-balancer add-target controlplane \\\n\n    --label-selector 'type=controlplane'\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the IP/DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines by issuing:\n\nCopy\n$ talosctl gen config talos-k8s-hcloud-tutorial https://<load balancer IP or DNS>:6443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatches which will be applied during the config generation.\n\nValidate the Configuration Files\n\nValidate any edited machine configs with:\n\nCopy\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the Servers\n\nWe can now create our servers. Note that you can find IMAGE_ID in the snapshot section of the console: https://console.hetzner.cloud/projects/$PROJECT_ID/servers/snapshots.\n\nCreate the Control Plane Nodes\n\nCreate the control plane nodes with:\n\nCopy\nexport IMAGE_ID=<your-image-id>\n\n\n\nhcloud server create --name talos-control-plane-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-2 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location fsn1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\n\n\nhcloud server create --name talos-control-plane-3 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location nbg1 \\\n\n    --label 'type=controlplane' \\\n\n    --user-data-from-file controlplane.yaml\n\nCreate the Worker Nodes\n\nCreate the worker nodes with the following command, repeating (and incrementing the name counter) as many times as desired.\n\nCopy\nhcloud server create --name talos-worker-1 \\\n\n    --image ${IMAGE_ID} \\\n\n    --type cx21 --location hel1 \\\n\n    --label 'type=worker' \\\n\n    --user-data-from-file worker.yaml\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP. This can be found by issuing:\n\nCopy\nhcloud server list | grep talos-control-plane\n\n\nSet the endpoints and nodes for your talosconfig with:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control-plane-1-IP>\n\ntalosctl --talosconfig talosconfig config node <control-plane-1-IP>\n\n\nBootstrap etcd on the first control plane node with:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "GCP | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/gcp/",
    "html": "Creating a Cluster via the CLI\nManual Setup\nEnvironment Setup\nCreate the Image\nUpload the Image\nRegister the image\nNetwork Infrastructure\nLoad Balancers and Firewalls\nCluster Configuration\nCompute Creation\nBootstrap Etcd\nRetrieve the kubeconfig\nCleanup\nUsing GCP Deployment manager\nGetting the deployment manifests\nUpdating the config\nEnabling external cloud provider\nCreating the deployment\nRetrieving the outputs\nDownloading talos and kube config\nDeploying the cloud controller manager\nCheck cluster status\nCleanup deployment\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nGCP\nGCP\nCreating a cluster via the CLI on Google Cloud Platform.\nCreating a Cluster via the CLI\n\nIn this guide, we will create an HA Kubernetes cluster in GCP with 1 worker node. We will assume an existing Cloud Storage bucket, and some familiarity with Google Cloud. If you need more information on Google Cloud specifics, please see the official Google documentation.\n\njq and talosctl also needs to be installed\n\nManual Setup\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\nCopy\n# Storage account to use\n\nexport STORAGE_BUCKET=\"StorageBucketName\"\n\n# Region\n\nexport REGION=\"us-central1\"\n\nCreate the Image\n\nFirst, download the Google Cloud image from a Talos release. These images are called gcp-$ARCH.tar.gz.\n\nUpload the Image\n\nOnce you have downloaded the image, you can upload it to your storage bucket with:\n\nCopy\ngsutil cp /path/to/gcp-amd64.tar.gz gs://$STORAGE_BUCKET\n\nRegister the image\n\nNow that the image is present in our bucket, we’ll register it.\n\nCopy\ngcloud compute images create talos \\\n\n --source-uri=gs://$STORAGE_BUCKET/gcp-amd64.tar.gz \\\n\n --guest-os-features=VIRTIO_SCSI_MULTIQUEUE\n\nNetwork Infrastructure\nLoad Balancers and Firewalls\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a firewall, load balancer, and their required components.\n\n130.211.0.0/22 and 35.191.0.0/16 are the GCP Load Balancer IP ranges\n\nCopy\n# Create Instance Group\n\ngcloud compute instance-groups unmanaged create talos-ig \\\n\n  --zone $REGION-b\n\n\n\n# Create port for IG\n\ngcloud compute instance-groups set-named-ports talos-ig \\\n\n    --named-ports tcp6443:6443 \\\n\n    --zone $REGION-b\n\n\n\n# Create health check\n\ngcloud compute health-checks create tcp talos-health-check --port 6443\n\n\n\n# Create backend\n\ngcloud compute backend-services create talos-be \\\n\n    --global \\\n\n    --protocol TCP \\\n\n    --health-checks talos-health-check \\\n\n    --timeout 5m \\\n\n    --port-name tcp6443\n\n\n\n# Add instance group to backend\n\ngcloud compute backend-services add-backend talos-be \\\n\n    --global \\\n\n    --instance-group talos-ig \\\n\n    --instance-group-zone $REGION-b\n\n\n\n# Create tcp proxy\n\ngcloud compute target-tcp-proxies create talos-tcp-proxy \\\n\n    --backend-service talos-be \\\n\n    --proxy-header NONE\n\n\n\n# Create LB IP\n\ngcloud compute addresses create talos-lb-ip --global\n\n\n\n# Forward 443 from LB IP to tcp proxy\n\ngcloud compute forwarding-rules create talos-fwd-rule \\\n\n    --global \\\n\n    --ports 443 \\\n\n    --address talos-lb-ip \\\n\n    --target-tcp-proxy talos-tcp-proxy\n\n\n\n# Create firewall rule for health checks\n\ngcloud compute firewall-rules create talos-controlplane-firewall \\\n\n     --source-ranges 130.211.0.0/22,35.191.0.0/16 \\\n\n     --target-tags talos-controlplane \\\n\n     --allow tcp:6443\n\n\n\n# Create firewall rule to allow talosctl access\n\ngcloud compute firewall-rules create talos-controlplane-talosctl \\\n\n  --source-ranges 0.0.0.0/0 \\\n\n  --target-tags talos-controlplane \\\n\n  --allow tcp:50000\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nCopy\nLB_PUBLIC_IP=$(gcloud compute forwarding-rules describe talos-fwd-rule \\\n\n               --global \\\n\n               --format json \\\n\n               | jq -r .IPAddress)\n\n\n\ntalosctl gen config talos-k8s-gcp-tutorial https://${LB_PUBLIC_IP}:443\n\n\nAdditionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nCompute Creation\n\nWe are now ready to create our GCP nodes.\n\nCopy\n# Create the control plane nodes.\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instances create talos-controlplane-$i \\\n\n    --image talos \\\n\n    --zone $REGION-b \\\n\n    --tags talos-controlplane \\\n\n    --boot-disk-size 20GB \\\n\n    --metadata-from-file=user-data=./controlplane.yaml\n\n    --tags talos-controlplane-$i\n\ndone\n\n\n\n# Add control plane nodes to instance group\n\nfor i in $( seq 1 3 ); do\n\n  gcloud compute instance-groups unmanaged add-instances talos-ig \\\n\n      --zone $REGION-b \\\n\n      --instances talos-controlplane-$i\n\ndone\n\n\n\n# Create worker\n\ngcloud compute instances create talos-worker-0 \\\n\n  --image talos \\\n\n  --zone $REGION-b \\\n\n  --boot-disk-size 20GB \\\n\n  --metadata-from-file=user-data=./worker.yaml\n\n  --tags talos-worker-$i\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCopy\nCONTROL_PLANE_0_IP=$(gcloud compute instances describe talos-controlplane-0 \\\n\n                     --zone $REGION-b \\\n\n                     --format json \\\n\n                     | jq -r '.networkInterfaces[0].accessConfigs[0].natIP')\n\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\nCleanup\nCopy\n# cleanup VM's\n\ngcloud compute instances delete \\\n\n  talos-worker-0 \\\n\n  talos-controlplane-0 \\\n\n  talos-controlplane-1 \\\n\n  talos-controlplane-2\n\n\n\n# cleanup firewall rules\n\ngcloud compute firewall-rules delete \\\n\n  talos-controlplane-talosctl \\\n\n  talos-controlplane-firewall\n\n\n\n# cleanup forwarding rules\n\ngcloud compute forwarding-rules delete \\\n\n  talos-fwd-rule\n\n\n\n# cleanup addresses\n\ngcloud compute addresses delete \\\n\n  talos-lb-ip\n\n\n\n# cleanup proxies\n\ngcloud compute target-tcp-proxies delete \\\n\n  talos-tcp-proxy\n\n\n\n# cleanup backend services\n\ngcloud compute backend-services delete \\\n\n  talos-be\n\n\n\n# cleanup health checks\n\ngcloud compute health-checks delete \\\n\n  talos-health-check\n\n\n\n# cleanup unmanaged instance groups\n\ngcloud compute instance-groups unmanaged delete \\\n\n  talos-ig\n\n\n\n# cleanup Talos image\n\ngcloud compute images delete \\\n\n  talos\n\nUsing GCP Deployment manager\n\nUsing GCP deployment manager automatically creates a Google Storage bucket and uploads the Talos image to it. Once the deployment is complete the generated talosconfig and kubeconfig files are uploaded to the bucket.\n\nBy default this setup creates a three node control plane and a single worker in us-west1-b\n\nFirst we need to create a folder to store our deployment manifests and perform all subsequent operations from that folder.\n\nCopy\nmkdir -p talos-gcp-deployment\n\ncd talos-gcp-deployment\n\nGetting the deployment manifests\n\nWe need to download two deployment manifests for the deployment from the Talos github repository.\n\nCopy\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/config.yaml\"\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/talos-ha.jinja\"\n\n# if using ccm\n\ncurl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/cloud-platforms/gcp/gcp-ccm.yaml\"\n\nUpdating the config\n\nNow we need to update the local config.yaml file with any required changes such as changing the default zone, Talos version, machine sizes, nodes count etc.\n\nAn example config.yaml file is shown below:\n\nCopy\nimports:\n\n  - path: talos-ha.jinja\n\n\n\nresources:\n\n  - name: talos-ha\n\n    type: talos-ha.jinja\n\n    properties:\n\n      zone: us-west1-b\n\n      talosVersion: v1.6.2\n\n      externalCloudProvider: false\n\n      controlPlaneNodeCount: 5\n\n      controlPlaneNodeType: n1-standard-1\n\n      workerNodeCount: 3\n\n      workerNodeType: n1-standard-1\n\noutputs:\n\n  - name: bucketName\n\n    value: $(ref.talos-ha.bucketName)\n\nEnabling external cloud provider\n\nNote: The externalCloudProvider property is set to false by default. The manifest used for deploying the ccm (cloud controller manager) is currently using the GCP ccm provided by openshift since there are no public images for the ccm yet.\n\nSince the routes controller is disabled while deploying the CCM, the CNI pods needs to be restarted after the CCM deployment is complete to remove the node.kubernetes.io/network-unavailable taint. See Nodes network-unavailable taint not removed after installing ccm for more information\n\nUse a custom built image for the ccm deployment if required.\n\nCreating the deployment\n\nNow we are ready to create the deployment. Confirm with y for any prompts. Run the following command to create the deployment:\n\nCopy\n# use a unique name for the deployment, resources are prefixed with the deployment name\n\nexport DEPLOYMENT_NAME=\"<deployment name>\"\n\ngcloud deployment-manager deployments create \"${DEPLOYMENT_NAME}\" --config config.yaml\n\nRetrieving the outputs\n\nFirst we need to get the deployment outputs.\n\nCopy\n# first get the outputs\n\nOUTPUTS=$(gcloud deployment-manager deployments describe \"${DEPLOYMENT_NAME}\" --format json | jq '.outputs[]')\n\n\n\nBUCKET_NAME=$(jq -r '. | select(.name == \"bucketName\").finalValue' <<< \"${OUTPUTS}\")\n\n# used when cloud controller is enabled\n\nSERVICE_ACCOUNT=$(jq -r '. | select(.name == \"serviceAccount\").finalValue' <<< \"${OUTPUTS}\")\n\nPROJECT=$(jq -r '. | select(.name == \"project\").finalValue' <<< \"${OUTPUTS}\")\n\n\nNote: If cloud controller manager is enabled, the below command needs to be run to allow the controller custom role to access cloud resources\n\nCopy\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects add-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\nDownloading talos and kube config\n\nIn addition to the talosconfig and kubeconfig files, the storage bucket contains the controlplane.yaml and worker.yaml files used to join additional nodes to the cluster.\n\nCopy\ngsutil cp \"gs://${BUCKET_NAME}/generated/talosconfig\" .\n\ngsutil cp \"gs://${BUCKET_NAME}/generated/kubeconfig\" .\n\nDeploying the cloud controller manager\nCopy\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  apply \\\n\n  --filename gcp-ccm.yaml\n\n#  wait for the ccm to be up\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset cloud-controller-manager\n\n\nIf the cloud controller manager is enabled, we need to restart the CNI pods to remove the node.kubernetes.io/network-unavailable taint.\n\nCopy\n# restart the CNI pods, in this case flannel\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout restart \\\n\n  daemonset kube-flannel\n\n# wait for the pods to be restarted\n\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  --namespace kube-system \\\n\n  rollout status \\\n\n  daemonset kube-flannel\n\nCheck cluster status\nCopy\nkubectl \\\n\n  --kubeconfig kubeconfig \\\n\n  get nodes\n\nCleanup deployment\n\nWarning: This will delete the deployment and all resources associated with it.\n\nRun below if cloud controller manager is enabled\n\nCopy\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member \"serviceAccount:${SERVICE_ACCOUNT}\" \\\n\n    --role roles/iam.serviceAccountUser\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.admin\n\n\n\ngcloud projects remove-iam-policy-binding \\\n\n    \"${PROJECT}\" \\\n\n    --member serviceAccount:\"${SERVICE_ACCOUNT}\" \\\n\n    --role roles/compute.loadBalancerAdmin\n\n\nNow we can finally remove the deployment\n\nCopy\n# delete the objects in the bucket first\n\ngsutil -m rm -r \"gs://${BUCKET_NAME}\"\n\ngcloud deployment-manager deployments delete \"${DEPLOYMENT_NAME}\" --quiet\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Exoscale | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/exoscale/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nExoscale\nExoscale\nCreating a cluster via the CLI using exoscale.com\n\nTalos is known to work on exoscale.com; however, it is currently undocumented.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "DigitalOcean | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/digitalocean/",
    "html": "Creating a Talos Linux Cluster on Digital Ocean via the CLI\nCreate the Image\nCreate a Load Balancer\nCreate the Machine Configuration Files\nCreate the Droplets\nCreate a dummy SSH key\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nDigitalOcean\nDigitalOcean\nCreating a cluster via the CLI on DigitalOcean.\nCreating a Talos Linux Cluster on Digital Ocean via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node, in the NYC region. We assume an existing Space, and some familiarity with DigitalOcean. If you need more information on DigitalOcean specifics, please see the official DigitalOcean documentation.\n\nCreate the Image\n\nDownload the DigitalOcean image digital-ocean-amd64.raw.gz from the latest Talos release.\n\nNote: the minimum version of Talos required to support Digital Ocean is v1.3.3.\n\nUsing an upload method of your choice (doctl does not have Spaces support), upload the image to a space. (It’s easy to drag the image file to the space using DigitalOcean’s web console.)\n\nNote: Make sure you upload the file as public.\n\nNow, create an image using the URL of the uploaded image:\n\nCopy\nexport REGION=nyc3\n\n\n\ndoctl compute image create \\\n\n    --region $REGION \\\n\n    --image-description talos-digital-ocean-tutorial \\\n\n    --image-url https://$SPACENAME.$REGION.digitaloceanspaces.com/digital-ocean-amd64.raw.gz \\\n\n    Talos\n\n\nSave the image ID. We will need it when creating droplets.\n\nCreate a Load Balancer\nCopy\ndoctl compute load-balancer create \\\n\n    --region $REGION \\\n\n    --name talos-digital-ocean-tutorial-lb \\\n\n    --tag-name talos-digital-ocean-tutorial-control-plane \\\n\n    --health-check protocol:tcp,port:6443,check_interval_seconds:10,response_timeout_seconds:5,healthy_threshold:5,unhealthy_threshold:3 \\\n\n    --forwarding-rules entry_protocol:tcp,entry_port:443,target_protocol:tcp,target_port:6443\n\n\nNote the returned ID of the load balancer.\n\nWe will need the IP of the load balancer. Using the ID of the load balancer, run:\n\nCopy\ndoctl compute load-balancer get --format IP <load balancer ID>\n\n\nNote that it may take a few minutes before the load balancer is provisioned, so repeat this command until it returns with the IP address.\n\nCreate the Machine Configuration Files\n\nUsing the IP address (or DNS name, if you have created one) of the loadbalancer, generate the base configuration files for the Talos machines. Also note that the load balancer forwards port 443 to port 6443 on the associated nodes, so we should use 443 as the port in the config definition:\n\nCopy\n$ talosctl gen config talos-k8s-digital-ocean-tutorial https://<load balancer IP or DNS>:443\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCreate the Droplets\nCreate a dummy SSH key\n\nAlthough SSH is not used by Talos, DigitalOcean requires that an SSH key be associated with a droplet during creation. We will create a dummy key that can be used to satisfy this requirement.\n\nCopy\ndoctl compute ssh-key create --public-key \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDbl0I1s/yOETIKjFr7mDLp8LmJn6OIZ68ILjVCkoN6lzKmvZEqEm1YYeWoI0xgb80hQ1fKkl0usW6MkSqwrijoUENhGFd6L16WFL53va4aeJjj2pxrjOr3uBFm/4ATvIfFTNVs+VUzFZ0eGzTgu1yXydX8lZMWnT4JpsMraHD3/qPP+pgyNuI51LjOCG0gVCzjl8NoGaQuKnl8KqbSCARIpETg1mMw+tuYgaKcbqYCMbxggaEKA0ixJ2MpFC/kwm3PcksTGqVBzp3+iE5AlRe1tnbr6GhgT839KLhOB03j7lFl1K9j1bMTOEj5Io8z7xo/XeF2ZQKHFWygAJiAhmKJ dummy@dummy.local\" dummy\n\n\nNote the ssh key ID that is returned - we will use it in creating the droplets.\n\nCreate the Control Plane Nodes\n\nRun the following commands to create three control plane nodes:\n\nCopy\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-1\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-2\n\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --tag-names talos-digital-ocean-tutorial-control-plane \\\n\n    --user-data-file controlplane.yaml \\\n\n    --ssh-keys <ssh key ID> \\\n\n    talos-control-plane-3\n\n\nNote the droplet ID returned for the first control plane node.\n\nCreate the Worker Nodes\n\nRun the following to create a worker node:\n\nCopy\ndoctl compute droplet create \\\n\n    --region $REGION \\\n\n    --image <image ID> \\\n\n    --size s-2vcpu-4gb \\\n\n    --enable-private-networking \\\n\n    --user-data-file worker.yaml \\\n\n    --ssh-keys <ssh key ID>  \\\n\n    talos-worker-1\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nCopy\ndoctl compute droplet get --format PublicIPv4 <droplet ID>\n\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nWe can also watch the cluster bootstrap via:\n\nCopy\ntalosctl --talosconfig talosconfig health\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Azure | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/azure/",
    "html": "Creating a Cluster via the CLI\nEnvironment Setup\nChoose an Image\nCreate the Image\nUpload the VHD\nRegister the Image\nNetwork Infrastructure\nVirtual Networks and Security Groups\nLoad Balancer\nNetwork Interfaces\nCluster Configuration\nCompute Creation\nManual Image Upload\nAzure Community Gallery Image\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nAzure\nAzure\nCreating a cluster via the CLI on Azure.\nCreating a Cluster via the CLI\n\nIn this guide we will create an HA Kubernetes cluster with 1 worker node. We assume existing Blob Storage, and some familiarity with Azure. If you need more information on Azure specifics, please see the official Azure documentation.\n\nEnvironment Setup\n\nWe’ll make use of the following environment variables throughout the setup. Edit the variables below with your correct information.\n\nCopy\n# Storage account to use\n\nexport STORAGE_ACCOUNT=\"StorageAccountName\"\n\n\n\n# Storage container to upload to\n\nexport STORAGE_CONTAINER=\"StorageContainerName\"\n\n\n\n# Resource group name\n\nexport GROUP=\"ResourceGroupName\"\n\n\n\n# Location\n\nexport LOCATION=\"centralus\"\n\n\n\n# Get storage account connection string based on info above\n\nexport CONNECTION=$(az storage account show-connection-string \\\n\n                    -n $STORAGE_ACCOUNT \\\n\n                    -g $GROUP \\\n\n                    -o tsv)\n\nChoose an Image\n\nThere are two methods of deployment in this tutorial.\n\nIf you would like to use the official Talos image uploaded to Azure Community Galleries by SideroLabs, you may skip ahead to setting up your network infrastructure.\n\nNetwork Infrastructure\n\nOtherwise, if you would like to upload your own image to Azure and use it to deploy Talos, continue to Creating an Image.\n\nCreate the Image\n\nFirst, download the Azure image from a Talos release. Once downloaded, untar with tar -xvf /path/to/azure-amd64.tar.gz\n\nUpload the VHD\n\nOnce you have pulled down the image, you can upload it to blob storage with:\n\nCopy\naz storage blob upload \\\n\n  --connection-string $CONNECTION \\\n\n  --container-name $STORAGE_CONTAINER \\\n\n  -f /path/to/extracted/talos-azure.vhd \\\n\n  -n talos-azure.vhd\n\nRegister the Image\n\nNow that the image is present in our blob storage, we’ll register it.\n\nCopy\naz image create \\\n\n  --name talos \\\n\n  --source https://$STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER/talos-azure.vhd \\\n\n  --os-type linux \\\n\n  -g $GROUP\n\nNetwork Infrastructure\nVirtual Networks and Security Groups\n\nOnce the image is prepared, we’ll want to work through setting up the network. Issue the following to create a network security group and add rules to it.\n\nCopy\n# Create vnet\n\naz network vnet create \\\n\n  --resource-group $GROUP \\\n\n  --location $LOCATION \\\n\n  --name talos-vnet \\\n\n  --subnet-name talos-subnet\n\n\n\n# Create network security group\n\naz network nsg create -g $GROUP -n talos-sg\n\n\n\n# Client -> apid\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n apid \\\n\n  --priority 1001 \\\n\n  --destination-port-ranges 50000 \\\n\n  --direction inbound\n\n\n\n# Trustd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n trustd \\\n\n  --priority 1002 \\\n\n  --destination-port-ranges 50001 \\\n\n  --direction inbound\n\n\n\n# etcd\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n etcd \\\n\n  --priority 1003 \\\n\n  --destination-port-ranges 2379-2380 \\\n\n  --direction inbound\n\n\n\n# Kubernetes API Server\n\naz network nsg rule create \\\n\n  -g $GROUP \\\n\n  --nsg-name talos-sg \\\n\n  -n kube \\\n\n  --priority 1004 \\\n\n  --destination-port-ranges 6443 \\\n\n  --direction inbound\n\nLoad Balancer\n\nWe will create a public ip, load balancer, and a health check that we will use for our control plane.\n\nCopy\n# Create public ip\n\naz network public-ip create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-public-ip \\\n\n  --allocation-method static\n\n\n\n# Create lb\n\naz network lb create \\\n\n  --resource-group $GROUP \\\n\n  --name talos-lb \\\n\n  --public-ip-address talos-public-ip \\\n\n  --frontend-ip-name talos-fe \\\n\n  --backend-pool-name talos-be-pool\n\n\n\n# Create health check\n\naz network lb probe create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-lb-health \\\n\n  --protocol tcp \\\n\n  --port 6443\n\n\n\n# Create lb rule for 6443\n\naz network lb rule create \\\n\n  --resource-group $GROUP \\\n\n  --lb-name talos-lb \\\n\n  --name talos-6443 \\\n\n  --protocol tcp \\\n\n  --frontend-ip-name talos-fe \\\n\n  --frontend-port 6443 \\\n\n  --backend-pool-name talos-be-pool \\\n\n  --backend-port 6443 \\\n\n  --probe-name talos-lb-health\n\nNetwork Interfaces\n\nIn Azure, we have to pre-create the NICs for our control plane so that they can be associated with our load balancer.\n\nCopy\nfor i in $( seq 0 1 2 ); do\n\n  # Create public IP for each nic\n\n  az network public-ip create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-public-ip-$i \\\n\n    --allocation-method static\n\n\n\n\n\n  # Create nic\n\n  az network nic create \\\n\n    --resource-group $GROUP \\\n\n    --name talos-controlplane-nic-$i \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --network-security-group talos-sg \\\n\n    --public-ip-address talos-controlplane-public-ip-$i\\\n\n    --lb-name talos-lb \\\n\n    --lb-address-pools talos-be-pool\n\ndone\n\n\n\n# NOTES:\n\n# Talos can detect PublicIPs automatically if PublicIP SKU is Basic.\n\n# Use `--sku Basic` to set SKU to Basic.\n\nCluster Configuration\n\nWith our networking bits setup, we’ll fetch the IP for our load balancer and create our configuration files.\n\nCopy\nLB_PUBLIC_IP=$(az network public-ip show \\\n\n              --resource-group $GROUP \\\n\n              --name talos-public-ip \\\n\n              --query \"ipAddress\" \\\n\n              --output tsv)\n\n\n\ntalosctl gen config talos-k8s-azure-tutorial https://${LB_PUBLIC_IP}:6443\n\nCompute Creation\n\nWe are now ready to create our azure nodes. Azure allows you to pass Talos machine configuration to the virtual machine at bootstrap time via user-data or custom-data methods.\n\nTalos supports only custom-data method, machine configuration is available to the VM only on the first boot.\n\nUse the steps below depending on whether you have manually uploaded a Talos image or if you are using the Community Gallery image.\n\nManual Image Upload\nAzure Community Gallery Image\nManual Image Upload\nCopy\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image talos \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image talos \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nAzure Community Gallery Image\n\nTalos is updated in Azure’s Community Galleries (Preview) on every release.\n\nTo use the Talos image for the current release create the following environment variables.\n\nEdit the variables below if you would like to use a different architecture or version.\n\nCopy\n# The architecture you would like to use. Options are \"talos-x64\" or \"talos-arm64\"\n\nARCHITECTURE=\"talos-x64\"\n\n\n\n# This will use the latest version of Talos. The version must be \"latest\" or in the format Major(int).Minor(int).Patch(int), e.g. 1.5.0\n\nVERSION=\"latest\"\n\n\nCreate the Virtual Machines.\n\nCopy\n# Create availability set\n\naz vm availability-set create \\\n\n  --name talos-controlplane-av-set \\\n\n  -g $GROUP\n\n\n\n# Create the controlplane nodes\n\nfor i in $( seq 0 1 2 ); do\n\n  az vm create \\\n\n    --name talos-controlplane-$i \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --custom-data ./controlplane.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --os-disk-size-gb 20 \\\n\n    --nics talos-controlplane-nic-$i \\\n\n    --availability-set talos-controlplane-av-set \\\n\n    --no-wait\n\ndone\n\n\n\n# Create worker node\n\n  az vm create \\\n\n    --name talos-worker-0 \\\n\n    --image /CommunityGalleries/siderolabs-c4d707c0-343e-42de-b597-276e4f7a5b0b/Images/${ARCHITECTURE}/Versions/${VERSION} \\\n\n    --vnet-name talos-vnet \\\n\n    --subnet talos-subnet \\\n\n    --custom-data ./worker.yaml \\\n\n    -g $GROUP \\\n\n    --admin-username talos \\\n\n    --generate-ssh-keys \\\n\n    --verbose \\\n\n    --boot-diagnostics-storage $STORAGE_ACCOUNT \\\n\n    --nsg talos-sg \\\n\n    --os-disk-size-gb 20 \\\n\n    --no-wait\n\n\n\n# NOTES:\n\n# `--admin-username` and `--generate-ssh-keys` are required by the az cli,\n\n# but are not actually used by talos\n\n# `--os-disk-size-gb` is the backing disk for Kubernetes and any workload containers\n\n# `--boot-diagnostics-storage` is to enable console output which may be necessary\n\n# for troubleshooting\n\nBootstrap Etcd\n\nYou should now be able to interact with your cluster with talosctl. We will need to discover the public IP for our first control plane node first.\n\nCopy\nCONTROL_PLANE_0_IP=$(az network public-ip show \\\n\n                    --resource-group $GROUP \\\n\n                    --name talos-controlplane-public-ip-0 \\\n\n                    --query \"ipAddress\" \\\n\n                    --output tsv)\n\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint $CONTROL_PLANE_0_IP\n\ntalosctl --talosconfig talosconfig config node $CONTROL_PLANE_0_IP\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "AWS | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/aws/",
    "html": "Creating a Cluster via the AWS CLI\nSet the needed info\nCreate the Subnet\nOfficial AMI Images\nCreate your own AMIs\nCreate the S3 Bucket\nCreate the vmimport Role\nCreate the Image Snapshot\nRegister the Image\nCreate a Security Group\nCreate a Load Balancer\nCreate the Machine Configuration Files\nValidate the Configuration Files\nCreate the EC2 Instances\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nConfigure the Load Balancer\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nAWS\nAWS\nCreating a cluster via the AWS CLI.\nCreating a Cluster via the AWS CLI\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing VPC, and some familiarity with AWS. If you need more information on AWS specifics, please see the official AWS documentation.\n\nSet the needed info\n\nChange to your desired region:\n\nCopy\nREGION=\"us-west-2\"\n\naws ec2 describe-vpcs --region $REGION\n\n\n\nVPC=\"(the VpcId from the above command)\"\n\nCreate the Subnet\n\nUse a CIDR block that is present on the VPC specified above.\n\nCopy\naws ec2 create-subnet \\\n\n    --region $REGION \\\n\n    --vpc-id $VPC \\\n\n    --cidr-block ${CIDR_BLOCK}\n\n\nNote the subnet ID that was returned, and assign it to a variable for ease of later use:\n\nCopy\nSUBNET=\"(the subnet ID of the created subnet)\"\n\nOfficial AMI Images\n\nOfficial AMI image ID can be found in the cloud-images.json file attached to the Talos release:\n\nCopy\nAMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.6.2/cloud-images.json | \\\n\n    jq -r '.[] | select(.region == \"'$REGION'\") | select (.arch == \"amd64\") | .id'`\n\necho $AMI\n\n\nReplace amd64 in the line above with the desired architecture. Note the AMI id that is returned is assigned to an environment variable: it will be used later when booting instances.\n\nIf using the official AMIs, you can skip to Creating the Security group\n\nCreate your own AMIs\n\nThe use of the official Talos AMIs are recommended, but if you wish to build your own AMIs, follow the procedure below.\n\nCreate the S3 Bucket\nCopy\naws s3api create-bucket \\\n\n    --bucket $BUCKET \\\n\n    --create-bucket-configuration LocationConstraint=$REGION \\\n\n    --acl private\n\nCreate the vmimport Role\n\nIn order to create an AMI, ensure that the vmimport role exists as described in the official AWS documentation.\n\nNote that the role should be associated with the S3 bucket we created above.\n\nCreate the Image Snapshot\n\nFirst, download the AWS image from a Talos release:\n\nCopy\ncurl -L https://github.com/siderolabs/talos/releases/download/v1.6.2/aws-amd64.raw.xz | xz -d > disk.raw\n\n\nCopy the RAW disk to S3 and import it as a snapshot:\n\nCopy\naws s3 cp disk.raw s3://$BUCKET/talos-aws-tutorial.raw\n\naws ec2 import-snapshot \\\n\n    --region $REGION \\\n\n    --description \"Talos kubernetes tutorial\" \\\n\n    --disk-container \"Format=raw,UserBucket={S3Bucket=$BUCKET,S3Key=talos-aws-tutorial.raw}\"\n\n\nSave the SnapshotId, as we will need it once the import is done. To check on the status of the import, run:\n\nCopy\naws ec2 describe-import-snapshot-tasks \\\n\n    --region $REGION \\\n\n    --import-task-ids\n\n\nOnce the SnapshotTaskDetail.Status indicates completed, we can register the image.\n\nRegister the Image\nCopy\naws ec2 register-image \\\n\n    --region $REGION \\\n\n    --block-device-mappings \"DeviceName=/dev/xvda,VirtualName=talos,Ebs={DeleteOnTermination=true,SnapshotId=$SNAPSHOT,VolumeSize=4,VolumeType=gp2}\" \\\n\n    --root-device-name /dev/xvda \\\n\n    --virtualization-type hvm \\\n\n    --architecture x86_64 \\\n\n    --ena-support \\\n\n    --name talos-aws-tutorial-ami\n\n\nWe now have an AMI we can use to create our cluster. Save the AMI ID, as we will need it when we create EC2 instances.\n\nCopy\nAMI=\"(AMI ID of the register image command)\"\n\nCreate a Security Group\nCopy\naws ec2 create-security-group \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --description \"Security Group for EC2 instances to allow ports required by Talos\"\n\n\n\nSECURITY_GROUP=\"(security group id that is returned)\"\n\n\nUsing the security group from above, allow all internal traffic within the same security group:\n\nCopy\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol all \\\n\n    --port 0 \\\n\n    --source-group talos-aws-tutorial-sg\n\n\nand expose the Talos and Kubernetes APIs:\n\nCopy\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 6443 \\\n\n    --cidr 0.0.0.0/0\n\n\n\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol tcp \\\n\n    --port 50000-50001 \\\n\n    --cidr 0.0.0.0/0\n\n\nIf you are using KubeSpan and will be adding workers outside of AWS, you need to allow inbound UDP for the Wireguard port:\n\nCopy\naws ec2 authorize-security-group-ingress \\\n\n    --region $REGION \\\n\n    --group-name talos-aws-tutorial-sg \\\n\n    --protocol udp --port 51820 --cidr 0.0.0.0/0\n\nCreate a Load Balancer\nCopy\naws elbv2 create-load-balancer \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-lb \\\n\n    --type network --subnets $SUBNET\n\n\nTake note of the DNS name and ARN. We will need these soon.\n\nCopy\nLOAD_BALANCER_ARN=\"(arn of the load balancer)\"\n\nCopy\naws elbv2 create-target-group \\\n\n    --region $REGION \\\n\n    --name talos-aws-tutorial-tg \\\n\n    --protocol TCP \\\n\n    --port 6443 \\\n\n    --target-type ip \\\n\n    --vpc-id $VPC\n\n\nAlso note the TargetGroupArn that is returned.\n\nCopy\nTARGET_GROUP_ARN=\"(target group arn)\"\n\nCreate the Machine Configuration Files\n\nUsing the DNS name of the loadbalancer created earlier, generate the base configuration files for the Talos machines.\n\nNote that the port used here is the externally accessible port configured on the load balancer - 443 - not the internal port of 6443:\n\nCopy\n$ talosctl gen config talos-k8s-aws-tutorial https://<load balancer DNS>:<port> --with-examples=false --with-docs=false\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nNote that the generated configs are too long for AWS userdata field if the --with-examples and --with-docs flags are not passed.\n\nAt this point, you can modify the generated configs to your liking.\n\nOptionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\nCopy\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nCreate the EC2 Instances\n\nchange the instance type if desired. Note: There is a known issue that prevents Talos from running on T2 instance types. Please use T3 if you need burstable instance types.\n\nCreate the Control Plane Nodes\nCopy\nCP_COUNT=1\n\nwhile [[ \"$CP_COUNT\" -lt 4 ]]; do\n\n  aws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 1 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://controlplane.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP \\\n\n    --associate-public-ip-address \\\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\"\n\n  ((CP_COUNT++))\n\ndone\n\n\nMake a note of the resulting PrivateIpAddress from the controlplane nodes for later use.\n\nCreate the Worker Nodes\nCopy\naws ec2 run-instances \\\n\n    --region $REGION \\\n\n    --image-id $AMI \\\n\n    --count 3 \\\n\n    --instance-type t3.small \\\n\n    --user-data file://worker.yaml \\\n\n    --subnet-id $SUBNET \\\n\n    --security-group-ids $SECURITY_GROUP\n\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\"\n\nConfigure the Load Balancer\n\nNow, using the load balancer target group’s ARN, and the PrivateIpAddress from the controlplane instances that you created :\n\nCopy\naws elbv2 register-targets \\\n\n    --region $REGION \\\n\n    --target-group-arn $TARGET_GROUP_ARN \\\n\n    --targets Id=$CP_NODE_1_IP  Id=$CP_NODE_2_IP  Id=$CP_NODE_3_IP\n\n\nUsing the ARNs of the load balancer and target group from previous steps, create the listener:\n\nCopy\naws elbv2 create-listener \\\n\n    --region $REGION \\\n\n    --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n    --protocol TCP \\\n\n    --port 443 \\\n\n    --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN\n\nBootstrap Etcd\n\nSet the endpoints (the control plane node to which talosctl commands are sent) and nodes (the nodes that the command operates on):\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 PUBLIC IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 PUBLIC IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n\nThe different control plane nodes should sendi/receive traffic via the load balancer, notice that one of the control plane has intiated the etcd cluster, and the others should join. You can now watch as your cluster bootstraps, by using\n\nCopy\ntalosctl --talosconfig talosconfig  health\n\n\nYou can also watch the performance of a node, via:\n\nCopy\ntalosctl  --talosconfig talosconfig dashboard\n\n\nAnd use standard kubectl commands.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Cloud Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/cloud-platforms/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nCloud Platforms\nCloud Platforms\nInstallation of Talos Linux on many cloud platforms.\nAWS\n\nCreating a cluster via the AWS CLI.\n\nAzure\n\nCreating a cluster via the CLI on Azure.\n\nDigitalOcean\n\nCreating a cluster via the CLI on DigitalOcean.\n\nExoscale\n\nCreating a cluster via the CLI using exoscale.com\n\nGCP\n\nCreating a cluster via the CLI on Google Cloud Platform.\n\nHetzner\n\nCreating a cluster via the CLI (hcloud) on Hetzner.\n\nNocloud\n\nCreating a cluster via the CLI using qemu.\n\nOpenstack\n\nCreating a cluster via the CLI on Openstack.\n\nOracle\n\nCreating a cluster via the CLI (oci) on OracleCloud.com.\n\nScaleway\n\nCreating a cluster via the CLI (scw) on scaleway.com.\n\nUpCloud\n\nCreating a cluster via the CLI (upctl) on UpCloud.com.\n\nVultr\n\nCreating a cluster via the CLI (vultr-cli) on Vultr.com.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Xen | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/xen/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nXen\nXen\n\nTalos is known to work on Xen. We don’t yet have a documented guide specific to Xen; however, you can follow the General Getting Started Guide. If you run into any issues, our community can probably help!\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "VMware | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/vmware/",
    "html": "Creating a Cluster via the govc CLI\nPrereqs/Assumptions\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nSet Environment Variables\nChoose Install Approach\nScripted Install\nImport OVA\nCreate Cluster\nManual Approach\nImport the OVA into vCenter\nCreate the Bootstrap Node\nUpdate Hardware Resources for the Bootstrap Node\nCreate the Remaining Control Plane Nodes\nUpdate Settings for the Worker Nodes\nBootstrap Cluster\nRetrieve the kubeconfig\nConfigure talos-vmtoolsd\nDocumentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nVMware\nVMware\nCreating Talos Kubernetes cluster using VMware.\nCreating a Cluster via the govc CLI\n\nIn this guide we will create an HA Kubernetes cluster with 2 worker nodes. We will use the govc cli which can be downloaded here.\n\nPrereqs/Assumptions\n\nThis guide will use the virtual IP (“VIP”) functionality that is built into Talos in order to provide a stable, known IP for the Kubernetes control plane. This simply means the user should pick an IP on their “VM Network” to designate for this purpose and keep it handy for future steps.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the VIP chosen in the prereq steps, we will now generate the base configuration files for the Talos machines. This can be done with the talosctl gen config ... command. Take note that we will also use a JSON6902 patch when creating the configs so that the control plane nodes get some special information about the VIP we chose earlier, as well as a daemonset to install vmware tools on talos nodes.\n\nFirst, download cp.patch.yaml to your local machine and edit the VIP to match your chosen IP. You can do this by issuing: curl -fsSLO https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/cp.patch.yaml. It’s contents should look like the following:\n\nCopy\n- op: add\n\n  path: /machine/network\n\n  value:\n\n    interfaces:\n\n    - interface: eth0\n\n      dhcp: true\n\n      vip:\n\n        ip: <VIP>\n\n- op: replace\n\n  path: /cluster/extraManifests\n\n  value:\n\n    - \"https://raw.githubusercontent.com/mologie/talos-vmtoolsd/master/deploy/unstable.yaml\"\n\n\nWith the patch in hand, generate machine configs with:\n\nCopy\n$ talosctl gen config vmware-test https://<VIP>:<port> --config-patch-control-plane @cp.patch.yaml\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking if needed. Optionally, you can specify additional patches by adding to the cp.patch.yaml file downloaded earlier, or create your own patch files.\n\nValidate the Configuration Files\nCopy\n$ talosctl validate --config controlplane.yaml --mode cloud\n\ncontrolplane.yaml is valid for cloud mode\n\n$ talosctl validate --config worker.yaml --mode cloud\n\nworker.yaml is valid for cloud mode\n\nSet Environment Variables\n\ngovc makes use of the following environment variables\n\nCopy\nexport GOVC_URL=<vCenter url>\n\nexport GOVC_USERNAME=<vCenter username>\n\nexport GOVC_PASSWORD=<vCenter password>\n\n\nNote: If your vCenter installation makes use of self signed certificates, you’ll want to export GOVC_INSECURE=true.\n\nThere are some additional variables that you may need to set:\n\nCopy\nexport GOVC_DATACENTER=<vCenter datacenter>\n\nexport GOVC_RESOURCE_POOL=<vCenter resource pool>\n\nexport GOVC_DATASTORE=<vCenter datastore>\n\nexport GOVC_NETWORK=<vCenter network>\n\nChoose Install Approach\n\nAs part of this guide, we have a more automated install script that handles some of the complexity of importing OVAs and creating VMs. If you wish to use this script, we will detail that next. If you wish to carry out the manual approach, simply skip ahead to the “Manual Approach” section.\n\nScripted Install\n\nDownload the vmware.sh script to your local machine. You can do this by issuing curl -fsSLO \"https://raw.githubusercontent.com/siderolabs/talos/master/website/content/v1.6/talos-guides/install/virtualized-platforms/vmware/vmware.sh\". This script has default variables for things like Talos version and cluster name that may be interesting to tweak before deploying.\n\nImport OVA\n\nTo create a content library and import the Talos OVA corresponding to the mentioned Talos version, simply issue:\n\nCopy\n./vmware.sh upload_ova\n\nCreate Cluster\n\nWith the OVA uploaded to the content library, you can create a 5 node (by default) cluster with 3 control plane and 2 worker nodes:\n\nCopy\n./vmware.sh create\n\n\nThis step will create a VM from the OVA, edit the settings based on the env variables used for VM size/specs, then power on the VMs.\n\nYou may now skip past the “Manual Approach” section down to “Bootstrap Cluster”.\n\nManual Approach\nImport the OVA into vCenter\n\nA talos.ova asset is published with each release. We will refer to the version of the release as $TALOS_VERSION below. It can be easily exported with export TALOS_VERSION=\"v0.3.0-alpha.10\" or similar.\n\nCopy\ncurl -LO https://github.com/siderolabs/talos/releases/download/$TALOS_VERSION/talos.ova\n\n\nCreate a content library (if needed) with:\n\nCopy\ngovc library.create <library name>\n\n\nImport the OVA to the library with:\n\nCopy\ngovc library.import -n talos-${TALOS_VERSION} <library name> /path/to/downloaded/talos.ova\n\nCreate the Bootstrap Node\n\nWe’ll clone the OVA to create the bootstrap node (our first control plane node).\n\nCopy\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-1\n\n\nTalos makes use of the guestinfo facility of VMware to provide the machine/cluster configuration. This can be set using the govc vm.change command. To facilitate persistent storage using the vSphere cloud provider integration with Kubernetes, disk.enableUUID=1 is used.\n\nCopy\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(cat controlplane.yaml | base64)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-1\n\nUpdate Hardware Resources for the Bootstrap Node\n-c is used to configure the number of cpus\n-m is used to configure the amount of memory (in MB)\nCopy\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-1\n\n\nThe following can be used to adjust the EPHEMERAL disk size.\n\nCopy\ngovc vm.disk.change -vm control-plane-1 -disk.name disk-1000-0 -size 10G\n\nCopy\ngovc vm.power -on control-plane-1\n\nCreate the Remaining Control Plane Nodes\nCopy\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-2\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} control-plane-3\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 controlplane.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm control-plane-3\n\nCopy\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-2\n\n\n\ngovc vm.change \\\n\n  -c 2 \\\n\n  -m 4096 \\\n\n  -vm control-plane-3\n\nCopy\ngovc vm.disk.change -vm control-plane-2 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm control-plane-3 -disk.name disk-1000-0 -size 10G\n\nCopy\ngovc vm.power -on control-plane-2\n\n\n\ngovc vm.power -on control-plane-3\n\nUpdate Settings for the Worker Nodes\nCopy\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-1\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-1\n\n\n\ngovc library.deploy <library name>/talos-${TALOS_VERSION} worker-2\n\ngovc vm.change \\\n\n  -e \"guestinfo.talos.config=$(base64 worker.yaml)\" \\\n\n  -e \"disk.enableUUID=1\" \\\n\n  -vm worker-2\n\nCopy\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-1\n\n\n\ngovc vm.change \\\n\n  -c 4 \\\n\n  -m 8192 \\\n\n  -vm worker-2\n\nCopy\ngovc vm.disk.change -vm worker-1 -disk.name disk-1000-0 -size 10G\n\n\n\ngovc vm.disk.change -vm worker-2 -disk.name disk-1000-0 -size 10G\n\nCopy\ngovc vm.power -on worker-1\n\n\n\ngovc vm.power -on worker-2\n\nBootstrap Cluster\n\nIn the vSphere UI, open a console to one of the control plane nodes. You should see some output stating that etcd should be bootstrapped. This text should look like:\n\nCopy\n\"etcd is waiting to join the cluster, if this node is the first node in the cluster, please run `talosctl bootstrap` against one of the following IPs:\n\n\nTake note of the IP mentioned here and issue:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap -e <control plane IP> -n <control plane IP>\n\n\nKeep this IP handy for the following steps as well.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane IP>\n\ntalosctl --talosconfig talosconfig config node <control plane IP>\n\ntalosctl --talosconfig talosconfig kubeconfig .\n\nConfigure talos-vmtoolsd\n\nThe talos-vmtoolsd application was deployed as a daemonset as part of the cluster creation; however, we must now provide a talos credentials file for it to use.\n\nCreate a new talosconfig with:\n\nCopy\ntalosctl --talosconfig talosconfig -n <control plane IP> config new vmtoolsd-secret.yaml --roles os:admin\n\n\nCreate a secret from the talosconfig:\n\nCopy\nkubectl -n kube-system create secret generic talos-vmtoolsd-config \\\n\n  --from-file=talosconfig=./vmtoolsd-secret.yaml\n\n\nClean up the generated file from local system:\n\nCopy\nrm vmtoolsd-secret.yaml\n\n\nOnce configured, you should now see these daemonset pods go into “Running” state and in vCenter, you will now see IPs and info from the Talos nodes present in the UI.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Proxmox | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/proxmox/",
    "html": "Installation\nHow to Get Proxmox\nInstall talosctl\nDownload ISO Image\nUpload ISO\nCreate VMs\nStart Control Plane Node\nWith DHCP server\nWithout DHCP server\nGenerate Machine Configurations\nCreate Control Plane Node\nCreate Worker Node\nUsing the Cluster\nBootstrap Etcd\nRetrieve the kubeconfig\nCleaning Up\nDocumentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nProxmox\nProxmox\nCreating Talos Kubernetes cluster using Proxmox.\n\nIn this guide we will create a Kubernetes cluster using Proxmox.\n\nVideo Walkthrough\n\nTo see a live demo of this writeup, visit Youtube here:\n\nInstallation\nHow to Get Proxmox\n\nIt is assumed that you have already installed Proxmox onto the server you wish to create Talos VMs on. Visit the Proxmox downloads page if necessary.\n\nInstall talosctl\n\nYou can download talosctl via\n\nCopy\ncurl -sL https://talos.dev/install | sh\n\nDownload ISO Image\n\nIn order to install Talos in Proxmox, you will need the ISO image from the Talos release page. You can download metal-amd64.iso via github.com/siderolabs/talos/releases\n\nCopy\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/<version>/metal-<arch>.iso -L -o _out/metal-<arch>.iso\n\n\nFor example version v1.6.2 for linux platform:\n\nCopy\nmkdir -p _out/\n\ncurl https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -L -o _out/metal-amd64.iso\n\nUpload ISO\n\nFrom the Proxmox UI, select the “local” storage and enter the “Content” section. Click the “Upload” button:\n\nSelect the ISO you downloaded previously, then hit “Upload”\n\nCreate VMs\n\nBefore starting, familiarise yourself with the system requirements for Talos and assign VM resources accordingly.\n\nCreate a new VM by clicking the “Create VM” button in the Proxmox UI:\n\nFill out a name for the new VM:\n\nIn the OS tab, select the ISO we uploaded earlier:\n\nKeep the defaults set in the “System” tab.\n\nKeep the defaults in the “Hard Disk” tab as well, only changing the size if desired.\n\nIn the “CPU” section, give at least 2 cores to the VM:\n\nNote: As of Talos v1.0 (which requires the x86-64-v2 microarchitecture), prior to Proxmox V8.0, booting with the default Processor Type kvm64 will not work. You can enable the required CPU features after creating the VM by adding the following line in the corresponding /etc/pve/qemu-server/<vmid>.conf file:\n\nCopy\nargs: -cpu kvm64,+cx16,+lahf_lm,+popcnt,+sse3,+ssse3,+sse4.1,+sse4.2\n\n\nAlternatively, you can set the Processor Type to host if your Proxmox host supports these CPU features, this however prevents using live VM migration.\n\nVerify that the RAM is set to at least 2GB:\n\nKeep the default values for networking, verifying that the VM is set to come up on the bridge interface:\n\nFinish creating the VM by clicking through the “Confirm” tab and then “Finish”.\n\nRepeat this process for a second VM to use as a worker node. You can also repeat this for additional nodes desired.\n\nNote: Talos doesn’t support memory hot plugging, if creating the VM programmatically don’t enable memory hotplug on your Talos VM’s. Doing so will cause Talos to be unable to see all available memory and have insufficient memory to complete installation of the cluster.\n\nStart Control Plane Node\n\nOnce the VMs have been created and updated, start the VM that will be the first control plane node. This VM will boot the ISO image specified earlier and enter “maintenance mode”.\n\nWith DHCP server\n\nOnce the machine has entered maintenance mode, there will be a console log that details the IP address that the node received. Take note of this IP address, which will be referred to as $CONTROL_PLANE_IP for the rest of this guide. If you wish to export this IP as a bash variable, simply issue a command like export CONTROL_PLANE_IP=1.2.3.4.\n\nWithout DHCP server\n\nTo apply the machine configurations in maintenance mode, VM has to have IP on the network. So you can set it on boot time manually.\n\nPress e on the boot time. And set the IP parameters for the VM. Format is:\n\nCopy\nip=<client-ip>:<srv-ip>:<gw-ip>:<netmask>:<host>:<device>:<autoconf>\n\n\nFor example $CONTROL_PLANE_IP will be 192.168.0.100 and gateway 192.168.0.1\n\nCopy\nlinux /boot/vmlinuz init_on_alloc=1 slab_nomerge pti=on panic=0 consoleblank=0 printk.devkmsg=on earlyprintk=ttyS0 console=tty0 console=ttyS0 talos.platform=metal ip=192.168.0.100::192.168.0.1:255.255.255.0::eth0:off\n\n\nThen press Ctrl-x or F10\n\nGenerate Machine Configurations\n\nWith the IP address above, you can now generate the machine configurations to use for installing Talos and Kubernetes. Issue the following command, updating the output directory, cluster name, and control plane IP as you see fit:\n\nCopy\ntalosctl gen config talos-proxmox-cluster https://$CONTROL_PLANE_IP:6443 --output-dir _out\n\n\nThis will create several files in the _out directory: controlplane.yaml, worker.yaml, and talosconfig.\n\nNote: The Talos config by default will install to /dev/sda. Depending on your setup the virtual disk may be mounted differently Eg: /dev/vda. You can check for disks running the following command:\n\nCopy\ntalosctl disks --insecure --nodes $CONTROL_PLANE_IP\n\n\nUpdate controlplane.yaml and worker.yaml config files to point to the correct disk location.\n\nCreate Control Plane Node\n\nUsing the controlplane.yaml generated above, you can now apply this config using talosctl. Issue:\n\nCopy\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file _out/controlplane.yaml\n\n\nYou should now see some action in the Proxmox console for this VM. Talos will be installed to disk, the VM will reboot, and then Talos will configure the Kubernetes control plane on this VM.\n\nNote: This process can be repeated multiple times to create an HA control plane.\n\nCreate Worker Node\n\nCreate at least a single worker node using a process similar to the control plane creation above. Start the worker node VM and wait for it to enter “maintenance mode”. Take note of the worker node’s IP address, which will be referred to as $WORKER_IP\n\nIssue:\n\nCopy\ntalosctl apply-config --insecure --nodes $WORKER_IP --file _out/worker.yaml\n\n\nNote: This process can be repeated multiple times to add additional workers.\n\nUsing the Cluster\n\nOnce the cluster is available, you can make use of talosctl and kubectl to interact with the cluster. For example, to view current running containers, run talosctl containers for a list of containers in the system namespace, or talosctl containers -k for the k8s.io namespace. To view the logs of a container, use talosctl logs <container> or talosctl logs -k <container>.\n\nFirst, configure talosctl to talk to your control plane node by issuing the following, updating paths and IPs as necessary:\n\nCopy\nexport TALOSCONFIG=\"_out/talosconfig\"\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\nBootstrap Etcd\nCopy\ntalosctl bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl kubeconfig .\n\nCleaning Up\n\nTo cleanup, simply stop and delete the virtual machines from the Proxmox UI.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Vagrant & Libvirt | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/vagrant-libvirt/",
    "html": "Pre-requisities\nOverview\nPreparing the environment\nBring up the nodes\nInstalling Talos\nInteracting with Kubernetes cluster\nCleanup\nDocumentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nVagrant & Libvirt\nVagrant & Libvirt\nPre-requisities\nLinux OS\nVagrant installed\nvagrant-libvirt plugin installed\ntalosctl installed\nkubectl installed\nOverview\n\nWe will use Vagrant and its libvirt plugin to create a KVM-based cluster with 3 control plane nodes and 1 worker node.\n\nFor this, we will mount Talos ISO into the VMs using a virtual CD-ROM, and configure the VMs to attempt to boot from the disk first with the fallback to the CD-ROM.\n\nWe will also configure a virtual IP address on Talos to achieve high-availability on kube-apiserver.\n\nPreparing the environment\n\nFirst, we download the latest metal-amd64.iso ISO from GitHub releases into the /tmp directory.\n\nCopy\nwget --timestamping https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso -O /tmp/metal-amd64.iso\n\n\nCreate a Vagrantfile with the following contents:\n\nCopy\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define \"control-plane-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-2\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-2.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"control-plane-node-3\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 2\n\n      domain.memory = 2048\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/control-plane-node-3.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\n\n\n  config.vm.define \"worker-node-1\" do |vm|\n\n    vm.vm.provider :libvirt do |domain|\n\n      domain.cpus = 1\n\n      domain.memory = 1024\n\n      domain.serial :type => \"file\", :source => {:path => \"/tmp/worker-node-1.log\"}\n\n      domain.storage :file, :device => :cdrom, :path => \"/tmp/metal-amd64.iso\"\n\n      domain.storage :file, :size => '4G', :type => 'raw'\n\n      domain.boot 'hd'\n\n      domain.boot 'cdrom'\n\n    end\n\n  end\n\nend\n\nBring up the nodes\n\nCheck the status of vagrant VMs:\n\nCopy\nvagrant status\n\n\nYou should see the VMs in “not created” state:\n\nCopy\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      not created (libvirt)\n\ncontrol-plane-node-2      not created (libvirt)\n\ncontrol-plane-node-3      not created (libvirt)\n\nworker-node-1             not created (libvirt)\n\n\nBring up the vagrant environment:\n\nCopy\nvagrant up --provider=libvirt\n\n\nCheck the status again:\n\nCopy\nvagrant status\n\n\nNow you should see the VMs in “running” state:\n\nCopy\nCurrent machine states:\n\n\n\ncontrol-plane-node-1      running (libvirt)\n\ncontrol-plane-node-2      running (libvirt)\n\ncontrol-plane-node-3      running (libvirt)\n\nworker-node-1             running (libvirt)\n\n\nFind out the IP addresses assigned by the libvirt DHCP by running:\n\nCopy\nvirsh list | grep vagrant | awk '{print $2}' | xargs -t -L1 virsh domifaddr\n\n\nOutput will look like the following:\n\nCopy\nvirsh domifaddr vagrant_control-plane-node-2\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet0      52:54:00:f9:10:e5    ipv4         192.168.121.119/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet1      52:54:00:0f:ae:59    ipv4         192.168.121.203/24\n\n\n\nvirsh domifaddr vagrant_worker-node-1\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet2      52:54:00:6f:28:95    ipv4         192.168.121.69/24\n\n\n\nvirsh domifaddr vagrant_control-plane-node-3\n\n Name       MAC address          Protocol     Address\n\n-------------------------------------------------------------------------------\n\n vnet3      52:54:00:03:45:10    ipv4         192.168.121.125/24\n\n\nOur control plane nodes have the IPs: 192.168.121.203, 192.168.121.119, 192.168.121.125 and the worker node has the IP 192.168.121.69.\n\nNow you should be able to interact with Talos nodes that are in maintenance mode:\n\nCopy\ntalosctl -n 192.168.121.203 disks --insecure\n\n\nSample output:\n\nCopy\nDEV        MODEL   SERIAL   TYPE   UUID   WWID   MODALIAS                    NAME   SIZE     BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      8.6 GB   /pci0000:00/0000:00:03.0/virtio0/\n\nInstalling Talos\n\nPick an endpoint IP in the vagrant-libvirt subnet but not used by any nodes, for example 192.168.121.100.\n\nGenerate a machine configuration:\n\nCopy\ntalosctl gen config my-cluster https://192.168.121.100:6443 --install-disk /dev/vda\n\n\nEdit controlplane.yaml to add the virtual IP you picked to a network interface under .machine.network.interfaces, for example:\n\nCopy\nmachine:\n\n  network:\n\n    interfaces:\n\n      - interface: eth0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.121.100\n\n\nApply the configuration to the initial control plane node:\n\nCopy\ntalosctl -n 192.168.121.203 apply-config --insecure --file controlplane.yaml\n\n\nYou can tail the logs of the node:\n\nCopy\nsudo tail -f /tmp/control-plane-node-1.log\n\n\nSet up your shell to use the generated talosconfig and configure its endpoints (use the IPs of the control plane nodes):\n\nCopy\nexport TALOSCONFIG=$(realpath ./talosconfig)\n\ntalosctl config endpoint 192.168.121.203 192.168.121.119 192.168.121.125\n\n\nBootstrap the Kubernetes cluster from the initial control plane node:\n\nCopy\ntalosctl -n 192.168.121.203 bootstrap\n\n\nFinally, apply the machine configurations to the remaining nodes:\n\nCopy\ntalosctl -n 192.168.121.119 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.125 apply-config --insecure --file controlplane.yaml\n\ntalosctl -n 192.168.121.69 apply-config --insecure --file worker.yaml\n\n\nAfter a while, you should see that all the members have joined:\n\nCopy\ntalosctl -n 192.168.121.203 get members\n\n\nThe output will be like the following:\n\nCopy\nNODE              NAMESPACE   TYPE     ID                      VERSION   HOSTNAME                MACHINE TYPE   OS               ADDRESSES\n\n192.168.121.203   cluster     Member   talos-192-168-121-119   1         talos-192-168-121-119   controlplane   Talos (v1.1.0)   [\"192.168.121.119\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-69    1         talos-192-168-121-69    worker         Talos (v1.1.0)   [\"192.168.121.69\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-203   6         talos-192-168-121-203   controlplane   Talos (v1.1.0)   [\"192.168.121.100\",\"192.168.121.203\"]\n\n192.168.121.203   cluster     Member   talos-192-168-121-125   1         talos-192-168-121-125   controlplane   Talos (v1.1.0)   [\"192.168.121.125\"]\n\nInteracting with Kubernetes cluster\n\nRetrieve the kubeconfig from the cluster:\n\nCopy\ntalosctl -n 192.168.121.203 kubeconfig ./kubeconfig\n\n\nList the nodes in the cluster:\n\nCopy\nkubectl --kubeconfig ./kubeconfig get node -owide\n\n\nYou will see an output similar to:\n\nCopy\nNAME                    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-192-168-121-203   Ready    control-plane,master   3m10s   v1.24.2   192.168.121.203   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-69    Ready    <none>                 2m25s   v1.24.2   192.168.121.69    <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-119   Ready    control-plane,master   8m46s   v1.24.2   192.168.121.119   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\ntalos-192-168-121-125   Ready    control-plane,master   3m11s   v1.24.2   192.168.121.125   <none>        Talos (v1.1.0)   5.15.48-talos    containerd://1.6.6\n\n\nCongratulations, you have a highly-available Talos cluster running!\n\nCleanup\n\nYou can destroy the vagrant environment by running:\n\nCopy\nvagrant destroy -f\n\n\nAnd remove the ISO image you downloaded:\n\nCopy\nsudo rm -f /tmp/metal-amd64.iso\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "KVM | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/kvm/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nKVM\nKVM\n\nTalos is known to work on KVM.\n\nWe don’t yet have a documented guide specific to KVM; however, you can have a look at our Vagrant & Libvirt guide which uses KVM for virtualization.\n\nIf you run into any issues, our community can probably help!\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Hyper-V | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/hyper-v/",
    "html": "Pre-requisities\nPlan Overview\nSetup a Control Plane Node\nSetup Worker Nodes\nPushing Config to the Nodes\nPushing Config to Worker Nodes\nBootstrap Cluster\nDocumentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nHyper-V\nHyper-V\nCreating a Talos Kubernetes cluster using Hyper-V.\nPre-requisities\nDownload the latest metal-amd64.iso ISO from github releases page\nCreate a New-TalosVM folder in any of your PS Module Path folders $env:PSModulePath -split ';' and save the New-TalosVM.psm1 there\nPlan Overview\n\nHere we will create a basic 3 node cluster with a single control-plane node and two worker nodes. The only difference between control plane and worker node is the amount of RAM and an additional storage VHD. This is personal preference and can be configured to your liking.\n\nWe are using a VMNamePrefix argument for a VM Name prefix and not the full hostname. This command will find any existing VM with that prefix and “+1” the highest suffix it finds. For example, if VMs talos-cp01 and talos-cp02 exist, this will create VMs starting from talos-cp03, depending on NumberOfVMs argument.\n\nSetup a Control Plane Node\n\nUse the following command to create a single control plane node:\n\nCopy\nNew-TalosVM -VMNamePrefix talos-cp -CPUCount 2 -StartupMemory 4GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 1 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos'\n\n\nThis will create talos-cp01 VM and power it on.\n\nSetup Worker Nodes\n\nUse the following command to create 2 worker nodes:\n\nCopy\nNew-TalosVM -VMNamePrefix talos-worker -CPUCount 4 -StartupMemory 8GB -SwitchName LAB -TalosISOPath C:\\ISO\\metal-amd64.iso -NumberOfVMs 2 -VMDestinationBasePath 'D:\\Virtual Machines\\Test VMs\\Talos' -StorageVHDSize 50GB\n\n\nThis will create two VMs: talos-worker01 and talos-wworker02 and attach an additional VHD of 50GB for storage (which in my case will be passed to Mayastor).\n\nPushing Config to the Nodes\n\nNow that our VMs are ready, find their IP addresses from console of VM. With that information, push config to the control plane node with:\n\nCopy\n# set control plane IP variable\n\n$CONTROL_PLANE_IP='10.10.10.x'\n\n\n\n# Generate talos config\n\ntalosctl gen config talos-cluster https://$($CONTROL_PLANE_IP):6443 --output-dir .\n\n\n\n# Apply config to control plane node\n\ntalosctl apply-config --insecure --nodes $CONTROL_PLANE_IP --file .\\controlplane.yaml\n\nPushing Config to Worker Nodes\n\nSimilarly, for the workers:\n\nCopy\ntalosctl apply-config --insecure --nodes 10.10.10.x --file .\\worker.yaml\n\n\nApply the config to both nodes.\n\nBootstrap Cluster\n\nNow that our nodes are ready, we are ready to bootstrap the Kubernetes cluster.\n\nCopy\n# Use following command to set node and endpoint permanantly in config so you dont have to type it everytime\n\ntalosctl config endpoint $CONTROL_PLANE_IP\n\ntalosctl config node $CONTROL_PLANE_IP\n\n\n\n# Bootstrap cluster\n\ntalosctl bootstrap\n\n\n\n# Generate kubeconfig\n\ntalosctl kubeconfig .\n\n\nThis will generate the kubeconfig file, you can use to connect to the cluster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Virtualized Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/virtualized-platforms/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nVirtualized Platforms\nVirtualized Platforms\nInstallation of Talos Linux for virtualization platforms.\nHyper-V\n\nCreating a Talos Kubernetes cluster using Hyper-V.\n\nKVM\n\nProxmox\n\nCreating Talos Kubernetes cluster using Proxmox.\n\nVagrant & Libvirt\n\nVMware\n\nCreating Talos Kubernetes cluster using VMware.\n\nXen\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "SecureBoot | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/secureboot/",
    "html": "SecureBoot with Sidero Labs Images\nBooting Talos Linux in SecureBoot Mode\nUpgrading Talos Linux\nDisk Encryption with TPM\nOther Boot Options\nSecureBoot with Custom Keys\nGenerating the Keys\nGenerating the SecureBoot Assets\nDocumentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nSecureBoot\nSecureBoot\nBooting Talos in SecureBoot mode on UEFI platforms.\n\nTalos now supports booting on UEFI systems in SecureBoot mode. When combined with TPM-based disk encryption, this provides Trusted Boot experience.\n\nNote: SecureBoot is not supported on x86 platforms in BIOS mode.\n\nThe implementation is using systemd-boot as a boot menu implementation, while the Talos kernel, initramfs and cmdline arguments are combined into the Unified Kernel Image (UKI) format. UEFI firmware loads the systemd-boot bootloader, which then loads the UKI image. Both systemd-boot and Talos UKI image are signed with the key, which is enrolled into the UEFI firmware.\n\nAs Talos Linux is fully contained in the UKI image, the full operating system is verified and booted by the UEFI firmware.\n\nNote: There is no support at the moment to upgrade non-UKI (GRUB-based) Talos installation to use UKI/SecureBoot, so a fresh installation is required.\n\nSecureBoot with Sidero Labs Images\n\nSidero Labs provides Talos images signed with the Sidero Labs SecureBoot key via Image Factory.\n\nNote: The SecureBoot images are available for Talos releases starting from v1.5.0.\n\nThe easiest way to get started with SecureBoot is to download the ISO, and boot it on a UEFI-enabled system which has SecureBoot enabled in setup mode.\n\nThe ISO bootloader will enroll the keys in the UEFI firmware, and boot the Talos Linux in SecureBoot mode. The install should performed using SecureBoot installer (put it Talos machine configuration): factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2.\n\nNote: SecureBoot images can also be generated with custom keys.\n\nBooting Talos Linux in SecureBoot Mode\n\nIn this guide we will use the ISO image to boot Talos Linux in SecureBoot mode, followed by submitting machine configuration to the machine in maintenance mode. We will use one the ways to generate and submit machine configuration to the node, please refer to the Production Notes for the full guide.\n\nFirst, make sure SecureBoot is enabled in the UEFI firmware. For the first boot, the UEFI firmware should be in the setup mode, so that the keys can be enrolled into the UEFI firmware automatically. If the UEFI firmware does not support automatic enrollment, you may need to hit Esc to force the boot menu to appear, and select the Enroll Secure Boot keys: auto option.\n\nNote: There are other ways to enroll the keys into the UEFI firmware, but this is out of scope of this guide.\n\nOnce Talos is running in maintenance mode, verify that secure boot is enabled:\n\nCopy\n$ talosctl -n <IP> get securitystate --insecure\n\nNODE   NAMESPACE   TYPE            ID              VERSION   SECUREBOOT\n\n       runtime     SecurityState   securitystate   1         true\n\n\nNow we will generate the machine configuration for the node supplying the installer-secureboot container image, and applying the patch to enable TPM-based disk encryption (requires TPM 2.0):\n\nCopy\n# tpm-disk-encryption.yaml\n\nmachine:\n\n  systemDiskEncryption:\n\n    ephemeral:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n    state:\n\n      provider: luks2\n\n      keys:\n\n        - slot: 0\n\n          tpm: {}\n\n\nGenerate machine configuration:\n\nCopy\ntalosctl gen config <cluster-name> https://<endpoint>:6443 --install-image=factory.talos.dev/installer-secureboot/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.6.2 --install-disk=/dev/sda --config-patch @tpm-disk-encryption.yaml\n\n\nApply machine configuration to the node:\n\nCopy\ntalosctl -n <IP> apply-config --insecure -f controlplane.yaml\n\n\nTalos will perform the installation to the disk and reboot the node. Please make sure that the ISO image is not attached to the node anymore, otherwise the node will boot from the ISO image again.\n\nOnce the node is rebooted, verify that the node is running in secure boot mode:\n\nCopy\ntalosctl -n <IP> --talosconfig=talosconfig get securitystate\n\nUpgrading Talos Linux\n\nAny change to the boot asset (kernel, initramfs, kernel command line) requires the UKI to be regenerated and the installer image to be rebuilt. Follow the steps above to generate new installer image updating the boot assets: use new Talos version, add a system extension, or modify the kernel command line. Once the new installer image is pushed to the registry, upgrade the node using the new installer image.\n\nIt is important to preserve the UKI signing key and the PCR signing key, otherwise the node will not be able to boot with the new UKI and unlock the encrypted partitions.\n\nDisk Encryption with TPM\n\nWhen encrypting the disk partition for the first time, Talos Linux generates a random disk encryption key and seals (encrypts) it with the TPM device. The TPM unlock policy is configured to trust the expected policy signed by the PCR signing key. This way TPM unlocking doesn’t depend on the exact PCR measurements, but rather on the expected policy signed by the PCR signing key and the state of SecureBoot (PCR 7 measurement, including secureboot status and the list of enrolled keys).\n\nWhen the UKI image is generated, the UKI is measured and expected measurements are combined into TPM unlock policy and signed with the PCR signing key. During the boot process, systemd-stub component of the UKI performs measurements of the UKI sections into the TPM device. Talos Linux during the boot appends to the PCR register the measurements of the boot phases, and once the boot reaches the point of mounting the encrypted disk partition, the expected signed policy from the UKI is matched against measured values to unlock the TPM, and TPM unseals the disk encryption key which is then used to unlock the disk partition.\n\nDuring the upgrade, as long as the new UKI is contains PCR policy signed with the same PCR signing key, and SecureBoot state has not changed the disk partition will be unlocked successfully.\n\nDisk encryption is also tied to the state of PCR register 7, so that it unlocks only if SecureBoot is enabled and the set of enrolled keys hasn’t changed.\n\nOther Boot Options\n\nUnified Kernel Image (UKI) is a UEFI-bootable image which can be booted directly from the UEFI firmware skipping the systemd-boot bootloader. In network boot mode, the UKI can be used directly as well, as it contains the full set of boot assets required to boot Talos Linux.\n\nWhen SecureBoot is enabled, the UKI image ignores any kernel command line arguments passed to it, but rather uses the kernel command line arguments embedded into the UKI image itself. If kernel command line arguments need to be changed, the UKI image needs to be rebuilt with the new kernel command line arguments.\n\nSecureBoot with Custom Keys\nGenerating the Keys\n\nTalos requires two set of keys to be used for the SecureBoot process:\n\nSecureBoot key is used to sign the boot assets and it is enrolled into the UEFI firmware.\nPCR Signing Key is used to sign the TPM policy, which is used to seal the disk encryption key.\n\nThe same key might be used for both, but it is recommended to use separate keys for each purpose.\n\nTalos provides a utility to generate the keys, but existing PKI infrastructure can be used as well:\n\nCopy\n$ talosctl gen secureboot uki --common-name \"SecureBoot Key\"\n\nwriting _out/uki-signing-cert.pem\n\nwriting _out/uki-signing-cert.der\n\nwriting _out/uki-signing-key.pem\n\n\nThe generated certificate and private key are written to disk in PEM-encoded format (RSA 4096-bit key). The certificate is also written in DER format for the systems which expect the certificate in DER format.\n\nPCR signing key can be generated with:\n\nCopy\n$ talosctl gen secureboot pcr\n\nwriting _out/pcr-signing-key.pem\n\n\nThe file containing the private key is written to disk in PEM-encoded format (RSA 2048-bit key).\n\nOptionally, UEFI automatic key enrollment database can be generated using the _out/uki-signing-* files as input:\n\nCopy\n$ talosctl gen secureboot database\n\nwriting _out/db.auth\n\nwriting _out/KEK.auth\n\nwriting _out/PK.auth\n\n\nThese files can be used to enroll the keys into the UEFI firmware automatically when booting from a SecureBoot ISO while UEFI firmware is in the setup mode.\n\nGenerating the SecureBoot Assets\n\nOnce the keys are generated, they can be used to sign the Talos boot assets to generate required ISO images, PXE boot assets, disk images, installer containers, etc. In this guide we will generate a SecureBoot ISO image and an installer image.\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-iso\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.5.0-alpha.3-35-ge0f383598-dirty\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: iso\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\nISO ready\n\noutput asset path: /out/metal-amd64-secureboot.iso\n\n\nNext, the installer image should be generated to install Talos to disk on a SecureBoot-enabled system:\n\nCopy\n$ docker run --rm -t -v $PWD/_out:/secureboot:ro -v $PWD/_out:/out ghcr.io/siderolabs/imager:v1.6.2 secureboot-installer\n\nprofile ready:\n\narch: amd64\n\nplatform: metal\n\nsecureboot: true\n\nversion: v1.6.2\n\ninput:\n\n  kernel:\n\n    path: /usr/install/amd64/vmlinuz\n\n  initramfs:\n\n    path: /usr/install/amd64/initramfs.xz\n\n  sdStub:\n\n    path: /usr/install/amd64/systemd-stub.efi\n\n  sdBoot:\n\n    path: /usr/install/amd64/systemd-boot.efi\n\n  baseInstaller:\n\n    imageRef: ghcr.io/siderolabs/installer:v1.6.2\n\n  secureboot:\n\n    signingKeyPath: /secureboot/uki-signing-key.pem\n\n    signingCertPath: /secureboot/uki-signing-cert.pem\n\n    pcrSigningKeyPath: /secureboot/pcr-signing-key.pem\n\n    pcrPublicKeyPath: /secureboot/pcr-signing-public-key.pem\n\n    platformKeyPath: /secureboot/PK.auth\n\n    keyExchangeKeyPath: /secureboot/KEK.auth\n\n    signatureKeyPath: /secureboot/db.auth\n\noutput:\n\n  kind: installer\n\n  outFormat: raw\n\nskipped initramfs rebuild (no system extensions)\n\nkernel command line: talos.platform=metal console=ttyS0 console=tty0 init_on_alloc=1 slab_nomerge pti=on consoleblank=0 nvme_core.io_timeout=4294967295 printk.devkmsg=on ima_template=ima-ng ima_appraise=fix ima_hash=sha512 lockdown=confidentiality\n\nUKI ready\n\ninstaller container image ready\n\noutput asset path: /out/installer-amd64-secureboot.tar\n\n\nThe generated container image should be pushed to some container registry which Talos can access during the installation, e.g.:\n\nCopy\ncrane push _out/installer-amd64-secureboot.tar ghcr.io/<user>/installer-amd64-secureboot:v1.6.2\n\n\nThe generated ISO and installer images might be further customized with system extensions, extra kernel command line arguments, etc.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "PXE | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/pxe/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nPXE\nPXE\nBooting Talos over the network on bare-metal with PXE.\n\nTalos can be installed on bare-metal using PXE service. There are two more detailed guides for PXE booting using Matchbox and Digital Rebar.\n\nThis guide describes generic steps for PXE booting Talos on bare-metal.\n\nFirst, download the vmlinuz and initramfs assets from the Talos releases page. Set up the machines to PXE boot from the network (usually by setting the boot order in the BIOS). There might be options specific to the hardware being used, booting in BIOS or UEFI mode, using iPXE, etc.\n\nTalos requires the following kernel parameters to be set on the initial boot:\n\ntalos.platform=metal\nslab_nomerge\npti=on\n\nWhen booted from the network without machine configuration, Talos will start in maintenance mode.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from network. The boot order should prefer disk over network.\n\nTalos can automatically fetch the machine configuration from the network on the initial boot using talos.config kernel parameter. A metadata service (HTTP service) can be implemented to deliver customized configuration to each node for example by using the MAC address of the node:\n\nCopy\ntalos.config=https://metadata.service/talos/config?mac=${mac}\n\n\nNote: The talos.config kernel parameter supports other substitution variables, see kernel parameters reference for the full list.\n\nPXE booting can be also performed via Image Factory.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Network Configuration | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/network-config/",
    "html": "Kernel Command Line\nPlatform Network Configuration\nDocumentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nNetwork Configuration\nNetwork Configuration\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nBy default, Talos will run DHCP client on all interfaces which have a link, and that might be enough for most of the cases. If some advanced network configuration is required, it can be done via the machine configuration file.\n\nBut sometimes it is required to apply network configuration even before the machine configuration can be fetched from the network.\n\nKernel Command Line\n\nTalos supports some kernel command line parameters to configure network before the machine configuration is fetched.\n\nNote: Kernel command line parameters are not persisted after Talos installation, so proper network configuration should be done via the machine configuration.\n\nAddress, default gateway and DNS servers can be configured via ip= kernel command line parameter:\n\nCopy\nip=172.20.0.2::172.20.0.1:255.255.255.0::eth0.100:::::\n\n\nBonding can be configured via bond= kernel command line parameter:\n\nCopy\nbond=bond0:eth0,eth1:balance-rr\n\n\nVLANs can be configured via vlan= kernel command line parameter:\n\nCopy\nvlan=eth0.100:eth0\n\n\nSee kernel parameters reference for more details.\n\nPlatform Network Configuration\n\nSome platforms (e.g. AWS, Google Cloud, etc.) have their own network configuration mechanisms, which can be used to perform the initial network configuration. There is no such mechanism for bare-metal platforms, so Talos provides a way to use platform network config on the metal platform to submit the initial network configuration.\n\nThe platform network configuration is a YAML document which contains resource specifications for various network resources. For the metal platform, the interactive dashboard can be used to edit the platform network configuration, also the configuration can be created manually.\n\nThe current value of the platform network configuration can be retrieved using the MetaKeys resource (key 0xa):\n\nCopy\ntalosctl get meta 0xa\n\n\nThe platform network configuration can be updated using the talosctl meta command for the running node:\n\nCopy\ntalosctl meta write 0xa '{\"externalIPs\": [\"1.2.3.4\"]}'\n\ntalosctl meta delete 0xa\n\n\nThe initial platform network configuration for the metal platform can be also included into the generated Talos image:\n\nCopy\ndocker run --rm -i ghcr.io/siderolabs/imager:v1.6.2 iso --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\ndocker run --rm -i --privileged ghcr.io/siderolabs/imager:v1.6.2 image --platform metal --arch amd64 --tar-to-stdout --meta 0xa='{...}' | tar xz\n\n\nThe platform network configuration gets merged with other sources of network configuration, the details can be found in the network resources guide.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Matchbox | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/matchbox/",
    "html": "Creating a Cluster\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nPublishing the Machine Configuration Files\nCreate the Matchbox Configuration Files\nProfiles\nControl Plane Nodes\nWorker Nodes\nGroups\nBoot the Machines\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nMatchbox\nMatchbox\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\nCreating a Cluster\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes. We assume an existing load balancer, matchbox deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\nCopy\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\nCopy\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nIn bare-metal setups it is up to the user to provide the configuration files over HTTP(S). A special kernel parameter (talos.config) must be used to inform Talos about where it should retrieve its configuration file. To keep things simple we will place controlplane.yaml, and worker.yaml into Matchbox’s assets directory. This directory is automatically served by Matchbox.\n\nCreate the Matchbox Configuration Files\n\nThe profiles we will create will reference vmlinuz, and initramfs.xz. Download these files from the release of your choice, and place them in /var/lib/matchbox/assets.\n\nProfiles\nControl Plane Nodes\nCopy\n{\n\n  \"id\": \"control-plane\",\n\n  \"name\": \"control-plane\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/controlplane.yaml\"\n\n    ]\n\n  }\n\n}\n\n\nNote: Be sure to change http://matchbox.talos.dev to the endpoint of your matchbox server.\n\nWorker Nodes\nCopy\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"boot\": {\n\n    \"kernel\": \"/assets/vmlinuz\",\n\n    \"initrd\": [\"/assets/initramfs.xz\"],\n\n    \"args\": [\n\n      \"initrd=initramfs.xz\",\n\n      \"init_on_alloc=1\",\n\n      \"slab_nomerge\",\n\n      \"pti=on\",\n\n      \"console=tty0\",\n\n      \"console=ttyS0\",\n\n      \"printk.devkmsg=on\",\n\n      \"talos.platform=metal\",\n\n      \"talos.config=http://matchbox.talos.dev/assets/worker.yaml\"\n\n    ]\n\n  }\n\n}\n\nGroups\n\nNow, create the following groups, and ensure that the selectors are accurate for your specific setup.\n\nCopy\n{\n\n  \"id\": \"control-plane-1\",\n\n  \"name\": \"control-plane-1\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\nCopy\n{\n\n  \"id\": \"control-plane-2\",\n\n  \"name\": \"control-plane-2\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\nCopy\n{\n\n  \"id\": \"control-plane-3\",\n\n  \"name\": \"control-plane-3\",\n\n  \"profile\": \"control-plane\",\n\n  \"selector\": {\n\n    ...\n\n  }\n\n}\n\nCopy\n{\n\n  \"id\": \"default\",\n\n  \"name\": \"default\",\n\n  \"profile\": \"default\"\n\n}\n\nBoot the Machines\n\nNow that we have our configuration files in place, boot all the machines. Talos will come up on each machine, grab its configuration file, and bootstrap itself.\n\nBootstrap Etcd\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "ISO | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/iso/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nISO\nISO\nBooting Talos on bare-metal with ISO.\n\nTalos can be installed on bare-metal machine using an ISO image. ISO images for amd64 and arm64 architectures are available on the Talos releases page.\n\nTalos doesn’t install itself to disk when booted from an ISO until the machine configuration is applied.\n\nPlease follow the getting started guide for the generic steps on how to install Talos.\n\nNote: If there is already a Talos installation on the disk, the machine will boot into that installation when booting from a Talos ISO. The boot order should prefer disk over ISO, or the ISO should be removed after the installation to make Talos boot from disk.\n\nSee kernel parameters reference for the list of kernel parameters supported by Talos.\n\nThere are two flavors of ISO images available:\n\nmetal-<arch>.iso supports booting on BIOS and UEFI systems (for x86, UEFI only for arm64)\nmetal-<arch>-secureboot.iso supports booting on only UEFI systems in SecureBoot mode (via Image Factory)\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Equinix Metal | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/equinix-metal/",
    "html": "Define the Kubernetes Endpoint\nCreate the Machine Configuration Files\nGenerating Configurations\nValidate the Configuration Files\nPassing in the configuration as User Data\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\nCreating a Cluster via the Equinix Metal CLI\nNetwork Booting via iPXE\nCreate the Control Plane Nodes\nCreate the Worker Nodes\nUpdate the Kubernetes endpoint\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nEquinix Metal\nEquinix Metal\nCreating Talos clusters with Equinix Metal.\n\nYou can create a Talos Linux cluster on Equinix Metal in a variety of ways, such as through the EM web UI, the metal command line too, or through PXE booting. Talos Linux is a supported OS install option on Equinix Metal, so it’s an easy process.\n\nRegardless of the method, the process is:\n\nCreate a DNS entry for your Kubernetes endpoint.\nGenerate the configurations using talosctl.\nProvision your machines on Equinix Metal.\nPush the configurations to your servers (if not done as part of the machine provisioning).\nconfigure your Kubernetes endpoint to point to the newly created control plane nodes\nbootstrap the cluster\nDefine the Kubernetes Endpoint\n\nThere are a variety of ways to create an HA endpoint for the Kubernetes cluster. Some of the ways are:\n\nDNS\nLoad Balancer\nBGP\n\nWhatever way is chosen, it should result in an IP address/DNS name that routes traffic to all the control plane nodes. We do not know the control plane node IP addresses at this stage, but we should define the endpoint DNS entry so that we can use it in creating the cluster configuration. After the nodes are provisioned, we can use their addresses to create the endpoint A records, or bind them to the load balancer, etc.\n\nCreate the Machine Configuration Files\nGenerating Configurations\n\nUsing the DNS name of the loadbalancer defined above, generate the base configuration files for the Talos machines:\n\nCopy\n$ talosctl gen config talos-k8s-em-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe port used above should be 6443, unless your load balancer maps a different port to port 6443 on the control plane nodes.\n\nValidate the Configuration Files\nCopy\ntalosctl validate --config controlplane.yaml --mode metal\n\ntalosctl validate --config worker.yaml --mode metal\n\n\nNote: Validation of the install disk could potentially fail as validation is performed on your local machine and the specified disk may not exist.\n\nPassing in the configuration as User Data\n\nYou can use the metadata service provide by Equinix Metal to pass in the machines configuration. It is required to add a shebang to the top of the configuration file.\n\nThe convention we use is #!talos.\n\nProvision the machines in Equinix Metal\nUsing the Equinix Metal UI\n\nSimply select the location and type of machines in the Equinix Metal web interface. Select Talos as the Operating System, then select the number of servers to create, and name them (in lowercase only.) Under optional settings, you can optionally paste in the contents of controlplane.yaml that was generated, above (ensuring you add a first line of #!talos).\n\nYou can repeat this process to create machines of different types for control plane and worker nodes (although you would pass in worker.yaml for the worker nodes, as user data).\n\nIf you did not pass in the machine configuration as User Data, you need to provide it to each machine, with the following command:\n\ntalosctl apply-config --insecure --nodes <Node IP> --file ./controlplane.yaml\n\nCreating a Cluster via the Equinix Metal CLI\n\nThis guide assumes the user has a working API token,and the Equinix Metal CLI installed.\n\nBecause Talos Linux is a supported operating system, Talos Linux machines can be provisioned directly via the CLI, using the -O talos_v1 parameter (for Operating System).\n\nNote: Ensure you have prepended #!talos to the controlplane.yaml file.\n\nCopy\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --operating-system \"talos_v1\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\ne.g. metal device create -p <projectID> -f da11 -O talos_v1 -P c3.small.x86 -H steve.test.11 --userdata-file ./controlplane.yaml\n\nRepeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nNetwork Booting via iPXE\n\nTalos Linux can be PXE-booted on Equinix Metal using Image Factory, using the equinixMetal platform: e.g. https://pxe.factory.talos.dev/pxe/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba/v1.6.2/equinixMetal-amd64 (this URL references the default schematic and amd64 architecture).\n\nCreate the Control Plane Nodes\nCopy\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file controlplane.yaml\n\n\nNote: Repeat this to create each control plane node desired: there should usually be 3 for a HA cluster.\n\nCreate the Worker Nodes\nCopy\nmetal device create \\\n\n  --project-id $PROJECT_ID \\\n\n  --facility $FACILITY \\\n\n  --ipxe-script-url $PXE_SERVER \\\n\n  --operating-system \"custom_ipxe\" \\\n\n  --plan $PLAN\\\n\n  --hostname $HOSTNAME\\\n\n  --userdata-file worker.yaml\n\nUpdate the Kubernetes endpoint\n\nNow our control plane nodes have been created, and we know their IP addresses, we can associate them with the Kubernetes endpoint. Configure your load balancer to route traffic to these nodes, or add A records to your DNS entry for the endpoint, for each control plane node. e.g.\n\nCopy\nhost endpoint.mydomain.com\n\nendpoint.mydomain.com has address 145.40.90.201\n\nendpoint.mydomain.com has address 147.75.109.71\n\nendpoint.mydomain.com has address 145.40.90.177\n\nBootstrap Etcd\n\nSet the endpoints and nodes for talosctl:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\n\nThis only needs to be issued to one control plane node.\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Digital Rebar | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/digital-rebar/",
    "html": "Prerequisites\nCreating a Cluster\nCreate the Machine Configuration Files\nGenerating Base Configurations\nValidate the Configuration Files\nPublishing the Machine Configuration Files\nDownload the boot files\nBootenv BootParams\nBoot the Machines\nBootstrap Etcd\nRetrieve the kubeconfig\nDocumentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nDigital Rebar\nDigital Rebar\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\nPrerequisites\n3 nodes (please see hardware requirements)\nLoadbalancer\nDigital Rebar Server\nTalosctl access (see talosctl setup)\nCreating a Cluster\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes. We assume an existing digital rebar deployment, and some familiarity with iPXE.\n\nWe leave it up to the user to decide if they would like to use static networking, or DHCP. The setup and configuration of DHCP will not be covered.\n\nCreate the Machine Configuration Files\nGenerating Base Configurations\n\nUsing the DNS name of the load balancer, generate the base configuration files for the Talos machines:\n\nCopy\n$ talosctl gen config talos-k8s-metal-tutorial https://<load balancer IP or DNS>:<port>\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\n\nThe loadbalancer is used to distribute the load across multiple controlplane nodes. This isn’t covered in detail, because we assume some loadbalancing knowledge before hand. If you think this should be added to the docs, please create a issue.\n\nAt this point, you can modify the generated configs to your liking. Optionally, you can specify --config-patch with RFC6902 jsonpatch which will be applied during the config generation.\n\nValidate the Configuration Files\nCopy\n$ talosctl validate --config controlplane.yaml --mode metal\n\ncontrolplane.yaml is valid for metal mode\n\n$ talosctl validate --config worker.yaml --mode metal\n\nworker.yaml is valid for metal mode\n\nPublishing the Machine Configuration Files\n\nDigital Rebar has a built-in fileserver, which means we can use this feature to expose the talos configuration files. We will place controlplane.yaml, and worker.yaml into Digital Rebar file server by using the drpcli tools.\n\nCopy the generated files from the step above into your Digital Rebar installation.\n\nCopy\ndrpcli file upload <file>.yaml as <file>.yaml\n\n\nReplacing <file> with controlplane or worker.\n\nDownload the boot files\n\nDownload a recent version of boot.tar.gz from github.\n\nUpload to DRB:\n\nCopy\n$ drpcli isos upload boot.tar.gz as talos.tar.gz\n\n{\n\n  \"Path\": \"talos.tar.gz\",\n\n  \"Size\": 96470072\n\n}\n\n\nWe have some Digital Rebar example files in the Git repo you can use to provision Digital Rebar with drpcli.\n\nTo apply these configs you need to create them, and then apply them as follow:\n\nCopy\n$ drpcli bootenvs create talos\n\n{\n\n  \"Available\": true,\n\n  \"BootParams\": \"\",\n\n  \"Bundle\": \"\",\n\n  \"Description\": \"\",\n\n  \"Documentation\": \"\",\n\n  \"Endpoint\": \"\",\n\n  \"Errors\": [],\n\n  \"Initrds\": [],\n\n  \"Kernel\": \"\",\n\n  \"Meta\": {},\n\n  \"Name\": \"talos\",\n\n  \"OS\": {\n\n    \"Codename\": \"\",\n\n    \"Family\": \"\",\n\n    \"IsoFile\": \"\",\n\n    \"IsoSha256\": \"\",\n\n    \"IsoUrl\": \"\",\n\n    \"Name\": \"\",\n\n    \"SupportedArchitectures\": {},\n\n    \"Version\": \"\"\n\n  },\n\n  \"OnlyUnknown\": false,\n\n  \"OptionalParams\": [],\n\n  \"ReadOnly\": false,\n\n  \"RequiredParams\": [],\n\n  \"Templates\": [],\n\n  \"Validated\": true\n\n}\n\nCopy\ndrpcli bootenvs update talos - < bootenv.yaml\n\n\nYou need to do this for all files in the example directory. If you don’t have access to the drpcli tools you can also use the webinterface.\n\nIt’s important to have a corresponding SHA256 hash matching the boot.tar.gz\n\nBootenv BootParams\n\nWe’re using some of Digital Rebar built in templating to make sure the machine gets the correct role assigned.\n\ntalos.platform=metal talos.config={{ .ProvisionerURL }}/files/{{.Param \\\"talos/role\\\"}}.yaml\"\n\nThis is why we also include a params.yaml in the example directory to make sure the role is set to one of the following:\n\ncontrolplane\nworker\n\nThe {{.Param \\\"talos/role\\\"}} then gets populated with one of the above roles.\n\nBoot the Machines\n\nIn the UI of Digital Rebar you need to select the machines you want to provision. Once selected, you need to assign to following:\n\nProfile\nWorkflow\n\nThis will provision the Stage and Bootenv with the talos values. Once this is done, you can boot the machine.\n\nBootstrap Etcd\n\nTo configure talosctl we will need the first control plane node’s IP:\n\nSet the endpoints and nodes:\n\nCopy\ntalosctl --talosconfig talosconfig config endpoint <control plane 1 IP>\n\ntalosctl --talosconfig talosconfig config node <control plane 1 IP>\n\n\nBootstrap etcd:\n\nCopy\ntalosctl --talosconfig talosconfig bootstrap\n\nRetrieve the kubeconfig\n\nAt this point we can retrieve the admin kubeconfig by running:\n\nCopy\ntalosctl --talosconfig talosconfig kubeconfig .\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Bare Metal Platforms | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nBare Metal Platforms\nBare Metal Platforms\nInstallation of Talos Linux on various bare-metal platforms.\nDigital Rebar\n\nIn this guide we will create an Kubernetes cluster with 1 worker node, and 2 controlplane nodes using an existing digital rebar deployment.\n\nEquinix Metal\n\nCreating Talos clusters with Equinix Metal.\n\nISO\n\nBooting Talos on bare-metal with ISO.\n\nMatchbox\n\nIn this guide we will create an HA Kubernetes cluster with 3 worker nodes using an existing load balancer and matchbox deployment.\n\nNetwork Configuration\n\nIn this guide we will describe how network can be configured on bare-metal platforms.\n\nPXE\n\nBooting Talos over the network on bare-metal with PXE.\n\nSecureBoot\n\nBooting Talos in SecureBoot mode on UEFI platforms.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Installation | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/install/",
    "html": "Documentation\nTalos Linux Guides\nInstallation\nInstallation\nHow to install Talos Linux on various platforms\nBare Metal Platforms\n\nInstallation of Talos Linux on various bare-metal platforms.\n\nVirtualized Platforms\n\nInstallation of Talos Linux for virtualization platforms.\n\nCloud Platforms\n\nInstallation of Talos Linux on many cloud platforms.\n\nLocal Platforms\n\nInstallation of Talos Linux on local platforms, helpful for testing and developing.\n\nSingle Board Computers\n\nInstallation of Talos Linux on single-board computers.\n\nBoot Assets\n\nCreating customized Talos boot assets, disk images, ISO and installer images.\n\nOmni SaaS\n\nOmni is a project created by the Talos team that has native support for Talos Linux.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Talos Linux Guides | Talos Linux",
    "url": "https://www.talos.dev/v1.6/talos-guides/",
    "html": "Documentation\nTalos Linux Guides\nTalos Linux Guides\nDocumentation on how to manage Talos Linux\nInstallation\n\nHow to install Talos Linux on various platforms\n\nConfiguration\n\nGuides on how to configure Talos Linux machines\n\nHow Tos\n\nHow to guide for common tasks in Talos Linux\n\nNetwork\n\nSet up networking layers for Talos Linux\n\nDiscovery Service\n\nTalos Linux Node discovery services\n\nInteractive Dashboard\n\nA tool to inspect the running Talos machine state on the physical video console.\n\nResetting a Machine\n\nSteps on how to reset a Talos Linux machine to a clean state.\n\nUpgrading Talos Linux\n\nGuide to upgrading a Talos Linux machine.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Troubleshooting | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/troubleshooting/",
    "html": "Network Configuration\nTalos API\nFirewall Issues\nClient Configuration Issues\nConflict on Kubernetes and Host Subnets\nWrong Endpoints\nTCP Loadbalancer\nSystem Requirements\nRunning Health Checks\nGathering Logs\nDiscovery and Cluster Membership\nSome Members are Missing\nDuplicate Members\nRemoved Members are Still Present\netcd Issues\nAll etcd Services are Stuck in Pre State\nSome etcd Services are Stuck in Pre State\netcd Reports and Alarm\netcd Quorum is Lost\nOther Issues\nkubelet and Kubernetes Node Issues\nkubelet is not running\nTalos Complains about Node Not Found\nkubectl get nodes Reports Wrong Internal IP\nkubectl get nodes Reports Wrong External IP\nkubectl get nodes Reports Wrong Node Name\nNode Is Not Ready\nDuplicate/Stale Nodes\nTalos Complains about Certificate Errors on kubelet API\nKubernetes Control Plane\nControl Plane Static Pods\nControl Plane Pod Status\nKubernetes Bootstrap Manifests\nOther Control Plane Components\nKubernetes API\nKubernetes Control Plane Endpoint\nMiscellaneous\nChecking Controller Runtime Logs\nDocumentation\nIntroduction\nTroubleshooting\nTroubleshooting\nTroubleshoot control plane and other failures for Talos Linux clusters.\n\nIn this guide we assume that Talos is configured with default features enabled, such as Discovery Service and KubePrism. If these features are disabled, some of the troubleshooting steps may not apply or may need to be adjusted.\n\nThis guide is structured so that it can be followed step-by-step, skip sections which are not relevant to your issue.\n\nNetwork Configuration\n\nAs Talos Linux is an API-based operating system, it is important to have networking configured so that the API can be accessed. Some information can be gathered from the Interactive Dashboard which is available on the machine console.\n\nWhen running in the cloud the networking should be configured automatically. Whereas when running on bare-metal it may need more specific configuration, see networking metal configuration guide.\n\nTalos API\n\nThe Talos API runs on port 50000. Control plane nodes should always serve the Talos API, while worker nodes require access to the control plane nodes to issue TLS certificates for the workers.\n\nFirewall Issues\n\nMake sure that the firewall is not blocking port 50000, and communication on ports 50000/50001 inside the cluster.\n\nClient Configuration Issues\n\nMake sure to use correct talosconfig client configuration file matching your cluster. See getting started for more information.\n\nThe most common issue is that talosctl gen config writes talosconfig to the file in the current directory, while talosctl by default picks up the configuration from the default location (~/.talos/config). The path to the configuration file can be specified with --talosconfig flag to talosctl.\n\nConflict on Kubernetes and Host Subnets\n\nIf talosctl returns an error saying that certificate IPs are empty, it might be due to a conflict between Kubernetes and host subnets. The Talos API runs on the host network, but it automatically excludes Kubernetes pod & network subnets from the useable set of addresses.\n\nTalos default machine configuration specifies the following Kubernetes pod and subnet IPv4 CIDRs: 10.244.0.0/16 and 10.96.0.0/12. If the host network is configured with one of these subnets, change the machine configuration to use a different subnet.\n\nWrong Endpoints\n\nThe talosctl CLI connects to the Talos API via the specified endpoints, which should be a list of control plane machine addresses. The client will automatically retry on other endpoints if there are unavailable endpoints.\n\nWorker nodes should not be used as the endpoint, as they are not able to forward request to other nodes.\n\nThe VIP should never be used as Talos API endpoint.\n\nTCP Loadbalancer\n\nWhen using a TCP loadbalancer, make sure the loadbalancer endpoint is included in the .machine.certSANs list in the machine configuration.\n\nSystem Requirements\n\nIf minimum system requirements are not met, this might manifest itself in various ways, such as random failures when starting services, or failures to pull images from the container registry.\n\nRunning Health Checks\n\nTalos Linux provides a set of basic health checks with talosctl health command which can be used to check the health of the cluster.\n\nIn the default mode, talosctl health uses information from the discovery to get the information about cluster members. This can be overridden with command line flags --control-plane-nodes and --worker-nodes.\n\nGathering Logs\n\nWhile the logs and state of the system can be queried via the Talos API, it is often useful to gather the logs from all nodes in the cluster, and analyze them offline. The talosctl support command can be used to gather logs and other information from the nodes specified with --nodes flag (multiple nodes are supported).\n\nDiscovery and Cluster Membership\n\nTalos Linux uses Discovery Service to discover other nodes in the cluster.\n\nThe list of members on each machine should be consistent: talosctl -n <IP> get members.\n\nSome Members are Missing\n\nEnsure connectivity to the discovery service (default is discovery.talos.dev:443), and that the discovery registry is not disabled.\n\nDuplicate Members\n\nDon’t use same base secrets to generate machine configuration for multiple clusters, as some secrets are used to identify members of the same cluster. So if the same machine configuration (or secrets) are used to repeatedly create and destroy clusters, the discovery service will see the same nodes as members of different clusters.\n\nRemoved Members are Still Present\n\nTalos Linux removes itself from the discovery service when it is reset. If the machine was not reset, it might show up as a member of the cluster for the maximum TTL of the discovery service (30 minutes), and after that it will be automatically removed.\n\netcd Issues\n\netcd is the distributed key-value store used by Kubernetes to store its state. Talos Linux provides automation to manage etcd members running on control plane nodes. If etcd is not healthy, the Kubernetes API server will not be able to function correctly.\n\nIt is always recommended to run an odd number of etcd members, as with 3 or more members it provides fault tolerance for less than quorum member failures.\n\nCommon troubleshooting steps:\n\ncheck etcd service state with talosctl -n IP service etcd for each control plane node\ncheck etcd membership on each control plane node with talosctl -n IP etcd member list\ncheck etcd logs with talosctl -n IP logs etcd\ncheck etcd alarms with talosctl -n IP etcd alarm list\nAll etcd Services are Stuck in Pre State\n\nMake sure that a single member was bootstrapped.\n\nCheck that the machine is able to pull the etcd container image, check talosctl dmesg for messages starting with retrying: prefix.\n\nSome etcd Services are Stuck in Pre State\n\nMake sure traffic is not blocked on port 2380 between controlplane nodes.\n\nCheck that etcd quorum is not lost.\n\nCheck that all control plane nodes are reported in talosctl get members output.\n\netcd Reports and Alarm\n\nSee etcd maintenance guide.\n\netcd Quorum is Lost\n\nSee disaster recovery guide.\n\nOther Issues\n\netcd will only run on control plane nodes. If a node is designated as a worker node, you should not expect etcd to be running on it.\n\nWhen a node boots for the first time, the etcd data directory (/var/lib/etcd) is empty, and it will only be populated when etcd is launched.\n\nIf the etcd service is crashing and restarting, check its logs with talosctl -n <IP> logs etcd. The most common reasons for crashes are:\n\nwrong arguments passed via extraArgs in the configuration;\nbooting Talos on non-empty disk with an existing Talos installation, /var/lib/etcd contains data from the old cluster.\nkubelet and Kubernetes Node Issues\n\nThe kubelet service should be running on all Talos nodes, and it is responsible for running Kubernetes pods, static pods (including control plane components), and registering the node with the Kubernetes API server.\n\nIf the kubelet doesn’t run on a control plane node, it will block the control plane components from starting.\n\nThe node will not be registered in Kubernetes until the Kubernetes API server is up and initial Kubernetes manifests are applied.\n\nkubelet is not running\n\nCheck that kubelet image is available (talosctl image ls --namespace system).\n\nCheck kubelet logs with talosctl -n IP logs kubelet for startup errors:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure kubelet extra arguments and extra configuration supplied with Talos machine configuration is valid\nTalos Complains about Node Not Found\n\nkubelet hasn’t yet registered the node with the Kubernetes API server, this is expected during initial cluster bootstrap, the error will go away. If the message persists, check Kubernetes API health.\n\nThe Kubernetes controller manager (kube-controller-manager) is responsible for monitoring the certificate signing requests (CSRs) and issuing certificates for each of them. The kubelet is responsible for generating and submitting the CSRs for its associated node.\n\nThe state of any CSRs can be checked with kubectl get csr:\n\nCopy\n$ kubectl get csr\n\nNAME        AGE   SIGNERNAME                                    REQUESTOR                 CONDITION\n\ncsr-jcn9j   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-p6b9q   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-sw6rm   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\ncsr-vlghg   14m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q9pyzr   Approved,Issued\n\nkubectl get nodes Reports Wrong Internal IP\n\nConfigure the correct internal IP address with .machine.kubelet.nodeIP\n\nkubectl get nodes Reports Wrong External IP\n\nTalos Linux doesn’t manage the external IP, it is managed with the Kubernetes Cloud Controller Manager.\n\nkubectl get nodes Reports Wrong Node Name\n\nBy default, the Kubernetes node name is derived from the hostname. Update the hostname using the machine configuration, cloud configuration, or via DHCP server.\n\nNode Is Not Ready\n\nA Node in Kubernetes is marked as Ready only once its CNI is up. It takes a minute or two for the CNI images to be pulled and for the CNI to start. If the node is stuck in this state for too long, check CNI pods and logs with kubectl. Usually, CNI-related resources are created in kube-system namespace.\n\nFor example, for the default Talos Flannel CNI:\n\nCopy\n$ kubectl -n kube-system get pods\n\nNAME                                             READY   STATUS    RESTARTS   AGE\n\n...\n\nkube-flannel-25drx                               1/1     Running   0          23m\n\nkube-flannel-8lmb6                               1/1     Running   0          23m\n\nkube-flannel-gl7nx                               1/1     Running   0          23m\n\nkube-flannel-jknt9                               1/1     Running   0          23m\n\n...\n\nDuplicate/Stale Nodes\n\nTalos Linux doesn’t remove Kubernetes nodes automatically, so if a node is removed from the cluster, it will still be present in Kubernetes. Remove the node from Kubernetes with kubectl delete node <node-name>.\n\nTalos Complains about Certificate Errors on kubelet API\n\nThis error might appear during initial cluster bootstrap, and it will go away once the Kubernetes API server is up and the node is registered.\n\nBy default configuration, kubelet issues a self-signed server certificate, but when rotate-server-certificates feature is enabled, kubelet issues its certificate using kube-apiserver. Make sure the kubelet CSR is approved by the Kubernetes API server.\n\nIn either case, this error is not critical, as it only affects reporting of the pod status to Talos Linux.\n\nKubernetes Control Plane\n\nThe Kubernetes control plane consists of the following components:\n\nkube-apiserver - the Kubernetes API server\nkube-controller-manager - the Kubernetes controller manager\nkube-scheduler - the Kubernetes scheduler\n\nOptionally, kube-proxy runs as a DaemonSet to provide pod-to-service communication.\n\ncoredns provides name resolution for the cluster.\n\nCNI is not part of the control plane, but it is required for Kubernetes pods using pod networking.\n\nTroubleshooting should always start with kube-apiserver, and then proceed to other components.\n\nTalos Linux configures kube-apiserver to talk to the etcd running on the same node, so etcd must be healthy before kube-apiserver can start. The kube-controller-manager and kube-scheduler are configured to talk to the kube-apiserver on the same node, so they will not start until kube-apiserver is healthy.\n\nControl Plane Static Pods\n\nTalos should generate the static pod definitions for the Kubernetes control plane as resources:\n\nCopy\n$ talosctl -n <IP> get staticpods\n\nNODE         NAMESPACE   TYPE        ID                        VERSION\n\n172.20.0.2   k8s         StaticPod   kube-apiserver            1\n\n172.20.0.2   k8s         StaticPod   kube-controller-manager   1\n\n172.20.0.2   k8s         StaticPod   kube-scheduler            1\n\n\nTalos should report that the static pod definitions are rendered for the kubelet:\n\nCopy\n$ talosctl -n <IP> dmesg | grep 'rendered new'\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.550527204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-apiserver\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.552186204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-controller-manager\"}\n\n172.20.0.2: user: warning: [2023-04-26T19:17:52.554607204Z]: [talos] rendered new static pod {\"component\": \"controller-runtime\", \"controller\": \"k8s.StaticPodServerController\", \"id\": \"kube-scheduler\"}\n\n\nIf the static pod definitions are not rendered, check etcd and kubelet service health (see above) and the controller runtime logs (talosctl logs controller-runtime).\n\nControl Plane Pod Status\n\nInitially the kube-apiserver component will not be running, and it takes some time before it becomes fully up during bootstrap (image should be pulled from the Internet, etc.)\n\nThe status of the control plane components on each of the control plane nodes can be checked with talosctl containers -k:\n\nCopy\n$ talosctl -n <IP> containers --kubernetes\n\nNODE         NAMESPACE   ID                                                                                            IMAGE                                               PID    STATUS\n\n172.20.0.2   k8s.io      kube-system/kube-apiserver-talos-default-controlplane-1                                       registry.k8s.io/pause:3.2                                2539   SANDBOX_READY\n\n172.20.0.2   k8s.io      └─ kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271        registry.k8s.io/kube-apiserver:v1.29.0 2572   CONTAINER_RUNNING\n\n\nThe logs of the control plane components can be checked with talosctl logs --kubernetes (or with -k as a shorthand):\n\nCopy\ntalosctl -n <IP> logs -k kube-system/kube-apiserver-talos-default-controlplane-1:kube-apiserver:51c3aad7a271\n\n\nIf the control plane component reports error on startup, check that:\n\nmake sure Kubernetes version is supported with this Talos release\nmake sure extra arguments and extra configuration supplied with Talos machine configuration is valid\nKubernetes Bootstrap Manifests\n\nAs part of the bootstrap process, Talos injects bootstrap manifests into Kubernetes API server. There are two kinds of these manifests: system manifests built-in into Talos and extra manifests downloaded (custom CNI, extra manifests in the machine config):\n\nCopy\n$ talosctl -n <IP> get manifests\n\nNODE         NAMESPACE      TYPE       ID                               VERSION\n\n172.20.0.2   controlplane   Manifest   00-kubelet-bootstrapping-token   1\n\n172.20.0.2   controlplane   Manifest   01-csr-approver-role-binding     1\n\n172.20.0.2   controlplane   Manifest   01-csr-node-bootstrap            1\n\n172.20.0.2   controlplane   Manifest   01-csr-renewal-role-binding      1\n\n172.20.0.2   controlplane   Manifest   02-kube-system-sa-role-binding   1\n\n172.20.0.2   controlplane   Manifest   03-default-pod-security-policy   1\n\n172.20.0.2   controlplane   Manifest   05-https://docs.projectcalico.org/manifests/calico.yaml   1\n\n172.20.0.2   controlplane   Manifest   10-kube-proxy                    1\n\n172.20.0.2   controlplane   Manifest   11-core-dns                      1\n\n172.20.0.2   controlplane   Manifest   11-core-dns-svc                  1\n\n172.20.0.2   controlplane   Manifest   11-kube-config-in-cluster        1\n\n\nDetails of each manifest can be queried by adding -o yaml:\n\nCopy\n$ talosctl -n <IP> get manifests 01-csr-approver-role-binding --namespace=controlplane -o yaml\n\nnode: 172.20.0.2\n\nmetadata:\n\n    namespace: controlplane\n\n    type: Manifests.kubernetes.talos.dev\n\n    id: 01-csr-approver-role-binding\n\n    version: 1\n\n    phase: running\n\nspec:\n\n    - apiVersion: rbac.authorization.k8s.io/v1\n\n      kind: ClusterRoleBinding\n\n      metadata:\n\n        name: system-bootstrap-approve-node-client-csr\n\n      roleRef:\n\n        apiGroup: rbac.authorization.k8s.io\n\n        kind: ClusterRole\n\n        name: system:certificates.k8s.io:certificatesigningrequests:nodeclient\n\n      subjects:\n\n        - apiGroup: rbac.authorization.k8s.io\n\n          kind: Group\n\n          name: system:bootstrappers\n\nOther Control Plane Components\n\nOnce the Kubernetes API server is up, other control plane components issues can be troubleshooted with kubectl:\n\nCopy\nkubectl get nodes -o wide\n\nkubectl get pods -o wide --all-namespaces\n\nkubectl describe pod -n NAMESPACE POD\n\nkubectl logs -n NAMESPACE POD\n\nKubernetes API\n\nThe Kubernetes API client configuration (kubeconfig) can be retrieved using Talos API with talosctl -n <IP> kubeconfig command. Talos Linux mostly doesn’t depend on the Kubernetes API endpoint for the cluster, but Kubernetes API endpoint should be configured correctly for external access to the cluster.\n\nKubernetes Control Plane Endpoint\n\nThe Kubernetes control plane endpoint is the single canonical URL by which the Kubernetes API is accessed. Especially with high-availability (HA) control planes, this endpoint may point to a load balancer or a DNS name which may have multiple A and AAAA records.\n\nLike Talos’ own API, the Kubernetes API uses mutual TLS, client certs, and a common Certificate Authority (CA). Unlike general-purpose websites, there is no need for an upstream CA, so tools such as cert-manager, Let’s Encrypt, or products such as validated TLS certificates are not required. Encryption, however, is, and hence the URL scheme will always be https://.\n\nBy default, the Kubernetes API server in Talos runs on port 6443. As such, the control plane endpoint URLs for Talos will almost always be of the form https://endpoint:6443. (The port, since it is not the https default of 443 is required.) The endpoint above may be a DNS name or IP address, but it should be directed to the set of all controlplane nodes, as opposed to a single one.\n\nAs mentioned above, this can be achieved by a number of strategies, including:\n\nan external load balancer\nDNS records\nTalos-builtin shared IP (VIP)\nBGP peering of a shared IP (such as with kube-vip)\n\nUsing a DNS name here is a good idea, since it allows any other option, while offering a layer of abstraction. It allows the underlying IP addresses to change without impacting the canonical URL.\n\nUnlike most services in Kubernetes, the API server runs with host networking, meaning that it shares the network namespace with the host. This means you can use the IP address(es) of the host to refer to the Kubernetes API server.\n\nFor availability of the API, it is important that any load balancer be aware of the health of the backend API servers, to minimize disruptions during common node operations like reboots and upgrades.\n\nMiscellaneous\nChecking Controller Runtime Logs\n\nTalos runs a set of controllers which operate on resources to build and support machine operations.\n\nSome debugging information can be queried from the controller logs with talosctl logs controller-runtime:\n\nCopy\ntalosctl -n <IP> logs controller-runtime\n\n\nControllers continuously run a reconcile loop, so at any time, they may be starting, failing, or restarting. This is expected behavior.\n\nIf there are no new messages in the controller-runtime log, it means that the controllers have successfully finished reconciling, and that the current system state is the desired system state.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Support Matrix | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/support-matrix/",
    "html": "Platform Tiers\nTier 1\nTier 2\nTier 3\nDocumentation\nIntroduction\nSupport Matrix\nSupport Matrix\nTable of supported Talos Linux versions and respective platforms.\nTalos Version\t1.6\t1.5\nRelease Date\t2023-12-15\t2023-08-17 (1.5.0)\nEnd of Community Support\t1.7.0 release (2024-04-15, TBD)\t1.6.0 release (2023-12-15)\nEnterprise Support\toffered by Sidero Labs Inc.\toffered by Sidero Labs Inc.\nKubernetes\t1.29, 1.28, 1.27, 1.26, 1.25, 1.24\t1.28, 1.27, 1.26\nArchitecture\tamd64, arm64\tamd64, arm64\nPlatforms\t\t\n- cloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\tAWS, GCP, Azure, Digital Ocean, Exoscale, Hetzner, OpenStack, Oracle Cloud, Scaleway, Vultr, Upcloud\n- bare metal\tx86: BIOS, UEFI, SecureBoot; arm64: UEFI, SecureBoot; boot: ISO, PXE, disk image\tx86: BIOS, UEFI; arm64: UEFI; boot: ISO, PXE, disk image\n- virtualized\tVMware, Hyper-V, KVM, Proxmox, Xen\tVMware, Hyper-V, KVM, Proxmox, Xen\n- SBCs\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\tBanana Pi M64, Jetson Nano, Libre Computer Board ALL-H3-CC, Nano Pi R4S, Pine64, Pine64 Rock64, Radxa ROCK Pi 4c, Raspberry Pi 4B, Raspberry Pi Compute Module 4\n- local\tDocker, QEMU\tDocker, QEMU\nCluster API\t\t\nCAPI Bootstrap Provider Talos\t>= 0.6.3\t>= 0.6.1\nCAPI Control Plane Provider Talos\t>= 0.5.4\t>= 0.5.2\nSidero\t>= 0.6.2\t>= 0.6.0\nPlatform Tiers\nTier 1: Automated tests, high-priority fixes.\nTier 2: Tested from time to time, medium-priority bugfixes.\nTier 3: Not tested by core Talos team, community tested.\nTier 1\nMetal\nAWS\nGCP\nTier 2\nAzure\nDigital Ocean\nOpenStack\nVMWare\nTier 3\nExoscale\nHetzner\nnocloud\nOracle Cloud\nScaleway\nVultr\nUpcloud\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "What's New in Talos 1.6.0 | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/what-is-new/",
    "html": "Breaking Changes\nLinux Firmware\nNetwork Device Selectors\ntalosctl images command\n.persist Machine Configuration Option\nNew Features\nKubernetes n-5 Version Support\nOAuth2 Machine Config Flow\nIngress Firewall\nImprovements\nComponent Updates\nExtension Services\nFlannel Configuration\nKernel Arguments\nkube-scheduler Configuration\nKubernetes Node Taint Configuration\nKubelet Credential Provider Configuration\nKubePrism\nSysctl\nUser Disks\nPacket Capture\nMemory Usage and Performance\nDocumentation\nIntroduction\nWhat's New in Talos 1.6.0\nWhat's New in Talos 1.6.0\nList of new and shiny features in Talos Linux.\n\nSee also upgrade notes for important changes.\n\nBreaking Changes\nLinux Firmware\n\nStarting with Talos 1.6, Linux firmware is not included in the default initramfs.\n\nUsers that need Linux firmware can pull them as an extension during install time using the Image Factory service. If the initial boot requires firmware, a custom ISO can be built with the firmware included using the Image Factory service or using the imager. This also ensures that the linux-firmware is not tied to a specific Talos version.\n\nThe list of firmware packages which were removed from the default initramfs and are now available as extensions:\n\nbnx2 and bnx2x firmware (Broadcom NetXtreme II)\nIntel ICE firmware (Intel(R) Ethernet Controller 800 Series)\nNetwork Device Selectors\n\nPreviously, network device selectors only matched the first link, now the configuration is applied to all matching links.\n\ntalosctl images command\n\nThe command images deprecated in Talos 1.5 was removed, please use talosctl images default instead.\n\n.persist Machine Configuration Option\n\nThe option .persist deprecated in Talos 1.5 was removed, the machine configuration is always persisted.\n\nNew Features\nKubernetes n-5 Version Support\n\nTalos Linux starting with version 1.6 supports the latest Kubernetes n-5 versions, for release 1.6.0 this means support for Kubernetes versions 1.24-1.29. This allows users to make it easier to upgrade to new Talos Linux versions without having to upgrade Kubernetes at the same time.\n\nSee Kubernetes release support for the list of supported versions by Kubernetes project.\n\nOAuth2 Machine Config Flow\n\nTalos Linux when running on the metal platform can be configured to authenticate the machine configuration download using OAuth2 device flow.\n\nIngress Firewall\n\nTalos Linux now supports configuring the ingress firewall rules.\n\nImprovements\nComponent Updates\nLinux: 6.1.67\nKubernetes: 1.29.0\ncontainerd: 1.7.10\nrunc: 1.1.10\netcd: 3.5.11\nCoreDNS: 1.11.1\nFlannel: 0.23.0\n\nTalos is built with Go 1.21.5.\n\nExtension Services\n\nTalos now starts Extension Services early in the boot process, this allows guest agents packaged as extension services to be started in maintenance mode.\n\nFlannel Configuration\n\nTalos Linux now supports customizing default Flannel manifest with extra arguments for flanneld:\n\nCopy\ncluster:\n\n  network:\n\n    cni:\n\n      flannel:\n\n        extraArgs:\n\n          - --iface-can-reach=192.168.1.1\n\nKernel Arguments\n\nTalos and Imager now supports dropping kernel arguments specified in .machine.install.extraKernelArgs or as --extra-kernel-arg to imager. Any kernel argument that starts with a - is dropped. Kernel arguments to be dropped can be specified either as -<key> which would remove all arguments that start with <key> or as -<key>=<value> which would remove the exact argument.\n\nFor example, console=ttyS0 can be dropped by specifying -console=ttyS0 as an extra argument.\n\nkube-scheduler Configuration\n\nTalos now supports specifying the kube-scheduler configuration in the Talos configuration file. It can be set under cluster.scheduler.config and kube-scheduler will be automatically configured to with the correct flags.\n\nKubernetes Node Taint Configuration\n\nSimilar to machine.nodeLabels Talos Linux now provides machine.nodeTaints machine configuration field to configure Kubernetes Node taints.\n\nKubelet Credential Provider Configuration\n\nTalos now supports specifying the kubelet credential provider configuration in the Talos configuration file. It can be set under machine.kubelet.credentialProviderConfig and kubelet will be automatically configured to with the correct flags. The credential binaries are expected to be present under /usr/local/lib/kubelet/credentialproviders. Talos System Extensions can be used to install the credential binaries.\n\nKubePrism\n\nKubePrism is enabled by default on port 7445.\n\nSysctl\n\nTalos now handles sysctl/sysfs key names in line with sysctl.conf(5):\n\nif the first separator is ‘/’, no conversion is done\nif the first separator is ‘.’, dots and slashes are remapped\n\nExample (both sysctls are equivalent):\n\nCopy\nmachine:\n\n  sysctls:\n\n    net/ipv6/conf/eth0.100/disable_ipv6: \"1\"\n\n    net.ipv6.conf.eth0/100.disable_ipv6: \"1\"\n\nUser Disks\n\nTalos Linux now supports specifying user disks in .machine.disks machine configuration links via udev symlinks, e.g. /dev/disk/by-id/XXXX.\n\nPacket Capture\n\nTalos Linux provides more performant implementation server-side for the packet capture API (talosctl pcap CLI).\n\nMemory Usage and Performance\n\nTalos Linux core components now use less memory and start faster.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "System Requirements | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/system-requirements/",
    "html": "Minimum Requirements\nRecommended\nStorage\nDocumentation\nIntroduction\nSystem Requirements\nSystem Requirements\nHardware requirements for running Talos Linux.\nMinimum Requirements\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t2 GiB\t2\t10 GiB\nWorker\t1 GiB\t1\t10 GiB\nRecommended\nRole\tMemory\tCores\tSystem Disk\nControl Plane\t4 GiB\t4\t100 GiB\nWorker\t2 GiB\t2\t100 GiB\n\nThese requirements are similar to that of Kubernetes.\n\nStorage\n\nTalos Linux itself only requires less than 100 MB of disk space, but the EPHEMERAL partition is used to store pulled images, container work directories, and so on. Thus a minimum is 10 GiB of disk space is required. 100 GiB is desired. Note, however, that because Talos Linux assumes complete control of the disk it is installed on, so that it can control the partition table for image based upgrades, you cannot partition the rest of the disk for use by workloads.\n\nThus it is recommended to install Talos Linux on a small, dedicated disk - using a Terabyte sized SSD for the Talos install disk would be wasteful. Sidero Labs recommends having separate disks (apart from the Talos install disk) to be used for storage.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Production Clusters | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/prodnotes/",
    "html": "Acquire the installation image\nAlternative Booting\nControl plane nodes\nDecide the Kubernetes Endpoint\nDedicated Load-balancer\nLayer 2 VIP Shared IP\nDNS records\nMultihoming\nMultihoming and etcd\nMultihoming and kubelets\nExample\nLoad balancing the Talos API\nConfigure Talos\nSeparating out secrets\nCustomizing Machine Configuration\nMachine Configs as Templates\nApply Configuration while validating the node identity\nFurther details about talosctl, endpoints and nodes\nEndpoints\nNodes\nDefault configuration file\nKubernetes Bootstrap\nDocumentation\nIntroduction\nProduction Clusters\nProduction Clusters\nRecommendations for setting up a Talos Linux cluster in production.\n\nThis document explains recommendations for running Talos Linux in production.\n\nAcquire the installation image\nAlternative Booting\n\nFor network booting and self-built media, you can use the published kernel and initramfs images:\n\nX86: vmlinuz-amd64 initramfs-amd64.xz\nARM64: vmlinuz-arm64 initramfs-arm64.xz\n\nNote that to use alternate booting, there are a number of required kernel parameters. Please see the kernel docs for more information.\n\nControl plane nodes\n\nFor a production, highly available Kubernetes cluster, it is recommended to use three control plane nodes. Using five nodes can provide greater fault tolerance, but imposes more replication overhead and can result in worse performance.\n\nBoot all three control plane nodes at this point. They will boot Talos Linux, and come up in maintenance mode, awaiting a configuration.\n\nDecide the Kubernetes Endpoint\n\nThe Kubernetes API Server endpoint, in order to be highly available, should be configured in a way that uses all available control plane nodes. There are three common ways to do this: using a load-balancer, using Talos Linux’s built in VIP functionality, or using multiple DNS records.\n\nDedicated Load-balancer\n\nIf you are using a cloud provider or have your own load-balancer (such as HAProxy, Nginx reverse proxy, or an F5 load-balancer), a dedicated load balancer is a natural choice. Create an appropriate frontend for the endpoint, listening on TCP port 6443, and point the backends at the addresses of each of the Talos control plane nodes. Your Kubernetes endpoint will be the IP address or DNS name of the load balancer front end, with the port appended (e.g. https://myK8s.mydomain.io:6443).\n\nNote: an HTTP load balancer can’t be used, as Kubernetes API server does TLS termination and mutual TLS authentication.\n\nLayer 2 VIP Shared IP\n\nTalos has integrated support for serving Kubernetes from a shared/virtual IP address. This requires Layer 2 connectivity between control plane nodes.\n\nChoose an unused IP address on the same subnet as the control plane nodes for the VIP. For instance, if your control plane node IPs are:\n\n192.168.0.10\n192.168.0.11\n192.168.0.12\n\nyou could choose the IP 192.168.0.15 as your VIP IP address. (Make sure that 192.168.0.15 is not used by any other machine and is excluded from DHCP ranges.)\n\nOnce chosen, form the full HTTPS URL from this IP:\n\nhttps://192.168.0.15:6443\n\n\nIf you create a DNS record for this IP, note you will need to use the IP address itself, not the DNS name, to configure the shared IP (machine.network.interfaces[].vip.ip) in the Talos configuration.\n\nAfter the machine configurations are generated, you will want to edit the controlplane.yaml file to activate the VIP:\n\nCopy\nmachine:\n\n    network:\n\n     interfaces:\n\n      - interface: enp2s0\n\n        dhcp: true\n\n        vip:\n\n          ip: 192.168.0.15\n\n\nFor more information about using a shared IP, see the related Guide\n\nDNS records\n\nAdd multiple A or AAAA records (one for each control plane node) to a DNS name.\n\nFor instance, you could add:\n\nkube.cluster1.mydomain.com  IN  A  192.168.0.10\nkube.cluster1.mydomain.com  IN  A  192.168.0.11\nkube.cluster1.mydomain.com  IN  A  192.168.0.12\n\n\nwhere the IP addresses are those of the control plane nodes.\n\nThen, your endpoint would be:\n\nhttps://kube.cluster1.mydomain.com:6443\n\nMultihoming\n\nIf your machines are multihomed, i.e., they have more than one IPv4 and/or IPv6 addresss other than loopback, then additional configuration is required. A point to note is that the machines may become multihomed via privileged workloads.\n\nMultihoming and etcd\n\nThe etcd cluster needs to establish a mesh of connections among the members. It is done using the so-called advertised address - each node learns the others’ addresses as they are advertised. It is crucial that these IP addresses are stable, i.e., that each node always advertises the same IP address. Moreover, it is beneficial to control them to establish the correct routes between the members and, e.g., avoid congested paths. In Talos, these addresses are controlled using the cluster.etcd.advertisedSubnets configuration key.\n\nMultihoming and kubelets\n\nStable IP addressing for kubelets (i.e., nodeIP) is not strictly necessary but highly recommended as it ensures that, e.g., kube-proxy and CNI routing take the desired routes. Analogously to etcd, for kubelets this is controlled via machine.kubelet.nodeIP.validSubnets.\n\nExample\n\nLet’s assume that we have a cluster with two networks:\n\npublic network\nprivate network 192.168.0.0/16\n\nWe want to use the private network for etcd and kubelet communication:\n\nCopy\nmachine:\n\n  kubelet:\n\n    nodeIP:\n\n      validSubnets:\n\n        - 192.168.0.0/16\n\n#...\n\ncluster:\n\n  etcd:\n\n    advertisedSubnets: # listenSubnets defaults to advertisedSubnets if not set explicitly\n\n      - 192.168.0.0/16\n\n\nThis way we ensure that the etcd cluster will use the private network for communication and the kubelets will use the private network for communication with the control plane.\n\nLoad balancing the Talos API\n\nThe talosctl tool provides built-in client-side load-balancing across control plane nodes, so usually you do not need to configure a load balancer for the Talos API.\n\nHowever, if the control plane nodes are not directly reachable from the workstation where you run talosctl, then configure a load balancer to forward TCP port 50000 to the control plane nodes.\n\nNote: Because the Talos Linux API uses gRPC and mutual TLS, it cannot be proxied by a HTTP/S proxy, but only by a TCP load balancer.\n\nIf you create a load balancer to forward the Talos API calls, the load balancer IP or hostname will be used as the endpoint for talosctl.\n\nAdd the load balancer IP or hostname to the .machine.certSANs field of the machine configuration file.\n\nDo not use Talos Linux’s built in VIP function for accessing the Talos API. In the event of an error in etcd, the VIP will not function, and you will not be able to access the Talos API to recover.\n\nConfigure Talos\n\nIn many installation methods, a configuration can be passed in on boot.\n\nFor example, Talos can be booted with the talos.config kernel argument set to an HTTP(s) URL from which it should receive its configuration. Where a PXE server is available, this is much more efficient than manually configuring each node. If you do use this method, note that Talos requires a number of other kernel commandline parameters. See required kernel parameters.\n\nSimilarly, if creating EC2 kubernetes clusters, the configuration file can be passed in as --user-data to the aws ec2 run-instances command. See generally the Installation Guide for the platform being deployed.\n\nSeparating out secrets\n\nWhen generating the configuration files for a Talos Linux cluster, it is recommended to start with generating a secrets bundle which should be saved in a secure location. This bundle can be used to generate machine or client configurations at any time:\n\nCopy\ntalosctl gen secrets -o secrets.yaml\n\n\nThe secrets.yaml can also be extracted from the existing controlplane machine configuration with talosctl gen secrets --from-controlplane-config controlplane.yaml -o secrets.yaml command.\n\nNow, we can generate the machine configuration for each node:\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml <cluster-name> <cluster-endpoint>\n\n\nHere, cluster-name is an arbitrary name for the cluster, used in your local client configuration as a label. It should be unique in the configuration on your local workstation.\n\nThe cluster-endpoint is the Kubernetes Endpoint you selected from above. This is the Kubernetes API URL, and it should be a complete URL, with https:// and port. (The default port is 6443, but you may have configured your load balancer to forward a different port.) For example:\n\nCopy\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ngenerating PKI and tokens\n\ncreated controlplane.yaml\n\ncreated worker.yaml\n\ncreated talosconfig\n\nCustomizing Machine Configuration\n\nThe generated machine configuration provides sane defaults for most cases, but can be modified to fit specific needs.\n\nSome machine configuration options are available as flags for the talosctl gen config command, for example setting a specific Kubernetes version:\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml --kubernetes-version 1.25.4 my-cluster https://192.168.64.15:6443\n\n\nOther modifications are done with machine configuration patches. Machine configuration patches can be applied with talosctl gen config command:\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml --config-patch-control-plane @cni.patch my-cluster https://192.168.64.15:6443\n\n\nNote: @cni.patch means that the patch is read from a file named cni.patch.\n\nMachine Configs as Templates\n\nIndividual machines may need different settings: for instance, each may have a different static IP address.\n\nWhen different files are needed for machines of the same type, there are two supported flows:\n\nUse the talosctl gen config command to generate a template, and then patch the template for each machine with talosctl machineconfig patch.\nGenerate each machine configuration file separately with talosctl gen config while applying patches.\n\nFor example, given a machine configuration patch which sets the static machine hostname:\n\nCopy\n# worker1.patch\n\nmachine:\n\n  network:\n\n    hostname: worker1\n\n\nEither of the following commands will generate a worker machine configuration file with the hostname set to worker1:\n\nCopy\n$ talosctl gen config --with-secrets secrets.yaml my-cluster https://192.168.64.15:6443\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n$ talosctl machineconfig patch worker.yaml --patch @worker1.patch --output worker1.yaml\n\nCopy\ntalosctl gen config --with-secrets secrets.yaml --config-patch-worker @worker1.patch --output-types worker -o worker1.yaml my-cluster https://192.168.64.15:6443\n\nApply Configuration while validating the node identity\n\nIf you have console access you can extract the server certificate fingerprint and use it for an additional layer of validation:\n\nCopy\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --cert-fingerprint xA9a1t2dMxB0NJ0qH1pDzilWbA3+DK/DjVbFaJBYheE= \\\n\n    --file cp0.yaml\n\n\nUsing the fingerprint allows you to be sure you are sending the configuration to the correct machine, but is completely optional. After the configuration is applied to a node, it will reboot. Repeat this process for each of the nodes in your cluster.\n\nFurther details about talosctl, endpoints and nodes\nEndpoints\n\nWhen passed multiple endpoints, talosctl will automatically load balance requests to, and fail over between, all endpoints.\n\nYou can pass in --endpoints <IP Address1>,<IP Address2> as a comma separated list of IP/DNS addresses to the current talosctl command. You can also set the endpoints in your talosconfig, by calling talosctl config endpoint <IP Address1> <IP Address2>. Note: these are space separated, not comma separated.\n\nAs an example, if the IP addresses of our control plane nodes are:\n\n192.168.0.2\n192.168.0.3\n192.168.0.4\n\nWe would set those in the talosconfig with:\n\nCopy\n  talosctl --talosconfig=./talosconfig \\\n\n    config endpoint 192.168.0.2 192.168.0.3 192.168.0.4\n\nNodes\n\nThe node is the target you wish to perform the API call on.\n\nIt is possible to set a default set of nodes in the talosconfig file, but our recommendation is to explicitly pass in the node or nodes to be operated on with each talosctl command. For a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nDefault configuration file\n\nYou can reference which configuration file to use directly with the --talosconfig parameter:\n\nCopy\n  talosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 version\n\n\nHowever, talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. This is done with the merge option.\n\nCopy\n  talosctl config merge ./talosconfig\n\n\nThis will merge your new talosconfig into the default configuration file ($XDG_CONFIG_HOME/talos/config.yaml), creating it if necessary. Like Kubernetes, the talosconfig configuration files has multiple “contexts” which correspond to multiple clusters. The <cluster-name> you chose above will be used as the context name.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster by simply calling the bootstrap command against any of your control plane nodes (or the loadbalancer, if used for the Talos API endpoint).:\n\nCopy\n  talosctl bootstrap --nodes 192.168.0.2\n\n\nThe bootstrap operation should only be called ONCE and only on a SINGLE control plane node!\n\nAt this point, Talos will form an etcd cluster, generate all of the core Kubernetes assets, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\nCopy\n  talosctl kubeconfig\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\nCopy\n  talosctl kubeconfig alternative-kubeconfig\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\nCopy\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\nCopy\n  talosctl -n <NODEIP> dashboard\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Getting Started | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/getting-started/",
    "html": "Prerequisites\ntalosctl\nNetwork access\nAcquire the Talos Linux image and boot machines\nAlternative Booting\nDefine the Kubernetes Endpoint\nAccessing the Talos API\nConfigure Talos Linux\nControlplane and Worker\nModifying the Machine configs\nUnderstand talosctl, endpoints and nodes\nEndpoints\nNodes\nApply Configuration\nDefault talosconfig configuration file\nKubernetes Bootstrap\nDocumentation\nIntroduction\nGetting Started\nGetting Started\nA guide to setting up a Talos Linux cluster.\n\nThis document will walk you through installing a simple Talos Cluster with a single control plane node and one or more worker nodes, explaining some of the concepts.\n\nIf this is your first use of Talos Linux, we recommend the Quickstart first, to quickly create a local virtual cluster in containers on your workstation.\n\nFor a production cluster, extra steps are needed - see Production Notes.\n\nRegardless of where you run Talos, the steps to create a Kubernetes cluster are:\n\nboot machines off the Talos Linux image\ndefine the endpoint for the Kubernetes API and generate your machine configurations\nconfigure Talos Linux by applying machine configurations to the machines\nconfigure talosctl\nbootstrap Kubernetes\nPrerequisites\ntalosctl\n\ntalosctl is a CLI tool which interfaces with the Talos API. Talos Linux has no SSH access: talosctl is the tool you use to interact with the operating system on the machines.\n\nInstall talosctl before continuing:\n\nCopy\ncurl -sL https://talos.dev/install | sh\n\n\nNote: If you boot systems off the ISO, Talos on the ISO image runs in RAM and acts as an installer. The version of talosctl that is used to create the machine configurations controls the version of Talos Linux that is installed on the machines - NOT the image that the machines are initially booted off. For example, booting a machine off the Talos 1.3.7 ISO, but creating the initial configuration with talosctl binary of version 1.4.1, will result in a machine running Talos Linux version 1.4.1.\n\nIt is advisable to use the same version of talosctl as the version of the boot media used.\n\nNetwork access\n\nThis guide assumes that the systems being installed have outgoing access to the internet, allowing them to pull installer and container images, query NTP, etc. If needed, see the documentation on registry proxies, local registries, and airgapped installation.\n\nAcquire the Talos Linux image and boot machines\n\nThe most general way to install Talos Linux is to use the ISO image.\n\nThe latest ISO image can be found on the Github Releases page:\n\nX86: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-amd64.iso\nARM64: https://github.com/siderolabs/talos/releases/download/v1.6.2/metal-arm64.iso\n\nWhen booted from the ISO, Talos will run in RAM and will not install to disk until provided a configuration. Thus, it is safe to boot any machine from the ISO.\n\nAt this point, you should:\n\nboot one machine off the ISO to be the control plane node\nboot one or more machines off the same ISO to be the workers\nAlternative Booting\n\nFor network booting and self-built media, see Production Notes. There are installation methods specific to specific platforms, such as pre-built AMIs for AWS - check the specific Installation Guides.)\n\nDefine the Kubernetes Endpoint\n\nIn order to configure Kubernetes, Talos needs to know what the endpoint of the Kubernetes API Server will be.\n\nBecause we are only creating a single control plane node in this guide, we can use the control plane node directly as the Kubernetes API endpoint.\n\nIdentify the IP address or DNS name of the control plane node that was booted above, and convert it to a fully-qualified HTTPS URL endpoint address for the Kubernetes API Server which (by default) runs on port 6443. The endpoint should be formatted like:\n\nhttps://192.168.0.2:6443\nhttps://kube.mycluster.mydomain.com:6443\n\nNOTE: For a production cluster, you should have three control plane nodes, and have the endpoint allocate traffic to all three - see Production Notes.\n\nAccessing the Talos API\n\nAdministrative tasks are performed by calling the Talos API (usually with talosctl) on Talos Linux control plane nodes - thus, ensure your control plane node is directly reachable on TCP port 50000 from the workstation where you run the talosctl client. This may require changing firewall rules or cloud provider access-lists.\n\nFor production configurations, see Production Notes.\n\nConfigure Talos Linux\n\nWhen Talos boots without a configuration, such as when booting off the Talos ISO, it enters maintenance mode and waits for a configuration to be provided.\n\nA configuration can be passed in on boot via kernel parameters or metadata servers. See Production Notes.\n\nUnlike traditional Linux, Talos Linux is not configured by SSHing to the server and issuing commands. Instead, the entire state of the machine is defined by a machine config file which is passed to the server. This allows machines to be managed in a declarative way, and lends itself to GitOps and modern operations paradigms. The state of a machine is completely defined by, and can be reproduced from, the machine configuration file.\n\nTo generate the machine configurations for a cluster, run this command on the workstation where you installed talosctl:\n\nCopy\ntalosctl gen config <cluster-name> <cluster-endpoint>\n\n\ncluster-name is an arbitrary name, used as a label in your local client configuration. It should be unique in the configuration on your local workstation.\n\ncluster-endpoint is the Kubernetes Endpoint you constructed from the control plane node’s IP address or DNS name above. It should be a complete URL, with https:// and port.\n\nFor example:\n\nCopy\n$ talosctl gen config mycluster https://192.168.0.2:6443\n\ngenerating PKI and tokens\n\ncreated /Users/taloswork/controlplane.yaml\n\ncreated /Users/taloswork/worker.yaml\n\ncreated /Users/taloswork/talosconfig\n\n\nWhen you run this command, three files are created in your current directory:\n\ncontrolplane.yaml\nworker.yaml\ntalosconfig\n\nThe .yaml files are Machine Configs: they describe everything from what disk Talos should be installed on, to network settings. The controlplane.yaml file also describes how Talos should form a Kubernetes cluster.\n\nThe talosconfig file is your local client configuration file, used to connect to and authenticate access to the cluster.\n\nControlplane and Worker\n\nThe two types of Machine Configs correspond to the two roles of Talos nodes, control plane nodes (which run both the Talos and Kubernetes control planes) and worker nodes (which run the workloads).\n\nThe main difference between Controlplane Machine Config files and Worker Machine Config files is that the former contains information about how to form the Kubernetes cluster.\n\nModifying the Machine configs\n\nThe generated Machine Configs have defaults that work for most cases. They use DHCP for interface configuration, and install to /dev/sda.\n\nSometimes, you will need to modify the generated files to work with your systems. A common case is needing to change the installation disk. If you try to to apply the machine config to a node, and get an error like the below, you need to specify a different installation disk:\n\nCopy\n$ talosctl apply-config --insecure -n 192.168.0.2 --file controlplane.yaml\n\nerror applying new configuration: rpc error: code = InvalidArgument desc = configuration validation failed: 1 error occurred:\n\n    * specified install disk does not exist: \"/dev/sda\"\n\n\nYou can verify which disks your nodes have by using the talosctl disks --insecure command.\n\nInsecure mode is needed at this point as the PKI infrastructure has not yet been set up.\n\nFor example, the talosctl disks command below shows that the system has a vda drive, not an sda:\n\nCopy\n$ talosctl -n 192.168.0.2 disks --insecure\n\nDEV        MODEL   SERIAL   TYPE   UUID   WWID  MODALIAS                    NAME   SIZE    BUS_PATH\n\n/dev/vda   -       -        HDD    -      -      virtio:d00000002v00001AF4   -      69 GB   /pci0000:00/0000:00:06.0/virtio2/\n\n\nIn this case, you would modify the controlplane.yaml and worker.yaml files and edit the line:\n\nCopy\ninstall:\n\n  disk: /dev/sda # The disk used for installations.\n\n\nto reflect vda instead of sda.\n\nFor information on customizing your machine configurations (such as to specify the version of Kubernetes), using machine configuration patches, or customizing configurations for individual machines (such as setting static IP addresses), see the Production Notes.\n\nUnderstand talosctl, endpoints and nodes\n\nIt is important to understand the concept of endpoints and nodes. In short: endpoints are where talosctl sends commands to, but the command operates on the specified nodes. The endpoint will forward the command to the nodes, if needed.\n\nEndpoints\n\nEndpoints are the IP addresses of control plane nodes, to which the talosctl client directly talks.\n\nEndpoints automatically proxy requests destined to another node in the cluster. This means that you only need access to the control plane nodes in order to manage the rest of the cluster.\n\nYou can pass in --endpoints <Control Plane IP Address> or -e <Control Plane IP Address> to the current talosctl command.\n\nIn this tutorial setup, the endpoint will always be the single control plane node.\n\nNodes\n\nNodes are the target(s) you wish to perform the operation on.\n\nWhen specifying nodes, the IPs and/or hostnames are as seen by the endpoint servers, not as from the client. This is because all connections are proxied through the endpoints.\n\nYou may provide -n or --nodes to any talosctl command to supply the node or (comma-separated) nodes on which you wish to perform the operation.\n\nFor example, to see the containers running on node 192.168.0.200, by routing the containers command through the control plane endpoint 192.168.0.2:\n\nCopy\ntalosctl -e 192.168.0.2 -n 192.168.0.200 containers\n\n\nTo see the etcd logs on both nodes 192.168.0.10 and 192.168.0.11:\n\nCopy\ntalosctl -e 192.168.0.2 -n 192.168.0.10,192.168.0.11 logs etcd\n\n\nFor a more in-depth discussion of Endpoints and Nodes, please see talosctl.\n\nApply Configuration\n\nTo apply the Machine Configs, you need to know the machines’ IP addresses.\n\nTalos prints the IP addresses of the machines on the console during the boot process:\n\n[4.605369] [talos] task loadConfig (1/1): this machine is reachable at:\n[4.607358] [talos] task loadConfig (1/1):   192.168.0.2\n\n\nIf you do not have console access, the IP address may also be discoverable from your DHCP server.\n\nOnce you have the IP address, you can then apply the correct configuration. Apply the controlplane.yaml file to the control plane node, and the worker.yaml file to all the worker node(s).\n\nCopy\n  talosctl apply-config --insecure \\\n\n    --nodes 192.168.0.2 \\\n\n    --file controlplane.yaml\n\n\nThe --insecure flag is necessary because the PKI infrastructure has not yet been made available to the node. Note: the connection will be encrypted, but not authenticated.\n\nWhen using the --insecure flag, it is not necessary to specify an endpoint.\n\nDefault talosconfig configuration file\n\nYou reference which configuration file to use by the --talosconfig parameter:\n\nCopy\ntalosctl --talosconfig=./talosconfig \\\n\n    --nodes 192.168.0.2 -e 192.168.0.2 version\n\n\nNote that talosctl comes with tooling to help you integrate and merge this configuration into the default talosctl configuration file. See Production Notes for more information.\n\nWhile getting started, a common mistake is referencing a configuration context for a different cluster, resulting in authentication or connection failures. Thus it is recommended to explicitly pass in the configuration file while becoming familiar with Talos Linux.\n\nKubernetes Bootstrap\n\nBootstrapping your Kubernetes cluster with Talos is as simple as calling talosctl bootstrap on your control plane node:\n\nCopy\ntalosctl bootstrap --nodes 192.168.0.2 --endpoints 192.168.0.2 \\\n\n  --talosconfig=./talosconfig\n\n\nThe bootstrap operation should only be called ONCE on a SINGLE control plane node. (If you have multiple control plane nodes, it doesn’t matter which one you issue the bootstrap command against.)\n\nAt this point, Talos will form an etcd cluster, and start the Kubernetes control plane components.\n\nAfter a few moments, you will be able to download your Kubernetes client configuration and get started:\n\nCopy\n  talosctl kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nRunning this command will add (merge) you new cluster into your local Kubernetes configuration.\n\nIf you would prefer the configuration to not be merged into your default Kubernetes configuration file, pass in a filename:\n\nCopy\n  talosctl kubeconfig alternative-kubeconfig --nodes 192.168.0.2 --endpoints 192.168.0.2\n\n\nYou should now be able to connect to Kubernetes and see your nodes:\n\nCopy\n  kubectl get nodes\n\n\nAnd use talosctl to explore your cluster:\n\nCopy\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 health \\\n\n   --talosconfig=./talosconfig\n\ntalosctl --nodes 192.168.0.2 --endpoints 192.168.0.2 dashboard \\\n\n   --talosconfig=./talosconfig\n\n\nFor a list of all the commands and operations that talosctl provides, see the CLI reference.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Quickstart | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/quickstart/",
    "html": "Local Docker Cluster\nPrerequisites\ntalosctl\nkubectl\nCreate the Cluster\nDestroy the Cluster\nDocumentation\nIntroduction\nQuickstart\nQuickstart\nA short guide on setting up a simple Talos Linux cluster locally with Docker.\nLocal Docker Cluster\n\nThe easiest way to try Talos is by using the CLI (talosctl) to create a cluster on a machine with docker installed.\n\nPrerequisites\ntalosctl\n\nDownload talosctl:\n\nCopy\ncurl -sL https://talos.dev/install | sh\n\nkubectl\n\nDownload kubectl via one of methods outlined in the documentation.\n\nCreate the Cluster\n\nNow run the following:\n\nCopy\ntalosctl cluster create\n\n\nYou can explore using Talos API commands:\n\nCopy\ntalosctl dashboard --nodes 10.5.0.2\n\n\nVerify that you can reach Kubernetes:\n\nCopy\n$ kubectl get nodes -o wide\n\nNAME                     STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION   CONTAINER-RUNTIME\n\ntalos-default-controlplane-1   Ready    master   115s   v1.29.0   10.5.0.2      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\ntalos-default-worker-1   Ready    <none>   115s   v1.29.0   10.5.0.3      <none>        Talos (v1.6.2)   <host kernel>    containerd://1.5.5\n\nDestroy the Cluster\n\nWhen you are all done, remove the cluster:\n\nCopy\ntalosctl cluster destroy\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "What is Talos? | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/what-is-talos/",
    "html": "Documentation\nIntroduction\nWhat is Talos?\nWhat is Talos?\nA quick introduction in to what Talos is and why it should be used.\n\nTalos is a container optimized Linux distro; a reimagining of Linux for distributed systems such as Kubernetes. Designed to be as minimal as possible while still maintaining practicality. For these reasons, Talos has a number of features unique to it:\n\nit is immutable\nit is atomic\nit is ephemeral\nit is minimal\nit is secure by default\nit is managed via a single declarative configuration file and gRPC API\n\nTalos can be deployed on container, cloud, virtualized, and bare metal platforms.\n\nWhy Talos\n\nIn having less, Talos offers more. Security. Efficiency. Resiliency. Consistency.\n\nAll of these areas are improved simply by having less.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Introduction | Talos Linux",
    "url": "https://www.talos.dev/v1.6/introduction/",
    "html": "Documentation\nIntroduction\nIntroduction\nWhat is Talos?\n\nA quick introduction in to what Talos is and why it should be used.\n\nQuickstart\n\nA short guide on setting up a simple Talos Linux cluster locally with Docker.\n\nGetting Started\n\nA guide to setting up a Talos Linux cluster.\n\nProduction Clusters\n\nRecommendations for setting up a Talos Linux cluster in production.\n\nSystem Requirements\n\nHardware requirements for running Talos Linux.\n\nWhat's New in Talos 1.6.0\n\nList of new and shiny features in Talos Linux.\n\nSupport Matrix\n\nTable of supported Talos Linux versions and respective platforms.\n\nTroubleshooting\n\nTroubleshoot control plane and other failures for Talos Linux clusters.\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  },
  {
    "title": "Welcome | Talos Linux",
    "url": "https://www.talos.dev/v1.6/",
    "html": "Welcome\nOpen Source\nCommunity\nOffice Hours\nEnterprise\nDocumentation\nWelcome\nWelcome\n\nWelcome to the Talos documentation. If you are just getting familiar with Talos, we recommend starting here:\n\nWhat is Talos: a quick description of Talos\nQuickstart: the fastest way to get a Talos cluster up and running\nGetting Started: a long-form, guided tour of getting a full Talos cluster deployed\nOpen Source\nCommunity\nGitHub: repo\nSupport: Questions, bugs, feature requests GitHub Discussions\nCommunity Slack: Join our slack channel\nMatrix: Join our Matrix channels:\nCommunity: #talos:matrix.org\nCommunity Support: #talos-support:matrix.org\nForum: community\nTwitter: @SideroLabs\nEmail: info@SideroLabs.com\n\nIf you’re interested in this project and would like to help in engineering efforts, or have general usage questions, we are happy to have you! We hold a weekly meeting that all audiences are welcome to attend.\n\nWe would appreciate your feedback so that we can make Talos even better! To do so, you can take our survey.\n\nOffice Hours\nWhen: Mondays at 16:30 UTC.\nWhere: Google Meet.\n\nYou can subscribe to this meeting by joining the community forum above.\n\nEnterprise\n\nIf you are using Talos in a production setting, and need consulting services to get started or to integrate Talos into your existing environment, we can help. Sidero Labs, Inc. offers support contracts with SLA (Service Level Agreement)-bound terms for mission-critical environments.\n\nLearn More\n\n© 2024 Sidero Labs, Inc. All Rights Reserved"
  }
]
